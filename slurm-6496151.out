share memory
ToM optimizer lr * 10
2024-04-07 01:18:18,063 : lr: 0.001
2024-04-07 01:18:18,063 : gamma: 0.1
2024-04-07 01:18:18,063 : gamma_rate: 0.002
2024-04-07 01:18:18,063 : gamma_final: 0.9
2024-04-07 01:18:18,063 : tau: 1.0
2024-04-07 01:18:18,063 : entropy: 0.005
2024-04-07 01:18:18,063 : grad_entropy: 1.0
2024-04-07 01:18:18,063 : seed: 1
2024-04-07 01:18:18,063 : workers: 6
2024-04-07 01:18:18,063 : A2C_steps: 10
2024-04-07 01:18:18,063 : env_steps: 10
2024-04-07 01:18:18,063 : start_eps: 2000
2024-04-07 01:18:18,063 : ToM_train_loops: 1
2024-04-07 01:18:18,063 : policy_train_loops: 1
2024-04-07 01:18:18,063 : test_eps: 20
2024-04-07 01:18:18,063 : ToM_frozen: 5
2024-04-07 01:18:18,063 : env: CN
2024-04-07 01:18:18,063 : optimizer: Adam
2024-04-07 01:18:18,063 : amsgrad: True
2024-04-07 01:18:18,063 : load_model_dir: None
2024-04-07 01:18:18,063 : load_executor_dir: None
2024-04-07 01:18:18,063 : log_dir: logs/CN/Apr07_01-18
2024-04-07 01:18:18,064 : model: ToM2C
2024-04-07 01:18:18,064 : gpu_id: [-1]
2024-04-07 01:18:18,064 : norm_reward: True
2024-04-07 01:18:18,064 : train_comm: False
2024-04-07 01:18:18,064 : random_target: True
2024-04-07 01:18:18,064 : mask_actions: False
2024-04-07 01:18:18,064 : mask: False
2024-04-07 01:18:18,064 : render: False
2024-04-07 01:18:18,064 : fix: False
2024-04-07 01:18:18,064 : shared_optimizer: True
2024-04-07 01:18:18,064 : train_mode: -1
2024-04-07 01:18:18,064 : lstm_out: 32
2024-04-07 01:18:18,064 : sleep_time: 0
2024-04-07 01:18:18,064 : max_step: 3000000
2024-04-07 01:18:18,064 : render_save: False
2024-04-07 01:18:18,064 : num_agents: -1
2024-04-07 01:18:18,064 : num_targets: -1
/home/josuetf/ToM2C/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  Variable._execution_engine.run_backward(
/home/josuetf/ToM2C/shared_optim.py:155: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2024-04-07 01:18:43,821 : Time 00h 00m 25s, ave eps reward [-31.91 -31.91 -31.91], ave eps length 10.0, reward step [-3.19 -3.19 -3.19], FPS 6.94, mean reward -31.9121217021274, std reward 6.165989510372239, AG 0.0
2024-04-07 01:19:10,521 : Time 00h 00m 52s, ave eps reward [-33.22 -33.22 -33.22], ave eps length 10.0, reward step [-3.32 -3.32 -3.32], FPS 6.69, mean reward -33.217691216211804, std reward 7.064140382753161, AG 0.0
2024-04-07 01:19:37,216 : Time 00h 01m 19s, ave eps reward [-32.72 -32.72 -32.72], ave eps length 10.0, reward step [-3.27 -3.27 -3.27], FPS 6.85, mean reward -32.715594886068445, std reward 8.383322554621568, AG 0.0
2024-04-07 01:20:03,890 : Time 00h 01m 45s, ave eps reward [-36.2 -36.2 -36.2], ave eps length 10.0, reward step [-3.62 -3.62 -3.62], FPS 7.94, mean reward -36.20411977606255, std reward 8.880812357313884, AG 0.0
2024-04-07 01:20:30,831 : Time 00h 02m 12s, ave eps reward [-25.57 -25.57 -25.57], ave eps length 10.0, reward step [-2.56 -2.56 -2.56], FPS 6.66, mean reward -25.565752638661575, std reward 11.604369935077717, AG 0.0
2024-04-07 01:20:57,525 : Time 00h 02m 39s, ave eps reward [-21.29 -21.29 -21.29], ave eps length 10.0, reward step [-2.13 -2.13 -2.13], FPS 6.55, mean reward -21.29098265060386, std reward 8.63879040303532, AG 0.0
2024-04-07 01:21:24,104 : Time 00h 03m 06s, ave eps reward [-22.01 -22.01 -22.01], ave eps length 10.0, reward step [-2.2 -2.2 -2.2], FPS 6.82, mean reward -22.006992000238988, std reward 10.846583342429497, AG 0.0
2024-04-07 01:21:50,605 : Time 00h 03m 32s, ave eps reward [-23.01 -23.01 -23.01], ave eps length 10.0, reward step [-2.3 -2.3 -2.3], FPS 8.0, mean reward -23.007235241634454, std reward 10.190011217113037, AG 0.0
2024-04-07 01:22:16,861 : Time 00h 03m 58s, ave eps reward [-24.52 -24.52 -24.52], ave eps length 10.0, reward step [-2.45 -2.45 -2.45], FPS 10.15, mean reward -24.516668450368286, std reward 9.963635029956373, AG 0.0
2024-04-07 01:22:44,191 : Time 00h 04m 26s, ave eps reward [-23.15 -23.15 -23.15], ave eps length 10.0, reward step [-2.32 -2.32 -2.32], FPS 6.66, mean reward -23.15189624752596, std reward 7.8251313143042225, AG 0.0
2024-04-07 01:23:10,851 : Time 00h 04m 52s, ave eps reward [-20.09 -20.09 -20.09], ave eps length 10.0, reward step [-2.01 -2.01 -2.01], FPS 7.14, mean reward -20.092572117950475, std reward 8.233447703374448, AG 0.0
2024-04-07 01:23:37,196 : Time 00h 05m 19s, ave eps reward [-19.56 -19.56 -19.56], ave eps length 10.0, reward step [-1.96 -1.96 -1.96], FPS 9.82, mean reward -19.55898481581952, std reward 6.369695214139644, AG 0.0
2024-04-07 01:24:03,844 : Time 00h 05m 45s, ave eps reward [-19.91 -19.91 -19.91], ave eps length 10.0, reward step [-1.99 -1.99 -1.99], FPS 7.5, mean reward -19.907379068269993, std reward 7.25152769207276, AG 0.0
2024-04-07 01:24:30,608 : Time 00h 06m 12s, ave eps reward [-25.13 -25.13 -25.13], ave eps length 10.0, reward step [-2.51 -2.51 -2.51], FPS 6.66, mean reward -25.127685839264032, std reward 8.999570523782387, AG 0.0
2024-04-07 01:24:57,524 : Time 00h 06m 39s, ave eps reward [-21.59 -21.59 -21.59], ave eps length 10.0, reward step [-2.16 -2.16 -2.16], FPS 8.93, mean reward -21.594005784666372, std reward 7.842329675397627, AG 0.0
2024-04-07 01:25:23,977 : Time 00h 07m 05s, ave eps reward [-17.59 -17.59 -17.59], ave eps length 10.0, reward step [-1.76 -1.76 -1.76], FPS 8.46, mean reward -17.59423456209643, std reward 5.981918580220185, AG 0.0
2024-04-07 01:25:50,768 : Time 00h 07m 32s, ave eps reward [-16.85 -16.85 -16.85], ave eps length 10.0, reward step [-1.68 -1.68 -1.68], FPS 6.5, mean reward -16.848949914410404, std reward 4.91933437404522, AG 0.0
2024-04-07 01:26:17,508 : Time 00h 07m 59s, ave eps reward [-17.75 -17.75 -17.75], ave eps length 10.0, reward step [-1.77 -1.77 -1.77], FPS 6.82, mean reward -17.74515730369033, std reward 6.625843014881424, AG 0.0
2024-04-07 01:26:44,212 : Time 00h 08m 26s, ave eps reward [-17.42 -17.42 -17.42], ave eps length 10.0, reward step [-1.74 -1.74 -1.74], FPS 6.72, mean reward -17.424486632360207, std reward 5.081745676914588, AG 0.0
2024-04-07 01:27:10,885 : Time 00h 08m 52s, ave eps reward [-17.4 -17.4 -17.4], ave eps length 10.0, reward step [-1.74 -1.74 -1.74], FPS 7.18, mean reward -17.40427259940301, std reward 6.0451878515532815, AG 0.0
2024-04-07 01:27:37,567 : Time 00h 09m 19s, ave eps reward [-17.8 -17.8 -17.8], ave eps length 10.0, reward step [-1.78 -1.78 -1.78], FPS 6.54, mean reward -17.80128041456775, std reward 6.208877539863135, AG 0.0
2024-04-07 01:28:04,201 : Time 00h 09m 46s, ave eps reward [-20.89 -20.89 -20.89], ave eps length 10.0, reward step [-2.09 -2.09 -2.09], FPS 7.11, mean reward -20.893113764084262, std reward 5.794876413332948, AG 0.0
2024-04-07 01:28:30,505 : Time 00h 10m 12s, ave eps reward [-17.45 -17.45 -17.45], ave eps length 10.0, reward step [-1.74 -1.74 -1.74], FPS 10.6, mean reward -17.447270519061142, std reward 6.447299069778551, AG 0.0
2024-04-07 01:28:57,730 : Time 00h 10m 39s, ave eps reward [-16.88 -16.88 -16.88], ave eps length 10.0, reward step [-1.69 -1.69 -1.69], FPS 6.59, mean reward -16.879693831830004, std reward 5.4320554829689325, AG 0.0
2024-04-07 01:29:24,166 : Time 00h 11m 06s, ave eps reward [-20.05 -20.05 -20.05], ave eps length 10.0, reward step [-2.01 -2.01 -2.01], FPS 8.27, mean reward -20.052179245421737, std reward 7.4990157019822625, AG 0.0
2024-04-07 01:29:50,587 : Time 00h 11m 32s, ave eps reward [-21.57 -21.57 -21.57], ave eps length 10.0, reward step [-2.16 -2.16 -2.16], FPS 9.07, mean reward -21.5730109256884, std reward 8.8080772759917, AG 0.0
2024-04-07 01:30:17,362 : Time 00h 11m 59s, ave eps reward [-19.66 -19.66 -19.66], ave eps length 10.0, reward step [-1.97 -1.97 -1.97], FPS 6.98, mean reward -19.662226250783945, std reward 6.5027395771046, AG 0.0
2024-04-07 01:30:44,119 : Time 00h 12m 26s, ave eps reward [-14.3 -14.3 -14.3], ave eps length 10.0, reward step [-1.43 -1.43 -1.43], FPS 6.61, mean reward -14.298190048459057, std reward 4.616737885979016, AG 0.0
2024-04-07 01:31:10,770 : Time 00h 12m 52s, ave eps reward [-16.26 -16.26 -16.26], ave eps length 10.0, reward step [-1.63 -1.63 -1.63], FPS 10.16, mean reward -16.25626740966786, std reward 3.9630776230049616, AG 0.0
2024-04-07 01:31:37,506 : Time 00h 13m 19s, ave eps reward [-13.23 -13.23 -13.23], ave eps length 10.0, reward step [-1.32 -1.32 -1.32], FPS 7.22, mean reward -13.226921599031389, std reward 3.1522017600200276, AG 0.0
2024-04-07 01:32:04,168 : Time 00h 13m 46s, ave eps reward [-17.78 -17.78 -17.78], ave eps length 10.0, reward step [-1.78 -1.78 -1.78], FPS 6.89, mean reward -17.784105188282876, std reward 6.856485030297423, AG 0.0
2024-04-07 01:32:30,666 : Time 00h 14m 12s, ave eps reward [-14.9 -14.9 -14.9], ave eps length 10.0, reward step [-1.49 -1.49 -1.49], FPS 8.01, mean reward -14.903458901009705, std reward 4.508547986910462, AG 0.0
2024-04-07 01:32:57,502 : Time 00h 14m 39s, ave eps reward [-18.93 -18.93 -18.93], ave eps length 10.0, reward step [-1.89 -1.89 -1.89], FPS 6.94, mean reward -18.933216506947936, std reward 4.706750326837611, AG 0.0
2024-04-07 01:33:24,195 : Time 00h 15m 06s, ave eps reward [-17.38 -17.38 -17.38], ave eps length 10.0, reward step [-1.74 -1.74 -1.74], FPS 6.77, mean reward -17.380297178420797, std reward 4.969359498398601, AG 0.0
2024-04-07 01:33:50,861 : Time 00h 15m 32s, ave eps reward [-17.42 -17.42 -17.42], ave eps length 10.0, reward step [-1.74 -1.74 -1.74], FPS 7.07, mean reward -17.42357630977452, std reward 5.3452495051688, AG 0.0
2024-04-07 01:34:17,163 : Time 00h 15m 59s, ave eps reward [-16.8 -16.8 -16.8], ave eps length 10.0, reward step [-1.68 -1.68 -1.68], FPS 10.1, mean reward -16.799745148762124, std reward 6.7467097583586035, AG 0.0
2024-04-07 01:34:43,744 : Time 00h 16m 25s, ave eps reward [-17.56 -17.56 -17.56], ave eps length 10.0, reward step [-1.76 -1.76 -1.76], FPS 8.28, mean reward -17.562624661730986, std reward 4.855714170516753, AG 0.0
2024-04-07 01:35:10,930 : Time 00h 16m 52s, ave eps reward [-15.02 -15.02 -15.02], ave eps length 10.0, reward step [-1.5 -1.5 -1.5], FPS 7.04, mean reward -15.015857132522402, std reward 4.113082691901658, AG 0.0
2024-04-07 01:35:37,139 : Time 00h 17m 19s, ave eps reward [-14.28 -14.28 -14.28], ave eps length 10.0, reward step [-1.43 -1.43 -1.43], FPS 9.91, mean reward -14.280105307034393, std reward 3.746661266024887, AG 0.0
2024-04-07 01:36:03,781 : Time 00h 17m 45s, ave eps reward [-15.23 -15.23 -15.23], ave eps length 10.0, reward step [-1.52 -1.52 -1.52], FPS 7.98, mean reward -15.232367814624865, std reward 5.679739440517317, AG 0.0
2024-04-07 01:36:30,478 : Time 00h 18m 12s, ave eps reward [-13.83 -13.83 -13.83], ave eps length 10.0, reward step [-1.38 -1.38 -1.38], FPS 6.66, mean reward -13.830928174797176, std reward 4.735960545528642, AG 0.0
2024-04-07 01:36:57,150 : Time 00h 18m 39s, ave eps reward [-16.16 -16.16 -16.16], ave eps length 10.0, reward step [-1.62 -1.62 -1.62], FPS 6.79, mean reward -16.157108384960203, std reward 6.841231469441139, AG 0.0
2024-04-07 01:37:23,909 : Time 00h 19m 05s, ave eps reward [-14.7 -14.7 -14.7], ave eps length 10.0, reward step [-1.47 -1.47 -1.47], FPS 9.38, mean reward -14.696716992717441, std reward 4.439997467639096, AG 0.0
2024-04-07 01:37:50,560 : Time 00h 19m 32s, ave eps reward [-14.98 -14.98 -14.98], ave eps length 10.0, reward step [-1.5 -1.5 -1.5], FPS 6.76, mean reward -14.979924000773039, std reward 4.08054881027802, AG 0.0
2024-04-07 01:38:17,269 : Time 00h 19m 59s, ave eps reward [-15.73 -15.73 -15.73], ave eps length 10.0, reward step [-1.57 -1.57 -1.57], FPS 6.66, mean reward -15.726103331221827, std reward 4.675978062982395, AG 0.0
2024-04-07 01:38:43,586 : Time 00h 20m 25s, ave eps reward [-13.82 -13.82 -13.82], ave eps length 10.0, reward step [-1.38 -1.38 -1.38], FPS 8.96, mean reward -13.820425667321846, std reward 5.374026794963008, AG 0.0
2024-04-07 01:39:10,563 : Time 00h 20m 52s, ave eps reward [-16.93 -16.93 -16.93], ave eps length 10.0, reward step [-1.69 -1.69 -1.69], FPS 6.73, mean reward -16.92978993482675, std reward 4.142985322714727, AG 0.0
2024-04-07 01:39:37,221 : Time 00h 21m 19s, ave eps reward [-17.38 -17.38 -17.38], ave eps length 10.0, reward step [-1.74 -1.74 -1.74], FPS 6.67, mean reward -17.38246725376616, std reward 5.044227053906736, AG 0.0
2024-04-07 01:40:03,816 : Time 00h 21m 45s, ave eps reward [-15.66 -15.66 -15.66], ave eps length 10.0, reward step [-1.57 -1.57 -1.57], FPS 7.09, mean reward -15.658297330619465, std reward 3.51219113970404, AG 0.0
2024-04-07 01:40:30,249 : Time 00h 22m 12s, ave eps reward [-14.7 -14.7 -14.7], ave eps length 10.0, reward step [-1.47 -1.47 -1.47], FPS 8.03, mean reward -14.695895498613018, std reward 4.790148695481969, AG 0.0
2024-04-07 01:40:56,665 : Time 00h 22m 38s, ave eps reward [-14.17 -14.17 -14.17], ave eps length 10.0, reward step [-1.42 -1.42 -1.42], FPS 9.57, mean reward -14.170640358583148, std reward 4.538129975722299, AG 0.0
2024-04-07 01:41:23,910 : Time 00h 23m 05s, ave eps reward [-14.39 -14.39 -14.39], ave eps length 10.0, reward step [-1.44 -1.44 -1.44], FPS 6.82, mean reward -14.387273572915941, std reward 3.972914264632766, AG 0.0
2024-04-07 01:41:50,412 : Time 00h 23m 32s, ave eps reward [-14.36 -14.36 -14.36], ave eps length 10.0, reward step [-1.44 -1.44 -1.44], FPS 8.98, mean reward -14.364011411832246, std reward 4.210452349034145, AG 0.0
2024-04-07 01:42:16,922 : Time 00h 23m 58s, ave eps reward [-14.42 -14.42 -14.42], ave eps length 10.0, reward step [-1.44 -1.44 -1.44], FPS 8.61, mean reward -14.42385523589526, std reward 4.6937418368311725, AG 0.0
2024-04-07 01:42:43,637 : Time 00h 24m 25s, ave eps reward [-12.63 -12.63 -12.63], ave eps length 10.0, reward step [-1.26 -1.26 -1.26], FPS 6.49, mean reward -12.629554934216436, std reward 3.7159610195870685, AG 0.0
2024-04-07 01:43:10,283 : Time 00h 24m 52s, ave eps reward [-14.77 -14.77 -14.77], ave eps length 10.0, reward step [-1.48 -1.48 -1.48], FPS 6.8, mean reward -14.772300211368329, std reward 5.269448753477226, AG 0.0
2024-04-07 01:43:36,925 : Time 00h 25m 18s, ave eps reward [-15.54 -15.54 -15.54], ave eps length 10.0, reward step [-1.55 -1.55 -1.55], FPS 7.53, mean reward -15.539523178413168, std reward 4.910870300001284, AG 0.0
2024-04-07 01:44:03,703 : Time 00h 25m 45s, ave eps reward [-17.87 -17.87 -17.87], ave eps length 10.0, reward step [-1.79 -1.79 -1.79], FPS 6.56, mean reward -17.873017662557892, std reward 4.575501888952743, AG 0.0
2024-04-07 01:44:30,134 : Time 00h 26m 12s, ave eps reward [-14.12 -14.12 -14.12], ave eps length 10.0, reward step [-1.41 -1.41 -1.41], FPS 8.16, mean reward -14.121320139165505, std reward 4.253017873222831, AG 0.0
2024-04-07 01:44:56,416 : Time 00h 26m 38s, ave eps reward [-17.67 -17.67 -17.67], ave eps length 10.0, reward step [-1.77 -1.77 -1.77], FPS 10.39, mean reward -17.668626190604417, std reward 5.198263857788319, AG 0.0
2024-04-07 01:45:23,657 : Time 00h 27m 05s, ave eps reward [-15.99 -15.99 -15.99], ave eps length 10.0, reward step [-1.6 -1.6 -1.6], FPS 6.8, mean reward -15.987161335671122, std reward 5.368910511206054, AG 0.0
2024-04-07 01:45:50,307 : Time 00h 27m 32s, ave eps reward [-14.64 -14.64 -14.64], ave eps length 10.0, reward step [-1.46 -1.46 -1.46], FPS 7.23, mean reward -14.636757663908522, std reward 5.3441921189275305, AG 0.0
2024-04-07 01:46:16,609 : Time 00h 27m 58s, ave eps reward [-13.82 -13.82 -13.82], ave eps length 10.0, reward step [-1.38 -1.38 -1.38], FPS 9.8, mean reward -13.818252053645441, std reward 6.403715842725028, AG 0.0
2024-04-07 01:46:43,201 : Time 00h 28m 25s, ave eps reward [-14.14 -14.14 -14.14], ave eps length 10.0, reward step [-1.41 -1.41 -1.41], FPS 8.16, mean reward -14.144963187009779, std reward 6.533741136659773, AG 0.0
2024-04-07 01:47:09,989 : Time 00h 28m 51s, ave eps reward [-13.99 -13.99 -13.99], ave eps length 10.0, reward step [-1.4 -1.4 -1.4], FPS 6.74, mean reward -13.986545402461081, std reward 6.642934175146679, AG 0.0
2024-04-07 01:47:36,739 : Time 00h 29m 18s, ave eps reward [-13.18 -13.18 -13.18], ave eps length 10.0, reward step [-1.32 -1.32 -1.32], FPS 10.7, mean reward -13.177601945620514, std reward 6.828656787182367, AG 0.0
2024-04-07 01:48:03,353 : Time 00h 29m 45s, ave eps reward [-14.12 -14.12 -14.12], ave eps length 10.0, reward step [-1.41 -1.41 -1.41], FPS 7.85, mean reward -14.1219591602771, std reward 6.878184021823594, AG 0.0
2024-04-07 01:48:30,563 : Time 00h 30m 12s, ave eps reward [-15.41 -15.41 -15.41], ave eps length 10.0, reward step [-1.54 -1.54 -1.54], FPS 6.78, mean reward -15.411888878445314, std reward 6.697947571577277, AG 0.0
2024-04-07 01:48:56,982 : Time 00h 30m 38s, ave eps reward [-14.7 -14.7 -14.7], ave eps length 10.0, reward step [-1.47 -1.47 -1.47], FPS 9.15, mean reward -14.697273174023618, std reward 5.543365242029284, AG 0.0
2024-04-07 01:49:24,117 : Time 00h 31m 06s, ave eps reward [-13.47 -13.47 -13.47], ave eps length 10.0, reward step [-1.35 -1.35 -1.35], FPS 6.73, mean reward -13.473449222330322, std reward 6.186839266103645, AG 0.0
2024-04-07 01:49:50,735 : Time 00h 31m 32s, ave eps reward [-15.87 -15.87 -15.87], ave eps length 10.0, reward step [-1.59 -1.59 -1.59], FPS 6.86, mean reward -15.874901515497513, std reward 8.287302593781128, AG 0.0
2024-04-07 01:50:17,123 : Time 00h 31m 59s, ave eps reward [-10.68 -10.68 -10.68], ave eps length 10.0, reward step [-1.07 -1.07 -1.07], FPS 9.06, mean reward -10.677286449383988, std reward 6.1922754420980475, AG 0.0
2024-04-07 01:50:43,741 : Time 00h 32m 25s, ave eps reward [-10.54 -10.54 -10.54], ave eps length 10.0, reward step [-1.05 -1.05 -1.05], FPS 7.59, mean reward -10.541483086746958, std reward 8.512784001797472, AG 0.0
2024-04-07 01:51:10,457 : Time 00h 32m 52s, ave eps reward [-12.66 -12.66 -12.66], ave eps length 10.0, reward step [-1.27 -1.27 -1.27], FPS 6.87, mean reward -12.66118928624662, std reward 7.721223810717725, AG 0.0
2024-04-07 01:51:37,346 : Time 00h 33m 19s, ave eps reward [-15.56 -15.56 -15.56], ave eps length 10.0, reward step [-1.56 -1.56 -1.56], FPS 9.85, mean reward -15.56499713934071, std reward 8.306737264857006, AG 0.0
2024-04-07 01:52:03,972 : Time 00h 33m 45s, ave eps reward [-13.67 -13.67 -13.67], ave eps length 10.0, reward step [-1.37 -1.37 -1.37], FPS 8.58, mean reward -13.666101240061142, std reward 8.304855430995888, AG 0.0
2024-04-07 01:52:30,707 : Time 00h 34m 12s, ave eps reward [-10.45 -10.45 -10.45], ave eps length 10.0, reward step [-1.04 -1.04 -1.04], FPS 6.45, mean reward -10.448757227024375, std reward 7.585074176146101, AG 0.0
2024-04-07 01:52:57,306 : Time 00h 34m 39s, ave eps reward [-12.68 -12.68 -12.68], ave eps length 10.0, reward step [-1.27 -1.27 -1.27], FPS 7.17, mean reward -12.68082128737035, std reward 8.494692504618639, AG 0.0
2024-04-07 01:53:24,183 : Time 00h 35m 06s, ave eps reward [-13.88 -13.88 -13.88], ave eps length 10.0, reward step [-1.39 -1.39 -1.39], FPS 6.68, mean reward -13.881830748544497, std reward 7.366190323013791, AG 0.0
2024-04-07 01:53:50,929 : Time 00h 35m 32s, ave eps reward [-12.47 -12.47 -12.47], ave eps length 10.0, reward step [-1.25 -1.25 -1.25], FPS 6.59, mean reward -12.47260408685457, std reward 8.133475275890376, AG 0.0
2024-04-07 01:54:17,408 : Time 00h 35m 59s, ave eps reward [-17.75 -17.75 -17.75], ave eps length 10.0, reward step [-1.77 -1.77 -1.77], FPS 7.74, mean reward -17.748741250772657, std reward 4.590772170115151, AG 0.0
2024-04-07 01:54:43,808 : Time 00h 36m 25s, ave eps reward [-14.58 -14.58 -14.58], ave eps length 10.0, reward step [-1.46 -1.46 -1.46], FPS 10.04, mean reward -14.57663452958504, std reward 6.222590198537788, AG 0.0
2024-04-07 01:55:10,556 : Time 00h 36m 52s, ave eps reward [-10.74 -10.74 -10.74], ave eps length 10.0, reward step [-1.07 -1.07 -1.07], FPS 6.8, mean reward -10.741355963571378, std reward 7.898048338215963, AG 0.0
2024-04-07 01:55:37,424 : Time 00h 37m 19s, ave eps reward [-12.01 -12.01 -12.01], ave eps length 10.0, reward step [-1.2 -1.2 -1.2], FPS 8.9, mean reward -12.01000719168779, std reward 6.730549442458577, AG 0.0
2024-04-07 01:56:03,937 : Time 00h 37m 45s, ave eps reward [-14.61 -14.61 -14.61], ave eps length 10.0, reward step [-1.46 -1.46 -1.46], FPS 8.78, mean reward -14.606795853844881, std reward 7.6514094695382, AG 0.0
2024-04-07 01:56:30,746 : Time 00h 38m 12s, ave eps reward [-14.04 -14.04 -14.04], ave eps length 10.0, reward step [-1.4 -1.4 -1.4], FPS 6.58, mean reward -14.039135473095996, std reward 7.029633227175841, AG 0.0
2024-04-07 01:56:57,402 : Time 00h 38m 39s, ave eps reward [-12.64 -12.64 -12.64], ave eps length 10.0, reward step [-1.26 -1.26 -1.26], FPS 6.95, mean reward -12.6384054220939, std reward 7.55823600915365, AG 0.0
2024-04-07 01:57:24,104 : Time 00h 39m 06s, ave eps reward [-13.24 -13.24 -13.24], ave eps length 10.0, reward step [-1.32 -1.32 -1.32], FPS 6.72, mean reward -13.236240308043682, std reward 6.792800155058197, AG 0.0
2024-04-07 01:57:50,780 : Time 00h 39m 32s, ave eps reward [-13.76 -13.76 -13.76], ave eps length 10.0, reward step [-1.38 -1.38 -1.38], FPS 6.77, mean reward -13.759397542341455, std reward 7.002392140150565, AG 0.0
2024-04-07 01:58:17,493 : Time 00h 39m 59s, ave eps reward [-11.23 -11.23 -11.23], ave eps length 10.0, reward step [-1.12 -1.12 -1.12], FPS 6.79, mean reward -11.229133595472472, std reward 6.643583778323382, AG 0.0
2024-04-07 01:58:43,815 : Time 00h 40m 25s, ave eps reward [-11.61 -11.61 -11.61], ave eps length 10.0, reward step [-1.16 -1.16 -1.16], FPS 9.76, mean reward -11.607063928237405, std reward 7.234344296419261, AG 0.0
2024-04-07 01:59:10,488 : Time 00h 40m 52s, ave eps reward [-10.56 -10.56 -10.56], ave eps length 10.0, reward step [-1.06 -1.06 -1.06], FPS 7.51, mean reward -10.560742046041296, std reward 7.2966460421920045, AG 0.0
2024-04-07 01:59:37,514 : Time 00h 41m 19s, ave eps reward [-13.54 -13.54 -13.54], ave eps length 10.0, reward step [-1.35 -1.35 -1.35], FPS 8.16, mean reward -13.544819586266968, std reward 6.64723823403994, AG 0.0
2024-04-07 02:00:03,971 : Time 00h 41m 45s, ave eps reward [-8.34 -8.34 -8.34], ave eps length 10.0, reward step [-0.83 -0.83 -0.83], FPS 8.95, mean reward -8.336079110575772, std reward 6.263518489518825, AG 0.0
2024-04-07 02:00:30,643 : Time 00h 42m 12s, ave eps reward [-10.31 -10.31 -10.31], ave eps length 10.0, reward step [-1.03 -1.03 -1.03], FPS 7.84, mean reward -10.313447154678377, std reward 6.153309467004114, AG 0.0
2024-04-07 02:00:57,471 : Time 00h 42m 39s, ave eps reward [-10.44 -10.44 -10.44], ave eps length 10.0, reward step [-1.04 -1.04 -1.04], FPS 6.66, mean reward -10.43593993828101, std reward 5.146137750306896, AG 0.0
2024-04-07 02:01:24,286 : Time 00h 43m 06s, ave eps reward [-12.98 -12.98 -12.98], ave eps length 10.0, reward step [-1.3 -1.3 -1.3], FPS 6.6, mean reward -12.984339273977653, std reward 5.925993251494934, AG 0.0
2024-04-07 02:01:50,943 : Time 00h 43m 32s, ave eps reward [-11.33 -11.33 -11.33], ave eps length 10.0, reward step [-1.13 -1.13 -1.13], FPS 7.91, mean reward -11.334993391226336, std reward 7.573472553903023, AG 0.0
2024-04-07 02:02:17,665 : Time 00h 43m 59s, ave eps reward [-11.98 -11.98 -11.98], ave eps length 10.0, reward step [-1.2 -1.2 -1.2], FPS 6.82, mean reward -11.97646636439612, std reward 7.977102004882292, AG 0.0
2024-04-07 02:02:44,148 : Time 00h 44m 26s, ave eps reward [-8.02 -8.02 -8.02], ave eps length 10.0, reward step [-0.8 -0.8 -0.8], FPS 7.64, mean reward -8.01788967852354, std reward 5.122191471130385, AG 0.0
2024-04-07 02:03:11,000 : Time 00h 44m 52s, ave eps reward [-10.74 -10.74 -10.74], ave eps length 10.0, reward step [-1.07 -1.07 -1.07], FPS 8.35, mean reward -10.739685446069759, std reward 6.540370295437243, AG 0.0
2024-04-07 02:03:38,307 : Time 00h 45m 20s, ave eps reward [-11.8 -11.8 -11.8], ave eps length 10.0, reward step [-1.18 -1.18 -1.18], FPS 6.57, mean reward -11.799831495440753, std reward 5.46932469647144, AG 0.0
2024-04-07 02:04:04,709 : Time 00h 45m 46s, ave eps reward [-12.26 -12.26 -12.26], ave eps length 10.0, reward step [-1.23 -1.23 -1.23], FPS 8.69, mean reward -12.258151510440822, std reward 6.330993929103991, AG 0.0
2024-04-07 02:04:31,245 : Time 00h 46m 13s, ave eps reward [-11.1 -11.1 -11.1], ave eps length 10.0, reward step [-1.11 -1.11 -1.11], FPS 8.87, mean reward -11.103899285613736, std reward 5.045673925970836, AG 0.0
2024-04-07 02:04:58,072 : Time 00h 46m 40s, ave eps reward [-8.54 -8.54 -8.54], ave eps length 10.0, reward step [-0.85 -0.85 -0.85], FPS 6.68, mean reward -8.544486405145715, std reward 6.782899431561413, AG 0.0
2024-04-07 02:05:24,775 : Time 00h 47m 06s, ave eps reward [-8.65 -8.65 -8.65], ave eps length 10.0, reward step [-0.86 -0.86 -0.86], FPS 6.58, mean reward -8.64632339932515, std reward 6.634805967960142, AG 0.0
2024-04-07 02:05:51,543 : Time 00h 47m 33s, ave eps reward [-11.29 -11.29 -11.29], ave eps length 10.0, reward step [-1.13 -1.13 -1.13], FPS 7.62, mean reward -11.28629066453244, std reward 6.344387364131566, AG 0.0
2024-04-07 02:06:18,226 : Time 00h 48m 00s, ave eps reward [-10.8 -10.8 -10.8], ave eps length 10.0, reward step [-1.08 -1.08 -1.08], FPS 6.72, mean reward -10.801307528784708, std reward 7.441414653527385, AG 0.0
2024-04-07 02:06:44,823 : Time 00h 48m 26s, ave eps reward [-9.34 -9.34 -9.34], ave eps length 10.0, reward step [-0.93 -0.93 -0.93], FPS 7.63, mean reward -9.338239336417377, std reward 6.511187947593593, AG 0.0
2024-04-07 02:07:11,151 : Time 00h 48m 53s, ave eps reward [-10.61 -10.61 -10.61], ave eps length 10.0, reward step [-1.06 -1.06 -1.06], FPS 9.38, mean reward -10.607703489271803, std reward 6.12638593775607, AG 0.0
2024-04-07 02:07:38,510 : Time 00h 49m 20s, ave eps reward [-9.96 -9.96 -9.96], ave eps length 10.0, reward step [-1. -1. -1.], FPS 6.8, mean reward -9.963976664407301, std reward 7.47096410241818, AG 0.0
2024-04-07 02:08:04,842 : Time 00h 49m 46s, ave eps reward [-9.13 -9.13 -9.13], ave eps length 10.0, reward step [-0.91 -0.91 -0.91], FPS 9.8, mean reward -9.130454237952788, std reward 5.7340242462069195, AG 0.0
2024-04-07 02:08:31,508 : Time 00h 50m 13s, ave eps reward [-9.71 -9.71 -9.71], ave eps length 10.0, reward step [-0.97 -0.97 -0.97], FPS 7.63, mean reward -9.71054566420488, std reward 6.598430707167005, AG 0.0
2024-04-07 02:08:58,366 : Time 00h 50m 40s, ave eps reward [-9.63 -9.63 -9.63], ave eps length 10.0, reward step [-0.96 -0.96 -0.96], FPS 6.55, mean reward -9.626066379261891, std reward 6.0850892323412005, AG 0.0
2024-04-07 02:09:25,093 : Time 00h 51m 07s, ave eps reward [-9.87 -9.87 -9.87], ave eps length 10.0, reward step [-0.99 -0.99 -0.99], FPS 6.76, mean reward -9.869684535473704, std reward 5.777280900847292, AG 0.0
2024-04-07 02:09:51,907 : Time 00h 51m 33s, ave eps reward [-10.38 -10.38 -10.38], ave eps length 10.0, reward step [-1.04 -1.04 -1.04], FPS 7.97, mean reward -10.383528267407964, std reward 6.768043792726286, AG 0.0
2024-04-07 02:10:18,658 : Time 00h 52m 00s, ave eps reward [-8.44 -8.44 -8.44], ave eps length 10.0, reward step [-0.84 -0.84 -0.84], FPS 6.73, mean reward -8.438399717612942, std reward 7.836493266013911, AG 0.0
2024-04-07 02:10:45,244 : Time 00h 52m 27s, ave eps reward [-11.26 -11.26 -11.26], ave eps length 10.0, reward step [-1.13 -1.13 -1.13], FPS 7.6, mean reward -11.256873483727919, std reward 6.9466590507362, AG 0.0
2024-04-07 02:11:11,643 : Time 00h 52m 53s, ave eps reward [-9.78 -9.78 -9.78], ave eps length 10.0, reward step [-0.98 -0.98 -0.98], FPS 10.42, mean reward -9.777534872798627, std reward 6.815612303397029, AG 0.0
2024-04-07 02:11:38,845 : Time 00h 53m 20s, ave eps reward [-5.95 -5.95 -5.95], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 6.78, mean reward -5.954683253275872, std reward 3.6937881832233312, AG 0.0
2024-04-07 02:12:05,362 : Time 00h 53m 47s, ave eps reward [-7.94 -7.94 -7.94], ave eps length 10.0, reward step [-0.79 -0.79 -0.79], FPS 7.78, mean reward -7.940232596546181, std reward 5.5098813971877405, AG 0.0
2024-04-07 02:12:31,712 : Time 00h 54m 13s, ave eps reward [-6.27 -6.27 -6.27], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 10.09, mean reward -6.265994559211527, std reward 4.593010277587387, AG 0.0
2024-04-07 02:12:58,489 : Time 00h 54m 40s, ave eps reward [-5.48 -5.48 -5.48], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.84, mean reward -5.484536360697437, std reward 4.529847519766703, AG 0.0
2024-04-07 02:13:25,255 : Time 00h 55m 07s, ave eps reward [-6.77 -6.77 -6.77], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 6.69, mean reward -6.770541998881311, std reward 4.927027442018615, AG 0.0
2024-04-07 02:13:51,969 : Time 00h 55m 33s, ave eps reward [-13.02 -13.02 -13.02], ave eps length 10.0, reward step [-1.3 -1.3 -1.3], FPS 8.67, mean reward -13.02240666009348, std reward 11.385166309173139, AG 0.0
2024-04-07 02:14:18,754 : Time 00h 56m 00s, ave eps reward [-8.49 -8.49 -8.49], ave eps length 10.0, reward step [-0.85 -0.85 -0.85], FPS 6.73, mean reward -8.492109703883578, std reward 6.031390412984015, AG 0.0
2024-04-07 02:14:45,457 : Time 00h 56m 27s, ave eps reward [-9.27 -9.27 -9.27], ave eps length 10.0, reward step [-0.93 -0.93 -0.93], FPS 7.08, mean reward -9.27302787488474, std reward 9.031486199433635, AG 0.0
2024-04-07 02:15:11,793 : Time 00h 56m 53s, ave eps reward [-7.31 -7.31 -7.31], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 9.91, mean reward -7.307594301076532, std reward 6.435773505675806, AG 0.0
2024-04-07 02:15:39,071 : Time 00h 57m 20s, ave eps reward [-7.37 -7.37 -7.37], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 6.84, mean reward -7.369776487905606, std reward 5.817655842828927, AG 0.0
2024-04-07 02:16:05,787 : Time 00h 57m 47s, ave eps reward [-8.74 -8.74 -8.74], ave eps length 10.0, reward step [-0.87 -0.87 -0.87], FPS 7.03, mean reward -8.742059620716095, std reward 6.6355780682289565, AG 0.0
2024-04-07 02:16:32,150 : Time 00h 58m 14s, ave eps reward [-7.68 -7.68 -7.68], ave eps length 10.0, reward step [-0.77 -0.77 -0.77], FPS 10.13, mean reward -7.6847436524944985, std reward 4.375839895843258, AG 0.0
2024-04-07 02:16:58,813 : Time 00h 58m 40s, ave eps reward [-6.2 -6.2 -6.2], ave eps length 10.0, reward step [-0.62 -0.62 -0.62], FPS 7.71, mean reward -6.196439038877678, std reward 5.529263758289708, AG 0.0
2024-04-07 02:17:25,531 : Time 00h 59m 07s, ave eps reward [-9.66 -9.66 -9.66], ave eps length 10.0, reward step [-0.97 -0.97 -0.97], FPS 6.76, mean reward -9.655456021650094, std reward 7.830140999040426, AG 0.0
2024-04-07 02:17:52,204 : Time 00h 59m 34s, ave eps reward [-6.57 -6.57 -6.57], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 9.23, mean reward -6.56764300664436, std reward 6.1315658993006, AG 0.0
2024-04-07 02:18:18,919 : Time 01h 00m 00s, ave eps reward [-9.01 -9.01 -9.01], ave eps length 10.0, reward step [-0.9 -0.9 -0.9], FPS 6.69, mean reward -9.011036217877134, std reward 6.506141222729341, AG 0.0
2024-04-07 02:18:45,660 : Time 01h 00m 27s, ave eps reward [-8.66 -8.66 -8.66], ave eps length 10.0, reward step [-0.87 -0.87 -0.87], FPS 6.93, mean reward -8.662129223135565, std reward 6.373176433932784, AG 0.0
2024-04-07 02:19:11,991 : Time 01h 00m 53s, ave eps reward [-6.71 -6.71 -6.71], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 9.44, mean reward -6.713991131450359, std reward 5.535623334248899, AG 0.0
2024-04-07 02:19:39,052 : Time 01h 01m 20s, ave eps reward [-6.29 -6.29 -6.29], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 6.7, mean reward -6.293891011126055, std reward 4.451931228275113, AG 0.0
2024-04-07 02:20:05,664 : Time 01h 01m 47s, ave eps reward [-11.07 -11.07 -11.07], ave eps length 10.0, reward step [-1.11 -1.11 -1.11], FPS 7.28, mean reward -11.07117607530217, std reward 6.882151207245262, AG 0.0
2024-04-07 02:20:31,992 : Time 01h 02m 13s, ave eps reward [-8.31 -8.31 -8.31], ave eps length 10.0, reward step [-0.83 -0.83 -0.83], FPS 9.84, mean reward -8.307411268959582, std reward 5.944731784699952, AG 0.0
2024-04-07 02:20:58,658 : Time 01h 02m 40s, ave eps reward [-9.32 -9.32 -9.32], ave eps length 10.0, reward step [-0.93 -0.93 -0.93], FPS 7.96, mean reward -9.320278427266027, std reward 7.184637721620115, AG 0.0
2024-04-07 02:21:25,457 : Time 01h 03m 07s, ave eps reward [-6.88 -6.88 -6.88], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 6.53, mean reward -6.877147173243332, std reward 5.67589376166642, AG 0.0
2024-04-07 02:21:52,528 : Time 01h 03m 34s, ave eps reward [-9.46 -9.46 -9.46], ave eps length 10.0, reward step [-0.95 -0.95 -0.95], FPS 8.27, mean reward -9.457964565724485, std reward 6.8603269139667695, AG 0.0
2024-04-07 02:22:19,002 : Time 01h 04m 00s, ave eps reward [-8.9 -8.9 -8.9], ave eps length 10.0, reward step [-0.89 -0.89 -0.89], FPS 10.26, mean reward -8.903855582965305, std reward 6.725483584210473, AG 0.0
2024-04-07 02:22:45,713 : Time 01h 04m 27s, ave eps reward [-8.98 -8.98 -8.98], ave eps length 10.0, reward step [-0.9 -0.9 -0.9], FPS 7.88, mean reward -8.97774822797272, std reward 7.842016584129316, AG 0.0
2024-04-07 02:23:12,497 : Time 01h 04m 54s, ave eps reward [-7.25 -7.25 -7.25], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 6.52, mean reward -7.250874122470248, std reward 5.862840013653653, AG 0.0
2024-04-07 02:23:39,247 : Time 01h 05m 21s, ave eps reward [-8.32 -8.32 -8.32], ave eps length 10.0, reward step [-0.83 -0.83 -0.83], FPS 6.69, mean reward -8.320362150121992, std reward 6.366022411178083, AG 0.0
2024-04-07 02:24:06,012 : Time 01h 05m 47s, ave eps reward [-7.91 -7.91 -7.91], ave eps length 10.0, reward step [-0.79 -0.79 -0.79], FPS 7.27, mean reward -7.906725426986448, std reward 6.895919388422263, AG 0.0
2024-04-07 02:24:32,671 : Time 01h 06m 14s, ave eps reward [-5.59 -5.59 -5.59], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.75, mean reward -5.592595869720106, std reward 4.708249255865325, AG 0.0
2024-04-07 02:24:59,137 : Time 01h 06m 41s, ave eps reward [-5.4 -5.4 -5.4], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 8.14, mean reward -5.404572337921725, std reward 6.037947406375273, AG 0.0
2024-04-07 02:25:25,556 : Time 01h 07m 07s, ave eps reward [-8.24 -8.24 -8.24], ave eps length 10.0, reward step [-0.82 -0.82 -0.82], FPS 9.5, mean reward -8.239576674008031, std reward 6.194882645919391, AG 0.0
2024-04-07 02:25:53,012 : Time 01h 07m 34s, ave eps reward [-4.77 -4.77 -4.77], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.64, mean reward -4.7718909687066215, std reward 4.921825502924744, AG 0.0
2024-04-07 02:26:19,634 : Time 01h 08m 01s, ave eps reward [-6.03 -6.03 -6.03], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 8.23, mean reward -6.026649319292389, std reward 5.710333783507839, AG 0.0
2024-04-07 02:26:46,316 : Time 01h 08m 28s, ave eps reward [-8.45 -8.45 -8.45], ave eps length 10.0, reward step [-0.84 -0.84 -0.84], FPS 9.34, mean reward -8.446161629932178, std reward 6.8574315292566785, AG 0.0
2024-04-07 02:27:13,413 : Time 01h 08m 55s, ave eps reward [-7.82 -7.82 -7.82], ave eps length 10.0, reward step [-0.78 -0.78 -0.78], FPS 6.96, mean reward -7.819076116275022, std reward 5.9220571541026965, AG 0.0
2024-04-07 02:27:40,257 : Time 01h 09m 22s, ave eps reward [-8.68 -8.68 -8.68], ave eps length 10.0, reward step [-0.87 -0.87 -0.87], FPS 6.66, mean reward -8.680450994466113, std reward 6.228282924317317, AG 0.0
2024-04-07 02:28:07,101 : Time 01h 09m 49s, ave eps reward [-8.12 -8.12 -8.12], ave eps length 10.0, reward step [-0.81 -0.81 -0.81], FPS 8.56, mean reward -8.119809578472971, std reward 5.255534258187378, AG 0.0
2024-04-07 02:28:33,908 : Time 01h 10m 15s, ave eps reward [-6.97 -6.97 -6.97], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 6.75, mean reward -6.965462421703528, std reward 6.005346309969537, AG 0.0
2024-04-07 02:29:00,827 : Time 01h 10m 42s, ave eps reward [-9.68 -9.68 -9.68], ave eps length 10.0, reward step [-0.97 -0.97 -0.97], FPS 6.44, mean reward -9.68404958946942, std reward 6.9065558585046345, AG 0.0
2024-04-07 02:29:27,584 : Time 01h 11m 09s, ave eps reward [-8.9 -8.9 -8.9], ave eps length 10.0, reward step [-0.89 -0.89 -0.89], FPS 7.25, mean reward -8.902095053642162, std reward 6.969955868413299, AG 0.0
2024-04-07 02:29:54,786 : Time 01h 11m 36s, ave eps reward [-6.53 -6.53 -6.53], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 6.73, mean reward -6.529998260373111, std reward 6.3161495612916845, AG 0.0
2024-04-07 02:30:21,819 : Time 01h 12m 03s, ave eps reward [-9.13 -9.13 -9.13], ave eps length 10.0, reward step [-0.91 -0.91 -0.91], FPS 6.62, mean reward -9.131677996124395, std reward 7.275716034967387, AG 0.0
2024-04-07 02:30:48,696 : Time 01h 12m 30s, ave eps reward [-7.95 -7.95 -7.95], ave eps length 10.0, reward step [-0.79 -0.79 -0.79], FPS 7.15, mean reward -7.945615962734832, std reward 6.302455153408792, AG 0.0
2024-04-07 02:31:15,366 : Time 01h 12m 57s, ave eps reward [-6.88 -6.88 -6.88], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 9.74, mean reward -6.878271281665403, std reward 5.512672259505692, AG 0.0
2024-04-07 02:31:42,263 : Time 01h 13m 24s, ave eps reward [-6.09 -6.09 -6.09], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 8.24, mean reward -6.094976466396899, std reward 5.230111987226495, AG 0.0
2024-04-07 02:32:09,744 : Time 01h 13m 51s, ave eps reward [-8.09 -8.09 -8.09], ave eps length 10.0, reward step [-0.81 -0.81 -0.81], FPS 6.81, mean reward -8.087269548334977, std reward 5.902210239475216, AG 0.0
2024-04-07 02:32:36,411 : Time 01h 14m 18s, ave eps reward [-6.85 -6.85 -6.85], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 8.63, mean reward -6.850441374869888, std reward 6.257856634572237, AG 0.0
2024-04-07 02:33:03,248 : Time 01h 14m 45s, ave eps reward [-6.85 -6.85 -6.85], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 9.27, mean reward -6.849788249366085, std reward 5.455429054181964, AG 0.0
2024-04-07 02:33:30,388 : Time 01h 15m 12s, ave eps reward [-8.58 -8.58 -8.58], ave eps length 10.0, reward step [-0.86 -0.86 -0.86], FPS 6.5, mean reward -8.579535237957959, std reward 6.292556050820251, AG 0.0
2024-04-07 02:33:57,414 : Time 01h 15m 39s, ave eps reward [-7.2 -7.2 -7.2], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 6.68, mean reward -7.197131911845839, std reward 6.141446847602371, AG 0.0
2024-04-07 02:34:24,403 : Time 01h 16m 06s, ave eps reward [-5.57 -5.57 -5.57], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 10.22, mean reward -5.565100894662725, std reward 5.695715825099609, AG 0.0
2024-04-07 02:34:51,364 : Time 01h 16m 33s, ave eps reward [-7.34 -7.34 -7.34], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 8.29, mean reward -7.341990965069021, std reward 6.258385651908915, AG 0.0
2024-04-07 02:35:18,413 : Time 01h 17m 00s, ave eps reward [-6.9 -6.9 -6.9], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 6.73, mean reward -6.895999964717058, std reward 5.333179073018371, AG 0.0
2024-04-07 02:35:45,380 : Time 01h 17m 27s, ave eps reward [-8.41 -8.41 -8.41], ave eps length 10.0, reward step [-0.84 -0.84 -0.84], FPS 6.68, mean reward -8.409618136360107, std reward 6.71391217789289, AG 0.0
2024-04-07 02:36:12,417 : Time 01h 17m 54s, ave eps reward [-7.11 -7.11 -7.11], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 6.72, mean reward -7.1053864439432814, std reward 6.934877663878337, AG 0.0
2024-04-07 02:36:39,348 : Time 01h 18m 21s, ave eps reward [-9.73 -9.73 -9.73], ave eps length 10.0, reward step [-0.97 -0.97 -0.97], FPS 6.68, mean reward -9.726385891140184, std reward 7.1808417294209725, AG 0.0
2024-04-07 02:37:06,372 : Time 01h 18m 48s, ave eps reward [-8.26 -8.26 -8.26], ave eps length 10.0, reward step [-0.83 -0.83 -0.83], FPS 6.63, mean reward -8.26160170677417, std reward 6.2651533304813904, AG 0.0
2024-04-07 02:37:33,231 : Time 01h 19m 15s, ave eps reward [-8.2 -8.2 -8.2], ave eps length 10.0, reward step [-0.82 -0.82 -0.82], FPS 7.47, mean reward -8.203219230139073, std reward 8.69366440732149, AG 0.0
2024-04-07 02:37:59,839 : Time 01h 19m 41s, ave eps reward [-8.05 -8.05 -8.05], ave eps length 10.0, reward step [-0.8 -0.8 -0.8], FPS 9.96, mean reward -8.046058760923726, std reward 6.876348907609609, AG 0.0
2024-04-07 02:38:27,273 : Time 01h 20m 09s, ave eps reward [-7.06 -7.06 -7.06], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 6.65, mean reward -7.058143068046698, std reward 5.452888168147101, AG 0.0
2024-04-07 02:38:54,290 : Time 01h 20m 36s, ave eps reward [-7. -7. -7.], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 6.6, mean reward -7.004054977627116, std reward 5.605804825148691, AG 0.0
2024-04-07 02:39:20,864 : Time 01h 21m 02s, ave eps reward [-5.57 -5.57 -5.57], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 9.53, mean reward -5.574405629533914, std reward 4.832655428047899, AG 0.0
2024-04-07 02:39:47,744 : Time 01h 21m 29s, ave eps reward [-7.83 -7.83 -7.83], ave eps length 10.0, reward step [-0.78 -0.78 -0.78], FPS 7.96, mean reward -7.834201679100216, std reward 6.2686611880614045, AG 0.0
2024-04-07 02:40:14,605 : Time 01h 21m 56s, ave eps reward [-5.78 -5.78 -5.78], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 6.74, mean reward -5.781272962446271, std reward 7.206043200326653, AG 0.0
2024-04-07 02:40:41,607 : Time 01h 22m 23s, ave eps reward [-5.52 -5.52 -5.52], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 9.83, mean reward -5.52102876809536, std reward 5.660939577843341, AG 0.0
2024-04-07 02:41:08,695 : Time 01h 22m 50s, ave eps reward [-7.39 -7.39 -7.39], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 9.33, mean reward -7.389144670849058, std reward 5.933595256550781, AG 0.0
2024-04-07 02:41:35,880 : Time 01h 23m 17s, ave eps reward [-8.91 -8.91 -8.91], ave eps length 10.0, reward step [-0.89 -0.89 -0.89], FPS 7.18, mean reward -8.914843228574249, std reward 5.174333647298062, AG 0.0
2024-04-07 02:42:02,706 : Time 01h 23m 44s, ave eps reward [-7.5 -7.5 -7.5], ave eps length 10.0, reward step [-0.75 -0.75 -0.75], FPS 6.84, mean reward -7.5039995238751445, std reward 5.966959842184971, AG 0.0
2024-04-07 02:42:29,692 : Time 01h 24m 11s, ave eps reward [-5.57 -5.57 -5.57], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.82, mean reward -5.56586835921912, std reward 5.2069757920824085, AG 0.0
2024-04-07 02:42:56,565 : Time 01h 24m 38s, ave eps reward [-5.8 -5.8 -5.8], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 9.24, mean reward -5.795968723108706, std reward 6.073477673059065, AG 0.0
2024-04-07 02:43:23,481 : Time 01h 25m 05s, ave eps reward [-8.3 -8.3 -8.3], ave eps length 10.0, reward step [-0.83 -0.83 -0.83], FPS 6.93, mean reward -8.298115647611166, std reward 6.919351624535703, AG 0.0
2024-04-07 02:43:50,466 : Time 01h 25m 32s, ave eps reward [-7.59 -7.59 -7.59], ave eps length 10.0, reward step [-0.76 -0.76 -0.76], FPS 6.56, mean reward -7.588125172968686, std reward 7.455502631269333, AG 0.0
2024-04-07 02:44:17,336 : Time 01h 25m 59s, ave eps reward [-6.01 -6.01 -6.01], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 7.52, mean reward -6.013326240656999, std reward 5.038675833427383, AG 0.0
2024-04-07 02:44:44,579 : Time 01h 26m 26s, ave eps reward [-4.08 -4.08 -4.08], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 6.59, mean reward -4.079977109390385, std reward 3.3167666363947146, AG 0.0
2024-04-07 02:45:11,568 : Time 01h 26m 53s, ave eps reward [-7.95 -7.95 -7.95], ave eps length 10.0, reward step [-0.8 -0.8 -0.8], FPS 6.73, mean reward -7.951636345657957, std reward 7.769027321582722, AG 0.0
2024-04-07 02:45:38,614 : Time 01h 27m 20s, ave eps reward [-9.9 -9.9 -9.9], ave eps length 10.0, reward step [-0.99 -0.99 -0.99], FPS 7.55, mean reward -9.899095923856263, std reward 7.9013818417998465, AG 0.0
2024-04-07 02:46:05,275 : Time 01h 27m 47s, ave eps reward [-7.35 -7.35 -7.35], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 10.02, mean reward -7.353939645524828, std reward 6.114140115062402, AG 0.0
2024-04-07 02:46:32,353 : Time 01h 28m 14s, ave eps reward [-7.92 -7.92 -7.92], ave eps length 10.0, reward step [-0.79 -0.79 -0.79], FPS 7.22, mean reward -7.921352716756512, std reward 6.0074218091258, AG 0.0
2024-04-07 02:46:59,775 : Time 01h 28m 41s, ave eps reward [-13.95 -13.95 -13.95], ave eps length 10.0, reward step [-1.4 -1.4 -1.4], FPS 7.43, mean reward -13.953163199723097, std reward 9.524641941027086, AG 0.0
2024-04-07 02:47:26,597 : Time 01h 29m 08s, ave eps reward [-7.31 -7.31 -7.31], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 8.71, mean reward -7.313134969187175, std reward 6.7925513168457785, AG 0.0
2024-04-07 02:47:53,350 : Time 01h 29m 35s, ave eps reward [-6.2 -6.2 -6.2], ave eps length 10.0, reward step [-0.62 -0.62 -0.62], FPS 8.88, mean reward -6.2017718199246525, std reward 5.854173283084934, AG 0.0
2024-04-07 02:48:20,370 : Time 01h 30m 02s, ave eps reward [-6.95 -6.95 -6.95], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 6.85, mean reward -6.9497185578696605, std reward 6.548915553902742, AG 0.0
2024-04-07 02:48:47,512 : Time 01h 30m 29s, ave eps reward [-7.8 -7.8 -7.8], ave eps length 10.0, reward step [-0.78 -0.78 -0.78], FPS 6.53, mean reward -7.79594051486157, std reward 6.024233393615917, AG 0.0
2024-04-07 02:49:14,473 : Time 01h 30m 56s, ave eps reward [-7.79 -7.79 -7.79], ave eps length 10.0, reward step [-0.78 -0.78 -0.78], FPS 9.52, mean reward -7.79361525644226, std reward 6.135143735379383, AG 0.0
2024-04-07 02:49:41,525 : Time 01h 31m 23s, ave eps reward [-7.6 -7.6 -7.6], ave eps length 10.0, reward step [-0.76 -0.76 -0.76], FPS 7.2, mean reward -7.59846049164838, std reward 6.238067286322955, AG 0.0
2024-04-07 02:50:08,548 : Time 01h 31m 50s, ave eps reward [-8.75 -8.75 -8.75], ave eps length 10.0, reward step [-0.88 -0.88 -0.88], FPS 6.63, mean reward -8.750468156545278, std reward 5.469743983101409, AG 0.0
2024-04-07 02:50:35,521 : Time 01h 32m 17s, ave eps reward [-9.57 -9.57 -9.57], ave eps length 10.0, reward step [-0.96 -0.96 -0.96], FPS 7.29, mean reward -9.573811912989338, std reward 7.587574443896698, AG 0.0
2024-04-07 02:51:02,687 : Time 01h 32m 44s, ave eps reward [-9.62 -9.62 -9.62], ave eps length 10.0, reward step [-0.96 -0.96 -0.96], FPS 6.81, mean reward -9.620345325901273, std reward 7.430782064685252, AG 0.0
2024-04-07 02:51:29,709 : Time 01h 33m 11s, ave eps reward [-7.97 -7.97 -7.97], ave eps length 10.0, reward step [-0.8 -0.8 -0.8], FPS 6.62, mean reward -7.968046967416184, std reward 6.117038741215769, AG 0.0
2024-04-07 02:51:56,773 : Time 01h 33m 38s, ave eps reward [-5.86 -5.86 -5.86], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 6.64, mean reward -5.860042541739165, std reward 5.569489242021122, AG 0.0
2024-04-07 02:52:23,471 : Time 01h 34m 05s, ave eps reward [-5.04 -5.04 -5.04], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 8.58, mean reward -5.040666146783602, std reward 4.9218655728978495, AG 0.0
2024-04-07 02:52:50,184 : Time 01h 34m 32s, ave eps reward [-9.23 -9.23 -9.23], ave eps length 10.0, reward step [-0.92 -0.92 -0.92], FPS 9.68, mean reward -9.227222830109989, std reward 7.759707590256029, AG 0.0
2024-04-07 02:53:17,764 : Time 01h 34m 59s, ave eps reward [-7.34 -7.34 -7.34], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 6.71, mean reward -7.338196921238163, std reward 5.475143982760201, AG 0.0
2024-04-07 02:53:44,744 : Time 01h 35m 26s, ave eps reward [-5.67 -5.67 -5.67], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 6.95, mean reward -5.66737932336141, std reward 5.312898517268065, AG 0.0
2024-04-07 02:54:11,475 : Time 01h 35m 53s, ave eps reward [-8.32 -8.32 -8.32], ave eps length 10.0, reward step [-0.83 -0.83 -0.83], FPS 8.97, mean reward -8.32381568234472, std reward 7.117424587381548, AG 0.0
2024-04-07 02:54:38,322 : Time 01h 36m 20s, ave eps reward [-8.74 -8.74 -8.74], ave eps length 10.0, reward step [-0.87 -0.87 -0.87], FPS 8.62, mean reward -8.73813476748526, std reward 5.9746068859254216, AG 0.0
2024-04-07 02:55:05,390 : Time 01h 36m 47s, ave eps reward [-6.38 -6.38 -6.38], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 6.59, mean reward -6.382797662258843, std reward 4.678811800692811, AG 0.0
2024-04-07 02:55:32,460 : Time 01h 37m 14s, ave eps reward [-8.95 -8.95 -8.95], ave eps length 10.0, reward step [-0.9 -0.9 -0.9], FPS 9.59, mean reward -8.950123112699261, std reward 7.40071884416332, AG 0.0
2024-04-07 02:55:59,348 : Time 01h 37m 41s, ave eps reward [-9.95 -9.95 -9.95], ave eps length 10.0, reward step [-1. -1. -1.], FPS 8.23, mean reward -9.952739109803431, std reward 10.085929624067207, AG 0.0
2024-04-07 02:56:26,247 : Time 01h 38m 08s, ave eps reward [-7.42 -7.42 -7.42], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 6.49, mean reward -7.41620945548623, std reward 5.6286541360910185, AG 0.0
2024-04-07 02:56:53,257 : Time 01h 38m 35s, ave eps reward [-13.36 -13.36 -13.36], ave eps length 10.0, reward step [-1.34 -1.34 -1.34], FPS 6.65, mean reward -13.356558936611453, std reward 8.421913127012862, AG 0.0
2024-04-07 02:57:20,281 : Time 01h 39m 02s, ave eps reward [-7.5 -7.5 -7.5], ave eps length 10.0, reward step [-0.75 -0.75 -0.75], FPS 6.56, mean reward -7.496531232949581, std reward 6.421036416970329, AG 0.0
2024-04-07 02:57:47,184 : Time 01h 39m 29s, ave eps reward [-7.29 -7.29 -7.29], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 6.84, mean reward -7.290818839510701, std reward 5.729634443803649, AG 0.0
2024-04-07 02:58:14,265 : Time 01h 39m 56s, ave eps reward [-6.44 -6.44 -6.44], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 6.59, mean reward -6.440228623579587, std reward 5.7111306813616345, AG 0.0
2024-04-07 02:58:41,005 : Time 01h 40m 22s, ave eps reward [-5.49 -5.49 -5.49], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 7.89, mean reward -5.488716427934653, std reward 5.893118214309717, AG 0.0
2024-04-07 02:59:07,698 : Time 01h 40m 49s, ave eps reward [-7.45 -7.45 -7.45], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 10.35, mean reward -7.449664298541478, std reward 5.850209578613101, AG 0.0
2024-04-07 02:59:35,258 : Time 01h 41m 17s, ave eps reward [-8.59 -8.59 -8.59], ave eps length 10.0, reward step [-0.86 -0.86 -0.86], FPS 6.69, mean reward -8.592364956996125, std reward 5.009127672802972, AG 0.0
2024-04-07 03:00:02,233 : Time 01h 41m 44s, ave eps reward [-6.98 -6.98 -6.98], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 6.65, mean reward -6.975737186287835, std reward 5.916534723167123, AG 0.0
2024-04-07 03:00:28,886 : Time 01h 42m 10s, ave eps reward [-6.89 -6.89 -6.89], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 8.23, mean reward -6.892577695207308, std reward 6.8634804598558885, AG 0.0
2024-04-07 03:00:55,618 : Time 01h 42m 37s, ave eps reward [-7.36 -7.36 -7.36], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 9.49, mean reward -7.356430757842264, std reward 7.399522454662827, AG 0.0
2024-04-07 03:01:22,688 : Time 01h 43m 04s, ave eps reward [-9.15 -9.15 -9.15], ave eps length 10.0, reward step [-0.92 -0.92 -0.92], FPS 6.87, mean reward -9.151114000177916, std reward 7.467468193394988, AG 0.0
2024-04-07 03:01:49,968 : Time 01h 43m 31s, ave eps reward [-6.77 -6.77 -6.77], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 7.96, mean reward -6.773749662496691, std reward 7.505448439164051, AG 0.0
2024-04-07 03:02:16,637 : Time 01h 43m 58s, ave eps reward [-7.32 -7.32 -7.32], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 10.25, mean reward -7.3230972455761405, std reward 6.471485445738209, AG 0.0
2024-04-07 03:02:43,640 : Time 01h 44m 25s, ave eps reward [-7.17 -7.17 -7.17], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 7.52, mean reward -7.1696073663919675, std reward 7.568720855812245, AG 0.0
2024-04-07 03:03:10,713 : Time 01h 44m 52s, ave eps reward [-8.96 -8.96 -8.96], ave eps length 10.0, reward step [-0.9 -0.9 -0.9], FPS 6.51, mean reward -8.956241098708023, std reward 7.0476930386468775, AG 0.0
2024-04-07 03:03:37,657 : Time 01h 45m 19s, ave eps reward [-6.13 -6.13 -6.13], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 6.66, mean reward -6.1322174227929045, std reward 5.4331248404828845, AG 0.0
2024-04-07 03:04:04,703 : Time 01h 45m 46s, ave eps reward [-7.09 -7.09 -7.09], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 10.52, mean reward -7.087531227461232, std reward 7.09271230599239, AG 0.0
2024-04-07 03:04:31,701 : Time 01h 46m 13s, ave eps reward [-4.37 -4.37 -4.37], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 8.08, mean reward -4.371688601897488, std reward 3.583047817212509, AG 0.0
2024-04-07 03:04:58,694 : Time 01h 46m 40s, ave eps reward [-6.3 -6.3 -6.3], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 6.64, mean reward -6.296598575367635, std reward 5.567965603429632, AG 0.0
2024-04-07 03:05:25,644 : Time 01h 47m 07s, ave eps reward [-7.09 -7.09 -7.09], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 7.36, mean reward -7.09442360310531, std reward 4.5063605629392525, AG 0.0
2024-04-07 03:05:52,835 : Time 01h 47m 34s, ave eps reward [-6.37 -6.37 -6.37], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 6.72, mean reward -6.367116151243316, std reward 5.681059417475861, AG 0.0
2024-04-07 03:06:19,855 : Time 01h 48m 01s, ave eps reward [-6.44 -6.44 -6.44], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 6.64, mean reward -6.436983463079825, std reward 4.03448716024369, AG 0.0
2024-04-07 03:06:46,934 : Time 01h 48m 28s, ave eps reward [-8.84 -8.84 -8.84], ave eps length 10.0, reward step [-0.88 -0.88 -0.88], FPS 6.75, mean reward -8.83521183212019, std reward 6.775612871004795, AG 0.0
2024-04-07 03:07:13,555 : Time 01h 48m 55s, ave eps reward [-7.23 -7.23 -7.23], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 9.09, mean reward -7.231721290209786, std reward 6.342109692722426, AG 0.0
2024-04-07 03:07:40,457 : Time 01h 49m 22s, ave eps reward [-6.86 -6.86 -6.86], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 8.27, mean reward -6.861131637587468, std reward 6.211080781298063, AG 0.0
2024-04-07 03:08:07,955 : Time 01h 49m 49s, ave eps reward [-8.17 -8.17 -8.17], ave eps length 10.0, reward step [-0.82 -0.82 -0.82], FPS 6.64, mean reward -8.173591106688367, std reward 6.297813050335337, AG 0.0
2024-04-07 03:08:34,621 : Time 01h 50m 16s, ave eps reward [-8.39 -8.39 -8.39], ave eps length 10.0, reward step [-0.84 -0.84 -0.84], FPS 9.17, mean reward -8.391695479635482, std reward 6.41156660769426, AG 0.0
2024-04-07 03:09:01,557 : Time 01h 50m 43s, ave eps reward [-8.08 -8.08 -8.08], ave eps length 10.0, reward step [-0.81 -0.81 -0.81], FPS 8.36, mean reward -8.07749867474649, std reward 6.599041470519945, AG 0.0
2024-04-07 03:09:28,573 : Time 01h 51m 10s, ave eps reward [-5.82 -5.82 -5.82], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 6.66, mean reward -5.819311552955815, std reward 5.328273160195969, AG 0.0
2024-04-07 03:09:55,617 : Time 01h 51m 37s, ave eps reward [-5.97 -5.97 -5.97], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 6.69, mean reward -5.967352709320646, std reward 6.9857144535626645, AG 0.0
2024-04-07 03:10:22,633 : Time 01h 52m 04s, ave eps reward [-6.13 -6.13 -6.13], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 8.53, mean reward -6.1278355437506296, std reward 4.988775491988275, AG 0.0
2024-04-07 03:10:49,808 : Time 01h 52m 31s, ave eps reward [-9.78 -9.78 -9.78], ave eps length 10.0, reward step [-0.98 -0.98 -0.98], FPS 6.63, mean reward -9.775933069504884, std reward 6.973289704498152, AG 0.0
2024-04-07 03:11:16,864 : Time 01h 52m 58s, ave eps reward [-9.71 -9.71 -9.71], ave eps length 10.0, reward step [-0.97 -0.97 -0.97], FPS 6.65, mean reward -9.714288413884995, std reward 9.907375177448879, AG 0.0
2024-04-07 03:11:43,546 : Time 01h 53m 25s, ave eps reward [-9.75 -9.75 -9.75], ave eps length 10.0, reward step [-0.98 -0.98 -0.98], FPS 8.84, mean reward -9.75307422072193, std reward 7.086085714623798, AG 0.0
2024-04-07 03:12:10,947 : Time 01h 53m 52s, ave eps reward [-8.83 -8.83 -8.83], ave eps length 10.0, reward step [-0.88 -0.88 -0.88], FPS 6.48, mean reward -8.831954762738572, std reward 7.643527096053878, AG 0.0
2024-04-07 03:12:37,953 : Time 01h 54m 19s, ave eps reward [-5.74 -5.74 -5.74], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 6.66, mean reward -5.737150573946535, std reward 4.442346787206079, AG 0.0
2024-04-07 03:13:04,836 : Time 01h 54m 46s, ave eps reward [-5.06 -5.06 -5.06], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 7.12, mean reward -5.05532340374245, std reward 4.0723385202773414, AG 0.0
2024-04-07 03:13:31,527 : Time 01h 55m 13s, ave eps reward [-7.49 -7.49 -7.49], ave eps length 10.0, reward step [-0.75 -0.75 -0.75], FPS 9.43, mean reward -7.492368600626891, std reward 7.333038287734403, AG 0.0
2024-04-07 03:13:58,349 : Time 01h 55m 40s, ave eps reward [-4.31 -4.31 -4.31], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 8.59, mean reward -4.313152049998135, std reward 4.474077582307029, AG 0.0
2024-04-07 03:14:25,869 : Time 01h 56m 07s, ave eps reward [-5.61 -5.61 -5.61], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.69, mean reward -5.614578818187452, std reward 5.970549901738294, AG 0.0
2024-04-07 03:14:52,593 : Time 01h 56m 34s, ave eps reward [-7.23 -7.23 -7.23], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 8.41, mean reward -7.230817140856733, std reward 5.085566100156351, AG 0.0
2024-04-07 03:15:19,280 : Time 01h 57m 01s, ave eps reward [-7.15 -7.15 -7.15], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 9.17, mean reward -7.146550553736054, std reward 5.060551894319605, AG 0.0
2024-04-07 03:15:46,396 : Time 01h 57m 28s, ave eps reward [-7.41 -7.41 -7.41], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 6.76, mean reward -7.409106701487724, std reward 6.561386447889653, AG 0.0
2024-04-07 03:16:13,425 : Time 01h 57m 55s, ave eps reward [-5.39 -5.39 -5.39], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 6.58, mean reward -5.390303795407121, std reward 5.594066921100092, AG 0.0
2024-04-07 03:16:40,415 : Time 01h 58m 22s, ave eps reward [-7.21 -7.21 -7.21], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 8.46, mean reward -7.214919443763636, std reward 5.759838735959894, AG 0.0
2024-04-07 03:17:07,464 : Time 01h 58m 49s, ave eps reward [-8.72 -8.72 -8.72], ave eps length 10.0, reward step [-0.87 -0.87 -0.87], FPS 6.6, mean reward -8.723655577263806, std reward 7.088571152764529, AG 0.0
2024-04-07 03:17:34,368 : Time 01h 59m 16s, ave eps reward [-7.98 -7.98 -7.98], ave eps length 10.0, reward step [-0.8 -0.8 -0.8], FPS 7.14, mean reward -7.976217905453927, std reward 8.08920907797606, AG 0.0
2024-04-07 03:18:00,921 : Time 01h 59m 42s, ave eps reward [-6.95 -6.95 -6.95], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 9.9, mean reward -6.950616117604634, std reward 5.943808657289111, AG 0.0
2024-04-07 03:18:28,452 : Time 02h 00m 10s, ave eps reward [-8.89 -8.89 -8.89], ave eps length 10.0, reward step [-0.89 -0.89 -0.89], FPS 6.61, mean reward -8.888175256470781, std reward 6.580967456950973, AG 0.0
2024-04-07 03:18:55,457 : Time 02h 00m 37s, ave eps reward [-5.54 -5.54 -5.54], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.7, mean reward -5.539257593963791, std reward 5.266888984153104, AG 0.0
2024-04-07 03:19:22,161 : Time 02h 01m 04s, ave eps reward [-6.5 -6.5 -6.5], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 8.32, mean reward -6.500468691739172, std reward 6.287513861436227, AG 0.0
2024-04-07 03:19:48,955 : Time 02h 01m 30s, ave eps reward [-7.72 -7.72 -7.72], ave eps length 10.0, reward step [-0.77 -0.77 -0.77], FPS 9.0, mean reward -7.716747523011643, std reward 6.694393894201668, AG 0.0
2024-04-07 03:20:16,480 : Time 02h 01m 58s, ave eps reward [-8.09 -8.09 -8.09], ave eps length 10.0, reward step [-0.81 -0.81 -0.81], FPS 6.53, mean reward -8.0938631132587, std reward 8.259092609571324, AG 0.0
2024-04-07 03:20:43,596 : Time 02h 02m 25s, ave eps reward [-7.12 -7.12 -7.12], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 10.67, mean reward -7.119921603760458, std reward 5.177487340153896, AG 0.0
2024-04-07 03:21:10,676 : Time 02h 02m 52s, ave eps reward [-9.43 -9.43 -9.43], ave eps length 10.0, reward step [-0.94 -0.94 -0.94], FPS 7.77, mean reward -9.427895800823851, std reward 7.403718845786504, AG 0.0
2024-04-07 03:21:37,682 : Time 02h 03m 19s, ave eps reward [-4.96 -4.96 -4.96], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 6.56, mean reward -4.9614295925599645, std reward 3.7074280334912486, AG 0.0
2024-04-07 03:22:04,573 : Time 02h 03m 46s, ave eps reward [-7.34 -7.34 -7.34], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 6.75, mean reward -7.344283210798942, std reward 6.6066899641750725, AG 0.0
2024-04-07 03:22:31,631 : Time 02h 04m 13s, ave eps reward [-7.08 -7.08 -7.08], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 6.76, mean reward -7.077861228778635, std reward 6.573956068292412, AG 0.0
2024-04-07 03:22:58,648 : Time 02h 04m 40s, ave eps reward [-5.09 -5.09 -5.09], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.52, mean reward -5.0899602894375535, std reward 4.706924509871343, AG 0.0
2024-04-07 03:23:25,712 : Time 02h 05m 07s, ave eps reward [-6.81 -6.81 -6.81], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 6.6, mean reward -6.805211935833173, std reward 6.998572703466016, AG 0.0
2024-04-07 03:23:52,412 : Time 02h 05m 34s, ave eps reward [-4.76 -4.76 -4.76], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 9.05, mean reward -4.756699705459648, std reward 3.9628996508006136, AG 0.0
2024-04-07 03:24:19,210 : Time 02h 06m 01s, ave eps reward [-6.66 -6.66 -6.66], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 8.73, mean reward -6.660503678038215, std reward 5.589941242478923, AG 0.0
2024-04-07 03:24:46,741 : Time 02h 06m 28s, ave eps reward [-4.79 -4.79 -4.79], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.9, mean reward -4.787567711817318, std reward 4.555884544112136, AG 0.0
2024-04-07 03:25:13,791 : Time 02h 06m 55s, ave eps reward [-7.25 -7.25 -7.25], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 6.63, mean reward -7.2537460043220054, std reward 5.600470658134835, AG 0.0
2024-04-07 03:25:40,444 : Time 02h 07m 22s, ave eps reward [-6.53 -6.53 -6.53], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 9.1, mean reward -6.527916647825674, std reward 5.044469455519198, AG 0.0
2024-04-07 03:26:07,291 : Time 02h 07m 49s, ave eps reward [-7.65 -7.65 -7.65], ave eps length 10.0, reward step [-0.76 -0.76 -0.76], FPS 8.43, mean reward -7.6492908000378295, std reward 5.858028123483595, AG 0.0
2024-04-07 03:26:34,242 : Time 02h 08m 16s, ave eps reward [-5.75 -5.75 -5.75], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 6.44, mean reward -5.75269911672048, std reward 5.275124144885528, AG 0.0
2024-04-07 03:27:01,450 : Time 02h 08m 43s, ave eps reward [-6.81 -6.81 -6.81], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 8.89, mean reward -6.813390607616748, std reward 6.939101591895133, AG 0.0
2024-04-07 03:27:28,277 : Time 02h 09m 10s, ave eps reward [-4.96 -4.96 -4.96], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 9.5, mean reward -4.964450467248298, std reward 4.721523466872419, AG 0.0
2024-04-07 03:27:55,370 : Time 02h 09m 37s, ave eps reward [-5.45 -5.45 -5.45], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 6.76, mean reward -5.445057562603678, std reward 5.1089518728970384, AG 0.0
2024-04-07 03:28:22,368 : Time 02h 10m 04s, ave eps reward [-7.12 -7.12 -7.12], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 6.51, mean reward -7.117544875039667, std reward 6.5832666747236575, AG 0.0
2024-04-07 03:28:49,400 : Time 02h 10m 31s, ave eps reward [-7.21 -7.21 -7.21], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 6.59, mean reward -7.212736937100415, std reward 6.726822241066389, AG 0.0
2024-04-07 03:29:16,362 : Time 02h 10m 58s, ave eps reward [-7.03 -7.03 -7.03], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 7.1, mean reward -7.031107967693584, std reward 5.087610790841214, AG 0.0
2024-04-07 03:29:43,307 : Time 02h 11m 25s, ave eps reward [-7.19 -7.19 -7.19], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 6.82, mean reward -7.192975821669805, std reward 6.386284760032572, AG 0.0
2024-04-07 03:30:10,203 : Time 02h 11m 52s, ave eps reward [-5.57 -5.57 -5.57], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 7.52, mean reward -5.566168614561652, std reward 5.302914915917602, AG 0.0
2024-04-07 03:30:36,940 : Time 02h 12m 18s, ave eps reward [-8.61 -8.61 -8.61], ave eps length 10.0, reward step [-0.86 -0.86 -0.86], FPS 9.54, mean reward -8.614894930451543, std reward 8.397616470845396, AG 0.0
2024-04-07 03:31:04,443 : Time 02h 12m 46s, ave eps reward [-6.66 -6.66 -6.66], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 6.71, mean reward -6.655830527977477, std reward 6.72653536071406, AG 0.0
2024-04-07 03:31:31,495 : Time 02h 13m 13s, ave eps reward [-7.67 -7.67 -7.67], ave eps length 10.0, reward step [-0.77 -0.77 -0.77], FPS 6.62, mean reward -7.6725785920156, std reward 7.090682357461895, AG 0.0
2024-04-07 03:31:58,161 : Time 02h 13m 40s, ave eps reward [-9.09 -9.09 -9.09], ave eps length 10.0, reward step [-0.91 -0.91 -0.91], FPS 8.44, mean reward -9.085636033455948, std reward 5.6754321119635565, AG 0.0
2024-04-07 03:32:24,978 : Time 02h 14m 06s, ave eps reward [-7.37 -7.37 -7.37], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 9.12, mean reward -7.371000436436868, std reward 6.333247118827222, AG 0.0
2024-04-07 03:32:51,970 : Time 02h 14m 33s, ave eps reward [-8.08 -8.08 -8.08], ave eps length 10.0, reward step [-0.81 -0.81 -0.81], FPS 6.97, mean reward -8.077071858641093, std reward 6.740425153571336, AG 0.0
2024-04-07 03:33:19,313 : Time 02h 15m 01s, ave eps reward [-7.75 -7.75 -7.75], ave eps length 10.0, reward step [-0.77 -0.77 -0.77], FPS 7.91, mean reward -7.749298049502732, std reward 5.667222455229384, AG 0.0
2024-04-07 03:33:45,946 : Time 02h 15m 27s, ave eps reward [-5.58 -5.58 -5.58], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 10.57, mean reward -5.581842433982256, std reward 5.851405149684759, AG 0.0
2024-04-07 03:34:12,904 : Time 02h 15m 54s, ave eps reward [-6.78 -6.78 -6.78], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 7.85, mean reward -6.778542585146701, std reward 5.033747996468883, AG 0.0
2024-04-07 03:34:39,937 : Time 02h 16m 21s, ave eps reward [-8.41 -8.41 -8.41], ave eps length 10.0, reward step [-0.84 -0.84 -0.84], FPS 6.68, mean reward -8.409424750161973, std reward 6.963729886200074, AG 0.0
2024-04-07 03:35:07,032 : Time 02h 16m 48s, ave eps reward [-4.83 -4.83 -4.83], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.52, mean reward -4.830314596215093, std reward 4.77123328093234, AG 0.0
2024-04-07 03:35:34,129 : Time 02h 17m 16s, ave eps reward [-5.65 -5.65 -5.65], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 7.37, mean reward -5.653146305421743, std reward 4.677446754396624, AG 0.0
2024-04-07 03:36:01,219 : Time 02h 17m 43s, ave eps reward [-6.28 -6.28 -6.28], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 6.58, mean reward -6.283434763874756, std reward 5.017944293689182, AG 0.0
2024-04-07 03:36:28,130 : Time 02h 18m 10s, ave eps reward [-5.61 -5.61 -5.61], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.78, mean reward -5.605523101181808, std reward 4.586293163802524, AG 0.0
2024-04-07 03:36:54,797 : Time 02h 18m 36s, ave eps reward [-9.33 -9.33 -9.33], ave eps length 10.0, reward step [-0.93 -0.93 -0.93], FPS 8.41, mean reward -9.333314103467796, std reward 6.311040865569199, AG 0.0
2024-04-07 03:37:22,167 : Time 02h 19m 04s, ave eps reward [-7.55 -7.55 -7.55], ave eps length 10.0, reward step [-0.75 -0.75 -0.75], FPS 6.69, mean reward -7.547421984708802, std reward 7.259524716392837, AG 0.0
2024-04-07 03:37:48,996 : Time 02h 19m 30s, ave eps reward [-5.66 -5.66 -5.66], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 6.58, mean reward -5.660895687119744, std reward 3.997612459302125, AG 0.0
2024-04-07 03:38:15,508 : Time 02h 19m 57s, ave eps reward [-4.7 -4.7 -4.7], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 7.12, mean reward -4.7000425027224635, std reward 4.0211015994322326, AG 0.0
2024-04-07 03:38:41,821 : Time 02h 20m 23s, ave eps reward [-6.56 -6.56 -6.56], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 10.04, mean reward -6.5606385532838685, std reward 5.313348547503432, AG 0.0
2024-04-07 03:39:08,433 : Time 02h 20m 50s, ave eps reward [-5.6 -5.6 -5.6], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 8.04, mean reward -5.596019806813773, std reward 4.976092708734269, AG 0.0
2024-04-07 03:39:35,482 : Time 02h 21m 17s, ave eps reward [-5.13 -5.13 -5.13], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 7.99, mean reward -5.131957125819527, std reward 4.134685528842163, AG 0.0
2024-04-07 03:40:01,942 : Time 02h 21m 43s, ave eps reward [-8.24 -8.24 -8.24], ave eps length 10.0, reward step [-0.82 -0.82 -0.82], FPS 9.95, mean reward -8.23806379498785, std reward 6.967736761837972, AG 0.0
2024-04-07 03:40:28,669 : Time 02h 22m 10s, ave eps reward [-4.09 -4.09 -4.09], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 6.92, mean reward -4.088409320199706, std reward 4.247181801528854, AG 0.0
2024-04-07 03:40:55,284 : Time 02h 22m 37s, ave eps reward [-6.27 -6.27 -6.27], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 6.86, mean reward -6.274656205288469, std reward 5.837643792554416, AG 0.0
2024-04-07 03:41:22,009 : Time 02h 23m 03s, ave eps reward [-7.52 -7.52 -7.52], ave eps length 10.0, reward step [-0.75 -0.75 -0.75], FPS 6.65, mean reward -7.521936254128586, std reward 7.085025542621655, AG 0.0
2024-04-07 03:41:48,781 : Time 02h 23m 30s, ave eps reward [-5.63 -5.63 -5.63], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.67, mean reward -5.627761549227765, std reward 5.362152147314212, AG 0.0
2024-04-07 03:42:15,591 : Time 02h 23m 57s, ave eps reward [-6.69 -6.69 -6.69], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 6.68, mean reward -6.692456633471278, std reward 5.701180435202364, AG 0.0
2024-04-07 03:42:42,226 : Time 02h 24m 24s, ave eps reward [-5.95 -5.95 -5.95], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 7.92, mean reward -5.950579713866519, std reward 5.583276155853035, AG 0.0
2024-04-07 03:43:08,825 : Time 02h 24m 50s, ave eps reward [-7.8 -7.8 -7.8], ave eps length 10.0, reward step [-0.78 -0.78 -0.78], FPS 8.88, mean reward -7.797853263044639, std reward 6.4154707846950165, AG 0.0
2024-04-07 03:43:36,340 : Time 02h 25m 18s, ave eps reward [-4.99 -4.99 -4.99], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 6.71, mean reward -4.985465770794185, std reward 4.8864140079809735, AG 0.0
2024-04-07 03:44:03,135 : Time 02h 25m 45s, ave eps reward [-7.81 -7.81 -7.81], ave eps length 10.0, reward step [-0.78 -0.78 -0.78], FPS 7.18, mean reward -7.807163294974812, std reward 6.251760472948702, AG 0.0
2024-04-07 03:44:29,654 : Time 02h 26m 11s, ave eps reward [-7.74 -7.74 -7.74], ave eps length 10.0, reward step [-0.77 -0.77 -0.77], FPS 10.42, mean reward -7.737794902390851, std reward 5.788386345014589, AG 0.0
2024-04-07 03:44:56,591 : Time 02h 26m 38s, ave eps reward [-8.34 -8.34 -8.34], ave eps length 10.0, reward step [-0.83 -0.83 -0.83], FPS 7.86, mean reward -8.336732171142984, std reward 6.228785028689856, AG 0.0
2024-04-07 03:45:23,496 : Time 02h 27m 05s, ave eps reward [-7.23 -7.23 -7.23], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 6.52, mean reward -7.226958162203845, std reward 7.415337881433403, AG 0.0
2024-04-07 03:45:50,410 : Time 02h 27m 32s, ave eps reward [-6.93 -6.93 -6.93], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 9.49, mean reward -6.934108716311554, std reward 5.597966435647137, AG 0.0
2024-04-07 03:46:17,162 : Time 02h 27m 59s, ave eps reward [-7.59 -7.59 -7.59], ave eps length 10.0, reward step [-0.76 -0.76 -0.76], FPS 8.26, mean reward -7.587804414226355, std reward 6.931239093907995, AG 0.0
2024-04-07 03:46:44,017 : Time 02h 28m 25s, ave eps reward [-5.57 -5.57 -5.57], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 7.95, mean reward -5.5679483796964755, std reward 4.927869633856586, AG 0.0
2024-04-07 03:47:10,938 : Time 02h 28m 52s, ave eps reward [-6.79 -6.79 -6.79], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 6.63, mean reward -6.786362473782143, std reward 5.410725654063705, AG 0.0
2024-04-07 03:47:37,923 : Time 02h 29m 19s, ave eps reward [-7.31 -7.31 -7.31], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 6.74, mean reward -7.308042624868987, std reward 5.976911429874871, AG 0.0
2024-04-07 03:48:04,949 : Time 02h 29m 46s, ave eps reward [-6.83 -6.83 -6.83], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 7.13, mean reward -6.831906364298158, std reward 5.514616878356493, AG 0.0
2024-04-07 03:48:31,825 : Time 02h 30m 13s, ave eps reward [-4.76 -4.76 -4.76], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.7, mean reward -4.75849266672789, std reward 5.113694408325624, AG 0.0
2024-04-07 03:48:58,569 : Time 02h 30m 40s, ave eps reward [-6.14 -6.14 -6.14], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 7.74, mean reward -6.139070656234799, std reward 5.732841419520067, AG 0.0
2024-04-07 03:49:25,007 : Time 02h 31m 06s, ave eps reward [-6.46 -6.46 -6.46], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 10.44, mean reward -6.46277662481514, std reward 6.675702263452434, AG 0.0
2024-04-07 03:49:52,520 : Time 02h 31m 34s, ave eps reward [-7.42 -7.42 -7.42], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 6.7, mean reward -7.417133100365033, std reward 7.495521680991729, AG 0.0
2024-04-07 03:50:19,355 : Time 02h 32m 01s, ave eps reward [-8. -8. -8.], ave eps length 10.0, reward step [-0.8 -0.8 -0.8], FPS 6.83, mean reward -7.996517558037887, std reward 6.149302182029665, AG 0.0
2024-04-07 03:50:46,059 : Time 02h 32m 27s, ave eps reward [-10.3 -10.3 -10.3], ave eps length 10.0, reward step [-1.03 -1.03 -1.03], FPS 8.61, mean reward -10.302013291232111, std reward 8.90616480206031, AG 0.0
2024-04-07 03:51:12,759 : Time 02h 32m 54s, ave eps reward [-5.56 -5.56 -5.56], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 8.67, mean reward -5.561576777877422, std reward 5.541292288496198, AG 0.0
2024-04-07 03:51:39,658 : Time 02h 33m 21s, ave eps reward [-5.79 -5.79 -5.79], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 6.52, mean reward -5.792029348756062, std reward 5.916941934898125, AG 0.0
2024-04-07 03:52:06,702 : Time 02h 33m 48s, ave eps reward [-6.37 -6.37 -6.37], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 9.05, mean reward -6.371985281294834, std reward 5.665579643317801, AG 0.0
2024-04-07 03:52:33,407 : Time 02h 34m 15s, ave eps reward [-6.39 -6.39 -6.39], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 8.65, mean reward -6.385311446724847, std reward 6.945286609055375, AG 0.0
2024-04-07 03:53:00,299 : Time 02h 34m 42s, ave eps reward [-6.12 -6.12 -6.12], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 6.42, mean reward -6.122228257461251, std reward 6.140673632075961, AG 0.0
2024-04-07 03:53:27,173 : Time 02h 35m 09s, ave eps reward [-6.43 -6.43 -6.43], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 6.86, mean reward -6.432729518705228, std reward 6.165080287912785, AG 0.0
2024-04-07 03:53:54,036 : Time 02h 35m 35s, ave eps reward [-8.55 -8.55 -8.55], ave eps length 10.0, reward step [-0.85 -0.85 -0.85], FPS 6.92, mean reward -8.547652791322323, std reward 6.357510546987994, AG 0.0
2024-04-07 03:54:20,978 : Time 02h 36m 02s, ave eps reward [-6.68 -6.68 -6.68], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 6.7, mean reward -6.680384071903314, std reward 5.131606141713988, AG 0.0
2024-04-07 03:54:47,858 : Time 02h 36m 29s, ave eps reward [-8.43 -8.43 -8.43], ave eps length 10.0, reward step [-0.84 -0.84 -0.84], FPS 7.1, mean reward -8.431459037353154, std reward 6.82159756569731, AG 0.0
2024-04-07 03:55:14,350 : Time 02h 36m 56s, ave eps reward [-6.89 -6.89 -6.89], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 9.29, mean reward -6.894470759339069, std reward 5.230791961092797, AG 0.0
2024-04-07 03:55:41,092 : Time 02h 37m 23s, ave eps reward [-6.12 -6.12 -6.12], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 8.59, mean reward -6.1193275913503005, std reward 5.280131918076533, AG 0.0
2024-04-07 03:56:08,628 : Time 02h 37m 50s, ave eps reward [-7.97 -7.97 -7.97], ave eps length 10.0, reward step [-0.8 -0.8 -0.8], FPS 6.77, mean reward -7.9703098883469705, std reward 7.660858205838317, AG 0.0
2024-04-07 03:56:35,362 : Time 02h 38m 17s, ave eps reward [-6.01 -6.01 -6.01], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 7.53, mean reward -6.012367867512089, std reward 5.497921211919455, AG 0.0
2024-04-07 03:57:01,928 : Time 02h 38m 43s, ave eps reward [-7.23 -7.23 -7.23], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 10.05, mean reward -7.229407225723165, std reward 6.138844245380189, AG 0.0
2024-04-07 03:57:28,884 : Time 02h 39m 10s, ave eps reward [-5.67 -5.67 -5.67], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 7.26, mean reward -5.665124252408163, std reward 6.709347733565673, AG 0.0
2024-04-07 03:57:55,743 : Time 02h 39m 37s, ave eps reward [-6.64 -6.64 -6.64], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 6.84, mean reward -6.640651495974687, std reward 4.285011754771001, AG 0.0
2024-04-07 03:58:22,690 : Time 02h 40m 04s, ave eps reward [-4.7 -4.7 -4.7], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 9.97, mean reward -4.700666537250755, std reward 4.892211077190758, AG 0.0
2024-04-07 03:58:49,581 : Time 02h 40m 31s, ave eps reward [-6.31 -6.31 -6.31], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 6.89, mean reward -6.312857302838099, std reward 6.045506305759833, AG 0.0
2024-04-07 03:59:16,425 : Time 02h 40m 58s, ave eps reward [-6.9 -6.9 -6.9], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 6.74, mean reward -6.900364443658809, std reward 5.28745486706557, AG 0.0
2024-04-07 03:59:43,015 : Time 02h 41m 24s, ave eps reward [-6.25 -6.25 -6.25], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 8.71, mean reward -6.252110097649253, std reward 5.635459488575445, AG 0.0
2024-04-07 04:00:10,295 : Time 02h 41m 52s, ave eps reward [-7.28 -7.28 -7.28], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 6.7, mean reward -7.282778603931358, std reward 6.702309173172494, AG 0.0
2024-04-07 04:00:37,203 : Time 02h 42m 19s, ave eps reward [-6.06 -6.06 -6.06], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 6.87, mean reward -6.0649101055250245, std reward 4.708755023303001, AG 0.0
2024-04-07 04:01:03,740 : Time 02h 42m 45s, ave eps reward [-5.2 -5.2 -5.2], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 9.54, mean reward -5.20245350583528, std reward 4.265753543810722, AG 0.0
2024-04-07 04:01:30,627 : Time 02h 43m 12s, ave eps reward [-4.28 -4.28 -4.28], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 7.94, mean reward -4.277109449765117, std reward 3.826440190073859, AG 0.0
training start after waiting for 1.1728074550628662 seconds
policy loss:4612.86083984375
value loss:65.77457427978516
entropies:197.74862670898438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2252099514007568 seconds
policy loss:1227.782958984375
value loss:102.22622680664062
entropies:197.74893188476562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201063632965088 seconds
policy loss:3009.983642578125
value loss:100.89456176757812
entropies:197.74935913085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1708810329437256 seconds
policy loss:-1406.297119140625
value loss:71.59493255615234
entropies:197.7491455078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2105376720428467 seconds
policy loss:-339.6561584472656
value loss:96.37620544433594
entropies:197.74900817871094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5017)
ToM Target loss= tensor(3701.2407)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.250218391418457 seconds
policy loss:-4866.859375
value loss:126.92108154296875
entropies:197.7494659423828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1992528438568115 seconds
policy loss:690.1175537109375
value loss:48.55030059814453
entropies:197.7494659423828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.234682321548462 seconds
policy loss:-6122.4033203125
value loss:134.1253662109375
entropies:197.74908447265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2273895740509033 seconds
policy loss:-1010.6181030273438
value loss:86.55024719238281
entropies:197.74949645996094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.22593355178833 seconds
policy loss:-4312.9365234375
value loss:124.53683471679688
entropies:197.74993896484375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4982)
ToM Target loss= tensor(3455.2832)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.231745958328247 seconds
policy loss:-3419.80615234375
value loss:90.94424438476562
entropies:197.74952697753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1684374809265137 seconds
policy loss:-413.73681640625
value loss:119.22612762451172
entropies:197.74984741210938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1888890266418457 seconds
policy loss:181.01473999023438
value loss:128.09857177734375
entropies:197.75
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2327818870544434 seconds
policy loss:-69.88629150390625
value loss:85.84820556640625
entropies:197.74981689453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2575502395629883 seconds
policy loss:-1790.0440673828125
value loss:80.92314910888672
entropies:197.74978637695312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5060)
ToM Target loss= tensor(3524.6758)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2058615684509277 seconds
policy loss:210.84109497070312
value loss:84.44721984863281
entropies:197.75
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2056643962860107 seconds
policy loss:-518.1715087890625
value loss:121.78569030761719
entropies:197.7498321533203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2202050685882568 seconds
policy loss:-59.19880294799805
value loss:101.92910766601562
entropies:197.75013732910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1749358177185059 seconds
policy loss:4828.0224609375
value loss:100.95364379882812
entropies:197.75015258789062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1747479438781738 seconds
policy loss:-1624.009765625
value loss:82.87879180908203
entropies:197.7500762939453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4917)
ToM Target loss= tensor(3464.5962)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2153027057647705 seconds
policy loss:7314.1298828125
value loss:82.82455444335938
entropies:197.7501678466797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1624679565429688 seconds
policy loss:5.675228118896484
value loss:115.93174743652344
entropies:197.7501678466797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2319064140319824 seconds
policy loss:1391.8292236328125
value loss:111.6690444946289
entropies:197.75013732910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2811696529388428 seconds
policy loss:-2121.2412109375
value loss:64.66790771484375
entropies:197.75001525878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2318012714385986 seconds
policy loss:-2426.3251953125
value loss:97.51637268066406
entropies:197.75009155273438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4941)
ToM Target loss= tensor(3446.2708)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2468435764312744 seconds
policy loss:2025.025146484375
value loss:84.01317596435547
entropies:197.75003051757812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2073352336883545 seconds
policy loss:911.5718383789062
value loss:91.74990844726562
entropies:197.7500762939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1609771251678467 seconds
policy loss:-1569.828857421875
value loss:87.41045379638672
entropies:197.7500762939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2304155826568604 seconds
policy loss:-4321.859375
value loss:164.8785400390625
entropies:197.75003051757812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2328376770019531 seconds
policy loss:-690.5054931640625
value loss:76.20794677734375
entropies:197.75013732910156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5024)
ToM Target loss= tensor(3463.7974)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.197885274887085 seconds
policy loss:-816.41162109375
value loss:117.95710754394531
entropies:197.7500457763672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177384376525879 seconds
policy loss:-5524.29443359375
value loss:148.52325439453125
entropies:197.7500762939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1737902164459229 seconds
policy loss:-1154.7247314453125
value loss:81.74557495117188
entropies:197.7500762939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1727898120880127 seconds
policy loss:2331.241943359375
value loss:119.12934112548828
entropies:197.75001525878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.284243106842041 seconds
policy loss:1932.1214599609375
value loss:120.23216247558594
entropies:197.75
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5010)
ToM Target loss= tensor(3452.9473)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1967766284942627 seconds
policy loss:4715.36669921875
value loss:119.99380493164062
entropies:197.7501220703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2260222434997559 seconds
policy loss:3753.975341796875
value loss:89.82616424560547
entropies:197.75006103515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2281510829925537 seconds
policy loss:3095.046142578125
value loss:71.22799682617188
entropies:197.74993896484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1676805019378662 seconds
policy loss:3620.75341796875
value loss:90.78953552246094
entropies:197.7500457763672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2069544792175293 seconds
policy loss:5221.0048828125
value loss:68.06440734863281
entropies:197.7501220703125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4995)
ToM Target loss= tensor(3441.5632)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1643397808074951 seconds
policy loss:3326.67578125
value loss:80.93274688720703
entropies:197.75006103515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2229070663452148 seconds
policy loss:3166.791748046875
value loss:80.47158813476562
entropies:197.74984741210938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1648869514465332 seconds
policy loss:4122.595703125
value loss:56.586273193359375
entropies:197.7493896484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2132656574249268 seconds
policy loss:-2587.85400390625
value loss:90.4215087890625
entropies:197.7498321533203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.216264247894287 seconds
policy loss:3662.73388671875
value loss:76.36808013916016
entropies:197.75003051757812
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4979)
ToM Target loss= tensor(3445.0864)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2107181549072266 seconds
policy loss:-2726.31884765625
value loss:176.8223114013672
entropies:197.74899291992188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.19840669631958 seconds
policy loss:2300.10888671875
value loss:98.41336059570312
entropies:197.74981689453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2808105945587158 seconds
policy loss:-2287.594970703125
value loss:119.89508056640625
entropies:197.7495880126953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1864230632781982 seconds
policy loss:-4924.56884765625
value loss:167.47474670410156
entropies:197.74966430664062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1661858558654785 seconds
policy loss:-5280.32421875
value loss:152.5604705810547
entropies:197.74978637695312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5171)
ToM Target loss= tensor(3448.7266)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2073218822479248 seconds
policy loss:-621.0319213867188
value loss:63.1807746887207
entropies:197.74942016601562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2108306884765625 seconds
policy loss:-1506.6099853515625
value loss:84.58087158203125
entropies:197.74960327148438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.223008155822754 seconds
policy loss:-4789.154296875
value loss:141.47494506835938
entropies:197.74803161621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1700446605682373 seconds
policy loss:-4199.392578125
value loss:103.9073715209961
entropies:197.74972534179688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2188658714294434 seconds
policy loss:-4130.69677734375
value loss:123.68421936035156
entropies:197.7481689453125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4913)
ToM Target loss= tensor(3444.2131)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2036819458007812 seconds
policy loss:-4561.97412109375
value loss:99.29741668701172
entropies:197.74876403808594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1842234134674072 seconds
policy loss:-242.82794189453125
value loss:99.12117767333984
entropies:197.74900817871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1703987121582031 seconds
policy loss:2790.36376953125
value loss:128.37240600585938
entropies:197.74842834472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2092735767364502 seconds
policy loss:7489.6650390625
value loss:121.68231201171875
entropies:197.74853515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1777448654174805 seconds
policy loss:-538.9647827148438
value loss:76.2827377319336
entropies:197.74810791015625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5035)
ToM Target loss= tensor(3440.1406)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.243924617767334 seconds
policy loss:-363.6315002441406
value loss:74.50288391113281
entropies:197.74867248535156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2118091583251953 seconds
policy loss:1239.610595703125
value loss:124.00877380371094
entropies:197.74783325195312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.211399793624878 seconds
policy loss:-1053.0103759765625
value loss:100.9330825805664
entropies:197.74635314941406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2040061950683594 seconds
policy loss:-3852.614990234375
value loss:113.18505096435547
entropies:197.7467498779297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.261303424835205 seconds
policy loss:1201.259765625
value loss:96.97010803222656
entropies:197.74696350097656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5068)
ToM Target loss= tensor(3441.4844)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1759543418884277 seconds
policy loss:2075.954833984375
value loss:66.20230865478516
entropies:197.74612426757812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2207622528076172 seconds
policy loss:1465.1597900390625
value loss:82.00456237792969
entropies:197.7467041015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2310502529144287 seconds
policy loss:1175.10595703125
value loss:105.08394622802734
entropies:197.7449493408203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2024807929992676 seconds
policy loss:-5183.9794921875
value loss:121.469482421875
entropies:197.74282836914062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1641430854797363 seconds
policy loss:1764.6505126953125
value loss:124.6168212890625
entropies:197.7438507080078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5160)
ToM Target loss= tensor(3443.3008)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2347321510314941 seconds
policy loss:-1541.7666015625
value loss:99.57379913330078
entropies:197.74081420898438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1764321327209473 seconds
policy loss:294.9472351074219
value loss:90.80598449707031
entropies:197.7451171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1583454608917236 seconds
policy loss:2324.81005859375
value loss:61.48236083984375
entropies:197.74171447753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1951303482055664 seconds
policy loss:3583.08349609375
value loss:71.5048599243164
entropies:197.74072265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2032594680786133 seconds
policy loss:-1050.515869140625
value loss:78.51231384277344
entropies:197.73992919921875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4911)
ToM Target loss= tensor(3441.4451)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2304508686065674 seconds
policy loss:-1603.713623046875
value loss:83.77100372314453
entropies:197.73951721191406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.168611764907837 seconds
policy loss:-3445.599853515625
value loss:97.6182632446289
entropies:197.7360076904297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2107429504394531 seconds
policy loss:-2537.3740234375
value loss:95.875732421875
entropies:197.73260498046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2195167541503906 seconds
policy loss:-293.2246398925781
value loss:111.71477508544922
entropies:197.73443603515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2262039184570312 seconds
policy loss:-5885.01513671875
value loss:207.33102416992188
entropies:197.73789978027344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5093)
ToM Target loss= tensor(3439.9153)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1777551174163818 seconds
policy loss:-258.912353515625
value loss:76.51882934570312
entropies:197.74154663085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.273710012435913 seconds
policy loss:2232.36572265625
value loss:74.53373718261719
entropies:197.72756958007812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2051465511322021 seconds
policy loss:-1094.12939453125
value loss:86.98863983154297
entropies:197.73074340820312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2084600925445557 seconds
policy loss:3005.160400390625
value loss:117.13525390625
entropies:197.7313232421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.261894941329956 seconds
policy loss:190.91409301757812
value loss:95.85916137695312
entropies:197.72291564941406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5168)
ToM Target loss= tensor(3441.0225)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2129161357879639 seconds
policy loss:3547.88134765625
value loss:103.07978057861328
entropies:197.73095703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.237459659576416 seconds
policy loss:-1856.586181640625
value loss:114.66817474365234
entropies:197.72897338867188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2134120464324951 seconds
policy loss:-1534.60888671875
value loss:132.52609252929688
entropies:197.7220458984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1716501712799072 seconds
policy loss:815.17041015625
value loss:75.62445831298828
entropies:197.69088745117188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2334413528442383 seconds
policy loss:714.3045043945312
value loss:73.78892517089844
entropies:197.7148895263672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5198)
ToM Target loss= tensor(3444.1360)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1955513954162598 seconds
policy loss:1631.6787109375
value loss:76.9944076538086
entropies:197.7264862060547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1571617126464844 seconds
policy loss:3162.55029296875
value loss:81.36161804199219
entropies:197.7332000732422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1750843524932861 seconds
policy loss:3416.147705078125
value loss:87.47235870361328
entropies:197.71234130859375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1700153350830078 seconds
policy loss:-585.3800659179688
value loss:99.067626953125
entropies:197.719970703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.20621919631958 seconds
policy loss:-485.37127685546875
value loss:81.53372955322266
entropies:197.71957397460938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5066)
ToM Target loss= tensor(3439.3843)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1685256958007812 seconds
policy loss:-1003.3781127929688
value loss:125.47306060791016
entropies:197.71299743652344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2065398693084717 seconds
policy loss:1564.256591796875
value loss:79.61083984375
entropies:197.7144317626953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2033390998840332 seconds
policy loss:4109.47802734375
value loss:78.78659057617188
entropies:197.70724487304688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1698336601257324 seconds
policy loss:-1054.1138916015625
value loss:134.1910400390625
entropies:197.70077514648438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2314181327819824 seconds
policy loss:564.6220703125
value loss:95.64837646484375
entropies:197.71429443359375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5120)
ToM Target loss= tensor(3439.9014)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1913049221038818 seconds
policy loss:1517.6070556640625
value loss:77.9316177368164
entropies:197.68832397460938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2257955074310303 seconds
policy loss:781.1373901367188
value loss:87.34265899658203
entropies:197.69082641601562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1952457427978516 seconds
policy loss:3280.69140625
value loss:101.10393524169922
entropies:197.70599365234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1682538986206055 seconds
policy loss:-4109.19921875
value loss:70.7632827758789
entropies:197.68765258789062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2336606979370117 seconds
policy loss:421.3438415527344
value loss:121.53777313232422
entropies:197.68405151367188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5068)
ToM Target loss= tensor(3442.2009)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2531986236572266 seconds
policy loss:-1106.1085205078125
value loss:87.53128051757812
entropies:197.7125701904297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2221853733062744 seconds
policy loss:3729.922119140625
value loss:105.2265396118164
entropies:197.67990112304688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2215018272399902 seconds
policy loss:-2455.981201171875
value loss:102.66004180908203
entropies:197.6360626220703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.170342206954956 seconds
policy loss:1032.1195068359375
value loss:62.60493469238281
entropies:197.68972778320312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2331855297088623 seconds
policy loss:-3766.699462890625
value loss:126.20651245117188
entropies:197.6748809814453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4915)
ToM Target loss= tensor(3441.8423)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.187868356704712 seconds
policy loss:-1524.055419921875
value loss:91.35882568359375
entropies:197.65769958496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2032675743103027 seconds
policy loss:-1509.0360107421875
value loss:198.56617736816406
entropies:197.70140075683594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.216334342956543 seconds
policy loss:2424.1630859375
value loss:62.50249099731445
entropies:197.67088317871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2278780937194824 seconds
policy loss:3375.593017578125
value loss:79.71421813964844
entropies:197.65464782714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2232770919799805 seconds
policy loss:603.4955444335938
value loss:100.4657974243164
entropies:197.66302490234375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4944)
ToM Target loss= tensor(3439.6008)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1248185634613037 seconds
policy loss:-3586.92138671875
value loss:125.21046447753906
entropies:197.65467834472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2049779891967773 seconds
policy loss:2683.051025390625
value loss:52.492454528808594
entropies:197.6516571044922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2259035110473633 seconds
policy loss:1313.6571044921875
value loss:61.790000915527344
entropies:197.65682983398438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2223095893859863 seconds
policy loss:-4.3963165283203125
value loss:68.55081176757812
entropies:197.67160034179688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2048556804656982 seconds
policy loss:1810.9447021484375
value loss:70.38624572753906
entropies:197.6543426513672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5054)
ToM Target loss= tensor(3439.2200)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1737074851989746 seconds
policy loss:302.8420104980469
value loss:39.86760330200195
entropies:197.67379760742188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.167992115020752 seconds
policy loss:-2801.540283203125
value loss:92.44017028808594
entropies:197.6435546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2144861221313477 seconds
policy loss:-1627.3389892578125
value loss:67.07392883300781
entropies:197.66461181640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2032477855682373 seconds
policy loss:-2102.418212890625
value loss:75.79608917236328
entropies:197.63804626464844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1761527061462402 seconds
policy loss:-6855.666015625
value loss:136.5805206298828
entropies:197.63699340820312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4897)
ToM Target loss= tensor(3440.1265)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2212903499603271 seconds
policy loss:-749.4003295898438
value loss:101.79026794433594
entropies:197.65966796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2191550731658936 seconds
policy loss:5521.64111328125
value loss:64.64457702636719
entropies:197.6610565185547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2242226600646973 seconds
policy loss:-386.3750915527344
value loss:94.4677734375
entropies:197.63140869140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2296977043151855 seconds
policy loss:-1124.3760986328125
value loss:108.08560180664062
entropies:197.6296844482422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.155407190322876 seconds
policy loss:1049.6754150390625
value loss:70.98487091064453
entropies:197.63783264160156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4714)
ToM Target loss= tensor(3440.1250)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2312140464782715 seconds
policy loss:1167.5216064453125
value loss:99.15518951416016
entropies:197.5992431640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2105236053466797 seconds
policy loss:954.6197509765625
value loss:85.35192108154297
entropies:197.62277221679688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2158317565917969 seconds
policy loss:8432.6669921875
value loss:101.111572265625
entropies:197.64939880371094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.214137315750122 seconds
policy loss:4906.7958984375
value loss:80.53861236572266
entropies:197.6680450439453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.3010199069976807 seconds
policy loss:-677.7555541992188
value loss:72.77127075195312
entropies:197.59774780273438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5024)
ToM Target loss= tensor(3438.2017)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1698188781738281 seconds
policy loss:-6179.005859375
value loss:159.6337890625
entropies:197.5405731201172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1713626384735107 seconds
policy loss:4183.24169921875
value loss:98.79051971435547
entropies:197.66123962402344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1869065761566162 seconds
policy loss:2154.216552734375
value loss:66.81991577148438
entropies:197.61654663085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2170822620391846 seconds
policy loss:3.11202335357666
value loss:83.20822143554688
entropies:197.66334533691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.220569133758545 seconds
policy loss:-433.337890625
value loss:105.29904174804688
entropies:197.6076202392578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5077)
ToM Target loss= tensor(3438.8770)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2024106979370117 seconds
policy loss:-980.2418823242188
value loss:58.30159378051758
entropies:197.63121032714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2402584552764893 seconds
policy loss:-1073.9581298828125
value loss:79.62395477294922
entropies:197.61790466308594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1645326614379883 seconds
policy loss:-1197.642333984375
value loss:99.61869812011719
entropies:197.65499877929688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2333366870880127 seconds
policy loss:-800.2046508789062
value loss:73.61326599121094
entropies:197.57672119140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1688001155853271 seconds
policy loss:2169.63037109375
value loss:93.24911499023438
entropies:197.66566467285156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5010)
ToM Target loss= tensor(3439.7417)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2247703075408936 seconds
policy loss:-420.466796875
value loss:96.95549774169922
entropies:197.59486389160156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1675739288330078 seconds
policy loss:-526.8587646484375
value loss:58.189300537109375
entropies:197.64376831054688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1919312477111816 seconds
policy loss:1438.863037109375
value loss:58.68748474121094
entropies:197.62608337402344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1678941249847412 seconds
policy loss:-2101.113525390625
value loss:62.26338195800781
entropies:197.6005859375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2272288799285889 seconds
policy loss:-570.1270141601562
value loss:41.92677307128906
entropies:197.6424560546875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4852)
ToM Target loss= tensor(3439.3728)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1923010349273682 seconds
policy loss:3599.67822265625
value loss:98.93220520019531
entropies:197.64280700683594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1758735179901123 seconds
policy loss:-1248.8116455078125
value loss:79.77002716064453
entropies:197.63418579101562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2205681800842285 seconds
policy loss:64.52029418945312
value loss:66.16730499267578
entropies:197.60243225097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1597011089324951 seconds
policy loss:4186.12255859375
value loss:66.00308227539062
entropies:197.65501403808594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1687724590301514 seconds
policy loss:4308.822265625
value loss:103.3026351928711
entropies:197.6456756591797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4957)
ToM Target loss= tensor(3438.2014)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.229384422302246 seconds
policy loss:596.5054931640625
value loss:120.81840515136719
entropies:197.52047729492188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2275922298431396 seconds
policy loss:-1401.889892578125
value loss:113.19501495361328
entropies:197.56350708007812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2156672477722168 seconds
policy loss:435.0771484375
value loss:29.364688873291016
entropies:197.60365295410156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.180018663406372 seconds
policy loss:2587.81005859375
value loss:67.75261688232422
entropies:197.5816650390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2229399681091309 seconds
policy loss:-2085.095458984375
value loss:93.51333618164062
entropies:197.57530212402344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4888)
ToM Target loss= tensor(3438.7329)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.224571704864502 seconds
policy loss:2546.2099609375
value loss:120.75027465820312
entropies:197.57501220703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.21049165725708 seconds
policy loss:-4051.388916015625
value loss:105.7507095336914
entropies:197.5703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2306528091430664 seconds
policy loss:-1007.9730224609375
value loss:111.3131103515625
entropies:197.52415466308594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2143597602844238 seconds
policy loss:-293.1678771972656
value loss:59.428199768066406
entropies:197.4931182861328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.17875075340271 seconds
policy loss:-2317.744140625
value loss:64.2000961303711
entropies:197.6111602783203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5144)
ToM Target loss= tensor(3439.2498)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.169424057006836 seconds
policy loss:611.3507080078125
value loss:89.14505004882812
entropies:197.54461669921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1674606800079346 seconds
policy loss:-939.574951171875
value loss:58.442317962646484
entropies:197.5806121826172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1637158393859863 seconds
policy loss:398.88494873046875
value loss:73.73487854003906
entropies:197.562744140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.221376657485962 seconds
policy loss:3403.2607421875
value loss:137.09478759765625
entropies:197.55690002441406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.202021598815918 seconds
policy loss:1265.04833984375
value loss:78.55931091308594
entropies:197.5311279296875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4451)
ToM Target loss= tensor(3438.6521)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2082006931304932 seconds
policy loss:-1797.406005859375
value loss:64.05643463134766
entropies:197.50765991210938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2297570705413818 seconds
policy loss:54.204036712646484
value loss:68.5406265258789
entropies:197.60092163085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2504510879516602 seconds
policy loss:2847.37744140625
value loss:46.216712951660156
entropies:197.59852600097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1732337474822998 seconds
policy loss:28.439353942871094
value loss:62.599891662597656
entropies:197.56033325195312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1562917232513428 seconds
policy loss:-165.51303100585938
value loss:50.198341369628906
entropies:197.5107879638672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5468)
ToM Target loss= tensor(3438.2935)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1747450828552246 seconds
policy loss:-1688.1953125
value loss:79.98078918457031
entropies:197.59095764160156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2204551696777344 seconds
policy loss:-287.22625732421875
value loss:57.40540313720703
entropies:197.4842529296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2299108505249023 seconds
policy loss:-286.6105041503906
value loss:72.69796752929688
entropies:197.50936889648438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2661378383636475 seconds
policy loss:2932.123291015625
value loss:94.340576171875
entropies:197.51710510253906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2356557846069336 seconds
policy loss:-1155.0888671875
value loss:41.98283386230469
entropies:197.50225830078125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4252)
ToM Target loss= tensor(3438.3345)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.218437910079956 seconds
policy loss:3554.530029296875
value loss:39.915626525878906
entropies:197.55540466308594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2152760028839111 seconds
policy loss:-1899.2572021484375
value loss:80.07498168945312
entropies:197.55029296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2200965881347656 seconds
policy loss:403.2212829589844
value loss:65.57341003417969
entropies:197.48728942871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2014942169189453 seconds
policy loss:1709.4837646484375
value loss:40.32823181152344
entropies:197.55984497070312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2008726596832275 seconds
policy loss:-1080.4140625
value loss:88.62664794921875
entropies:197.5592041015625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5034)
ToM Target loss= tensor(3437.9990)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1941909790039062 seconds
policy loss:-584.1747436523438
value loss:108.27163696289062
entropies:197.50616455078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2118656635284424 seconds
policy loss:-336.771240234375
value loss:86.48916625976562
entropies:197.4691925048828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1934394836425781 seconds
policy loss:-1557.3759765625
value loss:57.76899337768555
entropies:197.54783630371094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.175077199935913 seconds
policy loss:-1096.656494140625
value loss:64.24293518066406
entropies:197.50534057617188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2659006118774414 seconds
policy loss:-659.3102416992188
value loss:107.81394958496094
entropies:197.39268493652344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4724)
ToM Target loss= tensor(3438.3962)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1691555976867676 seconds
policy loss:-2130.885986328125
value loss:78.72239685058594
entropies:197.5079345703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1894137859344482 seconds
policy loss:5104.208984375
value loss:68.35150909423828
entropies:197.5101318359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2411603927612305 seconds
policy loss:3944.51416015625
value loss:69.47010803222656
entropies:197.552978515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169593334197998 seconds
policy loss:3538.70263671875
value loss:69.39347076416016
entropies:197.47154235839844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1841411590576172 seconds
policy loss:2027.7332763671875
value loss:82.91728973388672
entropies:197.53907775878906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.3899)
ToM Target loss= tensor(3437.9541)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1928305625915527 seconds
policy loss:2048.26904296875
value loss:43.66880798339844
entropies:197.56570434570312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.225714921951294 seconds
policy loss:542.2094116210938
value loss:97.76641845703125
entropies:197.52804565429688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2289702892303467 seconds
policy loss:-1297.7041015625
value loss:90.15309143066406
entropies:197.46275329589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1669280529022217 seconds
policy loss:-1844.1585693359375
value loss:68.66155242919922
entropies:197.50682067871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2168865203857422 seconds
policy loss:-909.4285278320312
value loss:57.6833381652832
entropies:197.52288818359375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4958)
ToM Target loss= tensor(3437.8677)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2343175411224365 seconds
policy loss:-2708.256591796875
value loss:73.53321075439453
entropies:197.4985809326172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2304918766021729 seconds
policy loss:-487.5111389160156
value loss:55.65738296508789
entropies:197.52090454101562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.218291997909546 seconds
policy loss:919.6881103515625
value loss:120.13654327392578
entropies:197.53021240234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2317166328430176 seconds
policy loss:86.51373291015625
value loss:96.23623657226562
entropies:197.4735107421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1693954467773438 seconds
policy loss:528.8372802734375
value loss:64.16854858398438
entropies:197.4241180419922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4032)
ToM Target loss= tensor(3438.1348)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2227542400360107 seconds
policy loss:2210.844482421875
value loss:59.27937316894531
entropies:197.4861602783203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2280771732330322 seconds
policy loss:150.05908203125
value loss:89.38557434082031
entropies:197.36082458496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1699836254119873 seconds
policy loss:462.4730529785156
value loss:88.87775421142578
entropies:197.48214721679688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.207068681716919 seconds
policy loss:2210.250732421875
value loss:53.290374755859375
entropies:197.51385498046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.237220287322998 seconds
policy loss:662.0612182617188
value loss:48.11189270019531
entropies:197.51101684570312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4014)
ToM Target loss= tensor(3438.3167)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2292873859405518 seconds
policy loss:-3040.727783203125
value loss:87.64978790283203
entropies:197.51339721679688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1674668788909912 seconds
policy loss:-4248.1015625
value loss:71.15280151367188
entropies:197.48728942871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.170114278793335 seconds
policy loss:-4425.56103515625
value loss:69.37794494628906
entropies:197.3577117919922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1625900268554688 seconds
policy loss:-1487.871826171875
value loss:58.115821838378906
entropies:197.43679809570312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1925277709960938 seconds
policy loss:-1336.08837890625
value loss:74.86160278320312
entropies:197.46527099609375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.3727)
ToM Target loss= tensor(3437.7986)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.23518705368042 seconds
policy loss:32.307926177978516
value loss:67.33944702148438
entropies:197.5121612548828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.199333667755127 seconds
policy loss:1137.4058837890625
value loss:88.80360412597656
entropies:197.43417358398438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1646997928619385 seconds
policy loss:3085.105712890625
value loss:54.7739372253418
entropies:197.44117736816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2226614952087402 seconds
policy loss:5260.92578125
value loss:131.28099060058594
entropies:197.46092224121094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2085232734680176 seconds
policy loss:1795.6363525390625
value loss:47.007476806640625
entropies:197.3922576904297
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4238)
ToM Target loss= tensor(3437.9768)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.239875078201294 seconds
policy loss:1587.3135986328125
value loss:80.14640808105469
entropies:197.4280242919922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.25787353515625 seconds
policy loss:-1905.5125732421875
value loss:60.59367370605469
entropies:197.4196014404297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.230109691619873 seconds
policy loss:-699.2132568359375
value loss:73.62079620361328
entropies:197.3707275390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2339410781860352 seconds
policy loss:2852.88330078125
value loss:75.20446014404297
entropies:197.39578247070312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2557902336120605 seconds
policy loss:-119.74295043945312
value loss:80.54716491699219
entropies:197.443603515625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4189)
ToM Target loss= tensor(3437.9534)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1706829071044922 seconds
policy loss:2481.912353515625
value loss:64.97918701171875
entropies:197.34896850585938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2250618934631348 seconds
policy loss:-6521.59912109375
value loss:126.8900146484375
entropies:197.38223266601562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1729254722595215 seconds
policy loss:2049.91455078125
value loss:62.51776123046875
entropies:197.29318237304688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.213857650756836 seconds
policy loss:-1944.2745361328125
value loss:76.4618148803711
entropies:197.2904052734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2328481674194336 seconds
policy loss:-986.5935668945312
value loss:81.07096099853516
entropies:197.39413452148438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.2684)
ToM Target loss= tensor(3438.0171)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1690194606781006 seconds
policy loss:2081.11181640625
value loss:57.18413162231445
entropies:197.3583221435547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1551234722137451 seconds
policy loss:-364.14239501953125
value loss:55.779788970947266
entropies:197.31063842773438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227001428604126 seconds
policy loss:-29.95665740966797
value loss:66.81511688232422
entropies:197.28366088867188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1675071716308594 seconds
policy loss:721.7622680664062
value loss:78.28645324707031
entropies:197.3397979736328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2026512622833252 seconds
policy loss:-431.7915344238281
value loss:74.68016815185547
entropies:197.3590850830078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.3171)
ToM Target loss= tensor(3437.8540)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2336273193359375 seconds
policy loss:-910.8302001953125
value loss:88.11821746826172
entropies:197.35861206054688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2336761951446533 seconds
policy loss:1054.3277587890625
value loss:50.125301361083984
entropies:197.23739624023438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.168513298034668 seconds
policy loss:295.02618408203125
value loss:57.030677795410156
entropies:197.37391662597656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2312147617340088 seconds
policy loss:1042.968994140625
value loss:64.49166107177734
entropies:197.3767547607422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2075262069702148 seconds
policy loss:296.8642883300781
value loss:71.18634796142578
entropies:197.34078979492188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.3835)
ToM Target loss= tensor(3437.7256)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2677850723266602 seconds
policy loss:2113.88134765625
value loss:104.92330169677734
entropies:197.32518005371094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2250292301177979 seconds
policy loss:2147.08251953125
value loss:96.58062744140625
entropies:197.2882080078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1708064079284668 seconds
policy loss:489.22528076171875
value loss:58.12158203125
entropies:197.34884643554688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.179344892501831 seconds
policy loss:-194.58511352539062
value loss:68.2732925415039
entropies:197.32705688476562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2115297317504883 seconds
policy loss:-6929.64892578125
value loss:144.58802795410156
entropies:197.32615661621094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5458)
ToM Target loss= tensor(3437.9688)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.195324420928955 seconds
policy loss:-6230.3251953125
value loss:132.14048767089844
entropies:197.32249450683594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1669397354125977 seconds
policy loss:-3158.23193359375
value loss:92.557373046875
entropies:197.40980529785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.241335153579712 seconds
policy loss:-3210.97314453125
value loss:92.8643798828125
entropies:197.29058837890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1678900718688965 seconds
policy loss:-135.12051391601562
value loss:63.92755889892578
entropies:197.28182983398438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.21993088722229 seconds
policy loss:2356.0419921875
value loss:45.11962890625
entropies:197.3754425048828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5435)
ToM Target loss= tensor(3437.5994)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2248814105987549 seconds
policy loss:2299.884521484375
value loss:63.6677131652832
entropies:197.24395751953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2289917469024658 seconds
policy loss:-588.5582885742188
value loss:104.98766326904297
entropies:197.2736358642578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2245471477508545 seconds
policy loss:4639.65673828125
value loss:68.43634796142578
entropies:197.1919403076172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2226243019104004 seconds
policy loss:663.7794799804688
value loss:60.25030517578125
entropies:197.38417053222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2358953952789307 seconds
policy loss:-785.5059204101562
value loss:70.01878356933594
entropies:197.26773071289062
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.2637)
ToM Target loss= tensor(3437.6279)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2268972396850586 seconds
policy loss:-3432.76708984375
value loss:64.17829895019531
entropies:197.2356414794922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2348411083221436 seconds
policy loss:736.2576293945312
value loss:66.74826049804688
entropies:197.2261199951172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1728262901306152 seconds
policy loss:2099.729248046875
value loss:59.03333282470703
entropies:197.2172393798828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.185694694519043 seconds
policy loss:2626.945068359375
value loss:66.7042465209961
entropies:197.28524780273438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1685633659362793 seconds
policy loss:-717.966064453125
value loss:69.48823547363281
entropies:197.31002807617188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.2849)
ToM Target loss= tensor(3437.6677)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2224454879760742 seconds
policy loss:-837.7570190429688
value loss:64.20406341552734
entropies:197.22756958007812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2268481254577637 seconds
policy loss:680.9700317382812
value loss:89.13884735107422
entropies:197.0501251220703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2184357643127441 seconds
policy loss:164.2332305908203
value loss:47.762996673583984
entropies:197.15757751464844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1979665756225586 seconds
policy loss:-1962.6307373046875
value loss:43.02796936035156
entropies:197.28118896484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1624655723571777 seconds
policy loss:-414.25738525390625
value loss:101.18266296386719
entropies:197.1854248046875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.1637)
ToM Target loss= tensor(3437.6326)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2019026279449463 seconds
policy loss:-156.66970825195312
value loss:58.42194366455078
entropies:197.23951721191406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1787033081054688 seconds
policy loss:-354.5931091308594
value loss:39.07642364501953
entropies:197.2529296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1916594505310059 seconds
policy loss:-772.5651245117188
value loss:46.16405487060547
entropies:197.17759704589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1877648830413818 seconds
policy loss:-1138.1500244140625
value loss:52.57984161376953
entropies:197.2037811279297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1700472831726074 seconds
policy loss:2594.008056640625
value loss:57.635169982910156
entropies:197.22178649902344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.3948)
ToM Target loss= tensor(3437.5735)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2286694049835205 seconds
policy loss:2283.291015625
value loss:54.08423614501953
entropies:196.94085693359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1708769798278809 seconds
policy loss:1908.729736328125
value loss:63.115325927734375
entropies:197.0770263671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2179100513458252 seconds
policy loss:2700.148681640625
value loss:51.937522888183594
entropies:197.0664520263672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2211968898773193 seconds
policy loss:157.10824584960938
value loss:45.61237335205078
entropies:197.11019897460938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1612963676452637 seconds
policy loss:-2004.348876953125
value loss:106.91403198242188
entropies:197.2020263671875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.1525)
ToM Target loss= tensor(3437.6104)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1969008445739746 seconds
policy loss:-2252.364013671875
value loss:116.52810668945312
entropies:197.0795440673828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1912829875946045 seconds
policy loss:1753.53564453125
value loss:48.5598030090332
entropies:197.2221221923828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2184374332427979 seconds
policy loss:-2324.874755859375
value loss:86.49982452392578
entropies:197.1195068359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2048566341400146 seconds
policy loss:-49.34248733520508
value loss:52.457061767578125
entropies:197.05630493164062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.160008430480957 seconds
policy loss:-3560.32861328125
value loss:77.8272476196289
entropies:196.8724822998047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.2406)
ToM Target loss= tensor(3437.7139)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2124965190887451 seconds
policy loss:-1340.2935791015625
value loss:73.0035400390625
entropies:197.0211181640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1715703010559082 seconds
policy loss:-4667.322265625
value loss:88.80633544921875
entropies:197.10165405273438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.3266839981079102 seconds
policy loss:5096.5166015625
value loss:62.683860778808594
entropies:197.11280822753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1683874130249023 seconds
policy loss:1310.56689453125
value loss:45.89704132080078
entropies:197.06312561035156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1817357540130615 seconds
policy loss:4802.23681640625
value loss:72.29305267333984
entropies:197.13951110839844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.8274)
ToM Target loss= tensor(3437.7754)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2084953784942627 seconds
policy loss:5423.3134765625
value loss:83.3283920288086
entropies:197.0536651611328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1683573722839355 seconds
policy loss:396.56768798828125
value loss:78.13494110107422
entropies:196.98220825195312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.188673734664917 seconds
policy loss:1305.9776611328125
value loss:39.90578842163086
entropies:197.23866271972656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1650993824005127 seconds
policy loss:-1243.3897705078125
value loss:56.75965118408203
entropies:197.01376342773438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2328853607177734 seconds
policy loss:1089.3311767578125
value loss:81.44245910644531
entropies:197.10211181640625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.0336)
ToM Target loss= tensor(3437.6519)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.170633316040039 seconds
policy loss:2021.239501953125
value loss:39.61454772949219
entropies:197.07481384277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.171189546585083 seconds
policy loss:-2276.228515625
value loss:50.00935363769531
entropies:197.06568908691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2196266651153564 seconds
policy loss:-3696.31884765625
value loss:67.72512817382812
entropies:197.03330993652344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1730551719665527 seconds
policy loss:-4138.5712890625
value loss:68.49665069580078
entropies:196.84344482421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2210328578948975 seconds
policy loss:-4230.23486328125
value loss:71.86173248291016
entropies:197.1318359375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4786)
ToM Target loss= tensor(3437.6021)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1901512145996094 seconds
policy loss:-2054.994140625
value loss:70.05374908447266
entropies:196.73692321777344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.167877197265625 seconds
policy loss:-766.506591796875
value loss:79.59989166259766
entropies:196.98326110839844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1222453117370605 seconds
policy loss:2545.760986328125
value loss:70.21528625488281
entropies:196.96397399902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.166774034500122 seconds
policy loss:-798.2139282226562
value loss:78.7876205444336
entropies:197.12510681152344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.209731101989746 seconds
policy loss:3978.017578125
value loss:57.651023864746094
entropies:197.00074768066406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.6937)
ToM Target loss= tensor(3437.7109)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1717591285705566 seconds
policy loss:3328.532470703125
value loss:57.82942199707031
entropies:196.9230194091797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2089836597442627 seconds
policy loss:405.9343566894531
value loss:51.86359405517578
entropies:197.05657958984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2128283977508545 seconds
policy loss:1264.763671875
value loss:69.13027954101562
entropies:196.8569793701172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1667983531951904 seconds
policy loss:6773.67724609375
value loss:96.37207794189453
entropies:196.9905548095703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1949355602264404 seconds
policy loss:2250.167236328125
value loss:67.5675048828125
entropies:197.0341033935547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.2473)
ToM Target loss= tensor(3437.5264)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2291243076324463 seconds
policy loss:477.0025329589844
value loss:62.69389724731445
entropies:197.00633239746094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2338123321533203 seconds
policy loss:-3630.0126953125
value loss:90.63703918457031
entropies:196.84596252441406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169116735458374 seconds
policy loss:-3386.649658203125
value loss:79.4142074584961
entropies:196.97540283203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1798124313354492 seconds
policy loss:-2504.12890625
value loss:49.81779479980469
entropies:196.92425537109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.194232702255249 seconds
policy loss:-4367.00341796875
value loss:79.34292602539062
entropies:196.93833923339844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.9355)
ToM Target loss= tensor(3437.5000)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2083852291107178 seconds
policy loss:-2332.826171875
value loss:66.75141906738281
entropies:197.0642852783203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1727616786956787 seconds
policy loss:-1311.403564453125
value loss:62.18598937988281
entropies:197.04037475585938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1908504962921143 seconds
policy loss:1597.8109130859375
value loss:80.9096908569336
entropies:197.01394653320312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2390711307525635 seconds
policy loss:-113.03511047363281
value loss:87.88188171386719
entropies:196.76791381835938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1828739643096924 seconds
policy loss:1968.2491455078125
value loss:112.51217651367188
entropies:196.8653564453125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.0150)
ToM Target loss= tensor(3437.5269)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2361063957214355 seconds
policy loss:2002.6011962890625
value loss:62.81235122680664
entropies:196.90560913085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2268216609954834 seconds
policy loss:3582.223876953125
value loss:63.69827651977539
entropies:197.0385284423828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.230966329574585 seconds
policy loss:1983.848388671875
value loss:64.2677230834961
entropies:196.752197265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.190363883972168 seconds
policy loss:3.7509512901306152
value loss:67.10911560058594
entropies:197.02334594726562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1673815250396729 seconds
policy loss:-503.4173889160156
value loss:59.0640869140625
entropies:196.9212646484375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4797)
ToM Target loss= tensor(3437.5957)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2039098739624023 seconds
policy loss:586.8280639648438
value loss:50.93136215209961
entropies:196.82681274414062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1964383125305176 seconds
policy loss:-3933.841064453125
value loss:57.467041015625
entropies:196.8721466064453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1656584739685059 seconds
policy loss:-772.8150634765625
value loss:53.53694152832031
entropies:196.85430908203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.22914719581604 seconds
policy loss:-3254.759033203125
value loss:89.7515640258789
entropies:196.97439575195312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1619303226470947 seconds
policy loss:-1021.18701171875
value loss:39.10106658935547
entropies:196.86048889160156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.8744)
ToM Target loss= tensor(3437.5962)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2409300804138184 seconds
policy loss:-528.2787475585938
value loss:67.95246124267578
entropies:196.69537353515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.231628656387329 seconds
policy loss:1923.9278564453125
value loss:68.87581634521484
entropies:196.85806274414062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2209300994873047 seconds
policy loss:-653.4987182617188
value loss:87.9576644897461
entropies:196.8224334716797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2309033870697021 seconds
policy loss:-1148.9453125
value loss:89.21257019042969
entropies:196.8011474609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2217803001403809 seconds
policy loss:1802.2872314453125
value loss:71.19656372070312
entropies:196.9245147705078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.3220)
ToM Target loss= tensor(3437.5549)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1682934761047363 seconds
policy loss:1232.0848388671875
value loss:53.82246398925781
entropies:196.85484313964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2271277904510498 seconds
policy loss:1553.5284423828125
value loss:77.98854064941406
entropies:196.77639770507812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.187436819076538 seconds
policy loss:4312.28125
value loss:62.63369369506836
entropies:196.82794189453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2073335647583008 seconds
policy loss:2417.5712890625
value loss:84.32513427734375
entropies:196.87547302246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.230375051498413 seconds
policy loss:-315.09735107421875
value loss:53.60340118408203
entropies:196.72271728515625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.7998)
ToM Target loss= tensor(3437.4729)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1484551429748535 seconds
policy loss:-1001.0472412109375
value loss:54.54081726074219
entropies:196.69635009765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.221637487411499 seconds
policy loss:-1178.238525390625
value loss:70.41803741455078
entropies:196.70115661621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1614100933074951 seconds
policy loss:-1501.37255859375
value loss:122.9323501586914
entropies:196.7238006591797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1909716129302979 seconds
policy loss:2879.3779296875
value loss:74.28099060058594
entropies:196.7576141357422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173668622970581 seconds
policy loss:649.2061157226562
value loss:65.64502716064453
entropies:196.80419921875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.8009)
ToM Target loss= tensor(3437.5491)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.167917013168335 seconds
policy loss:-2083.517333984375
value loss:54.72483825683594
entropies:196.75343322753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1980011463165283 seconds
policy loss:-890.374267578125
value loss:56.692832946777344
entropies:196.59092712402344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1845893859863281 seconds
policy loss:681.2611694335938
value loss:74.65214538574219
entropies:196.58685302734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1708390712738037 seconds
policy loss:-1288.123779296875
value loss:86.82991790771484
entropies:196.52279663085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.16422700881958 seconds
policy loss:-1451.8876953125
value loss:72.66661834716797
entropies:196.77566528320312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1978.1069)
ToM Target loss= tensor(3437.5989)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1692936420440674 seconds
policy loss:-1680.0833740234375
value loss:70.27017211914062
entropies:196.62489318847656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.220036268234253 seconds
policy loss:2483.42529296875
value loss:63.98154830932617
entropies:196.71644592285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2302777767181396 seconds
policy loss:2925.037109375
value loss:98.12027740478516
entropies:196.5333251953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2307040691375732 seconds
policy loss:4613.400390625
value loss:56.9205436706543
entropies:196.64910888671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2146284580230713 seconds
policy loss:2397.4921875
value loss:37.29551696777344
entropies:196.5079345703125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.0106)
ToM Target loss= tensor(3437.5288)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2354881763458252 seconds
policy loss:4954.86328125
value loss:62.86514663696289
entropies:196.4550323486328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201676845550537 seconds
policy loss:-664.6354370117188
value loss:59.12375259399414
entropies:196.51507568359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2092692852020264 seconds
policy loss:-3946.3173828125
value loss:61.20335388183594
entropies:196.32594299316406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1782832145690918 seconds
policy loss:1866.591552734375
value loss:44.742462158203125
entropies:196.46937561035156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2030408382415771 seconds
policy loss:-4035.389892578125
value loss:86.33688354492188
entropies:196.31907653808594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.6948)
ToM Target loss= tensor(3437.4055)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2048568725585938 seconds
policy loss:-1765.728271484375
value loss:44.39263153076172
entropies:196.5328369140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.198164463043213 seconds
policy loss:-2234.34033203125
value loss:64.0857162475586
entropies:196.44656372070312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2272353172302246 seconds
policy loss:-4971.70068359375
value loss:69.36780548095703
entropies:196.2093505859375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2420697212219238 seconds
policy loss:-1883.059814453125
value loss:63.8406982421875
entropies:196.37704467773438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2198901176452637 seconds
policy loss:4249.60595703125
value loss:66.19320678710938
entropies:196.4009246826172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.8800)
ToM Target loss= tensor(3437.4177)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1683943271636963 seconds
policy loss:3436.5830078125
value loss:80.43501281738281
entropies:196.23275756835938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1947710514068604 seconds
policy loss:5042.53955078125
value loss:92.84981536865234
entropies:196.34506225585938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1726806163787842 seconds
policy loss:2972.65673828125
value loss:75.33860778808594
entropies:196.3644561767578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2356188297271729 seconds
policy loss:2070.5419921875
value loss:59.073577880859375
entropies:196.4244384765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2121083736419678 seconds
policy loss:1584.867431640625
value loss:68.01630401611328
entropies:196.37059020996094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.3417)
ToM Target loss= tensor(3437.4890)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1690261363983154 seconds
policy loss:-3273.837646484375
value loss:79.2900619506836
entropies:196.4308319091797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.166649341583252 seconds
policy loss:-2734.47607421875
value loss:84.07769775390625
entropies:196.28187561035156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2061436176300049 seconds
policy loss:391.87847900390625
value loss:83.1508560180664
entropies:196.2958984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2215917110443115 seconds
policy loss:-533.2224731445312
value loss:68.73274993896484
entropies:196.1444854736328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1643986701965332 seconds
policy loss:-3162.2744140625
value loss:79.13790893554688
entropies:196.13893127441406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1975.4012)
ToM Target loss= tensor(3437.4163)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2111454010009766 seconds
policy loss:3469.67138671875
value loss:75.599365234375
entropies:196.4149627685547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228933572769165 seconds
policy loss:1064.435791015625
value loss:74.35433959960938
entropies:196.188232421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2290854454040527 seconds
policy loss:2099.756591796875
value loss:56.11222839355469
entropies:196.427001953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1657755374908447 seconds
policy loss:-3080.8642578125
value loss:56.65120315551758
entropies:196.35008239746094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173818826675415 seconds
policy loss:-3352.274169921875
value loss:70.3021011352539
entropies:196.32135009765625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.6912)
ToM Target loss= tensor(3437.5149)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1803438663482666 seconds
policy loss:-619.3887939453125
value loss:50.06344223022461
entropies:196.04725646972656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1668932437896729 seconds
policy loss:-2405.80810546875
value loss:47.22601318359375
entropies:196.2570037841797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169248104095459 seconds
policy loss:2061.510986328125
value loss:46.3129997253418
entropies:196.19854736328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1701414585113525 seconds
policy loss:2355.293701171875
value loss:82.20547485351562
entropies:195.99676513671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.193789005279541 seconds
policy loss:3043.6962890625
value loss:40.33958053588867
entropies:196.20144653320312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.6111)
ToM Target loss= tensor(3437.4436)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.243084192276001 seconds
policy loss:4832.86376953125
value loss:61.60670852661133
entropies:196.1768798828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2172951698303223 seconds
policy loss:321.4335632324219
value loss:59.885719299316406
entropies:196.07223510742188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1868412494659424 seconds
policy loss:-1296.421142578125
value loss:87.36166381835938
entropies:196.17343139648438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2292957305908203 seconds
policy loss:-1580.25634765625
value loss:67.34886169433594
entropies:195.92401123046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2158856391906738 seconds
policy loss:-2745.437255859375
value loss:79.97313690185547
entropies:196.15151977539062
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1978.4873)
ToM Target loss= tensor(3437.4331)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.176685094833374 seconds
policy loss:-2232.649169921875
value loss:45.866241455078125
entropies:196.0248565673828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2262990474700928 seconds
policy loss:-532.1016235351562
value loss:44.29236602783203
entropies:195.96084594726562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2146525382995605 seconds
policy loss:-2524.706787109375
value loss:79.86903381347656
entropies:196.11802673339844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.213331937789917 seconds
policy loss:-562.9510498046875
value loss:50.03426742553711
entropies:195.93093872070312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1991231441497803 seconds
policy loss:4796.89404296875
value loss:60.20613479614258
entropies:195.85313415527344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.9951)
ToM Target loss= tensor(3437.6047)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2351510524749756 seconds
policy loss:3501.8564453125
value loss:67.55789184570312
entropies:195.7544708251953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2175462245941162 seconds
policy loss:2939.482421875
value loss:75.66775512695312
entropies:195.91090393066406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2140014171600342 seconds
policy loss:3153.31982421875
value loss:71.92070007324219
entropies:195.9501190185547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2155396938323975 seconds
policy loss:-120.95034790039062
value loss:73.56739044189453
entropies:195.6114959716797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2008585929870605 seconds
policy loss:2001.2685546875
value loss:66.29701232910156
entropies:195.7030029296875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1979.0609)
ToM Target loss= tensor(3437.3845)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2184264659881592 seconds
policy loss:2858.470458984375
value loss:64.4605712890625
entropies:195.8536376953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1714959144592285 seconds
policy loss:-939.8331909179688
value loss:96.0136489868164
entropies:195.81309509277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2403533458709717 seconds
policy loss:-4060.056640625
value loss:110.54741668701172
entropies:195.73854064941406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2084417343139648 seconds
policy loss:662.6537475585938
value loss:40.70559310913086
entropies:195.7333526611328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1708333492279053 seconds
policy loss:-1509.0401611328125
value loss:77.24867248535156
entropies:195.90036010742188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1978.5713)
ToM Target loss= tensor(3437.7202)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1926488876342773 seconds
policy loss:-8.792530059814453
value loss:55.477752685546875
entropies:195.76715087890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201767921447754 seconds
policy loss:-572.0655517578125
value loss:93.89740753173828
entropies:195.70437622070312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2079668045043945 seconds
policy loss:-5106.13916015625
value loss:92.84486389160156
entropies:195.67315673828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1991829872131348 seconds
policy loss:-2810.535888671875
value loss:61.589988708496094
entropies:195.66592407226562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1610949039459229 seconds
policy loss:-2759.858154296875
value loss:91.20696258544922
entropies:195.41696166992188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1978.0110)
ToM Target loss= tensor(3437.7708)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.159149169921875 seconds
policy loss:1764.7601318359375
value loss:47.71532440185547
entropies:195.83932495117188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1743354797363281 seconds
policy loss:816.4259643554688
value loss:48.48673629760742
entropies:195.75421142578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2250313758850098 seconds
policy loss:2999.8330078125
value loss:57.32147979736328
entropies:195.52769470214844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1759631633758545 seconds
policy loss:1458.010009765625
value loss:45.5811882019043
entropies:195.7596893310547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2014572620391846 seconds
policy loss:-110.35306549072266
value loss:64.05937194824219
entropies:195.40521240234375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.7849)
ToM Target loss= tensor(3437.4253)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1601722240447998 seconds
policy loss:3079.590576171875
value loss:55.20693588256836
entropies:195.66766357421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2272605895996094 seconds
policy loss:-1418.3609619140625
value loss:69.76606750488281
entropies:195.2053680419922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2139410972595215 seconds
policy loss:-1839.513427734375
value loss:42.34725570678711
entropies:195.28895568847656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2104341983795166 seconds
policy loss:-1590.3115234375
value loss:50.22684097290039
entropies:195.57131958007812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1654994487762451 seconds
policy loss:725.064208984375
value loss:38.79509353637695
entropies:195.47251892089844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.2018)
ToM Target loss= tensor(3437.4536)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1927173137664795 seconds
policy loss:604.1680297851562
value loss:65.99916076660156
entropies:195.46475219726562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1744790077209473 seconds
policy loss:486.1538391113281
value loss:52.93667221069336
entropies:195.58119201660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2403273582458496 seconds
policy loss:-1243.3468017578125
value loss:54.67005157470703
entropies:195.3532257080078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2019240856170654 seconds
policy loss:-1969.5308837890625
value loss:49.21608352661133
entropies:195.27198791503906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2157511711120605 seconds
policy loss:1762.7410888671875
value loss:59.1949577331543
entropies:195.319580078125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4441)
ToM Target loss= tensor(3437.5518)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2162530422210693 seconds
policy loss:1384.740234375
value loss:74.09435272216797
entropies:195.49740600585938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1738181114196777 seconds
policy loss:-325.0725402832031
value loss:64.18215942382812
entropies:195.232666015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201902151107788 seconds
policy loss:730.8759765625
value loss:48.93526840209961
entropies:195.35150146484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1784839630126953 seconds
policy loss:3244.284423828125
value loss:56.75856018066406
entropies:195.49644470214844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.215700387954712 seconds
policy loss:-5278.7236328125
value loss:89.80589294433594
entropies:195.16354370117188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1978.3894)
ToM Target loss= tensor(3437.4702)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1894266605377197 seconds
policy loss:-1711.8612060546875
value loss:41.50214385986328
entropies:195.22923278808594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.235870122909546 seconds
policy loss:-3788.993896484375
value loss:78.25016784667969
entropies:195.062744140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.180628776550293 seconds
policy loss:1153.1182861328125
value loss:35.907691955566406
entropies:195.2914276123047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1853160858154297 seconds
policy loss:1450.9033203125
value loss:54.46062088012695
entropies:194.99664306640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2124319076538086 seconds
policy loss:-2473.96728515625
value loss:65.92153930664062
entropies:195.1865692138672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5541)
ToM Target loss= tensor(3437.3926)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1304552555084229 seconds
policy loss:-514.1461791992188
value loss:59.842472076416016
entropies:195.14320373535156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1654834747314453 seconds
policy loss:-1622.09228515625
value loss:71.59542083740234
entropies:195.14208984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.220463752746582 seconds
policy loss:1183.195556640625
value loss:59.30724334716797
entropies:194.9462890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228816270828247 seconds
policy loss:597.4475708007812
value loss:48.058998107910156
entropies:195.28631591796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1883306503295898 seconds
policy loss:-961.96826171875
value loss:47.023658752441406
entropies:195.03302001953125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.6821)
ToM Target loss= tensor(3437.3838)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2194464206695557 seconds
policy loss:2851.64453125
value loss:59.10330581665039
entropies:195.0521697998047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2243189811706543 seconds
policy loss:-1810.5240478515625
value loss:59.7040901184082
entropies:194.87664794921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2119128704071045 seconds
policy loss:-1318.49267578125
value loss:93.57606506347656
entropies:194.96734619140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1279935836791992 seconds
policy loss:2589.204345703125
value loss:66.09095764160156
entropies:194.94107055664062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2159819602966309 seconds
policy loss:3269.796630859375
value loss:78.98577117919922
entropies:194.774169921875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.6547)
ToM Target loss= tensor(3437.3650)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2240188121795654 seconds
policy loss:3672.4580078125
value loss:55.7839469909668
entropies:195.0110321044922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1623311042785645 seconds
policy loss:-224.84466552734375
value loss:61.0021858215332
entropies:194.53448486328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1625967025756836 seconds
policy loss:-2535.5888671875
value loss:43.22407531738281
entropies:194.80906677246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1649067401885986 seconds
policy loss:1352.0386962890625
value loss:57.9859504699707
entropies:194.7243194580078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2151765823364258 seconds
policy loss:-1518.3587646484375
value loss:80.92194366455078
entropies:194.6624298095703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.9031)
ToM Target loss= tensor(3437.3621)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1638450622558594 seconds
policy loss:-1434.3409423828125
value loss:84.13736724853516
entropies:194.5950927734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.165485143661499 seconds
policy loss:-1348.952880859375
value loss:43.7274055480957
entropies:194.83192443847656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2306134700775146 seconds
policy loss:-215.9014129638672
value loss:38.93391036987305
entropies:194.54571533203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1672711372375488 seconds
policy loss:-109.05213928222656
value loss:52.851383209228516
entropies:194.25784301757812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1975319385528564 seconds
policy loss:4010.295654296875
value loss:70.8710708618164
entropies:194.56170654296875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5798)
ToM Target loss= tensor(3437.3899)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.180633544921875 seconds
policy loss:2750.9658203125
value loss:48.978271484375
entropies:194.5615997314453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2298078536987305 seconds
policy loss:1955.6826171875
value loss:57.4696044921875
entropies:194.52403259277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.190039873123169 seconds
policy loss:146.25901794433594
value loss:71.65116882324219
entropies:194.55433654785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1669423580169678 seconds
policy loss:-4500.91796875
value loss:94.22698974609375
entropies:194.20578002929688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173945426940918 seconds
policy loss:-1344.7725830078125
value loss:62.285438537597656
entropies:193.9547119140625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1978.0697)
ToM Target loss= tensor(3437.3647)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.219451904296875 seconds
policy loss:-1474.8267822265625
value loss:68.28475189208984
entropies:194.20904541015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2957618236541748 seconds
policy loss:-1359.48291015625
value loss:68.95244598388672
entropies:194.1845245361328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2329494953155518 seconds
policy loss:2999.546875
value loss:45.89250183105469
entropies:194.46890258789062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201112985610962 seconds
policy loss:497.94219970703125
value loss:61.1158332824707
entropies:194.43133544921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2549421787261963 seconds
policy loss:500.2663269042969
value loss:37.70582580566406
entropies:194.50045776367188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1978.1497)
ToM Target loss= tensor(3437.3127)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1970171928405762 seconds
policy loss:560.810791015625
value loss:45.40800476074219
entropies:194.48214721679688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2309691905975342 seconds
policy loss:-711.015869140625
value loss:59.12852096557617
entropies:194.3425750732422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1945362091064453 seconds
policy loss:-354.72515869140625
value loss:59.43535232543945
entropies:194.06173706054688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1602494716644287 seconds
policy loss:495.538818359375
value loss:53.33890914916992
entropies:194.3887176513672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.165299892425537 seconds
policy loss:-211.5767364501953
value loss:42.60316467285156
entropies:194.29830932617188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.7867)
ToM Target loss= tensor(3437.3572)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2296760082244873 seconds
policy loss:-4937.58154296875
value loss:52.859073638916016
entropies:193.97329711914062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.232079029083252 seconds
policy loss:-4381.59765625
value loss:74.67023468017578
entropies:193.784423828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1606948375701904 seconds
policy loss:-825.4678955078125
value loss:35.80805969238281
entropies:194.0437469482422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1601142883300781 seconds
policy loss:-476.00238037109375
value loss:84.55071258544922
entropies:194.03485107421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2331337928771973 seconds
policy loss:260.55517578125
value loss:41.85870361328125
entropies:194.23341369628906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.7980)
ToM Target loss= tensor(3437.3618)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2282094955444336 seconds
policy loss:1667.233642578125
value loss:50.51905822753906
entropies:193.8027801513672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2223782539367676 seconds
policy loss:-1251.309326171875
value loss:53.32793045043945
entropies:193.88455200195312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2073454856872559 seconds
policy loss:3344.866943359375
value loss:81.50753784179688
entropies:193.96832275390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2147340774536133 seconds
policy loss:1909.96533203125
value loss:43.24991989135742
entropies:193.68748474121094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2269492149353027 seconds
policy loss:2190.87060546875
value loss:45.77165603637695
entropies:193.87017822265625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.7836)
ToM Target loss= tensor(3437.3499)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2209711074829102 seconds
policy loss:289.4354553222656
value loss:57.126853942871094
entropies:193.57150268554688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1623871326446533 seconds
policy loss:1373.147216796875
value loss:50.116966247558594
entropies:193.85633850097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2312750816345215 seconds
policy loss:53.35946273803711
value loss:56.38376235961914
entropies:193.49017333984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2334177494049072 seconds
policy loss:-5073.3740234375
value loss:80.99217987060547
entropies:193.5194091796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1711840629577637 seconds
policy loss:-6158.24609375
value loss:82.27061462402344
entropies:193.42344665527344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1978.6797)
ToM Target loss= tensor(3437.3105)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2313101291656494 seconds
policy loss:-6890.60791015625
value loss:106.57347106933594
entropies:193.51174926757812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2184598445892334 seconds
policy loss:-5876.1416015625
value loss:76.88008117675781
entropies:193.63345336914062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.164172649383545 seconds
policy loss:-4657.56884765625
value loss:88.09699249267578
entropies:193.4495849609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.242863655090332 seconds
policy loss:-2891.285400390625
value loss:58.857391357421875
entropies:193.3050079345703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2014799118041992 seconds
policy loss:673.609130859375
value loss:64.97954559326172
entropies:193.1912841796875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1978.0359)
ToM Target loss= tensor(3437.3623)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1708242893218994 seconds
policy loss:-4078.91845703125
value loss:70.71343231201172
entropies:193.3465118408203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2337453365325928 seconds
policy loss:1539.9794921875
value loss:35.95018005371094
entropies:193.2713623046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2322075366973877 seconds
policy loss:3884.010009765625
value loss:61.66762924194336
entropies:193.3095245361328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2299549579620361 seconds
policy loss:1025.717529296875
value loss:72.4908447265625
entropies:192.76544189453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2149624824523926 seconds
policy loss:2141.04931640625
value loss:37.250972747802734
entropies:193.1977996826172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.5205)
ToM Target loss= tensor(3437.4253)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2120118141174316 seconds
policy loss:2783.130615234375
value loss:97.48736572265625
entropies:192.99647521972656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2038733959197998 seconds
policy loss:2984.6533203125
value loss:83.50220489501953
entropies:193.080810546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.198472499847412 seconds
policy loss:-1097.9061279296875
value loss:55.330223083496094
entropies:193.02073669433594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1729931831359863 seconds
policy loss:-1160.9586181640625
value loss:55.25883865356445
entropies:192.8946533203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2329316139221191 seconds
policy loss:-2150.82421875
value loss:32.96097183227539
entropies:193.05352783203125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.7721)
ToM Target loss= tensor(3437.3276)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2289087772369385 seconds
policy loss:-3916.41748046875
value loss:89.34591674804688
entropies:193.07826232910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.205091953277588 seconds
policy loss:-1275.3046875
value loss:62.05558776855469
entropies:192.90235900878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2049846649169922 seconds
policy loss:1049.740478515625
value loss:40.65837097167969
entropies:193.081787109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1625802516937256 seconds
policy loss:1984.2364501953125
value loss:52.63081741333008
entropies:192.82647705078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1736879348754883 seconds
policy loss:2284.735595703125
value loss:73.55604553222656
entropies:192.58590698242188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.6688)
ToM Target loss= tensor(3437.3025)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.199502944946289 seconds
policy loss:958.2869262695312
value loss:66.08551788330078
entropies:192.63446044921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201493740081787 seconds
policy loss:-1071.3653564453125
value loss:75.31451416015625
entropies:192.58804321289062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1486811637878418 seconds
policy loss:-1279.18115234375
value loss:57.8033561706543
entropies:192.99549865722656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2206182479858398 seconds
policy loss:-1990.2620849609375
value loss:37.356170654296875
entropies:192.9272918701172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1636452674865723 seconds
policy loss:579.5865478515625
value loss:65.5893325805664
entropies:192.7963104248047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.7109)
ToM Target loss= tensor(3437.3445)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2346413135528564 seconds
policy loss:-433.7899169921875
value loss:51.68587112426758
entropies:192.34039306640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2021210193634033 seconds
policy loss:-3160.343017578125
value loss:107.76017761230469
entropies:192.07054138183594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1602377891540527 seconds
policy loss:272.9158020019531
value loss:75.23102569580078
entropies:192.50604248046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2285897731781006 seconds
policy loss:716.7214965820312
value loss:57.668800354003906
entropies:192.7652587890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1847639083862305 seconds
policy loss:689.7886352539062
value loss:65.12987518310547
entropies:192.16326904296875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.0011)
ToM Target loss= tensor(3437.2681)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2186448574066162 seconds
policy loss:1727.44287109375
value loss:69.46094512939453
entropies:192.6110076904297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2244813442230225 seconds
policy loss:-962.15966796875
value loss:40.36744689941406
entropies:192.34805297851562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1721904277801514 seconds
policy loss:825.95703125
value loss:50.59320068359375
entropies:192.62803649902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1708605289459229 seconds
policy loss:975.2540893554688
value loss:54.33707046508789
entropies:192.82061767578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1723802089691162 seconds
policy loss:-908.474853515625
value loss:43.972721099853516
entropies:191.8539276123047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.4153)
ToM Target loss= tensor(3437.2407)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2011287212371826 seconds
policy loss:-4691.43505859375
value loss:97.80795288085938
entropies:192.02490234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2212133407592773 seconds
policy loss:-2583.51318359375
value loss:63.426353454589844
entropies:192.09495544433594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2219927310943604 seconds
policy loss:-1749.0037841796875
value loss:57.30729675292969
entropies:192.31265258789062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.198946237564087 seconds
policy loss:-161.99484252929688
value loss:43.778846740722656
entropies:192.0746612548828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2200837135314941 seconds
policy loss:-977.2833862304688
value loss:44.790672302246094
entropies:191.98443603515625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.8369)
ToM Target loss= tensor(3437.1951)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.221944808959961 seconds
policy loss:2831.5263671875
value loss:76.44998931884766
entropies:191.75753784179688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2198362350463867 seconds
policy loss:-775.088134765625
value loss:29.44458770751953
entropies:191.60888671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.232313632965088 seconds
policy loss:3594.9619140625
value loss:86.13618469238281
entropies:191.57398986816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2229831218719482 seconds
policy loss:4032.241455078125
value loss:72.45687103271484
entropies:191.7947998046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1640994548797607 seconds
policy loss:1880.708740234375
value loss:41.1491584777832
entropies:191.6962127685547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.0399)
ToM Target loss= tensor(3437.2974)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1984691619873047 seconds
policy loss:2178.188720703125
value loss:99.99964904785156
entropies:191.4369354248047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2253711223602295 seconds
policy loss:1692.5263671875
value loss:64.54845428466797
entropies:191.46157836914062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2154741287231445 seconds
policy loss:2100.525634765625
value loss:76.15705108642578
entropies:191.8319549560547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2218499183654785 seconds
policy loss:-5426.63037109375
value loss:78.1617431640625
entropies:191.24786376953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2264351844787598 seconds
policy loss:-5132.8408203125
value loss:76.44750213623047
entropies:191.5205078125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.4038)
ToM Target loss= tensor(3437.2651)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1757011413574219 seconds
policy loss:-3593.482421875
value loss:74.21162414550781
entropies:191.63107299804688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.160191535949707 seconds
policy loss:-5727.9677734375
value loss:89.00174713134766
entropies:191.64495849609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2344558238983154 seconds
policy loss:-7313.9375
value loss:111.31047058105469
entropies:191.5558624267578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1712901592254639 seconds
policy loss:-4263.97216796875
value loss:56.16017150878906
entropies:191.76097106933594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1635727882385254 seconds
policy loss:2445.8662109375
value loss:51.083160400390625
entropies:191.89064025878906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1975.7676)
ToM Target loss= tensor(3437.2402)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1788723468780518 seconds
policy loss:-2528.58349609375
value loss:64.22233581542969
entropies:191.58782958984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1651709079742432 seconds
policy loss:2287.6689453125
value loss:73.53687286376953
entropies:191.4545440673828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1460046768188477 seconds
policy loss:919.2429809570312
value loss:56.462520599365234
entropies:191.2592010498047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2226042747497559 seconds
policy loss:2102.750244140625
value loss:70.56998443603516
entropies:191.01507568359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169970989227295 seconds
policy loss:3518.809326171875
value loss:80.2359619140625
entropies:191.36705017089844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1975.9071)
ToM Target loss= tensor(3437.3157)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1951327323913574 seconds
policy loss:2429.95849609375
value loss:66.8245849609375
entropies:191.42977905273438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1562466621398926 seconds
policy loss:1238.4229736328125
value loss:58.4550895690918
entropies:191.17489624023438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2235994338989258 seconds
policy loss:-2697.1416015625
value loss:69.98303985595703
entropies:191.0719757080078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1665277481079102 seconds
policy loss:-5815.71435546875
value loss:91.08067321777344
entropies:191.20591735839844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.172269344329834 seconds
policy loss:965.7427978515625
value loss:39.307823181152344
entropies:191.62757873535156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1973.3435)
ToM Target loss= tensor(3437.2153)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2138323783874512 seconds
policy loss:-2657.01171875
value loss:37.15768051147461
entropies:191.30125427246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1951956748962402 seconds
policy loss:-3948.94580078125
value loss:64.62381744384766
entropies:191.40208435058594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1627051830291748 seconds
policy loss:-2919.93115234375
value loss:65.30345916748047
entropies:191.10800170898438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1738576889038086 seconds
policy loss:-3619.90234375
value loss:68.75888061523438
entropies:190.77000427246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2241253852844238 seconds
policy loss:596.7261962890625
value loss:66.05731201171875
entropies:190.73875427246094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1973.1794)
ToM Target loss= tensor(3437.1248)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1691408157348633 seconds
policy loss:1388.80322265625
value loss:29.30972671508789
entropies:191.35728454589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1614887714385986 seconds
policy loss:453.58197021484375
value loss:62.32231521606445
entropies:191.29470825195312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2195518016815186 seconds
policy loss:3006.5458984375
value loss:37.38050842285156
entropies:191.42428588867188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2109589576721191 seconds
policy loss:2076.175537109375
value loss:53.36521530151367
entropies:191.314697265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2127506732940674 seconds
policy loss:-254.6006317138672
value loss:58.201534271240234
entropies:191.1006622314453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1975.5328)
ToM Target loss= tensor(3437.0283)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1853711605072021 seconds
policy loss:-3462.81982421875
value loss:61.457191467285156
entropies:190.72006225585938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2101829051971436 seconds
policy loss:1081.838623046875
value loss:72.57804870605469
entropies:190.88816833496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.164217472076416 seconds
policy loss:-4931.69189453125
value loss:72.54007720947266
entropies:191.13636779785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2030823230743408 seconds
policy loss:224.6265869140625
value loss:44.169090270996094
entropies:191.00051879882812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1637790203094482 seconds
policy loss:-2672.45068359375
value loss:49.85182571411133
entropies:191.04226684570312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1978.9282)
ToM Target loss= tensor(3437.1233)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1694483757019043 seconds
policy loss:-4199.38720703125
value loss:84.61798095703125
entropies:190.2991180419922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1889617443084717 seconds
policy loss:445.063720703125
value loss:61.769561767578125
entropies:191.21351623535156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1698946952819824 seconds
policy loss:-1340.2646484375
value loss:45.53094482421875
entropies:190.6872100830078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1710264682769775 seconds
policy loss:218.54014587402344
value loss:28.974679946899414
entropies:190.90225219726562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2157082557678223 seconds
policy loss:571.1022338867188
value loss:48.22551345825195
entropies:190.3505401611328
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1970.5258)
ToM Target loss= tensor(3436.9446)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1730966567993164 seconds
policy loss:-129.19110107421875
value loss:77.57120513916016
entropies:190.54730224609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1902267932891846 seconds
policy loss:587.7537841796875
value loss:39.77545166015625
entropies:190.31072998046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2208762168884277 seconds
policy loss:239.0377960205078
value loss:42.028717041015625
entropies:190.923583984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2330307960510254 seconds
policy loss:-1467.1767578125
value loss:39.861114501953125
entropies:190.33407592773438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1709771156311035 seconds
policy loss:223.205078125
value loss:63.39931106567383
entropies:190.7047576904297
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1970.3284)
ToM Target loss= tensor(3436.8452)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1914503574371338 seconds
policy loss:-813.1866455078125
value loss:55.02159118652344
entropies:190.37484741210938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1815853118896484 seconds
policy loss:-2628.34521484375
value loss:83.1689453125
entropies:190.91842651367188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2111179828643799 seconds
policy loss:-1052.7125244140625
value loss:47.80641555786133
entropies:190.4638214111328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1641888618469238 seconds
policy loss:555.18359375
value loss:37.91599655151367
entropies:190.6570281982422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.219618320465088 seconds
policy loss:-2450.614501953125
value loss:65.37555694580078
entropies:189.77752685546875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1971.0496)
ToM Target loss= tensor(3436.7795)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1977651119232178 seconds
policy loss:-1374.2996826171875
value loss:41.79209518432617
entropies:190.35305786132812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2164337635040283 seconds
policy loss:2709.696533203125
value loss:47.23503494262695
entropies:190.44638061523438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1798834800720215 seconds
policy loss:1150.4464111328125
value loss:32.75904846191406
entropies:189.86203002929688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1793336868286133 seconds
policy loss:-1362.9024658203125
value loss:101.38969421386719
entropies:190.11761474609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1823151111602783 seconds
policy loss:1806.083251953125
value loss:51.690433502197266
entropies:190.18345642089844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.9468)
ToM Target loss= tensor(3436.6257)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.137279987335205 seconds
policy loss:2028.5252685546875
value loss:68.1282730102539
entropies:189.29891967773438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1569998264312744 seconds
policy loss:-234.67027282714844
value loss:49.20200729370117
entropies:189.93374633789062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2475297451019287 seconds
policy loss:2101.456298828125
value loss:44.328739166259766
entropies:190.3173065185547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2422206401824951 seconds
policy loss:-906.6494140625
value loss:75.10135650634766
entropies:189.90167236328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1651747226715088 seconds
policy loss:-2644.421630859375
value loss:86.31319427490234
entropies:189.8843231201172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1970.9993)
ToM Target loss= tensor(3437.9231)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.252615213394165 seconds
policy loss:-824.1865844726562
value loss:31.824501037597656
entropies:190.2008514404297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2195079326629639 seconds
policy loss:-1057.3797607421875
value loss:27.033811569213867
entropies:189.9409942626953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1943199634552002 seconds
policy loss:-1623.9527587890625
value loss:56.85310745239258
entropies:189.74191284179688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2319416999816895 seconds
policy loss:707.064208984375
value loss:75.06110382080078
entropies:189.9415283203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1641924381256104 seconds
policy loss:934.6322631835938
value loss:57.77873229980469
entropies:189.20724487304688
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1966.3011)
ToM Target loss= tensor(3437.2153)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1703779697418213 seconds
policy loss:1845.4644775390625
value loss:91.32019805908203
entropies:189.34585571289062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2312798500061035 seconds
policy loss:3138.751708984375
value loss:43.824668884277344
entropies:189.81565856933594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2156264781951904 seconds
policy loss:2541.28564453125
value loss:74.45915985107422
entropies:189.41275024414062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2251262664794922 seconds
policy loss:-1075.03564453125
value loss:41.0284309387207
entropies:189.93359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228623867034912 seconds
policy loss:-1949.7174072265625
value loss:53.053836822509766
entropies:189.6977996826172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1974.0939)
ToM Target loss= tensor(3437.1084)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2318289279937744 seconds
policy loss:-1852.028076171875
value loss:44.836734771728516
entropies:190.1666717529297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2130916118621826 seconds
policy loss:-950.5743408203125
value loss:41.52880096435547
entropies:189.76504516601562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2225284576416016 seconds
policy loss:-962.5082397460938
value loss:55.247005462646484
entropies:189.78939819335938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1771283149719238 seconds
policy loss:2109.111083984375
value loss:66.79106140136719
entropies:189.8631134033203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2041916847229004 seconds
policy loss:1666.115234375
value loss:44.73228073120117
entropies:189.66712951660156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.7646)
ToM Target loss= tensor(3437.0432)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.169419765472412 seconds
policy loss:1480.137451171875
value loss:40.489105224609375
entropies:190.0272216796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.193352460861206 seconds
policy loss:1497.47265625
value loss:49.12126159667969
entropies:189.5686798095703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1748592853546143 seconds
policy loss:1001.7867431640625
value loss:47.66933822631836
entropies:189.4369659423828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.221311092376709 seconds
policy loss:-2821.15576171875
value loss:66.7719955444336
entropies:189.3473358154297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1956501007080078 seconds
policy loss:-3740.8818359375
value loss:45.32122039794922
entropies:189.59930419921875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1971.0762)
ToM Target loss= tensor(3436.0901)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2018938064575195 seconds
policy loss:-4976.220703125
value loss:56.14215850830078
entropies:189.52670288085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1706345081329346 seconds
policy loss:-4346.39208984375
value loss:63.3233528137207
entropies:188.7203826904297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1638128757476807 seconds
policy loss:-4092.3740234375
value loss:71.90962219238281
entropies:189.1869659423828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.126011610031128 seconds
policy loss:-734.88037109375
value loss:41.48710250854492
entropies:189.3370819091797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.152756929397583 seconds
policy loss:-45.55781555175781
value loss:34.22030258178711
entropies:189.08001708984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1970.0256)
ToM Target loss= tensor(3437.1714)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2119355201721191 seconds
policy loss:1080.5780029296875
value loss:56.30059814453125
entropies:188.8000946044922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1689271926879883 seconds
policy loss:-2470.92041015625
value loss:84.87958526611328
entropies:188.3300018310547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.269197940826416 seconds
policy loss:3590.15283203125
value loss:72.45181274414062
entropies:188.58465576171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2281806468963623 seconds
policy loss:4376.9814453125
value loss:93.9063720703125
entropies:189.10238647460938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.17033052444458 seconds
policy loss:3288.302734375
value loss:70.56352996826172
entropies:188.99049377441406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1970.3990)
ToM Target loss= tensor(3437.2222)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1858952045440674 seconds
policy loss:3959.76953125
value loss:55.21249008178711
entropies:188.2305908203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.161999225616455 seconds
policy loss:1885.8843994140625
value loss:60.109474182128906
entropies:188.83291625976562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2220449447631836 seconds
policy loss:-2105.3486328125
value loss:56.576419830322266
entropies:188.23245239257812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2267255783081055 seconds
policy loss:-1242.626953125
value loss:57.4627571105957
entropies:188.5913543701172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1980869770050049 seconds
policy loss:-2569.952392578125
value loss:47.58864212036133
entropies:189.0157470703125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1966.4426)
ToM Target loss= tensor(3437.0146)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.22999906539917 seconds
policy loss:-1619.8035888671875
value loss:45.249305725097656
entropies:189.65623474121094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1630017757415771 seconds
policy loss:-2137.25634765625
value loss:53.100738525390625
entropies:188.8697967529297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1614952087402344 seconds
policy loss:-1842.399658203125
value loss:57.87239074707031
entropies:188.1719207763672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1648750305175781 seconds
policy loss:-263.2816162109375
value loss:56.89547348022461
entropies:187.92965698242188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2064273357391357 seconds
policy loss:5040.615234375
value loss:42.140560150146484
entropies:189.2684783935547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1974.9697)
ToM Target loss= tensor(3437.0481)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.224858283996582 seconds
policy loss:4109.83740234375
value loss:94.87810516357422
entropies:188.2749481201172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2330803871154785 seconds
policy loss:1227.7264404296875
value loss:58.836822509765625
entropies:188.1845703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2272238731384277 seconds
policy loss:4903.25244140625
value loss:52.46355438232422
entropies:188.3498077392578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.225203514099121 seconds
policy loss:99.13336181640625
value loss:32.52336502075195
entropies:188.2074432373047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.214545726776123 seconds
policy loss:3723.4912109375
value loss:44.3538703918457
entropies:188.72918701171875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.7490)
ToM Target loss= tensor(3437.4463)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1859595775604248 seconds
policy loss:1676.5948486328125
value loss:67.00152587890625
entropies:188.28341674804688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1611838340759277 seconds
policy loss:-3063.44287109375
value loss:60.29658126831055
entropies:187.75106811523438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1983051300048828 seconds
policy loss:-1340.2744140625
value loss:45.37591552734375
entropies:187.94476318359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1661152839660645 seconds
policy loss:-655.5428466796875
value loss:38.910953521728516
entropies:188.14564514160156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1572411060333252 seconds
policy loss:-1568.0118408203125
value loss:49.022560119628906
entropies:188.3560028076172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1971.0472)
ToM Target loss= tensor(3436.1333)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1679491996765137 seconds
policy loss:-1619.2296142578125
value loss:53.220497131347656
entropies:187.91934204101562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.222426414489746 seconds
policy loss:1543.39013671875
value loss:92.7446060180664
entropies:188.16757202148438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2028648853302002 seconds
policy loss:1589.011962890625
value loss:72.5850601196289
entropies:188.40264892578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1931500434875488 seconds
policy loss:3045.58447265625
value loss:54.138031005859375
entropies:188.86276245117188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1659660339355469 seconds
policy loss:814.5347290039062
value loss:42.569557189941406
entropies:188.3428497314453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1971.7201)
ToM Target loss= tensor(3436.0500)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1954290866851807 seconds
policy loss:-2953.80419921875
value loss:75.77798461914062
entropies:187.81500244140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2231817245483398 seconds
policy loss:-762.1028442382812
value loss:39.9373779296875
entropies:188.7931365966797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2295527458190918 seconds
policy loss:-165.47396850585938
value loss:51.48941421508789
entropies:188.0945587158203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2222373485565186 seconds
policy loss:-594.3485107421875
value loss:77.61090087890625
entropies:187.53671264648438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2407047748565674 seconds
policy loss:-1990.086669921875
value loss:42.75417709350586
entropies:188.24749755859375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1971.8105)
ToM Target loss= tensor(3435.5188)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2329552173614502 seconds
policy loss:-4351.2470703125
value loss:85.58219146728516
entropies:187.39576721191406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2241334915161133 seconds
policy loss:-2210.20068359375
value loss:53.686336517333984
entropies:187.6007080078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2042183876037598 seconds
policy loss:-1910.35205078125
value loss:64.5088119506836
entropies:186.8811492919922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2302427291870117 seconds
policy loss:62.01716995239258
value loss:46.64159393310547
entropies:187.19049072265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.172088623046875 seconds
policy loss:2083.74462890625
value loss:59.28943634033203
entropies:188.18141174316406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1967.6575)
ToM Target loss= tensor(3436.5891)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2214717864990234 seconds
policy loss:1542.1158447265625
value loss:84.91315460205078
entropies:186.40447998046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2341818809509277 seconds
policy loss:2745.081298828125
value loss:66.78570556640625
entropies:186.05149841308594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1667871475219727 seconds
policy loss:2299.193359375
value loss:68.37202453613281
entropies:187.84031677246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2098393440246582 seconds
policy loss:3430.6591796875
value loss:62.61539077758789
entropies:186.9226531982422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1592276096343994 seconds
policy loss:-1586.9378662109375
value loss:49.13819885253906
entropies:186.8409881591797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1971.0621)
ToM Target loss= tensor(3435.7983)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2132785320281982 seconds
policy loss:-1761.2335205078125
value loss:47.491573333740234
entropies:186.900146484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1687414646148682 seconds
policy loss:-1294.663330078125
value loss:52.43523406982422
entropies:187.3998260498047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1731486320495605 seconds
policy loss:-281.9997863769531
value loss:39.15171813964844
entropies:186.9507598876953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177422046661377 seconds
policy loss:-4017.8359375
value loss:108.02200317382812
entropies:185.49131774902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2169692516326904 seconds
policy loss:1484.491455078125
value loss:53.495460510253906
entropies:187.2065887451172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1965.1338)
ToM Target loss= tensor(3436.5872)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2271966934204102 seconds
policy loss:-543.0165405273438
value loss:66.16207885742188
entropies:186.21945190429688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2363343238830566 seconds
policy loss:787.388916015625
value loss:49.98119354248047
entropies:186.4314422607422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1677742004394531 seconds
policy loss:-3356.6826171875
value loss:70.0735092163086
entropies:187.6484832763672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.124540090560913 seconds
policy loss:268.2135925292969
value loss:53.76987075805664
entropies:186.43783569335938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1665728092193604 seconds
policy loss:-989.7627563476562
value loss:60.96847915649414
entropies:187.03807067871094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1975.0641)
ToM Target loss= tensor(3437.4001)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1243853569030762 seconds
policy loss:-2071.5546875
value loss:45.90021514892578
entropies:187.15011596679688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1682970523834229 seconds
policy loss:113.78113555908203
value loss:73.1781005859375
entropies:185.7135772705078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2502374649047852 seconds
policy loss:4199.3212890625
value loss:53.77049255371094
entropies:187.05572509765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1739442348480225 seconds
policy loss:4029.745849609375
value loss:64.37479400634766
entropies:187.5167694091797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169752597808838 seconds
policy loss:2767.5625
value loss:60.780784606933594
entropies:184.00657653808594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1974.2970)
ToM Target loss= tensor(3435.6042)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1647977828979492 seconds
policy loss:-1331.623779296875
value loss:45.98967742919922
entropies:187.15975952148438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.218628168106079 seconds
policy loss:978.6873168945312
value loss:42.877296447753906
entropies:187.51463317871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2336418628692627 seconds
policy loss:-1325.9351806640625
value loss:59.97955322265625
entropies:186.553466796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1662304401397705 seconds
policy loss:-2949.466796875
value loss:63.10029983520508
entropies:186.7413787841797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2136523723602295 seconds
policy loss:-1312.2357177734375
value loss:49.22867202758789
entropies:185.83689880371094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1967.0693)
ToM Target loss= tensor(3436.9717)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1986277103424072 seconds
policy loss:-367.8503112792969
value loss:75.81568908691406
entropies:185.7266387939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1529028415679932 seconds
policy loss:993.5052490234375
value loss:48.78261184692383
entropies:185.86810302734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1942050457000732 seconds
policy loss:-1594.3909912109375
value loss:66.93193054199219
entropies:186.07684326171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2104570865631104 seconds
policy loss:407.056640625
value loss:69.92182159423828
entropies:185.44631958007812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2083935737609863 seconds
policy loss:-1097.0091552734375
value loss:59.32012176513672
entropies:186.61790466308594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1970.1766)
ToM Target loss= tensor(3436.4285)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.231321096420288 seconds
policy loss:114.65367126464844
value loss:36.68394088745117
entropies:186.00115966796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.200599193572998 seconds
policy loss:1217.8826904296875
value loss:58.78548812866211
entropies:186.04666137695312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1630911827087402 seconds
policy loss:21.983863830566406
value loss:56.17225646972656
entropies:183.3534698486328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2121448516845703 seconds
policy loss:-134.25408935546875
value loss:63.95790481567383
entropies:185.0880889892578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2305827140808105 seconds
policy loss:733.6796875
value loss:68.29279327392578
entropies:184.53109741210938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1969.5020)
ToM Target loss= tensor(3436.2400)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2001287937164307 seconds
policy loss:-1785.5418701171875
value loss:71.05094909667969
entropies:184.9459686279297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2331864833831787 seconds
policy loss:-915.00341796875
value loss:44.76089859008789
entropies:183.3309326171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2366034984588623 seconds
policy loss:-3429.197998046875
value loss:62.11389923095703
entropies:186.31272888183594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1980013847351074 seconds
policy loss:-2209.72119140625
value loss:44.70579147338867
entropies:185.692138671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2232155799865723 seconds
policy loss:-101.30425262451172
value loss:54.01762390136719
entropies:185.9713897705078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1966.5339)
ToM Target loss= tensor(3435.6177)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2347509860992432 seconds
policy loss:3421.296142578125
value loss:99.7125244140625
entropies:182.8839874267578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2263965606689453 seconds
policy loss:1000.2393188476562
value loss:57.2761116027832
entropies:184.58651733398438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2246253490447998 seconds
policy loss:-887.4041748046875
value loss:42.0958137512207
entropies:186.7843475341797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.217346429824829 seconds
policy loss:1394.813232421875
value loss:68.19680786132812
entropies:183.4427032470703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1624045372009277 seconds
policy loss:-56.96153259277344
value loss:67.8186264038086
entropies:182.03614807128906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1964.9667)
ToM Target loss= tensor(3433.3381)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2009530067443848 seconds
policy loss:2621.48046875
value loss:47.17476272583008
entropies:185.43540954589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2325339317321777 seconds
policy loss:-2988.52001953125
value loss:62.02067565917969
entropies:182.36036682128906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1692447662353516 seconds
policy loss:-1128.82666015625
value loss:54.74778747558594
entropies:183.0177459716797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1768863201141357 seconds
policy loss:-319.3517150878906
value loss:43.58821487426758
entropies:184.63192749023438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2033512592315674 seconds
policy loss:-5974.23828125
value loss:98.27841186523438
entropies:181.2458038330078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.6340)
ToM Target loss= tensor(3432.4395)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2292101383209229 seconds
policy loss:-2044.2767333984375
value loss:62.29677963256836
entropies:183.0665283203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2062814235687256 seconds
policy loss:2076.34228515625
value loss:40.43817138671875
entropies:185.32489013671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2210659980773926 seconds
policy loss:2953.081787109375
value loss:53.6036491394043
entropies:185.81143188476562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2353034019470215 seconds
policy loss:257.6494140625
value loss:58.0933837890625
entropies:184.12570190429688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1668150424957275 seconds
policy loss:-676.427734375
value loss:64.87999725341797
entropies:179.16574096679688
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1970.5978)
ToM Target loss= tensor(3431.6174)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2358975410461426 seconds
policy loss:-950.1349487304688
value loss:53.43316650390625
entropies:185.84945678710938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2185330390930176 seconds
policy loss:2104.753173828125
value loss:95.43888854980469
entropies:180.9263153076172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1688213348388672 seconds
policy loss:-659.3225708007812
value loss:53.27926254272461
entropies:182.8128204345703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1635093688964844 seconds
policy loss:-2327.703369140625
value loss:52.204166412353516
entropies:184.00613403320312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1684963703155518 seconds
policy loss:-1908.7879638671875
value loss:68.79271697998047
entropies:180.8995361328125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1970.7301)
ToM Target loss= tensor(3430.6750)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.161613941192627 seconds
policy loss:-946.866455078125
value loss:40.90926742553711
entropies:182.7371063232422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1766245365142822 seconds
policy loss:848.6118774414062
value loss:73.70992279052734
entropies:178.99496459960938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.160146713256836 seconds
policy loss:2778.766845703125
value loss:81.46847534179688
entropies:178.24578857421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2122552394866943 seconds
policy loss:2569.776611328125
value loss:51.099212646484375
entropies:179.62356567382812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.229891061782837 seconds
policy loss:1980.357177734375
value loss:56.39813232421875
entropies:184.99740600585938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1979.0331)
ToM Target loss= tensor(3430.7539)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.164778232574463 seconds
policy loss:949.4972534179688
value loss:49.879066467285156
entropies:184.0675048828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2102982997894287 seconds
policy loss:-3917.6064453125
value loss:52.456626892089844
entropies:178.93878173828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1989209651947021 seconds
policy loss:-1139.7833251953125
value loss:67.11837005615234
entropies:175.34413146972656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2227706909179688 seconds
policy loss:-131.82774353027344
value loss:75.2088394165039
entropies:181.93382263183594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2028326988220215 seconds
policy loss:1306.38330078125
value loss:87.23775482177734
entropies:175.32122802734375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1984.3765)
ToM Target loss= tensor(3429.0103)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2302701473236084 seconds
policy loss:-6357.7451171875
value loss:108.10059356689453
entropies:174.7198486328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1686272621154785 seconds
policy loss:-6750.41796875
value loss:146.81541442871094
entropies:173.64666748046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2285196781158447 seconds
policy loss:-3208.9736328125
value loss:67.16848754882812
entropies:183.2747802734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.211233139038086 seconds
policy loss:-1895.693359375
value loss:49.03471755981445
entropies:177.1511993408203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.223456859588623 seconds
policy loss:1613.25830078125
value loss:64.61467742919922
entropies:178.0867919921875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1973.9814)
ToM Target loss= tensor(3428.4766)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.172353744506836 seconds
policy loss:-415.9423522949219
value loss:49.24895477294922
entropies:176.36148071289062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1786956787109375 seconds
policy loss:357.464111328125
value loss:63.95851516723633
entropies:172.88047790527344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1715946197509766 seconds
policy loss:1320.412841796875
value loss:56.036563873291016
entropies:182.13803100585938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1978273391723633 seconds
policy loss:706.9425659179688
value loss:90.7728271484375
entropies:172.71237182617188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2210426330566406 seconds
policy loss:-3812.44091796875
value loss:93.57649993896484
entropies:179.06436157226562
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1971.9731)
ToM Target loss= tensor(3429.2368)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.163330316543579 seconds
policy loss:-1994.2230224609375
value loss:110.22216796875
entropies:173.40830993652344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.164863109588623 seconds
policy loss:-1209.7025146484375
value loss:65.31737518310547
entropies:164.65682983398438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2165086269378662 seconds
policy loss:-3306.4697265625
value loss:54.57143020629883
entropies:176.0809783935547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1647915840148926 seconds
policy loss:-1307.6697998046875
value loss:49.19145584106445
entropies:177.88198852539062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2471325397491455 seconds
policy loss:2271.970703125
value loss:57.13238525390625
entropies:177.93106079101562
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1976.1250)
ToM Target loss= tensor(3424.0405)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.22749924659729 seconds
policy loss:777.931884765625
value loss:50.216041564941406
entropies:169.37286376953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2265510559082031 seconds
policy loss:1692.6788330078125
value loss:48.9546012878418
entropies:173.33465576171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1724421977996826 seconds
policy loss:-1025.82373046875
value loss:82.52304077148438
entropies:174.2151641845703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1628367900848389 seconds
policy loss:148.37416076660156
value loss:65.88053131103516
entropies:177.89370727539062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2392661571502686 seconds
policy loss:457.3223571777344
value loss:64.77667236328125
entropies:174.92408752441406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1978.3326)
ToM Target loss= tensor(3425.4805)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2757501602172852 seconds
policy loss:1164.358154296875
value loss:85.63525390625
entropies:166.74952697753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1961865425109863 seconds
policy loss:-2221.74951171875
value loss:52.80807876586914
entropies:181.74429321289062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.207634449005127 seconds
policy loss:-2920.52392578125
value loss:78.47030639648438
entropies:175.4499053955078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1552150249481201 seconds
policy loss:-3035.33984375
value loss:56.031166076660156
entropies:169.24855041503906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2234106063842773 seconds
policy loss:-3633.5283203125
value loss:59.06861877441406
entropies:174.13394165039062
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1969.3436)
ToM Target loss= tensor(3417.2100)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.158937931060791 seconds
policy loss:-2199.8037109375
value loss:53.40250778198242
entropies:171.8981475830078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1605489253997803 seconds
policy loss:-148.01084899902344
value loss:66.71762084960938
entropies:161.60797119140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.221628189086914 seconds
policy loss:-1397.843994140625
value loss:91.19857788085938
entropies:168.20657348632812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2164599895477295 seconds
policy loss:1719.911865234375
value loss:67.57965087890625
entropies:164.8640899658203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1630189418792725 seconds
policy loss:2834.450927734375
value loss:64.9303207397461
entropies:166.26705932617188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1959.9711)
ToM Target loss= tensor(3411.3579)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2073523998260498 seconds
policy loss:212.5406951904297
value loss:59.5904541015625
entropies:172.47894287109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1653106212615967 seconds
policy loss:661.135009765625
value loss:49.039031982421875
entropies:169.9658203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2098047733306885 seconds
policy loss:-1841.8887939453125
value loss:63.84580612182617
entropies:163.47119140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1935439109802246 seconds
policy loss:-884.5338745117188
value loss:47.566017150878906
entropies:165.02674865722656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2194452285766602 seconds
policy loss:-5146.03076171875
value loss:122.51144409179688
entropies:159.9369354248047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1949.7902)
ToM Target loss= tensor(3411.4663)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2296383380889893 seconds
policy loss:-3342.8515625
value loss:184.4498291015625
entropies:151.0251007080078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201373815536499 seconds
policy loss:-7921.6865234375
value loss:121.0294189453125
entropies:165.6237030029297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1680865287780762 seconds
policy loss:-2530.08349609375
value loss:73.3110580444336
entropies:151.70166015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2382853031158447 seconds
policy loss:-2782.507080078125
value loss:64.88589477539062
entropies:165.41619873046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2532243728637695 seconds
policy loss:-1160.0374755859375
value loss:60.72862243652344
entropies:158.91241455078125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1956.9003)
ToM Target loss= tensor(3409.5447)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2181715965270996 seconds
policy loss:4397.37060546875
value loss:77.25729370117188
entropies:168.91697692871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.186363935470581 seconds
policy loss:1909.863037109375
value loss:71.58433532714844
entropies:135.46702575683594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2247610092163086 seconds
policy loss:2763.727294921875
value loss:89.79454803466797
entropies:153.9444580078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1672265529632568 seconds
policy loss:381.9462890625
value loss:120.07587432861328
entropies:154.1085662841797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2411344051361084 seconds
policy loss:1712.853271484375
value loss:73.06727600097656
entropies:158.77667236328125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1977.9053)
ToM Target loss= tensor(3405.3059)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2464666366577148 seconds
policy loss:-1479.6431884765625
value loss:70.13703918457031
entropies:157.61843872070312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169297456741333 seconds
policy loss:1494.686767578125
value loss:34.83664321899414
entropies:170.424560546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1841933727264404 seconds
policy loss:19.224050521850586
value loss:59.13457489013672
entropies:156.61781311035156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1775836944580078 seconds
policy loss:-3626.131591796875
value loss:82.10626220703125
entropies:148.18167114257812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2364399433135986 seconds
policy loss:-1482.0269775390625
value loss:50.03396987915039
entropies:154.83782958984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1939.1464)
ToM Target loss= tensor(3406.9204)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1650984287261963 seconds
policy loss:-3729.34375
value loss:84.9809341430664
entropies:155.13172912597656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1653523445129395 seconds
policy loss:-2719.672119140625
value loss:68.0894546508789
entropies:160.56005859375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.171722650527954 seconds
policy loss:-3315.10302734375
value loss:315.6304626464844
entropies:141.82864379882812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2219274044036865 seconds
policy loss:-179.7665557861328
value loss:48.25170135498047
entropies:175.43948364257812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1652498245239258 seconds
policy loss:-2227.530517578125
value loss:164.19595336914062
entropies:146.6664581298828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1968.8245)
ToM Target loss= tensor(3409.3174)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2204132080078125 seconds
policy loss:363.5115966796875
value loss:53.9991340637207
entropies:146.85125732421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1948528289794922 seconds
policy loss:1794.34326171875
value loss:71.74561309814453
entropies:160.9544219970703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2312638759613037 seconds
policy loss:2375.8857421875
value loss:100.03748321533203
entropies:146.7930908203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2201766967773438 seconds
policy loss:-1928.30810546875
value loss:202.34962463378906
entropies:128.2788848876953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2093477249145508 seconds
policy loss:192.25399780273438
value loss:67.05166625976562
entropies:141.25201416015625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1938.9839)
ToM Target loss= tensor(3394.6528)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1546704769134521 seconds
policy loss:1045.104248046875
value loss:89.51300048828125
entropies:134.87826538085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1727848052978516 seconds
policy loss:-1268.0350341796875
value loss:186.37237548828125
entropies:99.7406234741211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2226877212524414 seconds
policy loss:-6098.66845703125
value loss:890.3805541992188
entropies:142.35459899902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1728928089141846 seconds
policy loss:-3685.59716796875
value loss:89.67040252685547
entropies:148.72149658203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.215235710144043 seconds
policy loss:1313.008056640625
value loss:71.55934143066406
entropies:169.06532287597656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1952.0925)
ToM Target loss= tensor(3398.8528)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2317984104156494 seconds
policy loss:1473.07470703125
value loss:34.869205474853516
entropies:167.47975158691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1879138946533203 seconds
policy loss:452.0882568359375
value loss:95.59603118896484
entropies:119.70301055908203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.194502353668213 seconds
policy loss:-484.4429016113281
value loss:54.73137664794922
entropies:150.32745361328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1673853397369385 seconds
policy loss:30.108753204345703
value loss:93.63095092773438
entropies:133.85855102539062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1567130088806152 seconds
policy loss:-590.7920532226562
value loss:48.613216400146484
entropies:144.3529052734375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1918.8341)
ToM Target loss= tensor(3389.4153)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2254016399383545 seconds
policy loss:-1684.5284423828125
value loss:50.283267974853516
entropies:149.3509521484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2115130424499512 seconds
policy loss:-430.9425048828125
value loss:117.4110107421875
entropies:133.22019958496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227090835571289 seconds
policy loss:-3986.775390625
value loss:82.93754577636719
entropies:128.67971801757812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.167421579360962 seconds
policy loss:-700.9073486328125
value loss:68.95782470703125
entropies:146.78073120117188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.200120210647583 seconds
policy loss:-2558.68310546875
value loss:61.131771087646484
entropies:161.67942810058594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1908.2747)
ToM Target loss= tensor(3383.3037)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1585147380828857 seconds
policy loss:216.25514221191406
value loss:71.14469909667969
entropies:138.6488037109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2165732383728027 seconds
policy loss:770.588134765625
value loss:88.1158676147461
entropies:142.99644470214844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.196645736694336 seconds
policy loss:-1056.6622314453125
value loss:67.86890411376953
entropies:132.73362731933594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2137377262115479 seconds
policy loss:2098.384033203125
value loss:41.31689453125
entropies:164.05482482910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1634483337402344 seconds
policy loss:-1478.0740966796875
value loss:51.16078186035156
entropies:166.26779174804688
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1924.2281)
ToM Target loss= tensor(3388.9683)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1831300258636475 seconds
policy loss:-1456.188232421875
value loss:64.19544219970703
entropies:159.20550537109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2262306213378906 seconds
policy loss:-356.5486755371094
value loss:54.22800064086914
entropies:157.55712890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1836140155792236 seconds
policy loss:-2808.184326171875
value loss:65.54723358154297
entropies:157.5770263671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1871333122253418 seconds
policy loss:-2675.386474609375
value loss:149.32269287109375
entropies:138.85848999023438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.16060471534729 seconds
policy loss:-1674.822998046875
value loss:68.19080352783203
entropies:148.27169799804688
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1923.9338)
ToM Target loss= tensor(3379.1470)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.188201665878296 seconds
policy loss:-1484.74169921875
value loss:56.12098693847656
entropies:157.44837951660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1820082664489746 seconds
policy loss:-788.7171630859375
value loss:74.41862487792969
entropies:141.47064208984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2337298393249512 seconds
policy loss:-1877.995361328125
value loss:93.99177551269531
entropies:137.61093139648438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.225294589996338 seconds
policy loss:-548.3018798828125
value loss:68.41194915771484
entropies:148.3366241455078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.190762996673584 seconds
policy loss:2204.120849609375
value loss:93.17388153076172
entropies:139.61862182617188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1882.5980)
ToM Target loss= tensor(3369.7473)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1602349281311035 seconds
policy loss:-369.0203552246094
value loss:69.25238800048828
entropies:144.8498992919922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1635417938232422 seconds
policy loss:537.843994140625
value loss:88.70451354980469
entropies:149.32687377929688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1743848323822021 seconds
policy loss:1713.07275390625
value loss:60.097015380859375
entropies:159.22276306152344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1853201389312744 seconds
policy loss:-502.536865234375
value loss:97.5888442993164
entropies:148.02200317382812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.241396427154541 seconds
policy loss:-14.491189956665039
value loss:58.42328643798828
entropies:152.8427276611328
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1894.3413)
ToM Target loss= tensor(3370.4443)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2192292213439941 seconds
policy loss:-862.4365234375
value loss:47.85115432739258
entropies:149.04071044921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2173492908477783 seconds
policy loss:-1422.2613525390625
value loss:51.67991256713867
entropies:170.16891479492188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.235673189163208 seconds
policy loss:-4156.35009765625
value loss:90.735595703125
entropies:140.60397338867188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1695184707641602 seconds
policy loss:-3489.1455078125
value loss:119.8793716430664
entropies:148.78138732910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1660897731781006 seconds
policy loss:-2413.657470703125
value loss:63.991859436035156
entropies:148.37103271484375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1892.0110)
ToM Target loss= tensor(3378.4976)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1720426082611084 seconds
policy loss:-3104.156005859375
value loss:99.2839584350586
entropies:148.37396240234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1669788360595703 seconds
policy loss:-547.3003540039062
value loss:70.14335632324219
entropies:155.66143798828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1989853382110596 seconds
policy loss:893.1876220703125
value loss:41.60655975341797
entropies:163.72198486328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1711738109588623 seconds
policy loss:2809.833984375
value loss:60.330322265625
entropies:147.7823944091797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.232551097869873 seconds
policy loss:-686.8397827148438
value loss:112.39398193359375
entropies:155.05361938476562
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1946.6583)
ToM Target loss= tensor(3390.6274)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.228745460510254 seconds
policy loss:-498.64520263671875
value loss:68.18994140625
entropies:134.64122009277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2315418720245361 seconds
policy loss:1507.589111328125
value loss:56.11819076538086
entropies:146.9230194091797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1934566497802734 seconds
policy loss:-2219.9931640625
value loss:105.88247680664062
entropies:126.73515319824219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1699461936950684 seconds
policy loss:-1642.9129638671875
value loss:83.3372802734375
entropies:154.69837951660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1648955345153809 seconds
policy loss:-2608.17333984375
value loss:74.63905334472656
entropies:149.31777954101562
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1910.9817)
ToM Target loss= tensor(3373.2395)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.155792474746704 seconds
policy loss:-123.46464538574219
value loss:58.89057159423828
entropies:124.71915435791016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2018663883209229 seconds
policy loss:-1850.5401611328125
value loss:84.96643829345703
entropies:128.97140502929688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2137889862060547 seconds
policy loss:964.8112182617188
value loss:64.51441192626953
entropies:151.8544921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2284178733825684 seconds
policy loss:-2406.3916015625
value loss:561.5187377929688
entropies:113.22621154785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2136898040771484 seconds
policy loss:-160.62120056152344
value loss:67.5914535522461
entropies:129.06082153320312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1861.5874)
ToM Target loss= tensor(3370.4912)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1645829677581787 seconds
policy loss:978.0150146484375
value loss:60.71113586425781
entropies:153.25689697265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2209928035736084 seconds
policy loss:1220.4678955078125
value loss:34.42903137207031
entropies:159.39031982421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1878738403320312 seconds
policy loss:-1621.696533203125
value loss:58.39547348022461
entropies:128.95082092285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1792290210723877 seconds
policy loss:-3475.55810546875
value loss:70.62847137451172
entropies:131.90589904785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2233529090881348 seconds
policy loss:-2401.78564453125
value loss:49.78566360473633
entropies:149.046630859375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1870.3497)
ToM Target loss= tensor(3357.1179)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1715741157531738 seconds
policy loss:-4233.568359375
value loss:84.54476928710938
entropies:130.25823974609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2246556282043457 seconds
policy loss:-2714.11328125
value loss:63.465232849121094
entropies:157.9757843017578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2122085094451904 seconds
policy loss:-1685.4267578125
value loss:120.74607849121094
entropies:101.96629333496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.234055519104004 seconds
policy loss:634.3834838867188
value loss:72.74618530273438
entropies:132.3070068359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1677024364471436 seconds
policy loss:316.9278564453125
value loss:86.13127136230469
entropies:88.80313110351562
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1837.9492)
ToM Target loss= tensor(3348.1633)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1644370555877686 seconds
policy loss:1858.8480224609375
value loss:115.32891845703125
entropies:130.352294921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.15604829788208 seconds
policy loss:1574.2926025390625
value loss:115.2758560180664
entropies:100.6294174194336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2270045280456543 seconds
policy loss:4026.686767578125
value loss:114.77140808105469
entropies:114.03401947021484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2320845127105713 seconds
policy loss:1538.130859375
value loss:149.73544311523438
entropies:96.59727478027344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.230156660079956 seconds
policy loss:3152.554931640625
value loss:98.8277587890625
entropies:119.40853881835938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1823.9375)
ToM Target loss= tensor(3283.7004)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2071707248687744 seconds
policy loss:2141.393798828125
value loss:168.537109375
entropies:122.42123413085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2284162044525146 seconds
policy loss:854.6231689453125
value loss:158.48226928710938
entropies:101.48112487792969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2341532707214355 seconds
policy loss:575.9444580078125
value loss:53.08637237548828
entropies:132.0628204345703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2338690757751465 seconds
policy loss:359.1331481933594
value loss:171.31884765625
entropies:119.38741302490234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1691176891326904 seconds
policy loss:-2254.005859375
value loss:91.70445251464844
entropies:91.14599609375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1794.6753)
ToM Target loss= tensor(3276.6589)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2315125465393066 seconds
policy loss:-675.3345947265625
value loss:91.57592010498047
entropies:118.27118682861328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1913776397705078 seconds
policy loss:-2512.799560546875
value loss:231.20220947265625
entropies:112.27559661865234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2169618606567383 seconds
policy loss:-1301.8582763671875
value loss:71.8758544921875
entropies:114.5855941772461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.202061653137207 seconds
policy loss:-3233.01025390625
value loss:109.85386657714844
entropies:118.91610717773438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1639165878295898 seconds
policy loss:1152.578369140625
value loss:46.073307037353516
entropies:139.6572265625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1820.4160)
ToM Target loss= tensor(3279.6558)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2238008975982666 seconds
policy loss:-1699.986083984375
value loss:104.40177154541016
entropies:106.48735046386719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1928951740264893 seconds
policy loss:-613.3903198242188
value loss:94.53880310058594
entropies:107.60124969482422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2184617519378662 seconds
policy loss:-961.599365234375
value loss:86.8970947265625
entropies:112.25000762939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2353606224060059 seconds
policy loss:4.463998317718506
value loss:136.0316925048828
entropies:118.42621612548828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2178542613983154 seconds
policy loss:-864.2274780273438
value loss:86.54085540771484
entropies:106.13418579101562
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1766.2068)
ToM Target loss= tensor(3264.9128)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2295074462890625 seconds
policy loss:157.46644592285156
value loss:57.4724235534668
entropies:136.58824157714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2348504066467285 seconds
policy loss:-1738.146484375
value loss:108.19671630859375
entropies:124.6178207397461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2264163494110107 seconds
policy loss:-1952.955810546875
value loss:123.80451202392578
entropies:116.68122100830078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.199998140335083 seconds
policy loss:361.8428649902344
value loss:83.46942138671875
entropies:147.8432159423828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.161895990371704 seconds
policy loss:-27.67349624633789
value loss:51.66539764404297
entropies:155.8035125732422
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1797.2430)
ToM Target loss= tensor(3288.3052)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.168654203414917 seconds
policy loss:1609.04345703125
value loss:72.49935150146484
entropies:129.8233184814453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1691412925720215 seconds
policy loss:-2723.6083984375
value loss:202.94163513183594
entropies:112.70478820800781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2174072265625 seconds
policy loss:-1237.0953369140625
value loss:106.660888671875
entropies:113.9870376586914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2314190864562988 seconds
policy loss:-3605.00732421875
value loss:84.91122436523438
entropies:148.65451049804688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2252473831176758 seconds
policy loss:-4429.859375
value loss:98.08629608154297
entropies:117.92865753173828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1772.7847)
ToM Target loss= tensor(3296.8311)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1679205894470215 seconds
policy loss:-271.8089294433594
value loss:58.02711486816406
entropies:137.9449462890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1716525554656982 seconds
policy loss:-2930.045166015625
value loss:124.78103637695312
entropies:113.84396362304688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1637852191925049 seconds
policy loss:2032.614501953125
value loss:61.141422271728516
entropies:130.8948516845703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173882246017456 seconds
policy loss:676.375732421875
value loss:93.3177490234375
entropies:113.4941635131836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2360079288482666 seconds
policy loss:2212.12158203125
value loss:97.79092407226562
entropies:139.15333557128906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1762.6068)
ToM Target loss= tensor(3277.7695)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1632258892059326 seconds
policy loss:-193.8070831298828
value loss:53.398719787597656
entropies:152.856201171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1662869453430176 seconds
policy loss:-1520.0987548828125
value loss:72.36677551269531
entropies:140.43605041503906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1631193161010742 seconds
policy loss:-92.63142395019531
value loss:120.1607437133789
entropies:126.67578887939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.230252981185913 seconds
policy loss:-938.8505249023438
value loss:57.94969177246094
entropies:141.87158203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2220661640167236 seconds
policy loss:-948.8967895507812
value loss:61.983863830566406
entropies:138.98904418945312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1846.3218)
ToM Target loss= tensor(3352.1807)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.19209623336792 seconds
policy loss:2652.676025390625
value loss:107.9690933227539
entropies:123.75399780273438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2289609909057617 seconds
policy loss:622.167236328125
value loss:57.56608963012695
entropies:155.45870971679688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.16693115234375 seconds
policy loss:-1785.7581787109375
value loss:92.3149642944336
entropies:124.74862670898438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.206385612487793 seconds
policy loss:993.1328735351562
value loss:76.28994750976562
entropies:104.4044189453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1555426120758057 seconds
policy loss:312.29718017578125
value loss:111.71928405761719
entropies:118.7793960571289
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1784.0319)
ToM Target loss= tensor(3259.4060)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2172811031341553 seconds
policy loss:9.389551162719727
value loss:65.1032485961914
entropies:145.108154296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.223799705505371 seconds
policy loss:-1198.6080322265625
value loss:75.47136688232422
entropies:132.79583740234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1955370903015137 seconds
policy loss:-1077.6883544921875
value loss:42.330665588378906
entropies:150.07125854492188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2250542640686035 seconds
policy loss:-1555.788330078125
value loss:74.43521881103516
entropies:115.53781127929688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2274994850158691 seconds
policy loss:-1654.9495849609375
value loss:69.9267578125
entropies:147.24041748046875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1780.8534)
ToM Target loss= tensor(3305.1292)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.225950002670288 seconds
policy loss:-3663.006103515625
value loss:114.49275207519531
entropies:138.46270751953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2187769412994385 seconds
policy loss:-1091.3394775390625
value loss:73.60455322265625
entropies:141.0947723388672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1675219535827637 seconds
policy loss:1833.632568359375
value loss:68.80293273925781
entropies:127.11276245117188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1719276905059814 seconds
policy loss:3156.42822265625
value loss:144.71084594726562
entropies:132.72265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1972124576568604 seconds
policy loss:297.61328125
value loss:117.302978515625
entropies:127.31558227539062
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1754.0919)
ToM Target loss= tensor(3245.0520)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1680128574371338 seconds
policy loss:2893.438232421875
value loss:53.35161590576172
entropies:154.75814819335938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.190772533416748 seconds
policy loss:2617.28369140625
value loss:64.1476821899414
entropies:146.87701416015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169588327407837 seconds
policy loss:-336.88555908203125
value loss:68.34510040283203
entropies:146.15975952148438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1728358268737793 seconds
policy loss:-2499.75341796875
value loss:79.99903869628906
entropies:141.30506896972656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2174570560455322 seconds
policy loss:-3132.584228515625
value loss:110.92825317382812
entropies:148.38038635253906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1827.5157)
ToM Target loss= tensor(3301.4246)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2287306785583496 seconds
policy loss:-2959.655517578125
value loss:67.06300354003906
entropies:152.2392578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169754981994629 seconds
policy loss:-1479.415283203125
value loss:62.50979232788086
entropies:153.18714904785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2009482383728027 seconds
policy loss:162.03741455078125
value loss:135.08953857421875
entropies:123.74568939208984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2212185859680176 seconds
policy loss:85.67455291748047
value loss:53.27119445800781
entropies:159.62554931640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2235379219055176 seconds
policy loss:-2962.134521484375
value loss:86.35745239257812
entropies:148.5831756591797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1801.3906)
ToM Target loss= tensor(3282.1074)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2194623947143555 seconds
policy loss:667.2073364257812
value loss:61.339839935302734
entropies:120.82201385498047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2319211959838867 seconds
policy loss:1179.0318603515625
value loss:72.79804992675781
entropies:152.61456298828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1970956325531006 seconds
policy loss:1484.169921875
value loss:115.63816833496094
entropies:126.16436767578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1334235668182373 seconds
policy loss:1801.363525390625
value loss:61.52890396118164
entropies:139.5294952392578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1633617877960205 seconds
policy loss:-24.077594757080078
value loss:98.68904113769531
entropies:136.33404541015625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1738.1393)
ToM Target loss= tensor(3256.7803)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1994514465332031 seconds
policy loss:-3704.94970703125
value loss:66.15279388427734
entropies:151.22451782226562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.209627389907837 seconds
policy loss:-2962.6953125
value loss:95.77136993408203
entropies:127.59394836425781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2467191219329834 seconds
policy loss:-3866.96826171875
value loss:155.1328125
entropies:130.27963256835938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2262842655181885 seconds
policy loss:-1870.9429931640625
value loss:142.92276000976562
entropies:107.3003921508789
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.168639898300171 seconds
policy loss:-2710.84375
value loss:77.54486083984375
entropies:136.6780548095703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1729.1772)
ToM Target loss= tensor(3250.1470)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1746387481689453 seconds
policy loss:-678.95751953125
value loss:67.49278259277344
entropies:151.12088012695312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2116634845733643 seconds
policy loss:-2262.995849609375
value loss:124.06401062011719
entropies:136.7399444580078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2358884811401367 seconds
policy loss:-1706.1614990234375
value loss:67.9168701171875
entropies:138.36550903320312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2236409187316895 seconds
policy loss:2186.09814453125
value loss:48.241641998291016
entropies:158.0968475341797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.239305019378662 seconds
policy loss:4098.9443359375
value loss:66.04155731201172
entropies:156.8782958984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1802.5815)
ToM Target loss= tensor(3302.2095)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2253096103668213 seconds
policy loss:3577.396240234375
value loss:72.39127349853516
entropies:143.8477783203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2341442108154297 seconds
policy loss:1441.4560546875
value loss:63.8673210144043
entropies:139.88999938964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1676089763641357 seconds
policy loss:1728.66259765625
value loss:109.65293884277344
entropies:113.49607849121094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2191076278686523 seconds
policy loss:2300.01318359375
value loss:75.93863677978516
entropies:131.99314880371094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2402803897857666 seconds
policy loss:723.6464233398438
value loss:81.20128631591797
entropies:127.20586395263672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1703.2256)
ToM Target loss= tensor(3224.3796)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2224977016448975 seconds
policy loss:-746.2343139648438
value loss:138.83787536621094
entropies:108.81520080566406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.230128526687622 seconds
policy loss:-4100.8935546875
value loss:118.17884826660156
entropies:133.42320251464844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1806471347808838 seconds
policy loss:-2821.1240234375
value loss:73.52197265625
entropies:105.6917495727539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1736114025115967 seconds
policy loss:-3798.41943359375
value loss:82.21072387695312
entropies:124.2927474975586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.219585657119751 seconds
policy loss:-6320.50439453125
value loss:141.8723907470703
entropies:133.55613708496094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1669.9192)
ToM Target loss= tensor(3182.3396)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2063698768615723 seconds
policy loss:-4134.62060546875
value loss:95.49121856689453
entropies:123.36396789550781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2225260734558105 seconds
policy loss:-1757.5775146484375
value loss:55.252403259277344
entropies:155.191162109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1528644561767578 seconds
policy loss:-964.6412353515625
value loss:103.3050537109375
entropies:92.06646728515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2382094860076904 seconds
policy loss:1698.366455078125
value loss:115.46236419677734
entropies:128.0321044921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2084324359893799 seconds
policy loss:1536.478759765625
value loss:48.17356491088867
entropies:145.9805908203125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1702.8413)
ToM Target loss= tensor(3200.2639)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1826732158660889 seconds
policy loss:1717.2373046875
value loss:87.33797454833984
entropies:97.41815185546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1769282817840576 seconds
policy loss:106.07156372070312
value loss:80.55842590332031
entropies:111.76193237304688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1786673069000244 seconds
policy loss:2596.166259765625
value loss:99.67491912841797
entropies:112.62295532226562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1675009727478027 seconds
policy loss:428.95172119140625
value loss:79.99119567871094
entropies:121.67721557617188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.168832540512085 seconds
policy loss:-598.7860107421875
value loss:53.44263458251953
entropies:140.35289001464844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1591.4592)
ToM Target loss= tensor(3231.5125)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2013840675354004 seconds
policy loss:-552.6342163085938
value loss:47.50001525878906
entropies:125.31202697753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1748876571655273 seconds
policy loss:-2240.578125
value loss:60.0003776550293
entropies:141.34967041015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2320802211761475 seconds
policy loss:1449.5181884765625
value loss:65.01380920410156
entropies:106.5955810546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2203154563903809 seconds
policy loss:787.801025390625
value loss:69.84709167480469
entropies:129.7036590576172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.222766399383545 seconds
policy loss:388.423583984375
value loss:96.90798950195312
entropies:91.29750061035156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1610.8550)
ToM Target loss= tensor(3134.2598)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1959645748138428 seconds
policy loss:-2603.849609375
value loss:63.25495147705078
entropies:121.44166564941406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2176768779754639 seconds
policy loss:-3090.064697265625
value loss:101.32229614257812
entropies:108.46580505371094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1795320510864258 seconds
policy loss:-1790.182373046875
value loss:110.25975036621094
entropies:101.04119873046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1800963878631592 seconds
policy loss:-3404.40625
value loss:110.41400909423828
entropies:122.32386016845703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2336452007293701 seconds
policy loss:-1662.733642578125
value loss:69.97422790527344
entropies:116.50154113769531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1574.8407)
ToM Target loss= tensor(3153.8342)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2352566719055176 seconds
policy loss:-121.71691131591797
value loss:89.73869323730469
entropies:93.2624740600586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.219646692276001 seconds
policy loss:-1096.6092529296875
value loss:137.49070739746094
entropies:105.91200256347656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1951730251312256 seconds
policy loss:627.4835815429688
value loss:86.71121978759766
entropies:128.20899963378906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2167630195617676 seconds
policy loss:960.7554321289062
value loss:64.88783264160156
entropies:138.0491485595703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1736931800842285 seconds
policy loss:-722.0403442382812
value loss:84.70401000976562
entropies:102.04395294189453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1609.1625)
ToM Target loss= tensor(3145.2559)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2252469062805176 seconds
policy loss:952.0674438476562
value loss:76.85863494873047
entropies:137.01889038085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1734583377838135 seconds
policy loss:1227.65087890625
value loss:82.02714538574219
entropies:97.54843139648438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2173411846160889 seconds
policy loss:17.389005661010742
value loss:123.98673248291016
entropies:115.32119750976562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1694979667663574 seconds
policy loss:549.6951904296875
value loss:75.60429382324219
entropies:96.02400207519531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228271245956421 seconds
policy loss:-1908.3712158203125
value loss:136.25091552734375
entropies:111.66010284423828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1642.1691)
ToM Target loss= tensor(3085.1904)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2129707336425781 seconds
policy loss:-2402.53271484375
value loss:90.08729553222656
entropies:107.31893157958984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1764729022979736 seconds
policy loss:-1943.6812744140625
value loss:70.02696990966797
entropies:103.67662048339844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1705482006072998 seconds
policy loss:-2893.701904296875
value loss:125.22496032714844
entropies:101.84280395507812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2244091033935547 seconds
policy loss:-3085.315185546875
value loss:83.48544311523438
entropies:99.57665252685547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.209242343902588 seconds
policy loss:-1121.333984375
value loss:63.39243698120117
entropies:90.74861907958984
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1468.3862)
ToM Target loss= tensor(3032.3911)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2166154384613037 seconds
policy loss:498.72711181640625
value loss:87.7509536743164
entropies:97.3060302734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1733546257019043 seconds
policy loss:-1536.6546630859375
value loss:76.13223266601562
entropies:116.26527404785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2563176155090332 seconds
policy loss:-965.6728515625
value loss:56.64796829223633
entropies:108.14794921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2359027862548828 seconds
policy loss:1246.3023681640625
value loss:48.8221549987793
entropies:112.48388671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.17716383934021 seconds
policy loss:401.5278015136719
value loss:59.72016143798828
entropies:106.09293365478516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1546.7686)
ToM Target loss= tensor(3026.0278)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1824462413787842 seconds
policy loss:-967.1792602539062
value loss:93.83155059814453
entropies:90.00263977050781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2384448051452637 seconds
policy loss:-1181.662109375
value loss:142.018310546875
entropies:96.61229705810547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2350566387176514 seconds
policy loss:844.4519653320312
value loss:57.163421630859375
entropies:118.33965301513672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2445564270019531 seconds
policy loss:-90.99406433105469
value loss:102.15209197998047
entropies:109.83983612060547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2094602584838867 seconds
policy loss:-997.7963256835938
value loss:66.86691284179688
entropies:104.0164794921875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1518.2886)
ToM Target loss= tensor(2940.1033)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.241175651550293 seconds
policy loss:2303.22900390625
value loss:75.11042785644531
entropies:97.11493682861328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2139971256256104 seconds
policy loss:-2012.034912109375
value loss:273.97906494140625
entropies:110.42264556884766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2490453720092773 seconds
policy loss:-569.2689208984375
value loss:46.84762954711914
entropies:140.2572479248047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1846935749053955 seconds
policy loss:382.54034423828125
value loss:54.58842849731445
entropies:140.27041625976562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1796348094940186 seconds
policy loss:-1823.03515625
value loss:120.25601196289062
entropies:110.0804672241211
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1613.0027)
ToM Target loss= tensor(3044.4314)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2332196235656738 seconds
policy loss:-3601.439453125
value loss:101.95803833007812
entropies:136.39605712890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1723933219909668 seconds
policy loss:-2079.690673828125
value loss:72.57450866699219
entropies:114.32449340820312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2318048477172852 seconds
policy loss:-3100.737548828125
value loss:127.2070083618164
entropies:115.12554931640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1871826648712158 seconds
policy loss:-2522.7744140625
value loss:65.9301986694336
entropies:142.44497680664062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.203242540359497 seconds
policy loss:352.93768310546875
value loss:67.72274780273438
entropies:111.94309997558594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1575.0828)
ToM Target loss= tensor(3077.0046)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.18375825881958 seconds
policy loss:-729.6964721679688
value loss:130.74966430664062
entropies:101.88663482666016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.235903263092041 seconds
policy loss:-582.9347534179688
value loss:74.01118469238281
entropies:141.98707580566406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.182074785232544 seconds
policy loss:-2214.28759765625
value loss:58.025779724121094
entropies:137.2658233642578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2025794982910156 seconds
policy loss:-346.0635681152344
value loss:344.7980041503906
entropies:65.34044647216797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2507705688476562 seconds
policy loss:763.251708984375
value loss:55.0805549621582
entropies:135.75717163085938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1513.2495)
ToM Target loss= tensor(2983.2622)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2420308589935303 seconds
policy loss:2028.2762451171875
value loss:62.47515106201172
entropies:139.58168029785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2429156303405762 seconds
policy loss:74.73255157470703
value loss:96.54302978515625
entropies:137.67178344726562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1743016242980957 seconds
policy loss:2539.78271484375
value loss:63.387306213378906
entropies:126.51492309570312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2554481029510498 seconds
policy loss:1715.004150390625
value loss:51.181053161621094
entropies:143.61862182617188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2096261978149414 seconds
policy loss:606.3790283203125
value loss:46.900325775146484
entropies:121.64962768554688
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1715.6454)
ToM Target loss= tensor(3162.1731)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2270269393920898 seconds
policy loss:-1277.3431396484375
value loss:120.5406494140625
entropies:96.3189926147461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1850926876068115 seconds
policy loss:-765.6989135742188
value loss:28.747177124023438
entropies:148.19834899902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1952824592590332 seconds
policy loss:-5053.5751953125
value loss:153.755126953125
entropies:130.75613403320312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2354302406311035 seconds
policy loss:-2787.060302734375
value loss:54.14619445800781
entropies:132.98060607910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2159614562988281 seconds
policy loss:-1321.016845703125
value loss:39.426883697509766
entropies:103.43930053710938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1603.9294)
ToM Target loss= tensor(3065.1914)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.24464750289917 seconds
policy loss:-2157.642578125
value loss:85.14688110351562
entropies:121.7135009765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1379404067993164 seconds
policy loss:-43.11430358886719
value loss:103.97799682617188
entropies:109.24526977539062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2437443733215332 seconds
policy loss:-1358.17041015625
value loss:44.27359390258789
entropies:106.24515533447266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1807599067687988 seconds
policy loss:-1755.8358154296875
value loss:81.40812683105469
entropies:110.20880126953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1840934753417969 seconds
policy loss:-2395.93896484375
value loss:85.96702575683594
entropies:120.51789855957031
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1547.7748)
ToM Target loss= tensor(2961.5259)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.240659236907959 seconds
policy loss:-509.1484069824219
value loss:75.00869750976562
entropies:105.32011413574219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2208597660064697 seconds
policy loss:1153.65771484375
value loss:56.10773849487305
entropies:113.22692108154297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2395524978637695 seconds
policy loss:388.73577880859375
value loss:32.03971862792969
entropies:129.48533630371094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2460501194000244 seconds
policy loss:62.893394470214844
value loss:64.14361572265625
entropies:65.57171630859375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1806602478027344 seconds
policy loss:-730.4644165039062
value loss:94.96934509277344
entropies:105.05935668945312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1585.7372)
ToM Target loss= tensor(2893.2593)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2104568481445312 seconds
policy loss:-404.78533935546875
value loss:82.54383087158203
entropies:102.43519592285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2359275817871094 seconds
policy loss:-811.1430053710938
value loss:56.902801513671875
entropies:125.6395263671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2124576568603516 seconds
policy loss:-461.6156005859375
value loss:51.9399528503418
entropies:137.49874877929688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.216904640197754 seconds
policy loss:-190.96209716796875
value loss:120.92720031738281
entropies:71.61882019042969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2191991806030273 seconds
policy loss:-652.8707275390625
value loss:48.21820068359375
entropies:115.01217651367188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1468.0747)
ToM Target loss= tensor(2832.5479)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2309160232543945 seconds
policy loss:-257.20343017578125
value loss:64.77775573730469
entropies:118.78268432617188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2271795272827148 seconds
policy loss:-1572.962890625
value loss:115.83480072021484
entropies:80.4474105834961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2175097465515137 seconds
policy loss:77.14339447021484
value loss:91.71116638183594
entropies:90.26919555664062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1983561515808105 seconds
policy loss:-664.85888671875
value loss:119.42450714111328
entropies:93.64977264404297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1683142185211182 seconds
policy loss:-1216.0853271484375
value loss:127.6019515991211
entropies:73.17774963378906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1450.2858)
ToM Target loss= tensor(2755.8130)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.234356164932251 seconds
policy loss:840.30517578125
value loss:47.42859649658203
entropies:85.6185073852539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1843371391296387 seconds
policy loss:1174.070556640625
value loss:151.77806091308594
entropies:84.14292907714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2280256748199463 seconds
policy loss:-1367.5762939453125
value loss:52.570899963378906
entropies:128.9231414794922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2439539432525635 seconds
policy loss:1847.0223388671875
value loss:68.29180145263672
entropies:122.45527648925781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2308862209320068 seconds
policy loss:1279.801025390625
value loss:123.95966339111328
entropies:76.65643310546875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1560.2789)
ToM Target loss= tensor(2853.5959)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.244415283203125 seconds
policy loss:-708.763916015625
value loss:133.83670043945312
entropies:77.10469055175781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2086243629455566 seconds
policy loss:-115.12779998779297
value loss:103.67513275146484
entropies:73.09882354736328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2466871738433838 seconds
policy loss:233.92587280273438
value loss:94.3613510131836
entropies:109.25436401367188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.245802879333496 seconds
policy loss:1253.90625
value loss:74.15520477294922
entropies:97.02259063720703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.185018539428711 seconds
policy loss:896.3195190429688
value loss:73.93340301513672
entropies:101.82449340820312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1458.3507)
ToM Target loss= tensor(2774.7285)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2322320938110352 seconds
policy loss:-810.7675170898438
value loss:96.97149658203125
entropies:79.49984741210938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1861703395843506 seconds
policy loss:-2534.0693359375
value loss:229.2832489013672
entropies:92.92202758789062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1777448654174805 seconds
policy loss:-1201.4847412109375
value loss:99.05802917480469
entropies:76.3700942993164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2281725406646729 seconds
policy loss:-1564.50048828125
value loss:107.64061737060547
entropies:89.22895812988281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.192500352859497 seconds
policy loss:-883.3436889648438
value loss:62.27565002441406
entropies:124.15853881835938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1511.4866)
ToM Target loss= tensor(2779.2090)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2411503791809082 seconds
policy loss:176.20664978027344
value loss:52.635494232177734
entropies:96.51959991455078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2041387557983398 seconds
policy loss:-2083.2998046875
value loss:61.98749923706055
entropies:117.73558044433594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.225165605545044 seconds
policy loss:-1086.177734375
value loss:44.84937286376953
entropies:105.70086669921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178107500076294 seconds
policy loss:2.369879722595215
value loss:49.935874938964844
entropies:94.9383544921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2324423789978027 seconds
policy loss:-1739.266357421875
value loss:221.8581085205078
entropies:102.56112670898438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1507.5054)
ToM Target loss= tensor(2747.4312)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2399001121520996 seconds
policy loss:1208.5904541015625
value loss:60.606231689453125
entropies:98.97402954101562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.242222785949707 seconds
policy loss:-48.73186492919922
value loss:66.75530242919922
entropies:115.65343475341797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2279915809631348 seconds
policy loss:-1875.5880126953125
value loss:91.30926513671875
entropies:98.87042236328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1770987510681152 seconds
policy loss:-261.1988830566406
value loss:54.82122802734375
entropies:92.6668472290039
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227933645248413 seconds
policy loss:-1678.10498046875
value loss:101.82141876220703
entropies:95.91816711425781
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1499.3094)
ToM Target loss= tensor(2774.8994)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2100615501403809 seconds
policy loss:-485.88104248046875
value loss:46.8314208984375
entropies:98.90188598632812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1765127182006836 seconds
policy loss:-951.8982543945312
value loss:57.33771514892578
entropies:111.7229995727539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2277569770812988 seconds
policy loss:1330.9364013671875
value loss:175.1810302734375
entropies:91.58333587646484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.252434253692627 seconds
policy loss:-1249.54638671875
value loss:51.588924407958984
entropies:109.40374755859375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2464914321899414 seconds
policy loss:-262.11151123046875
value loss:61.283058166503906
entropies:101.86372375488281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1467.1696)
ToM Target loss= tensor(2705.6428)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.234344244003296 seconds
policy loss:-735.171630859375
value loss:75.99356079101562
entropies:81.8092269897461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1839241981506348 seconds
policy loss:-1147.982421875
value loss:50.349037170410156
entropies:125.13703155517578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1861608028411865 seconds
policy loss:-1665.1810302734375
value loss:70.4632797241211
entropies:119.88753509521484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2429296970367432 seconds
policy loss:-2184.926025390625
value loss:56.58525466918945
entropies:104.49758911132812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1760051250457764 seconds
policy loss:-415.6902770996094
value loss:112.19485473632812
entropies:68.72130584716797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1455.5143)
ToM Target loss= tensor(2705.8567)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1750497817993164 seconds
policy loss:-626.3909301757812
value loss:53.453678131103516
entropies:92.95037841796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2132837772369385 seconds
policy loss:-1883.6629638671875
value loss:76.99410247802734
entropies:102.35780334472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2181651592254639 seconds
policy loss:2670.14111328125
value loss:77.27317810058594
entropies:114.50940704345703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1824417114257812 seconds
policy loss:1827.3525390625
value loss:73.3884048461914
entropies:107.7614517211914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1838765144348145 seconds
policy loss:630.4095458984375
value loss:55.615943908691406
entropies:131.96481323242188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1479.0754)
ToM Target loss= tensor(2678.8320)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.178494930267334 seconds
policy loss:-698.8401489257812
value loss:134.66888427734375
entropies:60.25822448730469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1796233654022217 seconds
policy loss:-1586.44287109375
value loss:126.60577392578125
entropies:83.89300537109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2130606174468994 seconds
policy loss:17.149213790893555
value loss:106.61348724365234
entropies:119.84526062011719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2231791019439697 seconds
policy loss:22.767990112304688
value loss:69.14999389648438
entropies:117.42732238769531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2445743083953857 seconds
policy loss:-1390.052734375
value loss:95.39035034179688
entropies:82.96398162841797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1403.8291)
ToM Target loss= tensor(2624.9976)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1697475910186768 seconds
policy loss:-817.8442993164062
value loss:100.37876892089844
entropies:95.6644058227539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2866435050964355 seconds
policy loss:-1917.2789306640625
value loss:71.11509704589844
entropies:107.11805725097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2399139404296875 seconds
policy loss:931.90771484375
value loss:141.75694274902344
entropies:85.04991912841797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2495765686035156 seconds
policy loss:1717.282958984375
value loss:149.39022827148438
entropies:98.2909164428711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2377581596374512 seconds
policy loss:1610.3504638671875
value loss:71.3277359008789
entropies:94.43561553955078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1481.8937)
ToM Target loss= tensor(2668.1880)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2452337741851807 seconds
policy loss:998.7369384765625
value loss:155.53536987304688
entropies:115.5098648071289
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2483453750610352 seconds
policy loss:1419.6463623046875
value loss:69.57881164550781
entropies:73.55647277832031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1771764755249023 seconds
policy loss:-624.4224243164062
value loss:78.07857513427734
entropies:80.62776184082031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.240506649017334 seconds
policy loss:1445.90478515625
value loss:60.38637924194336
entropies:118.8941879272461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2362287044525146 seconds
policy loss:-1180.578857421875
value loss:120.34138488769531
entropies:89.72250366210938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1525.4441)
ToM Target loss= tensor(2699.6243)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.243913173675537 seconds
policy loss:-3028.998046875
value loss:132.00685119628906
entropies:87.66983795166016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2129027843475342 seconds
policy loss:-906.2711181640625
value loss:69.31808471679688
entropies:114.73214721679688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.243541955947876 seconds
policy loss:-2274.548095703125
value loss:60.423336029052734
entropies:101.29899597167969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.212228775024414 seconds
policy loss:-2342.08447265625
value loss:93.3271484375
entropies:89.26667022705078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177502155303955 seconds
policy loss:-2120.796142578125
value loss:68.12532043457031
entropies:77.16082000732422
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1524.2672)
ToM Target loss= tensor(2656.9121)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2471649646759033 seconds
policy loss:-3023.072509765625
value loss:64.50659942626953
entropies:92.4348373413086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2129511833190918 seconds
policy loss:-2182.21337890625
value loss:59.28181838989258
entropies:125.1805419921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1680893898010254 seconds
policy loss:-255.38641357421875
value loss:48.5704231262207
entropies:110.73460388183594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2373144626617432 seconds
policy loss:71.16102600097656
value loss:80.35997772216797
entropies:95.49825286865234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2373528480529785 seconds
policy loss:1047.3463134765625
value loss:124.33619689941406
entropies:72.65872955322266
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1555.9005)
ToM Target loss= tensor(2661.8430)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1894798278808594 seconds
policy loss:1011.2525024414062
value loss:68.38182067871094
entropies:102.20856475830078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2431190013885498 seconds
policy loss:-1781.4222412109375
value loss:150.38955688476562
entropies:115.7904281616211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1735649108886719 seconds
policy loss:1869.9896240234375
value loss:57.049076080322266
entropies:97.18743896484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2116758823394775 seconds
policy loss:-78.50116729736328
value loss:50.717506408691406
entropies:88.41342163085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.189788579940796 seconds
policy loss:802.5831909179688
value loss:60.50711441040039
entropies:110.24425506591797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1490.5850)
ToM Target loss= tensor(2622.0073)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2270333766937256 seconds
policy loss:-30.632232666015625
value loss:85.6839599609375
entropies:90.86676788330078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2117319107055664 seconds
policy loss:279.09765625
value loss:100.11971282958984
entropies:68.90631866455078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2387399673461914 seconds
policy loss:2282.00439453125
value loss:95.59529876708984
entropies:100.70852661132812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.238368272781372 seconds
policy loss:1080.2159423828125
value loss:54.49317169189453
entropies:107.82870483398438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1767685413360596 seconds
policy loss:-492.36517333984375
value loss:101.85659790039062
entropies:101.0999984741211
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1476.1123)
ToM Target loss= tensor(2622.8291)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.219747543334961 seconds
policy loss:-46.3299674987793
value loss:60.07344436645508
entropies:81.60084533691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1923179626464844 seconds
policy loss:-1883.4190673828125
value loss:94.89617919921875
entropies:89.1556167602539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1840605735778809 seconds
policy loss:-3158.76904296875
value loss:94.52396392822266
entropies:114.91217803955078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176351547241211 seconds
policy loss:-2536.15185546875
value loss:69.33565521240234
entropies:99.32139587402344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.18758225440979 seconds
policy loss:-2940.806884765625
value loss:186.8210906982422
entropies:105.81155395507812
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1512.8682)
ToM Target loss= tensor(2534.4419)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2326772212982178 seconds
policy loss:-2143.23193359375
value loss:87.34274291992188
entropies:107.35691833496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1838970184326172 seconds
policy loss:-1098.55029296875
value loss:104.27833557128906
entropies:73.3977279663086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2186036109924316 seconds
policy loss:-10.385762214660645
value loss:73.62387084960938
entropies:88.85346984863281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.250375509262085 seconds
policy loss:607.927734375
value loss:70.99620819091797
entropies:90.09273529052734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.249086618423462 seconds
policy loss:1742.3109130859375
value loss:119.68111419677734
entropies:102.97599792480469
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1485.7490)
ToM Target loss= tensor(2493.7415)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1753740310668945 seconds
policy loss:1043.20751953125
value loss:87.73577117919922
entropies:107.87065887451172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2044150829315186 seconds
policy loss:442.93475341796875
value loss:94.41926574707031
entropies:85.23151397705078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.186927318572998 seconds
policy loss:1380.6602783203125
value loss:85.89979553222656
entropies:92.26068878173828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1370580196380615 seconds
policy loss:2663.87255859375
value loss:88.11115264892578
entropies:82.53377532958984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2222611904144287 seconds
policy loss:983.1648559570312
value loss:71.09571075439453
entropies:103.28799438476562
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1463.4001)
ToM Target loss= tensor(2500.0708)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.170732021331787 seconds
policy loss:-2249.280029296875
value loss:100.1935806274414
entropies:107.13995361328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.193674087524414 seconds
policy loss:-1356.300537109375
value loss:46.896240234375
entropies:91.69479370117188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.168961524963379 seconds
policy loss:-901.219970703125
value loss:76.3563232421875
entropies:122.99929809570312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2361183166503906 seconds
policy loss:-3254.040771484375
value loss:63.92160415649414
entropies:120.29983520507812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.206207513809204 seconds
policy loss:-681.2584228515625
value loss:47.82905578613281
entropies:99.60946655273438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1513.7344)
ToM Target loss= tensor(2506.4304)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2385246753692627 seconds
policy loss:-1037.1160888671875
value loss:124.49134826660156
entropies:78.23530578613281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2335917949676514 seconds
policy loss:-1306.587646484375
value loss:58.111820220947266
entropies:113.17537689208984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2437052726745605 seconds
policy loss:-486.29876708984375
value loss:88.61046600341797
entropies:96.92542266845703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1740758419036865 seconds
policy loss:794.0032958984375
value loss:71.18612670898438
entropies:115.88172912597656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1768760681152344 seconds
policy loss:-2197.099853515625
value loss:84.8682632446289
entropies:120.65620422363281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1536.6772)
ToM Target loss= tensor(2452.3196)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.218878984451294 seconds
policy loss:1380.6212158203125
value loss:80.92427825927734
entropies:101.388427734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2150664329528809 seconds
policy loss:270.53680419921875
value loss:75.3519058227539
entropies:94.03375244140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2342321872711182 seconds
policy loss:-494.9104919433594
value loss:57.9189567565918
entropies:62.2968635559082
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2452595233917236 seconds
policy loss:-2109.515380859375
value loss:56.831634521484375
entropies:105.37337493896484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2475287914276123 seconds
policy loss:169.3177032470703
value loss:94.28411102294922
entropies:79.66812896728516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1468.2976)
ToM Target loss= tensor(2354.4695)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1895556449890137 seconds
policy loss:331.8096618652344
value loss:98.47221374511719
entropies:52.73326110839844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2477655410766602 seconds
policy loss:1195.8056640625
value loss:102.37444305419922
entropies:71.74368286132812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1999101638793945 seconds
policy loss:1254.4156494140625
value loss:53.839752197265625
entropies:126.39820861816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2434797286987305 seconds
policy loss:192.3462677001953
value loss:62.19027328491211
entropies:122.14139556884766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.175459861755371 seconds
policy loss:-1040.552978515625
value loss:50.276058197021484
entropies:115.31181335449219
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1514.5037)
ToM Target loss= tensor(2546.6875)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.179004192352295 seconds
policy loss:-2184.9267578125
value loss:40.35923385620117
entropies:110.18978118896484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2107384204864502 seconds
policy loss:-3758.387939453125
value loss:68.14169311523438
entropies:139.1358642578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2319424152374268 seconds
policy loss:-2878.314697265625
value loss:59.673091888427734
entropies:130.62432861328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.208561897277832 seconds
policy loss:-2951.162109375
value loss:64.6161117553711
entropies:90.62016296386719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.194760799407959 seconds
policy loss:-1967.6207275390625
value loss:55.407188415527344
entropies:120.16254425048828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1643.2771)
ToM Target loss= tensor(2588.6243)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1737909317016602 seconds
policy loss:-1981.016357421875
value loss:133.56248474121094
entropies:93.83836364746094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2364907264709473 seconds
policy loss:-581.1343994140625
value loss:65.42302703857422
entropies:101.11845397949219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1687572002410889 seconds
policy loss:-1411.3629150390625
value loss:80.5257797241211
entropies:99.18118286132812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1778242588043213 seconds
policy loss:704.8146362304688
value loss:81.784423828125
entropies:102.96148681640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.236602544784546 seconds
policy loss:-322.3837890625
value loss:92.42886352539062
entropies:63.270118713378906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1497.3956)
ToM Target loss= tensor(2483.7407)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1873364448547363 seconds
policy loss:3910.9287109375
value loss:71.4640884399414
entropies:136.80845642089844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.209946870803833 seconds
policy loss:-121.82823181152344
value loss:99.39424896240234
entropies:101.67707824707031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1926863193511963 seconds
policy loss:4545.845703125
value loss:154.8439483642578
entropies:107.95690155029297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1796460151672363 seconds
policy loss:548.3694458007812
value loss:225.90512084960938
entropies:102.92762756347656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2391488552093506 seconds
policy loss:1294.8658447265625
value loss:57.1632194519043
entropies:62.295372009277344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1537.4755)
ToM Target loss= tensor(2533.4575)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2244362831115723 seconds
policy loss:1038.3946533203125
value loss:35.810306549072266
entropies:132.97683715820312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2207245826721191 seconds
policy loss:693.716796875
value loss:72.09380340576172
entropies:65.315185546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2449817657470703 seconds
policy loss:-595.9413452148438
value loss:52.961299896240234
entropies:104.19981384277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2212655544281006 seconds
policy loss:-2023.51806640625
value loss:112.98228454589844
entropies:99.91947174072266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2281208038330078 seconds
policy loss:-1806.3148193359375
value loss:45.94469451904297
entropies:73.94380950927734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1492.8942)
ToM Target loss= tensor(2363.3809)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2451567649841309 seconds
policy loss:-1757.8006591796875
value loss:51.0198860168457
entropies:121.90131378173828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2416629791259766 seconds
policy loss:-5913.650390625
value loss:120.73846435546875
entropies:104.19761657714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.225017786026001 seconds
policy loss:-2537.350341796875
value loss:87.41452026367188
entropies:115.422119140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2436540126800537 seconds
policy loss:-807.5264892578125
value loss:49.36597442626953
entropies:82.6485595703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1826295852661133 seconds
policy loss:-1837.391845703125
value loss:49.05628967285156
entropies:114.56904602050781
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1602.7101)
ToM Target loss= tensor(2527.7292)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2339532375335693 seconds
policy loss:-2064.792236328125
value loss:67.50535583496094
entropies:109.25232696533203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173891544342041 seconds
policy loss:-266.2668762207031
value loss:38.72212219238281
entropies:103.94573974609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1320171356201172 seconds
policy loss:757.514404296875
value loss:46.13446044921875
entropies:111.7750244140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2342922687530518 seconds
policy loss:453.408935546875
value loss:49.85225296020508
entropies:62.23051071166992
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2379708290100098 seconds
policy loss:137.7350311279297
value loss:58.283897399902344
entropies:89.26383972167969
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1538.1251)
ToM Target loss= tensor(2504.5327)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1853210926055908 seconds
policy loss:-609.8914794921875
value loss:58.91741943359375
entropies:105.8029556274414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.250678300857544 seconds
policy loss:-527.29150390625
value loss:63.00178527832031
entropies:77.54593658447266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.241058111190796 seconds
policy loss:-534.2036743164062
value loss:48.08981704711914
entropies:77.55548095703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1798646450042725 seconds
policy loss:-1580.520751953125
value loss:37.20609664916992
entropies:118.81800842285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1700091361999512 seconds
policy loss:-1527.97314453125
value loss:103.33001708984375
entropies:81.22508239746094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1472.4515)
ToM Target loss= tensor(2415.1296)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.227226972579956 seconds
policy loss:-89.31082153320312
value loss:137.9683074951172
entropies:106.16072082519531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.20906400680542 seconds
policy loss:832.4714965820312
value loss:44.75227737426758
entropies:93.96619415283203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178307056427002 seconds
policy loss:-1858.255859375
value loss:72.50782012939453
entropies:131.894287109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2411389350891113 seconds
policy loss:-242.53297424316406
value loss:86.48884582519531
entropies:74.81388854980469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.276627779006958 seconds
policy loss:-80.8366470336914
value loss:43.54910659790039
entropies:120.3768310546875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1583.2483)
ToM Target loss= tensor(2570.9556)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.236100196838379 seconds
policy loss:932.96240234375
value loss:79.41871643066406
entropies:94.77642059326172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2004289627075195 seconds
policy loss:-2486.720458984375
value loss:93.12841796875
entropies:116.20266723632812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.179173231124878 seconds
policy loss:-214.52548217773438
value loss:30.35541534423828
entropies:121.16807556152344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1799490451812744 seconds
policy loss:750.8287963867188
value loss:58.52737045288086
entropies:133.3909912109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1829352378845215 seconds
policy loss:-1086.6868896484375
value loss:79.85806274414062
entropies:79.2601318359375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1579.2899)
ToM Target loss= tensor(2446.3110)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2404820919036865 seconds
policy loss:-1081.0369873046875
value loss:34.0858154296875
entropies:70.31388854980469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176408052444458 seconds
policy loss:-806.452392578125
value loss:77.55726623535156
entropies:115.05803680419922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1746997833251953 seconds
policy loss:-255.7200927734375
value loss:59.24943542480469
entropies:120.69921112060547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.205627679824829 seconds
policy loss:-1357.9783935546875
value loss:53.014156341552734
entropies:97.18901824951172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1842825412750244 seconds
policy loss:572.5089111328125
value loss:45.79674530029297
entropies:108.32987213134766
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1512.2419)
ToM Target loss= tensor(2559.0220)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1917297840118408 seconds
policy loss:-1316.7904052734375
value loss:80.630859375
entropies:87.04224395751953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1795768737792969 seconds
policy loss:328.3733825683594
value loss:59.69194793701172
entropies:100.74276733398438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1740734577178955 seconds
policy loss:-3064.829345703125
value loss:99.62120819091797
entropies:130.96470642089844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2329912185668945 seconds
policy loss:-3864.656005859375
value loss:147.85020446777344
entropies:90.0250015258789
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2251029014587402 seconds
policy loss:-967.7091674804688
value loss:88.49994659423828
entropies:86.97761535644531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1558.3013)
ToM Target loss= tensor(2451.1245)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1508948802947998 seconds
policy loss:-21.300912857055664
value loss:47.910213470458984
entropies:139.16966247558594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2254199981689453 seconds
policy loss:251.64308166503906
value loss:69.57418060302734
entropies:116.63518524169922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.247065544128418 seconds
policy loss:59.35776138305664
value loss:44.23378372192383
entropies:113.90888977050781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1273884773254395 seconds
policy loss:208.83872985839844
value loss:31.758760452270508
entropies:107.94464874267578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2294609546661377 seconds
policy loss:605.8793334960938
value loss:29.97286033630371
entropies:125.64675903320312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1610.4846)
ToM Target loss= tensor(2647.2009)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2427616119384766 seconds
policy loss:-4215.31103515625
value loss:76.07794952392578
entropies:111.74322509765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.247847080230713 seconds
policy loss:-4991.533203125
value loss:83.53148651123047
entropies:118.5169677734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1727876663208008 seconds
policy loss:-4365.0830078125
value loss:100.04898071289062
entropies:102.5350112915039
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228271484375 seconds
policy loss:-5121.6552734375
value loss:99.76873779296875
entropies:120.09881591796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1795625686645508 seconds
policy loss:-4684.8984375
value loss:92.01822662353516
entropies:101.40962982177734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1579.0779)
ToM Target loss= tensor(2470.5991)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.178886890411377 seconds
policy loss:-2845.9169921875
value loss:57.19842529296875
entropies:128.02484130859375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.238295555114746 seconds
policy loss:-837.1043090820312
value loss:73.58350372314453
entropies:124.91790771484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2386467456817627 seconds
policy loss:341.1809387207031
value loss:101.53404998779297
entropies:111.74323272705078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1739521026611328 seconds
policy loss:1489.663818359375
value loss:66.73919677734375
entropies:106.37065887451172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2297027111053467 seconds
policy loss:499.454345703125
value loss:76.46724700927734
entropies:85.82162475585938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1594.3887)
ToM Target loss= tensor(2377.8022)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.184169054031372 seconds
policy loss:2321.23095703125
value loss:86.70547485351562
entropies:95.88714599609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1832044124603271 seconds
policy loss:797.614990234375
value loss:108.60353088378906
entropies:80.46880340576172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1777045726776123 seconds
policy loss:2747.2939453125
value loss:70.49761962890625
entropies:101.69931030273438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2301170825958252 seconds
policy loss:3098.5712890625
value loss:67.18865966796875
entropies:129.95443725585938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2498283386230469 seconds
policy loss:1376.096435546875
value loss:53.030399322509766
entropies:100.20570373535156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1479.6349)
ToM Target loss= tensor(2323.1333)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2053651809692383 seconds
policy loss:126.85060119628906
value loss:128.1702880859375
entropies:121.16724395751953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1726553440093994 seconds
policy loss:53.05042266845703
value loss:51.43813705444336
entropies:102.07627868652344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1832659244537354 seconds
policy loss:717.364990234375
value loss:46.25982666015625
entropies:121.54225158691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2275192737579346 seconds
policy loss:-1533.855224609375
value loss:65.09408569335938
entropies:111.47969055175781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2178285121917725 seconds
policy loss:-1348.6641845703125
value loss:57.008941650390625
entropies:105.03860473632812
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1573.5024)
ToM Target loss= tensor(2373.0642)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1769039630889893 seconds
policy loss:-3287.06201171875
value loss:47.89723205566406
entropies:138.03515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2447781562805176 seconds
policy loss:-2362.5537109375
value loss:57.71841049194336
entropies:77.6263427734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2320971488952637 seconds
policy loss:-1677.8026123046875
value loss:72.70938873291016
entropies:114.36915588378906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1644177436828613 seconds
policy loss:-1883.8214111328125
value loss:51.194610595703125
entropies:94.1405029296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.179936408996582 seconds
policy loss:-1776.12109375
value loss:137.73318481445312
entropies:100.6783676147461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1528.8779)
ToM Target loss= tensor(2350.4602)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1701829433441162 seconds
policy loss:658.7528076171875
value loss:81.80239868164062
entropies:81.024658203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1774132251739502 seconds
policy loss:2125.696533203125
value loss:63.06694412231445
entropies:104.46682739257812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2438971996307373 seconds
policy loss:145.73089599609375
value loss:73.07366180419922
entropies:71.68119049072266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178100347518921 seconds
policy loss:1356.2027587890625
value loss:77.02128601074219
entropies:116.1204605102539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1883225440979004 seconds
policy loss:1001.4049682617188
value loss:76.53543090820312
entropies:94.70799255371094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1446.9242)
ToM Target loss= tensor(2425.0308)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2421209812164307 seconds
policy loss:1669.6053466796875
value loss:71.45074462890625
entropies:115.24649810791016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2473642826080322 seconds
policy loss:211.69082641601562
value loss:41.88226318359375
entropies:112.27894592285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.203141689300537 seconds
policy loss:-589.3980102539062
value loss:74.76710510253906
entropies:93.84638977050781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2333457469940186 seconds
policy loss:-4934.28466796875
value loss:93.6490478515625
entropies:141.5604705810547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2238068580627441 seconds
policy loss:-2614.62109375
value loss:53.32577896118164
entropies:103.93746948242188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1651.9196)
ToM Target loss= tensor(2420.1245)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2369284629821777 seconds
policy loss:-3809.080322265625
value loss:94.26615905761719
entropies:119.16647338867188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2352261543273926 seconds
policy loss:-2610.215087890625
value loss:90.37117004394531
entropies:122.18013763427734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1879229545593262 seconds
policy loss:-4001.914794921875
value loss:82.5189208984375
entropies:103.81685638427734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1866354942321777 seconds
policy loss:-3021.566162109375
value loss:47.963802337646484
entropies:117.46807861328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2406134605407715 seconds
policy loss:-2372.150634765625
value loss:44.90494918823242
entropies:124.59152221679688
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1588.2271)
ToM Target loss= tensor(2301.5437)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1921288967132568 seconds
policy loss:72.22016143798828
value loss:41.086238861083984
entropies:141.76991271972656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2325291633605957 seconds
policy loss:-856.3585205078125
value loss:215.83416748046875
entropies:101.50554656982422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.225391149520874 seconds
policy loss:1422.4088134765625
value loss:98.85606384277344
entropies:75.67826080322266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2029943466186523 seconds
policy loss:2765.8984375
value loss:101.18170928955078
entropies:101.44145202636719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2284152507781982 seconds
policy loss:874.504638671875
value loss:77.76392364501953
entropies:110.21932983398438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1462.3828)
ToM Target loss= tensor(2320.8560)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2347931861877441 seconds
policy loss:1389.8187255859375
value loss:70.93905639648438
entropies:79.01746368408203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2165751457214355 seconds
policy loss:1803.6475830078125
value loss:55.880428314208984
entropies:102.02285766601562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.24456787109375 seconds
policy loss:2159.3779296875
value loss:83.57756042480469
entropies:88.8111572265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1847302913665771 seconds
policy loss:-320.0718078613281
value loss:33.94371795654297
entropies:131.6505889892578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.250547170639038 seconds
policy loss:-2182.6904296875
value loss:72.35285186767578
entropies:119.0186996459961
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1516.2617)
ToM Target loss= tensor(2339.9329)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2275471687316895 seconds
policy loss:-2380.861083984375
value loss:72.23125457763672
entropies:126.33158874511719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2653834819793701 seconds
policy loss:-3177.25439453125
value loss:122.4039077758789
entropies:97.6278305053711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2237792015075684 seconds
policy loss:-1652.496337890625
value loss:51.217281341552734
entropies:106.0628433227539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.233638048171997 seconds
policy loss:-367.1098327636719
value loss:22.071964263916016
entropies:131.552001953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2500224113464355 seconds
policy loss:-655.7955322265625
value loss:38.42775344848633
entropies:97.1114730834961
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1565.8829)
ToM Target loss= tensor(2285.3037)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.232184648513794 seconds
policy loss:-210.44325256347656
value loss:48.41403579711914
entropies:118.0819091796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2391083240509033 seconds
policy loss:-309.9505615234375
value loss:52.71415710449219
entropies:129.89671325683594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1825227737426758 seconds
policy loss:-2325.687744140625
value loss:38.64529800415039
entropies:134.719970703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2522776126861572 seconds
policy loss:-820.6968994140625
value loss:45.2606315612793
entropies:111.61761474609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2376182079315186 seconds
policy loss:-616.8356323242188
value loss:41.96832275390625
entropies:116.40100860595703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1675.3142)
ToM Target loss= tensor(2545.7715)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2394201755523682 seconds
policy loss:393.6857604980469
value loss:38.559059143066406
entropies:145.85617065429688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1681544780731201 seconds
policy loss:-133.45863342285156
value loss:32.239566802978516
entropies:94.80358123779297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2382667064666748 seconds
policy loss:-1352.6937255859375
value loss:52.637718200683594
entropies:97.0011215209961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2394657135009766 seconds
policy loss:-2634.86083984375
value loss:56.232662200927734
entropies:118.72301483154297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1726253032684326 seconds
policy loss:-2869.61669921875
value loss:58.30022430419922
entropies:123.12710571289062
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1607.8452)
ToM Target loss= tensor(2513.4285)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.235809564590454 seconds
policy loss:-3403.66748046875
value loss:126.49195098876953
entropies:105.49732971191406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2335090637207031 seconds
policy loss:450.7033996582031
value loss:25.410709381103516
entropies:92.30291748046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1817874908447266 seconds
policy loss:-1214.6968994140625
value loss:51.06309127807617
entropies:94.38194274902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1763262748718262 seconds
policy loss:-1120.4759521484375
value loss:60.35148239135742
entropies:108.21754455566406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2933485507965088 seconds
policy loss:239.28892517089844
value loss:75.28732299804688
entropies:75.38945007324219
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1520.1437)
ToM Target loss= tensor(2414.3491)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1803007125854492 seconds
policy loss:-798.306396484375
value loss:147.1315155029297
entropies:102.25949096679688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1978631019592285 seconds
policy loss:332.28570556640625
value loss:54.50328063964844
entropies:126.75138854980469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.188767671585083 seconds
policy loss:-635.0223388671875
value loss:43.94750213623047
entropies:78.54833221435547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2431766986846924 seconds
policy loss:317.091552734375
value loss:31.61392593383789
entropies:59.256526947021484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2433769702911377 seconds
policy loss:-1199.2786865234375
value loss:72.58074188232422
entropies:99.99248504638672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1503.8552)
ToM Target loss= tensor(2357.5564)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.232539176940918 seconds
policy loss:-1365.134765625
value loss:42.82024383544922
entropies:103.20000457763672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2374534606933594 seconds
policy loss:196.68797302246094
value loss:53.638614654541016
entropies:117.7394790649414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2212390899658203 seconds
policy loss:336.89666748046875
value loss:46.784908294677734
entropies:73.27828979492188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2483344078063965 seconds
policy loss:345.8824462890625
value loss:35.3782844543457
entropies:63.606117248535156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2538814544677734 seconds
policy loss:-1192.85888671875
value loss:38.42710876464844
entropies:96.35206604003906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1476.2526)
ToM Target loss= tensor(2353.3015)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2394957542419434 seconds
policy loss:-2606.876220703125
value loss:64.94853973388672
entropies:100.30143737792969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1675219535827637 seconds
policy loss:-1459.216064453125
value loss:46.97821044921875
entropies:76.72146606445312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2434988021850586 seconds
policy loss:-2078.1748046875
value loss:73.52069091796875
entropies:102.04869842529297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.255887508392334 seconds
policy loss:-671.55419921875
value loss:70.77964782714844
entropies:58.86498260498047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1823163032531738 seconds
policy loss:-698.752197265625
value loss:65.1065902709961
entropies:101.26119995117188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1485.1411)
ToM Target loss= tensor(2288.5610)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.225611686706543 seconds
policy loss:-784.8955688476562
value loss:56.37395477294922
entropies:95.51959991455078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1816906929016113 seconds
policy loss:-1996.5972900390625
value loss:111.171875
entropies:87.16320037841797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1752355098724365 seconds
policy loss:998.3474731445312
value loss:81.08171081542969
entropies:87.18067169189453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2368378639221191 seconds
policy loss:1725.70849609375
value loss:90.5785140991211
entropies:72.67815399169922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2118990421295166 seconds
policy loss:2782.647705078125
value loss:75.74474334716797
entropies:106.7325439453125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1473.0377)
ToM Target loss= tensor(2256.5618)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2401249408721924 seconds
policy loss:2674.534423828125
value loss:127.90475463867188
entropies:87.68450927734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.253385066986084 seconds
policy loss:2639.16943359375
value loss:110.94277954101562
entropies:80.6953353881836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1676216125488281 seconds
policy loss:2284.71923828125
value loss:88.2969970703125
entropies:81.32161712646484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1820919513702393 seconds
policy loss:1238.29638671875
value loss:74.66992950439453
entropies:93.29557800292969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2492353916168213 seconds
policy loss:-390.2532958984375
value loss:72.33940887451172
entropies:95.42829895019531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1421.6256)
ToM Target loss= tensor(2294.9399)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1828088760375977 seconds
policy loss:788.4273681640625
value loss:49.268890380859375
entropies:97.75296783447266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2291748523712158 seconds
policy loss:-1022.702880859375
value loss:55.83522415161133
entropies:101.00582885742188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1831073760986328 seconds
policy loss:-1346.5938720703125
value loss:36.497581481933594
entropies:117.81328582763672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2427327632904053 seconds
policy loss:-819.8361206054688
value loss:110.98111724853516
entropies:104.92806243896484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2072772979736328 seconds
policy loss:-1896.7762451171875
value loss:51.0715217590332
entropies:127.88642883300781
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1634.4016)
ToM Target loss= tensor(2352.1379)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.246722936630249 seconds
policy loss:-1205.2593994140625
value loss:31.412673950195312
entropies:101.45207214355469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2283966541290283 seconds
policy loss:-1192.2052001953125
value loss:49.850101470947266
entropies:90.30957794189453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.17911696434021 seconds
policy loss:-266.6382141113281
value loss:30.69658851623535
entropies:91.28270721435547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.211665391921997 seconds
policy loss:604.768798828125
value loss:35.794376373291016
entropies:103.57109069824219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1786649227142334 seconds
policy loss:-1184.001708984375
value loss:53.717002868652344
entropies:90.83634185791016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1492.2487)
ToM Target loss= tensor(2346.7979)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1829874515533447 seconds
policy loss:-1.240107536315918
value loss:48.4285888671875
entropies:72.81278991699219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2485694885253906 seconds
policy loss:-3072.4736328125
value loss:74.0455322265625
entropies:76.72032165527344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2441325187683105 seconds
policy loss:-1594.861328125
value loss:28.547218322753906
entropies:102.8253402709961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1757941246032715 seconds
policy loss:-3634.30615234375
value loss:125.04521942138672
entropies:90.04931640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2112667560577393 seconds
policy loss:-1487.5289306640625
value loss:54.046295166015625
entropies:102.9730224609375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1407.6379)
ToM Target loss= tensor(2236.2695)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2536752223968506 seconds
policy loss:-2508.98974609375
value loss:98.12000274658203
entropies:90.97943115234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.242689609527588 seconds
policy loss:-1889.2010498046875
value loss:66.94564056396484
entropies:101.0356674194336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2426984310150146 seconds
policy loss:48.64921188354492
value loss:62.76885986328125
entropies:90.53756713867188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2430293560028076 seconds
policy loss:3097.748779296875
value loss:72.07270812988281
entropies:95.5195083618164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1944730281829834 seconds
policy loss:1866.718994140625
value loss:87.70548248291016
entropies:88.18339538574219
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1449.5383)
ToM Target loss= tensor(2387.5837)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2050988674163818 seconds
policy loss:1149.28466796875
value loss:59.52242660522461
entropies:80.52584838867188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.188493251800537 seconds
policy loss:1529.0537109375
value loss:76.95854949951172
entropies:46.115028381347656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1808803081512451 seconds
policy loss:495.5504455566406
value loss:74.37858581542969
entropies:71.91362762451172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.206216812133789 seconds
policy loss:-323.63568115234375
value loss:52.75149154663086
entropies:88.81356811523438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2144949436187744 seconds
policy loss:-915.8345336914062
value loss:78.91305541992188
entropies:82.66525268554688
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1326.5814)
ToM Target loss= tensor(2263.7131)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1984286308288574 seconds
policy loss:-1726.15234375
value loss:22.697856903076172
entropies:99.7078857421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1742630004882812 seconds
policy loss:-2166.8701171875
value loss:77.9674301147461
entropies:67.99322509765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2178726196289062 seconds
policy loss:-1121.2376708984375
value loss:27.579025268554688
entropies:82.08405303955078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1766204833984375 seconds
policy loss:-3038.945556640625
value loss:61.4206657409668
entropies:101.62651824951172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2363240718841553 seconds
policy loss:-713.2000732421875
value loss:61.96411895751953
entropies:56.563987731933594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1397.5282)
ToM Target loss= tensor(2290.3684)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1875231266021729 seconds
policy loss:-1309.7742919921875
value loss:65.11177062988281
entropies:67.61531066894531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.179814338684082 seconds
policy loss:-589.2461547851562
value loss:44.759220123291016
entropies:78.10152435302734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1919357776641846 seconds
policy loss:901.2193603515625
value loss:39.42357635498047
entropies:85.35078430175781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2171967029571533 seconds
policy loss:441.4405822753906
value loss:50.45671081542969
entropies:77.93150329589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.200659990310669 seconds
policy loss:630.3417358398438
value loss:95.13319396972656
entropies:74.2892074584961
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1362.3060)
ToM Target loss= tensor(2096.2686)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.203580617904663 seconds
policy loss:1618.6773681640625
value loss:54.687747955322266
entropies:65.34444427490234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2253413200378418 seconds
policy loss:498.84130859375
value loss:78.3291244506836
entropies:81.3205795288086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2063069343566895 seconds
policy loss:-290.95281982421875
value loss:41.20060348510742
entropies:78.26425170898438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2504730224609375 seconds
policy loss:1431.8682861328125
value loss:81.61446380615234
entropies:102.3316879272461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.232607126235962 seconds
policy loss:1056.4393310546875
value loss:49.93250274658203
entropies:89.70185852050781
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1433.4866)
ToM Target loss= tensor(2297.7744)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1859495639801025 seconds
policy loss:334.6044616699219
value loss:56.11872863769531
entropies:74.79100036621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2285401821136475 seconds
policy loss:-263.7332763671875
value loss:51.60054016113281
entropies:96.43831634521484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2353410720825195 seconds
policy loss:-947.3965454101562
value loss:39.939048767089844
entropies:109.85482025146484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.183852195739746 seconds
policy loss:-1122.6707763671875
value loss:55.03797149658203
entropies:81.96897888183594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1711437702178955 seconds
policy loss:-1664.64794921875
value loss:48.14113235473633
entropies:69.22747039794922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1406.2493)
ToM Target loss= tensor(2320.2346)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2043354511260986 seconds
policy loss:-1397.857421875
value loss:86.27167510986328
entropies:54.60540008544922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.184004306793213 seconds
policy loss:336.6183166503906
value loss:72.60318756103516
entropies:89.4440689086914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2426478862762451 seconds
policy loss:-882.020263671875
value loss:48.37267303466797
entropies:83.60335540771484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1782429218292236 seconds
policy loss:-1042.509033203125
value loss:84.6879653930664
entropies:76.34669494628906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2450697422027588 seconds
policy loss:-281.7359619140625
value loss:69.36400604248047
entropies:89.7059555053711
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1398.9163)
ToM Target loss= tensor(2079.3530)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.248072862625122 seconds
policy loss:176.16754150390625
value loss:132.59092712402344
entropies:51.31555938720703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2052433490753174 seconds
policy loss:1964.5159912109375
value loss:64.20832061767578
entropies:90.52693939208984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2425813674926758 seconds
policy loss:2571.529296875
value loss:111.03559112548828
entropies:79.83665466308594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1794993877410889 seconds
policy loss:1633.1845703125
value loss:113.93669891357422
entropies:76.61075592041016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2396280765533447 seconds
policy loss:1884.480712890625
value loss:74.04564666748047
entropies:76.54325866699219
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1371.1736)
ToM Target loss= tensor(2132.5632)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2454047203063965 seconds
policy loss:933.2584228515625
value loss:33.38508224487305
entropies:109.61206817626953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1930270195007324 seconds
policy loss:-943.6370849609375
value loss:87.86485290527344
entropies:111.57245635986328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178697109222412 seconds
policy loss:-27.837003707885742
value loss:32.25957107543945
entropies:91.56034851074219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1795251369476318 seconds
policy loss:-1063.6502685546875
value loss:111.54688262939453
entropies:89.12210083007812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2486248016357422 seconds
policy loss:-1676.8585205078125
value loss:61.13862609863281
entropies:66.23477935791016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1489.9304)
ToM Target loss= tensor(2327.5020)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.241624355316162 seconds
policy loss:-2898.391845703125
value loss:58.16329574584961
entropies:106.62271118164062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1759002208709717 seconds
policy loss:-839.401123046875
value loss:59.3988151550293
entropies:76.75360870361328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.188467025756836 seconds
policy loss:-2021.775634765625
value loss:70.57328033447266
entropies:131.955322265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2046830654144287 seconds
policy loss:-1392.20849609375
value loss:56.71298599243164
entropies:110.45760345458984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2226135730743408 seconds
policy loss:-892.2376708984375
value loss:50.84836196899414
entropies:65.16492462158203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1553.3060)
ToM Target loss= tensor(2372.8459)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2298080921173096 seconds
policy loss:174.04629516601562
value loss:60.83150863647461
entropies:90.69603729248047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169867753982544 seconds
policy loss:419.39495849609375
value loss:62.944129943847656
entropies:93.92608642578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1708345413208008 seconds
policy loss:755.455810546875
value loss:70.53865051269531
entropies:72.21306610107422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2244651317596436 seconds
policy loss:1113.0771484375
value loss:48.063507080078125
entropies:59.567298889160156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.246168613433838 seconds
policy loss:624.0225219726562
value loss:83.12066650390625
entropies:87.70703125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1378.0706)
ToM Target loss= tensor(2235.7769)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2323801517486572 seconds
policy loss:-205.3063507080078
value loss:29.26287269592285
entropies:97.22248077392578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2021112442016602 seconds
policy loss:-1349.4791259765625
value loss:66.76541900634766
entropies:97.87480163574219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2258377075195312 seconds
policy loss:-535.7527465820312
value loss:59.83272171020508
entropies:83.84175872802734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2474846839904785 seconds
policy loss:34.0974235534668
value loss:38.82555389404297
entropies:84.6622314453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.225816249847412 seconds
policy loss:-1300.628662109375
value loss:47.166927337646484
entropies:63.226497650146484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1452.4185)
ToM Target loss= tensor(2251.5276)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2113213539123535 seconds
policy loss:-2063.23779296875
value loss:65.97040557861328
entropies:94.73506927490234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1731305122375488 seconds
policy loss:-1411.761962890625
value loss:36.46979522705078
entropies:83.30155944824219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1517877578735352 seconds
policy loss:559.1024169921875
value loss:34.42588806152344
entropies:79.52314758300781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2176783084869385 seconds
policy loss:-2264.567626953125
value loss:62.888572692871094
entropies:75.12237548828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.243823766708374 seconds
policy loss:173.4700164794922
value loss:28.695703506469727
entropies:100.6695785522461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1386.4337)
ToM Target loss= tensor(2265.2212)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.184560775756836 seconds
policy loss:-1762.82763671875
value loss:38.567161560058594
entropies:100.9132308959961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2402608394622803 seconds
policy loss:-617.2137451171875
value loss:33.86550521850586
entropies:89.90171813964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2081151008605957 seconds
policy loss:-2467.66796875
value loss:49.147891998291016
entropies:109.12163543701172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2451722621917725 seconds
policy loss:-1606.9591064453125
value loss:40.36377716064453
entropies:83.18921661376953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2018864154815674 seconds
policy loss:-513.6572875976562
value loss:30.678136825561523
entropies:75.79531860351562
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1465.4657)
ToM Target loss= tensor(2363.9746)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2292742729187012 seconds
policy loss:-579.6133422851562
value loss:22.43010711669922
entropies:108.97355651855469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1745262145996094 seconds
policy loss:-754.6354370117188
value loss:45.43214797973633
entropies:57.41581726074219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2348463535308838 seconds
policy loss:-1961.594482421875
value loss:98.26441192626953
entropies:76.72480773925781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2273662090301514 seconds
policy loss:-495.2740478515625
value loss:90.02629089355469
entropies:90.02928161621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1647272109985352 seconds
policy loss:-1020.5798950195312
value loss:48.179439544677734
entropies:89.70259094238281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1454.7858)
ToM Target loss= tensor(2181.1021)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2437736988067627 seconds
policy loss:393.1794738769531
value loss:94.00011444091797
entropies:81.17643737792969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2035088539123535 seconds
policy loss:1822.551513671875
value loss:38.91162872314453
entropies:83.10828399658203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1776070594787598 seconds
policy loss:973.6505737304688
value loss:40.551116943359375
entropies:83.78076934814453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2118923664093018 seconds
policy loss:-1743.5072021484375
value loss:71.36476135253906
entropies:84.00616455078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.220470905303955 seconds
policy loss:-831.677978515625
value loss:48.835548400878906
entropies:72.56283569335938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1400.5558)
ToM Target loss= tensor(2263.2041)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2454898357391357 seconds
policy loss:-1499.31884765625
value loss:52.9851188659668
entropies:68.90385437011719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1824114322662354 seconds
policy loss:240.9110565185547
value loss:28.71038246154785
entropies:94.58016967773438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.234635829925537 seconds
policy loss:668.1746826171875
value loss:54.00752258300781
entropies:77.79000854492188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.229400634765625 seconds
policy loss:467.7547302246094
value loss:35.05853271484375
entropies:72.23867797851562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1694631576538086 seconds
policy loss:-221.81973266601562
value loss:33.522491455078125
entropies:90.72882843017578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1437.1978)
ToM Target loss= tensor(2255.9275)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2105271816253662 seconds
policy loss:-2630.451904296875
value loss:43.23398971557617
entropies:75.70014953613281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1784148216247559 seconds
policy loss:-2780.359375
value loss:34.95677947998047
entropies:85.10958862304688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2100038528442383 seconds
policy loss:-1299.4652099609375
value loss:64.59101104736328
entropies:117.44813537597656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2408514022827148 seconds
policy loss:-1055.1922607421875
value loss:109.60066986083984
entropies:69.91731262207031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2320613861083984 seconds
policy loss:-705.61865234375
value loss:46.769351959228516
entropies:67.85868072509766
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1480.7220)
ToM Target loss= tensor(2167.0127)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.175290584564209 seconds
policy loss:1452.9862060546875
value loss:62.70863342285156
entropies:71.56131744384766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1834111213684082 seconds
policy loss:2325.669189453125
value loss:68.75401306152344
entropies:83.87667083740234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2209091186523438 seconds
policy loss:577.5615234375
value loss:48.99300003051758
entropies:109.12495422363281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2084736824035645 seconds
policy loss:2001.8165283203125
value loss:47.933021545410156
entropies:130.55592346191406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1835172176361084 seconds
policy loss:-7.046321868896484
value loss:50.706565856933594
entropies:89.74622344970703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1501.6172)
ToM Target loss= tensor(2382.7932)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.243586540222168 seconds
policy loss:1760.2314453125
value loss:62.400150299072266
entropies:118.09593200683594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2192881107330322 seconds
policy loss:688.3558349609375
value loss:37.15118408203125
entropies:95.00045013427734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2351315021514893 seconds
policy loss:431.1296691894531
value loss:57.37723159790039
entropies:77.40521240234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2416343688964844 seconds
policy loss:-2402.8564453125
value loss:51.09109878540039
entropies:84.54740905761719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2452490329742432 seconds
policy loss:-1231.548583984375
value loss:24.662752151489258
entropies:82.3939208984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1491.1216)
ToM Target loss= tensor(2270.7063)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2510569095611572 seconds
policy loss:-1342.4371337890625
value loss:41.01516342163086
entropies:88.9251708984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2312133312225342 seconds
policy loss:-2468.838623046875
value loss:79.57020568847656
entropies:68.67982482910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1777598857879639 seconds
policy loss:-1878.7850341796875
value loss:35.56794738769531
entropies:82.66470336914062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1820707321166992 seconds
policy loss:-1639.8656005859375
value loss:40.480289459228516
entropies:88.06896209716797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1820449829101562 seconds
policy loss:-1236.837890625
value loss:60.00238037109375
entropies:92.8757095336914
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1449.3143)
ToM Target loss= tensor(2170.9688)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2414796352386475 seconds
policy loss:-572.9576416015625
value loss:43.33884048461914
entropies:74.59036254882812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2367403507232666 seconds
policy loss:-868.0193481445312
value loss:52.93327331542969
entropies:95.59941101074219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.246778964996338 seconds
policy loss:1473.153076171875
value loss:45.50691223144531
entropies:97.93699645996094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1746852397918701 seconds
policy loss:-316.5715637207031
value loss:34.6407356262207
entropies:110.22014617919922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2196028232574463 seconds
policy loss:-641.5259399414062
value loss:90.71438598632812
entropies:90.57972717285156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1589.1526)
ToM Target loss= tensor(2410.1560)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2546887397766113 seconds
policy loss:-2404.252197265625
value loss:54.97698211669922
entropies:65.64254760742188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2222118377685547 seconds
policy loss:327.4326477050781
value loss:33.7425537109375
entropies:105.11573028564453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2474250793457031 seconds
policy loss:-731.953125
value loss:38.98527145385742
entropies:86.5577621459961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.240279197692871 seconds
policy loss:-1316.1939697265625
value loss:24.376277923583984
entropies:67.16363525390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.246119499206543 seconds
policy loss:-1646.245849609375
value loss:38.175559997558594
entropies:97.27334594726562
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1434.5579)
ToM Target loss= tensor(2277.3386)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1980128288269043 seconds
policy loss:-340.6475524902344
value loss:46.18025207519531
entropies:90.58748626708984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2358856201171875 seconds
policy loss:-1408.82421875
value loss:62.9356575012207
entropies:79.6349105834961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2386534214019775 seconds
policy loss:-1302.9852294921875
value loss:52.94295120239258
entropies:74.5311508178711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1734256744384766 seconds
policy loss:-1199.850830078125
value loss:27.55691909790039
entropies:79.50336456298828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1720538139343262 seconds
policy loss:-1104.17041015625
value loss:36.053367614746094
entropies:102.58978271484375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1465.5288)
ToM Target loss= tensor(2372.6099)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2211296558380127 seconds
policy loss:-1800.9359130859375
value loss:40.65129089355469
entropies:101.5655517578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2314198017120361 seconds
policy loss:341.1268310546875
value loss:31.02246856689453
entropies:85.74691772460938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1876375675201416 seconds
policy loss:-2973.021484375
value loss:57.03533935546875
entropies:114.00413513183594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2135231494903564 seconds
policy loss:139.12075805664062
value loss:28.04775619506836
entropies:101.04489135742188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2427647113800049 seconds
policy loss:-479.8584289550781
value loss:33.497894287109375
entropies:106.47142791748047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1525.4281)
ToM Target loss= tensor(2339.1113)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.239600658416748 seconds
policy loss:-209.33657836914062
value loss:39.87220001220703
entropies:91.94989013671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2227435111999512 seconds
policy loss:415.84869384765625
value loss:18.602291107177734
entropies:80.47604370117188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2284724712371826 seconds
policy loss:-731.8335571289062
value loss:52.8801155090332
entropies:92.26686096191406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1779851913452148 seconds
policy loss:-1702.736083984375
value loss:35.565185546875
entropies:77.82030487060547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2416138648986816 seconds
policy loss:-4144.99267578125
value loss:46.774696350097656
entropies:97.81230926513672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1427.8497)
ToM Target loss= tensor(2248.8428)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2091548442840576 seconds
policy loss:-2084.505126953125
value loss:98.54462432861328
entropies:69.98975372314453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1957414150238037 seconds
policy loss:-101.37202453613281
value loss:38.16632080078125
entropies:113.76317596435547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1671292781829834 seconds
policy loss:-2276.84423828125
value loss:83.73014068603516
entropies:63.2171745300293
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.224449634552002 seconds
policy loss:-1116.0400390625
value loss:151.4581756591797
entropies:104.67662811279297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2346713542938232 seconds
policy loss:-1103.3348388671875
value loss:60.17070770263672
entropies:86.16815185546875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1408.1890)
ToM Target loss= tensor(2156.6753)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2283480167388916 seconds
policy loss:-108.15043640136719
value loss:40.15962600708008
entropies:103.59825897216797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2372572422027588 seconds
policy loss:730.173828125
value loss:61.71427917480469
entropies:86.05191802978516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2513275146484375 seconds
policy loss:32.69593048095703
value loss:91.90009307861328
entropies:53.9166259765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1825141906738281 seconds
policy loss:801.5982666015625
value loss:38.402809143066406
entropies:90.7601547241211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2485854625701904 seconds
policy loss:-241.4391326904297
value loss:31.326528549194336
entropies:58.228981018066406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1346.5391)
ToM Target loss= tensor(2194.4771)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2425692081451416 seconds
policy loss:-440.4292297363281
value loss:26.17325782775879
entropies:54.76343536376953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2436339855194092 seconds
policy loss:-1376.234619140625
value loss:33.34526443481445
entropies:79.41377258300781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1702959537506104 seconds
policy loss:-824.6085815429688
value loss:69.41688537597656
entropies:62.389076232910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1878552436828613 seconds
policy loss:-1831.96240234375
value loss:41.47010040283203
entropies:85.48384094238281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1786112785339355 seconds
policy loss:-2879.580078125
value loss:46.67994689941406
entropies:111.55038452148438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1381.2654)
ToM Target loss= tensor(2530.8042)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.175701379776001 seconds
policy loss:-1692.083740234375
value loss:51.46732711791992
entropies:68.64742279052734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2284913063049316 seconds
policy loss:-391.1799621582031
value loss:31.183866500854492
entropies:70.22065734863281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2130084037780762 seconds
policy loss:24.817289352416992
value loss:41.184776306152344
entropies:75.13247680664062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228379487991333 seconds
policy loss:87.94189453125
value loss:46.39311981201172
entropies:90.14869689941406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1704974174499512 seconds
policy loss:942.9713745117188
value loss:39.422080993652344
entropies:97.23070526123047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1336.5752)
ToM Target loss= tensor(2211.6128)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1811528205871582 seconds
policy loss:587.3930053710938
value loss:72.02713775634766
entropies:53.56062316894531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1748623847961426 seconds
policy loss:504.6651611328125
value loss:54.323455810546875
entropies:57.554962158203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1882238388061523 seconds
policy loss:-863.9197387695312
value loss:29.460468292236328
entropies:95.86791229248047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.166961908340454 seconds
policy loss:355.9735412597656
value loss:69.971923828125
entropies:59.729373931884766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1732749938964844 seconds
policy loss:-392.94635009765625
value loss:59.77202224731445
entropies:94.07771301269531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1339.6074)
ToM Target loss= tensor(2125.7043)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2322466373443604 seconds
policy loss:-387.0196533203125
value loss:56.347930908203125
entropies:110.05815124511719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173903465270996 seconds
policy loss:-1370.424560546875
value loss:40.870697021484375
entropies:87.30741882324219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2407000064849854 seconds
policy loss:-796.0018310546875
value loss:30.853395462036133
entropies:109.64688110351562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2392168045043945 seconds
policy loss:-1435.498779296875
value loss:80.09852600097656
entropies:68.79426574707031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.244882345199585 seconds
policy loss:-2039.254638671875
value loss:42.63780975341797
entropies:96.226318359375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1489.1907)
ToM Target loss= tensor(2314.9219)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1871991157531738 seconds
policy loss:-1964.2169189453125
value loss:42.0754508972168
entropies:67.51974487304688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1713533401489258 seconds
policy loss:-577.9539794921875
value loss:30.33482551574707
entropies:103.90785217285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2422778606414795 seconds
policy loss:-2562.116455078125
value loss:68.22927856445312
entropies:88.61820220947266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2477891445159912 seconds
policy loss:-823.0
value loss:63.483360290527344
entropies:69.17782592773438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1944739818572998 seconds
policy loss:-445.9990234375
value loss:38.7275276184082
entropies:96.08932495117188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1429.2535)
ToM Target loss= tensor(2265.9841)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1988935470581055 seconds
policy loss:-256.5856628417969
value loss:34.44419860839844
entropies:90.39199829101562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2346093654632568 seconds
policy loss:147.80111694335938
value loss:33.65212631225586
entropies:105.71379852294922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1978709697723389 seconds
policy loss:-1311.0498046875
value loss:42.9294319152832
entropies:123.79149627685547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2436192035675049 seconds
policy loss:-637.4906616210938
value loss:54.59242248535156
entropies:95.84451293945312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2142269611358643 seconds
policy loss:-1453.062744140625
value loss:29.691143035888672
entropies:92.35427856445312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1484.9534)
ToM Target loss= tensor(2371.1824)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2075412273406982 seconds
policy loss:-847.8961791992188
value loss:29.055952072143555
entropies:81.20388793945312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2119207382202148 seconds
policy loss:-3781.219482421875
value loss:127.14408111572266
entropies:65.58981323242188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2354850769042969 seconds
policy loss:-1469.7774658203125
value loss:56.73026657104492
entropies:85.3373794555664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1809871196746826 seconds
policy loss:-77.40351104736328
value loss:30.925601959228516
entropies:65.54915618896484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.238295078277588 seconds
policy loss:-97.9358901977539
value loss:33.44008255004883
entropies:75.58621978759766
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1275.4297)
ToM Target loss= tensor(2048.0229)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2443604469299316 seconds
policy loss:-798.2720336914062
value loss:41.170650482177734
entropies:92.76949310302734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1688108444213867 seconds
policy loss:217.72314453125
value loss:27.459211349487305
entropies:71.70575714111328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1824543476104736 seconds
policy loss:-330.1643981933594
value loss:32.81385040283203
entropies:73.76679229736328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.232469081878662 seconds
policy loss:-43.03835678100586
value loss:64.12895202636719
entropies:73.9824447631836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1817114353179932 seconds
policy loss:-1165.929931640625
value loss:29.67433738708496
entropies:73.39762115478516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1414.6979)
ToM Target loss= tensor(2304.1536)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1903626918792725 seconds
policy loss:-1490.950927734375
value loss:36.36769485473633
entropies:107.47412109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2480242252349854 seconds
policy loss:-2073.334228515625
value loss:38.394248962402344
entropies:84.6726303100586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1846115589141846 seconds
policy loss:-1414.093017578125
value loss:49.41184997558594
entropies:93.60309600830078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2126314640045166 seconds
policy loss:-2025.822998046875
value loss:79.34146118164062
entropies:63.89038848876953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1680097579956055 seconds
policy loss:-1102.6708984375
value loss:36.989742279052734
entropies:67.97120666503906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1461.4290)
ToM Target loss= tensor(2332.5393)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1848585605621338 seconds
policy loss:-766.8076171875
value loss:28.389251708984375
entropies:86.75145721435547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178272008895874 seconds
policy loss:219.93734741210938
value loss:46.439666748046875
entropies:65.46897888183594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1787307262420654 seconds
policy loss:390.77081298828125
value loss:72.84668731689453
entropies:99.45641326904297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1846203804016113 seconds
policy loss:158.5648651123047
value loss:53.569252014160156
entropies:66.0448226928711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.237597942352295 seconds
policy loss:39.621673583984375
value loss:49.898895263671875
entropies:99.16324615478516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1328.5422)
ToM Target loss= tensor(2149.0095)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1707916259765625 seconds
policy loss:312.36297607421875
value loss:52.72943878173828
entropies:78.90635681152344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2164127826690674 seconds
policy loss:-246.4337921142578
value loss:55.761558532714844
entropies:88.30474853515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2489593029022217 seconds
policy loss:60.938690185546875
value loss:35.81990432739258
entropies:71.12150573730469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2321605682373047 seconds
policy loss:606.3539428710938
value loss:43.32757568359375
entropies:43.03328323364258
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2224340438842773 seconds
policy loss:-344.95660400390625
value loss:61.394508361816406
entropies:40.60773468017578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1251.0563)
ToM Target loss= tensor(2077.6597)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1849818229675293 seconds
policy loss:-2223.286865234375
value loss:95.00881958007812
entropies:65.74549865722656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1844396591186523 seconds
policy loss:-914.8616333007812
value loss:42.4688720703125
entropies:94.26356506347656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2141571044921875 seconds
policy loss:-243.3784942626953
value loss:144.2747802734375
entropies:72.7671890258789
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1735451221466064 seconds
policy loss:-488.6290588378906
value loss:49.00482940673828
entropies:68.67628479003906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.209761142730713 seconds
policy loss:523.6831665039062
value loss:29.782150268554688
entropies:65.3481216430664
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1341.1439)
ToM Target loss= tensor(2275.7896)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1829750537872314 seconds
policy loss:-717.2968139648438
value loss:54.8370246887207
entropies:72.7894058227539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2330536842346191 seconds
policy loss:-1156.8876953125
value loss:71.7403564453125
entropies:105.16563415527344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2325916290283203 seconds
policy loss:-539.1082153320312
value loss:42.505489349365234
entropies:57.55906677246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1812448501586914 seconds
policy loss:-1317.9617919921875
value loss:88.40447998046875
entropies:83.60868072509766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1869947910308838 seconds
policy loss:773.0172119140625
value loss:30.78019905090332
entropies:64.2431411743164
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1450.6144)
ToM Target loss= tensor(2293.6082)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1820333003997803 seconds
policy loss:676.8005981445312
value loss:24.84931755065918
entropies:56.703556060791016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1766424179077148 seconds
policy loss:311.99249267578125
value loss:25.54642677307129
entropies:59.372230529785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2010307312011719 seconds
policy loss:-1591.6441650390625
value loss:72.47857666015625
entropies:71.79109191894531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.240713357925415 seconds
policy loss:-658.2985229492188
value loss:33.28386306762695
entropies:74.6006088256836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.172414779663086 seconds
policy loss:-1850.766845703125
value loss:58.55248260498047
entropies:68.8545150756836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1306.5027)
ToM Target loss= tensor(2215.4976)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2185430526733398 seconds
policy loss:-964.62548828125
value loss:45.910179138183594
entropies:62.03936004638672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1844630241394043 seconds
policy loss:-980.1107177734375
value loss:42.53147506713867
entropies:41.7401008605957
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2273640632629395 seconds
policy loss:-482.11334228515625
value loss:32.18907928466797
entropies:69.79379272460938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2373371124267578 seconds
policy loss:273.5087890625
value loss:33.925743103027344
entropies:85.5043716430664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2459874153137207 seconds
policy loss:-657.61474609375
value loss:25.66515350341797
entropies:70.09249114990234
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1358.4568)
ToM Target loss= tensor(2421.7991)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1816446781158447 seconds
policy loss:-2897.3740234375
value loss:40.30241394042969
entropies:98.54266357421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2259740829467773 seconds
policy loss:-3122.041748046875
value loss:61.68460464477539
entropies:94.35781860351562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2393383979797363 seconds
policy loss:-2846.85009765625
value loss:37.264827728271484
entropies:115.69032287597656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2098512649536133 seconds
policy loss:-1845.68798828125
value loss:40.11561965942383
entropies:93.64800262451172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2275416851043701 seconds
policy loss:-1248.0882568359375
value loss:35.22203826904297
entropies:100.17493438720703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1582.4116)
ToM Target loss= tensor(2343.9692)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1746132373809814 seconds
policy loss:-643.2086791992188
value loss:38.76474380493164
entropies:74.61282348632812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1808655261993408 seconds
policy loss:1164.2940673828125
value loss:43.607364654541016
entropies:85.53498077392578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.217099905014038 seconds
policy loss:472.7869567871094
value loss:42.84928894042969
entropies:90.00090789794922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2293353080749512 seconds
policy loss:204.07119750976562
value loss:46.47352981567383
entropies:78.5892333984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1760482788085938 seconds
policy loss:-89.31803131103516
value loss:57.91463088989258
entropies:80.16046905517578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1436.6210)
ToM Target loss= tensor(2319.0139)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2378478050231934 seconds
policy loss:-332.5048828125
value loss:26.677213668823242
entropies:89.11396789550781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2433698177337646 seconds
policy loss:-1486.9403076171875
value loss:53.83930587768555
entropies:110.92972564697266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2383899688720703 seconds
policy loss:-433.9847106933594
value loss:107.01937103271484
entropies:88.01529693603516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2360444068908691 seconds
policy loss:-1627.9561767578125
value loss:37.649742126464844
entropies:60.73720169067383
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1828546524047852 seconds
policy loss:-609.4395751953125
value loss:28.126516342163086
entropies:76.65699768066406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1447.8087)
ToM Target loss= tensor(2364.9707)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1801464557647705 seconds
policy loss:-897.0995483398438
value loss:25.8076171875
entropies:63.86905288696289
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.172154426574707 seconds
policy loss:-3441.78173828125
value loss:43.90949630737305
entropies:87.44219970703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2399928569793701 seconds
policy loss:-1720.7467041015625
value loss:29.548442840576172
entropies:75.11398315429688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2456583976745605 seconds
policy loss:-2759.14453125
value loss:53.64788055419922
entropies:75.03034973144531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2101600170135498 seconds
policy loss:-2384.165283203125
value loss:35.229007720947266
entropies:114.49193572998047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1481.5088)
ToM Target loss= tensor(2339.4250)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2464203834533691 seconds
policy loss:-1570.3306884765625
value loss:37.26241683959961
entropies:82.44779968261719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227818250656128 seconds
policy loss:193.66439819335938
value loss:49.70457077026367
entropies:78.28678894042969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1757032871246338 seconds
policy loss:944.3029174804688
value loss:42.9780158996582
entropies:99.7230224609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.166048526763916 seconds
policy loss:1657.8948974609375
value loss:53.995445251464844
entropies:89.89801788330078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1776776313781738 seconds
policy loss:-650.41455078125
value loss:43.92949295043945
entropies:75.22990417480469
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1454.4279)
ToM Target loss= tensor(2223.1943)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.186070203781128 seconds
policy loss:444.3211364746094
value loss:37.69654083251953
entropies:75.25193786621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2308292388916016 seconds
policy loss:161.0552978515625
value loss:30.492431640625
entropies:58.58637619018555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2489662170410156 seconds
policy loss:-111.08454132080078
value loss:35.72731018066406
entropies:72.85707092285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2367708683013916 seconds
policy loss:-45.33597946166992
value loss:32.41613006591797
entropies:83.60161590576172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1781322956085205 seconds
policy loss:-1524.1915283203125
value loss:66.4853744506836
entropies:79.65442657470703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1455.4463)
ToM Target loss= tensor(2647.5852)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1760528087615967 seconds
policy loss:-1833.0751953125
value loss:25.30936622619629
entropies:103.5419692993164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1860506534576416 seconds
policy loss:-1967.355712890625
value loss:30.608991622924805
entropies:104.41879272460938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1697537899017334 seconds
policy loss:-1445.8094482421875
value loss:46.41153335571289
entropies:84.1027603149414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1802833080291748 seconds
policy loss:-1032.7779541015625
value loss:50.334312438964844
entropies:75.08826446533203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2394237518310547 seconds
policy loss:-2153.390380859375
value loss:99.01655578613281
entropies:48.51911163330078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1435.5288)
ToM Target loss= tensor(2302.5684)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1637883186340332 seconds
policy loss:591.455078125
value loss:39.532745361328125
entropies:74.36580657958984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1747651100158691 seconds
policy loss:403.7069396972656
value loss:32.47557067871094
entropies:62.07027053833008
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1753108501434326 seconds
policy loss:699.4666137695312
value loss:55.30889892578125
entropies:85.77371978759766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1745986938476562 seconds
policy loss:-683.4966430664062
value loss:129.7996826171875
entropies:52.99613952636719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.217757225036621 seconds
policy loss:578.94384765625
value loss:52.34367370605469
entropies:100.43006134033203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1331.0195)
ToM Target loss= tensor(2281.5796)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2299532890319824 seconds
policy loss:-288.7212219238281
value loss:54.690792083740234
entropies:90.11962890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.247685194015503 seconds
policy loss:-822.0322875976562
value loss:60.81218338012695
entropies:83.32302856445312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2397613525390625 seconds
policy loss:-1154.934814453125
value loss:41.451416015625
entropies:82.25147247314453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2493345737457275 seconds
policy loss:-1140.85888671875
value loss:30.563764572143555
entropies:111.01652526855469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2430174350738525 seconds
policy loss:-1978.096923828125
value loss:56.616546630859375
entropies:75.53939819335938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1468.5951)
ToM Target loss= tensor(2204.0903)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1803133487701416 seconds
policy loss:-1532.6640625
value loss:106.7975082397461
entropies:75.8413314819336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1689972877502441 seconds
policy loss:-1368.4163818359375
value loss:27.346607208251953
entropies:97.28549194335938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.212632417678833 seconds
policy loss:-1074.3177490234375
value loss:50.34416961669922
entropies:108.12265014648438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.204125165939331 seconds
policy loss:-711.7250366210938
value loss:23.896198272705078
entropies:49.292869567871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1789524555206299 seconds
policy loss:-1331.1353759765625
value loss:50.24984359741211
entropies:81.87153625488281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1407.2942)
ToM Target loss= tensor(2354.2051)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2288846969604492 seconds
policy loss:45.97113800048828
value loss:52.63351058959961
entropies:78.62567138671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2014095783233643 seconds
policy loss:-581.3922729492188
value loss:49.53764724731445
entropies:80.49778747558594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1792902946472168 seconds
policy loss:-775.66162109375
value loss:37.130470275878906
entropies:83.59205627441406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.250800371170044 seconds
policy loss:351.8363037109375
value loss:32.65695571899414
entropies:80.9213638305664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1698880195617676 seconds
policy loss:-841.8383178710938
value loss:39.91632843017578
entropies:107.4775390625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1405.8174)
ToM Target loss= tensor(2326.7913)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1817407608032227 seconds
policy loss:-1605.738525390625
value loss:34.71229553222656
entropies:92.7179946899414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2376399040222168 seconds
policy loss:-2226.3857421875
value loss:22.364912033081055
entropies:100.78788757324219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1828136444091797 seconds
policy loss:-4058.277587890625
value loss:66.38146209716797
entropies:94.65142822265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2308456897735596 seconds
policy loss:-2548.267578125
value loss:70.85840606689453
entropies:79.44772338867188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2111904621124268 seconds
policy loss:-2174.321533203125
value loss:38.9671516418457
entropies:95.9703369140625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1500.2501)
ToM Target loss= tensor(2339.8801)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2396206855773926 seconds
policy loss:-1910.482666015625
value loss:35.38392639160156
entropies:77.03124237060547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1800403594970703 seconds
policy loss:-507.9805603027344
value loss:34.111446380615234
entropies:115.28142547607422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1837687492370605 seconds
policy loss:-94.65484619140625
value loss:41.26905059814453
entropies:87.69784545898438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1717994213104248 seconds
policy loss:-42.2838020324707
value loss:47.350189208984375
entropies:56.62452697753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.232320785522461 seconds
policy loss:1215.9954833984375
value loss:40.37864685058594
entropies:97.93415069580078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1402.9358)
ToM Target loss= tensor(2259.0830)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.246586799621582 seconds
policy loss:446.61846923828125
value loss:47.821258544921875
entropies:92.047119140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2371888160705566 seconds
policy loss:-792.507080078125
value loss:41.874027252197266
entropies:93.7921142578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1658813953399658 seconds
policy loss:-1270.6656494140625
value loss:37.44361114501953
entropies:92.9638671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1819899082183838 seconds
policy loss:-1995.188720703125
value loss:42.36066818237305
entropies:79.18138122558594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1800341606140137 seconds
policy loss:-465.55657958984375
value loss:29.924423217773438
entropies:61.38949966430664
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1441.4033)
ToM Target loss= tensor(2270.7366)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1997485160827637 seconds
policy loss:199.04135131835938
value loss:17.416698455810547
entropies:58.68211364746094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1720921993255615 seconds
policy loss:-1939.6141357421875
value loss:57.390533447265625
entropies:63.60532760620117
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1767964363098145 seconds
policy loss:-1554.2415771484375
value loss:37.53794860839844
entropies:76.02308654785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2153441905975342 seconds
policy loss:-894.8363647460938
value loss:34.13065719604492
entropies:68.25283813476562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2418880462646484 seconds
policy loss:320.2894592285156
value loss:33.45680236816406
entropies:99.16032409667969
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1280.9415)
ToM Target loss= tensor(2277.2698)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1861493587493896 seconds
policy loss:-1408.3133544921875
value loss:60.54902267456055
entropies:106.46125030517578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2257235050201416 seconds
policy loss:-1477.2265625
value loss:40.0871467590332
entropies:102.00900268554688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1790101528167725 seconds
policy loss:114.68209838867188
value loss:34.7681884765625
entropies:59.86960983276367
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1741001605987549 seconds
policy loss:-569.2445678710938
value loss:57.63562774658203
entropies:85.09027099609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1806883811950684 seconds
policy loss:-698.0547485351562
value loss:43.71828842163086
entropies:51.19059753417969
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1362.9407)
ToM Target loss= tensor(2161.2852)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2423691749572754 seconds
policy loss:-1373.4337158203125
value loss:33.915855407714844
entropies:64.91358947753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2196159362792969 seconds
policy loss:-367.7146301269531
value loss:44.18098831176758
entropies:75.17251586914062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2326595783233643 seconds
policy loss:-2401.8505859375
value loss:52.04793167114258
entropies:74.75369262695312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177487850189209 seconds
policy loss:-1451.8233642578125
value loss:22.44376564025879
entropies:64.94102478027344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.22835373878479 seconds
policy loss:301.4775085449219
value loss:14.638124465942383
entropies:57.462974548339844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1295.8569)
ToM Target loss= tensor(2211.7837)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1750075817108154 seconds
policy loss:-1778.1966552734375
value loss:60.17470932006836
entropies:80.4192123413086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2334518432617188 seconds
policy loss:-1806.572265625
value loss:39.85048294067383
entropies:94.46739959716797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1648585796356201 seconds
policy loss:-808.4746704101562
value loss:33.0286979675293
entropies:76.1833724975586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2048523426055908 seconds
policy loss:409.04766845703125
value loss:41.49968719482422
entropies:86.26620483398438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1800282001495361 seconds
policy loss:-858.046630859375
value loss:59.614044189453125
entropies:85.19483184814453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1426.8175)
ToM Target loss= tensor(2233.3069)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.218822717666626 seconds
policy loss:990.0361938476562
value loss:34.97783660888672
entropies:74.88371276855469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2120928764343262 seconds
policy loss:1875.4000244140625
value loss:48.30515670776367
entropies:65.62268829345703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2322523593902588 seconds
policy loss:-194.60977172851562
value loss:48.584930419921875
entropies:39.10277557373047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1692991256713867 seconds
policy loss:551.1739501953125
value loss:32.89876174926758
entropies:79.61479949951172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.180420160293579 seconds
policy loss:-90.33826446533203
value loss:79.64141082763672
entropies:87.83467864990234
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1363.8079)
ToM Target loss= tensor(2226.7742)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1811227798461914 seconds
policy loss:-316.4776306152344
value loss:47.713218688964844
entropies:56.92285919189453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2372517585754395 seconds
policy loss:-1238.0958251953125
value loss:31.744909286499023
entropies:68.50933837890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1738314628601074 seconds
policy loss:-2067.142333984375
value loss:47.46290969848633
entropies:89.7385025024414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.211212396621704 seconds
policy loss:-603.4508056640625
value loss:16.31954574584961
entropies:68.69331359863281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1824030876159668 seconds
policy loss:-717.752685546875
value loss:24.544633865356445
entropies:70.58259582519531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1293.7939)
ToM Target loss= tensor(2297.9231)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2461566925048828 seconds
policy loss:-2662.01220703125
value loss:46.71696472167969
entropies:98.60177612304688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176103115081787 seconds
policy loss:-2989.1953125
value loss:101.59867095947266
entropies:80.64547729492188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1780500411987305 seconds
policy loss:-936.4086303710938
value loss:24.13129997253418
entropies:60.09415054321289
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1753928661346436 seconds
policy loss:-54.90437698364258
value loss:34.54212188720703
entropies:88.87274169921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.235257625579834 seconds
policy loss:-1383.4093017578125
value loss:37.395240783691406
entropies:82.78607177734375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1534.4585)
ToM Target loss= tensor(2225.3481)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1752171516418457 seconds
policy loss:-210.9962615966797
value loss:44.40980911254883
entropies:108.82027435302734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1703236103057861 seconds
policy loss:-182.66392517089844
value loss:38.91847610473633
entropies:69.76075744628906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1745152473449707 seconds
policy loss:-1551.1815185546875
value loss:25.944068908691406
entropies:62.30238342285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1772489547729492 seconds
policy loss:-1761.4842529296875
value loss:35.1793327331543
entropies:102.99596405029297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1748578548431396 seconds
policy loss:-1157.398193359375
value loss:29.553133010864258
entropies:86.40967559814453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1412.2708)
ToM Target loss= tensor(2420.3306)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2255055904388428 seconds
policy loss:-1200.791259765625
value loss:49.895111083984375
entropies:76.30778503417969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2177751064300537 seconds
policy loss:532.6985473632812
value loss:38.676239013671875
entropies:79.84711456298828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1808724403381348 seconds
policy loss:-199.72608947753906
value loss:43.36431884765625
entropies:72.4643325805664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1985094547271729 seconds
policy loss:-644.0518798828125
value loss:45.76567840576172
entropies:58.78533935546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2238409519195557 seconds
policy loss:-1503.789794921875
value loss:47.89828109741211
entropies:74.2550048828125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1504.1335)
ToM Target loss= tensor(2351.4729)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1766445636749268 seconds
policy loss:-1808.4417724609375
value loss:47.63487243652344
entropies:79.43746185302734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2419655323028564 seconds
policy loss:-666.8491821289062
value loss:37.31575393676758
entropies:49.0887565612793
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1815378665924072 seconds
policy loss:-2201.93505859375
value loss:65.52705383300781
entropies:72.78255462646484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1823921203613281 seconds
policy loss:-1084.80712890625
value loss:32.70659637451172
entropies:65.60082244873047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.246612787246704 seconds
policy loss:-593.5902709960938
value loss:39.31446838378906
entropies:62.369911193847656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1228.8987)
ToM Target loss= tensor(2260.5347)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.177215337753296 seconds
policy loss:-77.18467712402344
value loss:56.93143844604492
entropies:56.141944885253906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2007129192352295 seconds
policy loss:-1899.1405029296875
value loss:68.69071197509766
entropies:78.31889343261719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.182002305984497 seconds
policy loss:737.3399047851562
value loss:49.56534957885742
entropies:93.37418365478516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1833839416503906 seconds
policy loss:928.2003784179688
value loss:47.77157211303711
entropies:62.42548751831055
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2193918228149414 seconds
policy loss:82.09678649902344
value loss:56.063865661621094
entropies:69.34272766113281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1380.7830)
ToM Target loss= tensor(2226.0474)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1796679496765137 seconds
policy loss:188.6580352783203
value loss:60.961429595947266
entropies:88.59347534179688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1787724494934082 seconds
policy loss:297.25897216796875
value loss:58.898948669433594
entropies:31.21622085571289
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2282757759094238 seconds
policy loss:-71.35143280029297
value loss:54.578285217285156
entropies:101.08826446533203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1814415454864502 seconds
policy loss:161.15899658203125
value loss:41.525604248046875
entropies:56.90469741821289
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1793067455291748 seconds
policy loss:492.6212158203125
value loss:28.022096633911133
entropies:65.36666870117188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1360.2312)
ToM Target loss= tensor(2238.3447)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2080562114715576 seconds
policy loss:-894.0711059570312
value loss:54.7882194519043
entropies:65.7151107788086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2101702690124512 seconds
policy loss:-29.175365447998047
value loss:52.62687683105469
entropies:60.134246826171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1847751140594482 seconds
policy loss:-1441.4044189453125
value loss:32.871543884277344
entropies:82.00570678710938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2454516887664795 seconds
policy loss:-574.3728637695312
value loss:35.5440788269043
entropies:60.4675407409668
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1978545188903809 seconds
policy loss:-510.9732666015625
value loss:57.81285095214844
entropies:63.88197326660156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1271.2810)
ToM Target loss= tensor(2223.6255)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2209267616271973 seconds
policy loss:-695.8897705078125
value loss:25.87574577331543
entropies:71.98436737060547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1783268451690674 seconds
policy loss:-561.3631591796875
value loss:17.531219482421875
entropies:59.56085968017578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1755759716033936 seconds
policy loss:-1450.0389404296875
value loss:263.4627685546875
entropies:42.264869689941406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1834361553192139 seconds
policy loss:-1379.00927734375
value loss:38.4357795715332
entropies:53.457542419433594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.236339807510376 seconds
policy loss:-675.93017578125
value loss:46.698020935058594
entropies:77.14251708984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1287.8040)
ToM Target loss= tensor(2292.5972)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2352564334869385 seconds
policy loss:-207.45672607421875
value loss:31.37716293334961
entropies:76.69931030273438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.175896406173706 seconds
policy loss:-876.0390625
value loss:38.04969787597656
entropies:85.98065948486328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2381818294525146 seconds
policy loss:-662.7677612304688
value loss:30.823928833007812
entropies:71.4254379272461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1759569644927979 seconds
policy loss:223.7041778564453
value loss:46.70069122314453
entropies:67.78054809570312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2159547805786133 seconds
policy loss:860.8952026367188
value loss:32.822425842285156
entropies:84.94244384765625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1399.2083)
ToM Target loss= tensor(2171.9546)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2250313758850098 seconds
policy loss:-1027.192138671875
value loss:40.11051940917969
entropies:60.97510528564453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2371139526367188 seconds
policy loss:9.630666732788086
value loss:46.123451232910156
entropies:87.44735717773438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1950678825378418 seconds
policy loss:153.8159637451172
value loss:23.181440353393555
entropies:74.6977767944336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1874570846557617 seconds
policy loss:-1142.93505859375
value loss:51.85676956176758
entropies:69.07553100585938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.222855567932129 seconds
policy loss:-1454.146484375
value loss:26.196866989135742
entropies:70.34619903564453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1399.3541)
ToM Target loss= tensor(2309.8569)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2384207248687744 seconds
policy loss:-736.5390014648438
value loss:20.218217849731445
entropies:66.32589721679688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2428548336029053 seconds
policy loss:-1236.3475341796875
value loss:25.952709197998047
entropies:61.25007247924805
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2073543071746826 seconds
policy loss:-1324.3443603515625
value loss:26.839733123779297
entropies:76.46392059326172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.19598388671875 seconds
policy loss:-1323.932861328125
value loss:35.73859786987305
entropies:68.396240234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.183551549911499 seconds
policy loss:-980.5494995117188
value loss:30.641036987304688
entropies:85.24443817138672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1353.6497)
ToM Target loss= tensor(2309.1494)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2386417388916016 seconds
policy loss:-3.205286979675293
value loss:40.09583282470703
entropies:76.98664093017578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.215005874633789 seconds
policy loss:-48.691410064697266
value loss:29.91187286376953
entropies:81.27552032470703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2291393280029297 seconds
policy loss:-157.92752075195312
value loss:35.31785583496094
entropies:87.3431625366211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2247302532196045 seconds
policy loss:-1205.750244140625
value loss:119.2883529663086
entropies:60.34904479980469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2405903339385986 seconds
policy loss:-1217.559326171875
value loss:37.42510223388672
entropies:81.1943588256836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1324.1519)
ToM Target loss= tensor(2325.1453)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1739130020141602 seconds
policy loss:-813.2871704101562
value loss:25.24660301208496
entropies:75.77635192871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.243293046951294 seconds
policy loss:-1755.0133056640625
value loss:37.8886604309082
entropies:81.64315032958984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1792502403259277 seconds
policy loss:-852.3671875
value loss:23.85758399963379
entropies:86.65347290039062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2377924919128418 seconds
policy loss:-43.00873947143555
value loss:25.991037368774414
entropies:98.59561157226562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1825039386749268 seconds
policy loss:-1649.567626953125
value loss:20.16775894165039
entropies:81.60890197753906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1442.7838)
ToM Target loss= tensor(2392.9189)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1693429946899414 seconds
policy loss:-1732.2647705078125
value loss:33.98124313354492
entropies:99.6463623046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2081685066223145 seconds
policy loss:-1278.4632568359375
value loss:38.29021072387695
entropies:73.44546508789062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1784131526947021 seconds
policy loss:-1759.2332763671875
value loss:42.095306396484375
entropies:78.39898681640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2235474586486816 seconds
policy loss:-505.6363830566406
value loss:17.714862823486328
entropies:45.23479461669922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2545654773712158 seconds
policy loss:421.1348571777344
value loss:27.51646614074707
entropies:63.33462142944336
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1363.0867)
ToM Target loss= tensor(2322.7236)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1775877475738525 seconds
policy loss:-1382.260986328125
value loss:52.987640380859375
entropies:76.64886474609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.248168706893921 seconds
policy loss:-264.119873046875
value loss:33.42124938964844
entropies:87.19517517089844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1810319423675537 seconds
policy loss:750.6602783203125
value loss:47.4686164855957
entropies:94.34783172607422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1644508838653564 seconds
policy loss:-401.500732421875
value loss:25.56344985961914
entropies:69.5918960571289
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2321646213531494 seconds
policy loss:57.62916946411133
value loss:27.877384185791016
entropies:57.038169860839844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1456.3611)
ToM Target loss= tensor(2438.4241)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.232389211654663 seconds
policy loss:-1489.1439208984375
value loss:37.13427734375
entropies:72.9049072265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2147314548492432 seconds
policy loss:-2338.005615234375
value loss:37.55827331542969
entropies:89.3111572265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2253949642181396 seconds
policy loss:-3176.29443359375
value loss:46.122886657714844
entropies:90.2193832397461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1809718608856201 seconds
policy loss:-3614.7041015625
value loss:83.84957885742188
entropies:62.25965118408203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1813302040100098 seconds
policy loss:-1933.2772216796875
value loss:23.489450454711914
entropies:85.27122497558594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1380.3539)
ToM Target loss= tensor(2327.1926)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2218928337097168 seconds
policy loss:-2000.3194580078125
value loss:25.081296920776367
entropies:66.59916687011719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.224942684173584 seconds
policy loss:-697.547607421875
value loss:17.055980682373047
entropies:58.30792236328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2425618171691895 seconds
policy loss:-233.54559326171875
value loss:27.230892181396484
entropies:78.00899505615234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1766974925994873 seconds
policy loss:-67.9322738647461
value loss:53.081092834472656
entropies:84.94310760498047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1743102073669434 seconds
policy loss:-43.092769622802734
value loss:37.60475158691406
entropies:76.72400665283203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1328.0654)
ToM Target loss= tensor(2345.8884)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2157416343688965 seconds
policy loss:945.0549926757812
value loss:38.65757369995117
entropies:62.707008361816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1732969284057617 seconds
policy loss:1469.5794677734375
value loss:37.389732360839844
entropies:91.26863098144531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2375493049621582 seconds
policy loss:-160.917724609375
value loss:29.43603515625
entropies:81.41908264160156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1862993240356445 seconds
policy loss:-1276.66845703125
value loss:41.70401382446289
entropies:81.24151611328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1751606464385986 seconds
policy loss:245.04965209960938
value loss:14.920587539672852
entropies:61.0844612121582
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1374.1381)
ToM Target loss= tensor(2200.7432)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2037160396575928 seconds
policy loss:-1308.8077392578125
value loss:31.2728214263916
entropies:101.79478454589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.240398645401001 seconds
policy loss:-1449.220703125
value loss:44.61064529418945
entropies:57.257225036621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1735610961914062 seconds
policy loss:-2515.227294921875
value loss:65.90438079833984
entropies:73.5594711303711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2360115051269531 seconds
policy loss:-1546.0247802734375
value loss:28.002737045288086
entropies:56.78801727294922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.221571445465088 seconds
policy loss:-403.572021484375
value loss:21.935462951660156
entropies:40.35910415649414
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1310.9987)
ToM Target loss= tensor(2288.0503)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1855597496032715 seconds
policy loss:-405.52490234375
value loss:19.403343200683594
entropies:67.17573547363281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1731505393981934 seconds
policy loss:761.5626220703125
value loss:21.209360122680664
entropies:68.15428161621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2432808876037598 seconds
policy loss:-1676.9654541015625
value loss:22.949546813964844
entropies:58.69001388549805
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.180112361907959 seconds
policy loss:-352.15838623046875
value loss:9.43372631072998
entropies:50.19647979736328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1486358642578125 seconds
policy loss:-1312.3123779296875
value loss:24.543621063232422
entropies:76.6830825805664
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1271.5125)
ToM Target loss= tensor(2245.8044)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.22111177444458 seconds
policy loss:-801.9295043945312
value loss:54.76042938232422
entropies:63.68064498901367
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1881327629089355 seconds
policy loss:-1061.7362060546875
value loss:23.53986167907715
entropies:58.58539962768555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2394731044769287 seconds
policy loss:-1299.6212158203125
value loss:37.06624221801758
entropies:75.41669464111328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1719434261322021 seconds
policy loss:-911.8778076171875
value loss:25.04404640197754
entropies:78.82368469238281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.186739206314087 seconds
policy loss:-928.816162109375
value loss:54.62868118286133
entropies:73.84906768798828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1268.2290)
ToM Target loss= tensor(2158.3035)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.186307430267334 seconds
policy loss:310.2503356933594
value loss:57.35747146606445
entropies:61.763214111328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2356865406036377 seconds
policy loss:266.7022705078125
value loss:53.461116790771484
entropies:63.00977325439453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1776211261749268 seconds
policy loss:-480.20550537109375
value loss:53.19295883178711
entropies:78.56389617919922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176377773284912 seconds
policy loss:413.4833984375
value loss:35.362510681152344
entropies:57.12357711791992
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.229659080505371 seconds
policy loss:883.3089599609375
value loss:29.76003074645996
entropies:64.61115264892578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1281.5847)
ToM Target loss= tensor(2186.0112)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2363109588623047 seconds
policy loss:-222.59112548828125
value loss:19.219627380371094
entropies:77.08977508544922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.234459400177002 seconds
policy loss:-289.4391784667969
value loss:27.467105865478516
entropies:70.45796966552734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2028770446777344 seconds
policy loss:-1310.2320556640625
value loss:25.095975875854492
entropies:64.59508514404297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2074639797210693 seconds
policy loss:-2499.701904296875
value loss:51.03495788574219
entropies:77.74772644042969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2364988327026367 seconds
policy loss:-2057.18798828125
value loss:78.68890380859375
entropies:53.86397171020508
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1345.3997)
ToM Target loss= tensor(2297.7820)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2394933700561523 seconds
policy loss:-1335.266845703125
value loss:28.70505714416504
entropies:57.03854751586914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1784770488739014 seconds
policy loss:-2776.81884765625
value loss:47.04646682739258
entropies:77.53093719482422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2355453968048096 seconds
policy loss:-1424.094970703125
value loss:49.68837356567383
entropies:49.77631378173828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.185171365737915 seconds
policy loss:-1062.701416015625
value loss:38.92937469482422
entropies:54.761138916015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2193012237548828 seconds
policy loss:-384.5711669921875
value loss:36.165409088134766
entropies:85.6738052368164
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1400.2257)
ToM Target loss= tensor(2311.0005)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.172656536102295 seconds
policy loss:13.794841766357422
value loss:28.81355094909668
entropies:43.69267272949219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1655831336975098 seconds
policy loss:68.40892028808594
value loss:46.63873291015625
entropies:69.67721557617188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1803510189056396 seconds
policy loss:1410.320556640625
value loss:68.27848815917969
entropies:97.24903869628906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2281835079193115 seconds
policy loss:1324.9161376953125
value loss:46.39908218383789
entropies:78.76555633544922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2416975498199463 seconds
policy loss:533.8594970703125
value loss:34.129417419433594
entropies:83.8364486694336
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1411.8533)
ToM Target loss= tensor(2351.5320)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2289397716522217 seconds
policy loss:-291.8188781738281
value loss:56.85693359375
entropies:96.9210433959961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2320044040679932 seconds
policy loss:-1332.4920654296875
value loss:37.20683670043945
entropies:67.11107635498047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2536578178405762 seconds
policy loss:-833.3004150390625
value loss:29.557384490966797
entropies:111.49832916259766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2251946926116943 seconds
policy loss:-606.4143676757812
value loss:38.09913635253906
entropies:65.06212615966797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1785101890563965 seconds
policy loss:-737.816162109375
value loss:17.01984977722168
entropies:47.02162170410156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1445.8282)
ToM Target loss= tensor(2399.3064)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2417850494384766 seconds
policy loss:-1815.3192138671875
value loss:35.85735321044922
entropies:61.751461029052734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1780314445495605 seconds
policy loss:31.502784729003906
value loss:50.47224807739258
entropies:48.956172943115234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2366108894348145 seconds
policy loss:-334.4475402832031
value loss:30.850555419921875
entropies:51.998714447021484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2480437755584717 seconds
policy loss:-1191.56494140625
value loss:25.79962158203125
entropies:116.78972625732422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2771427631378174 seconds
policy loss:-824.1339721679688
value loss:21.65364646911621
entropies:64.55768585205078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1379.2771)
ToM Target loss= tensor(2287.6038)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.186420202255249 seconds
policy loss:-1105.173095703125
value loss:53.287010192871094
entropies:64.90349578857422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1719496250152588 seconds
policy loss:-1537.6575927734375
value loss:56.86772155761719
entropies:49.79605484008789
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2286937236785889 seconds
policy loss:346.9477233886719
value loss:42.46250915527344
entropies:88.70699310302734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1831181049346924 seconds
policy loss:-405.292724609375
value loss:48.673091888427734
entropies:78.27288818359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2369346618652344 seconds
policy loss:1022.1246948242188
value loss:73.6226806640625
entropies:59.54078674316406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1332.7496)
ToM Target loss= tensor(2292.9292)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2262730598449707 seconds
policy loss:874.514892578125
value loss:44.241859436035156
entropies:55.855865478515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2433979511260986 seconds
policy loss:271.48736572265625
value loss:34.94413375854492
entropies:83.61228942871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2431590557098389 seconds
policy loss:61.2213134765625
value loss:27.313520431518555
entropies:82.59703063964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2462468147277832 seconds
policy loss:-606.2208251953125
value loss:30.99254035949707
entropies:61.063682556152344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1723322868347168 seconds
policy loss:-2052.36279296875
value loss:29.629440307617188
entropies:73.39530181884766
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1526.3489)
ToM Target loss= tensor(2293.0579)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.226609468460083 seconds
policy loss:-780.7109985351562
value loss:29.70551109313965
entropies:70.70946502685547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2187182903289795 seconds
policy loss:-2829.436767578125
value loss:43.67564392089844
entropies:83.03372192382812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2108759880065918 seconds
policy loss:-1848.40673828125
value loss:33.89311981201172
entropies:67.66301727294922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1902854442596436 seconds
policy loss:-1100.9976806640625
value loss:45.25861740112305
entropies:68.63468933105469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1991503238677979 seconds
policy loss:-646.5805053710938
value loss:39.57825469970703
entropies:60.53376388549805
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1401.4866)
ToM Target loss= tensor(2287.9517)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2290668487548828 seconds
policy loss:-1379.736083984375
value loss:44.431034088134766
entropies:67.4936752319336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1955921649932861 seconds
policy loss:-544.7218627929688
value loss:40.157005310058594
entropies:67.29991149902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2338995933532715 seconds
policy loss:-167.3552703857422
value loss:40.833763122558594
entropies:92.68114471435547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2000377178192139 seconds
policy loss:221.84567260742188
value loss:30.478811264038086
entropies:65.79857635498047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1855113506317139 seconds
policy loss:-445.2163391113281
value loss:25.22509002685547
entropies:77.78062438964844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1393.5524)
ToM Target loss= tensor(2165.0374)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1833760738372803 seconds
policy loss:146.10801696777344
value loss:29.806747436523438
entropies:76.54122924804688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1799476146697998 seconds
policy loss:-348.09698486328125
value loss:20.945547103881836
entropies:53.15008544921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1830320358276367 seconds
policy loss:-1424.093505859375
value loss:66.74357604980469
entropies:60.83447265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2336766719818115 seconds
policy loss:-1734.9454345703125
value loss:33.01814270019531
entropies:55.71931457519531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2272353172302246 seconds
policy loss:-2706.26025390625
value loss:53.14909744262695
entropies:80.3763198852539
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1335.9460)
ToM Target loss= tensor(2227.7710)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1841533184051514 seconds
policy loss:-3668.519287109375
value loss:65.97531127929688
entropies:91.18296813964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1757586002349854 seconds
policy loss:-5296.5830078125
value loss:72.8096923828125
entropies:91.82716369628906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2388169765472412 seconds
policy loss:-3265.46435546875
value loss:52.50689697265625
entropies:86.12269592285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.229924201965332 seconds
policy loss:-1078.8392333984375
value loss:18.138626098632812
entropies:69.82537078857422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1905341148376465 seconds
policy loss:-1057.71875
value loss:19.103591918945312
entropies:44.7104606628418
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1407.7769)
ToM Target loss= tensor(2264.0706)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2470176219940186 seconds
policy loss:-1451.2391357421875
value loss:34.64057922363281
entropies:88.46894836425781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.191309928894043 seconds
policy loss:280.9134216308594
value loss:38.080284118652344
entropies:85.20144653320312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1743528842926025 seconds
policy loss:1142.049072265625
value loss:39.525577545166016
entropies:74.92256164550781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2241506576538086 seconds
policy loss:872.8379516601562
value loss:58.63074493408203
entropies:58.841941833496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2377946376800537 seconds
policy loss:-372.8421630859375
value loss:53.3495979309082
entropies:73.58149719238281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1351.4182)
ToM Target loss= tensor(2156.8865)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2092030048370361 seconds
policy loss:590.9171752929688
value loss:41.25775146484375
entropies:70.86638641357422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.241328477859497 seconds
policy loss:1937.676513671875
value loss:54.12199401855469
entropies:73.02798461914062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1823351383209229 seconds
policy loss:-168.3113555908203
value loss:35.89196014404297
entropies:88.3233871459961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.238804817199707 seconds
policy loss:341.5549621582031
value loss:15.236953735351562
entropies:57.82224655151367
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2443382740020752 seconds
policy loss:-3114.3603515625
value loss:104.72637939453125
entropies:82.0266342163086
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1456.8011)
ToM Target loss= tensor(2247.5269)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1922078132629395 seconds
policy loss:-3338.797607421875
value loss:41.60286331176758
entropies:58.699913024902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2681176662445068 seconds
policy loss:-1963.0567626953125
value loss:43.20913314819336
entropies:56.24464416503906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2321393489837646 seconds
policy loss:-2120.138427734375
value loss:37.39046096801758
entropies:68.17575073242188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.186159610748291 seconds
policy loss:-2359.185546875
value loss:80.21994018554688
entropies:62.27578353881836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.234947681427002 seconds
policy loss:-470.1183776855469
value loss:31.866500854492188
entropies:69.29142761230469
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1293.3729)
ToM Target loss= tensor(2264.9111)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2614026069641113 seconds
policy loss:-1196.91064453125
value loss:36.21030044555664
entropies:61.537315368652344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1838085651397705 seconds
policy loss:-1936.0654296875
value loss:46.80242919921875
entropies:71.8662338256836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1803274154663086 seconds
policy loss:-997.0180053710938
value loss:44.522438049316406
entropies:81.40599822998047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.186286449432373 seconds
policy loss:-316.14886474609375
value loss:48.8365364074707
entropies:61.748252868652344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2265021800994873 seconds
policy loss:1321.3653564453125
value loss:35.163818359375
entropies:74.9900131225586
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1389.2109)
ToM Target loss= tensor(2325.8816)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1436028480529785 seconds
policy loss:963.533447265625
value loss:32.12802505493164
entropies:84.00045013427734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1857662200927734 seconds
policy loss:260.6055603027344
value loss:35.32965850830078
entropies:75.82877349853516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2177097797393799 seconds
policy loss:-734.8469848632812
value loss:25.150814056396484
entropies:87.16712951660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2259571552276611 seconds
policy loss:-1475.365966796875
value loss:27.189647674560547
entropies:79.57228088378906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1935348510742188 seconds
policy loss:-1878.0997314453125
value loss:32.66782760620117
entropies:90.61469268798828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1510.7697)
ToM Target loss= tensor(2211.9541)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.172152042388916 seconds
policy loss:-3700.10107421875
value loss:65.47981262207031
entropies:88.54371643066406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1927375793457031 seconds
policy loss:-1596.04052734375
value loss:24.97137451171875
entropies:51.138648986816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2286202907562256 seconds
policy loss:-1351.853515625
value loss:22.540843963623047
entropies:68.48822784423828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1747767925262451 seconds
policy loss:-526.7299194335938
value loss:19.916240692138672
entropies:78.1204833984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2027738094329834 seconds
policy loss:-1605.703369140625
value loss:43.874027252197266
entropies:100.56413269042969
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1441.1407)
ToM Target loss= tensor(2317.9795)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.184614658355713 seconds
policy loss:-692.6331176757812
value loss:34.19617462158203
entropies:54.9006233215332
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1738331317901611 seconds
policy loss:-1714.14892578125
value loss:76.21005249023438
entropies:66.20700073242188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.170360803604126 seconds
policy loss:-1424.2171630859375
value loss:39.52488708496094
entropies:88.82611083984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2168354988098145 seconds
policy loss:-13.526488304138184
value loss:34.46110534667969
entropies:59.912109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1786308288574219 seconds
policy loss:-235.08999633789062
value loss:40.48767852783203
entropies:56.010501861572266
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1276.8159)
ToM Target loss= tensor(2237.7615)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2404694557189941 seconds
policy loss:423.6877136230469
value loss:33.34050369262695
entropies:74.5356674194336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2368807792663574 seconds
policy loss:-824.5640869140625
value loss:31.458553314208984
entropies:66.82179260253906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2194983959197998 seconds
policy loss:-2353.960693359375
value loss:38.21224594116211
entropies:66.29539489746094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2514259815216064 seconds
policy loss:-1786.5155029296875
value loss:37.511024475097656
entropies:64.2443618774414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.24251127243042 seconds
policy loss:-919.5045776367188
value loss:23.36003875732422
entropies:64.96836853027344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1281.4005)
ToM Target loss= tensor(2141.4727)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.244624376296997 seconds
policy loss:-2630.7880859375
value loss:51.160072326660156
entropies:68.94839477539062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.22279953956604 seconds
policy loss:-1429.0146484375
value loss:55.749393463134766
entropies:63.737548828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2010111808776855 seconds
policy loss:-1174.4285888671875
value loss:43.4277229309082
entropies:65.64234924316406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.199751853942871 seconds
policy loss:-1401.3350830078125
value loss:27.007797241210938
entropies:71.7607421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2404747009277344 seconds
policy loss:-919.23974609375
value loss:44.613162994384766
entropies:65.24110412597656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1370.1556)
ToM Target loss= tensor(2199.4907)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1880571842193604 seconds
policy loss:659.18896484375
value loss:36.381126403808594
entropies:64.62621307373047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1871542930603027 seconds
policy loss:122.91414642333984
value loss:52.782867431640625
entropies:69.49072265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2513372898101807 seconds
policy loss:721.9068603515625
value loss:41.10154724121094
entropies:72.12458801269531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2159302234649658 seconds
policy loss:233.49281311035156
value loss:51.537109375
entropies:73.2909927368164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2802386283874512 seconds
policy loss:-847.254150390625
value loss:40.21771240234375
entropies:52.231754302978516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1331.7172)
ToM Target loss= tensor(2192.0923)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.199686050415039 seconds
policy loss:330.0069580078125
value loss:29.022125244140625
entropies:69.44734191894531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2363710403442383 seconds
policy loss:-1179.25341796875
value loss:43.230064392089844
entropies:62.493751525878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2316713333129883 seconds
policy loss:-639.3973388671875
value loss:24.45529556274414
entropies:59.155853271484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2334673404693604 seconds
policy loss:-1805.212890625
value loss:42.365055084228516
entropies:69.55361938476562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2031893730163574 seconds
policy loss:-1706.3656005859375
value loss:35.43659210205078
entropies:74.1535415649414
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1447.8989)
ToM Target loss= tensor(2251.4087)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1768519878387451 seconds
policy loss:-582.6183471679688
value loss:11.722225189208984
entropies:45.56572723388672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2032876014709473 seconds
policy loss:-1719.96337890625
value loss:28.494098663330078
entropies:73.91797637939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2394275665283203 seconds
policy loss:-2072.094970703125
value loss:42.98204040527344
entropies:60.13551330566406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.223189115524292 seconds
policy loss:-593.208984375
value loss:18.586328506469727
entropies:66.11192321777344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1743261814117432 seconds
policy loss:-725.87060546875
value loss:16.272382736206055
entropies:69.26515197753906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1348.6584)
ToM Target loss= tensor(2276.8125)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2415025234222412 seconds
policy loss:-379.52655029296875
value loss:23.255006790161133
entropies:61.61590576171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2295036315917969 seconds
policy loss:-1599.1092529296875
value loss:38.810211181640625
entropies:71.8880386352539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2061169147491455 seconds
policy loss:-1089.9091796875
value loss:63.77690124511719
entropies:99.02841186523438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2130382061004639 seconds
policy loss:909.6490478515625
value loss:31.364463806152344
entropies:71.01201629638672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1694591045379639 seconds
policy loss:630.522216796875
value loss:28.522375106811523
entropies:68.08735656738281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1382.4111)
ToM Target loss= tensor(2143.3132)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2447125911712646 seconds
policy loss:-321.47283935546875
value loss:37.99222183227539
entropies:72.14311218261719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.226909875869751 seconds
policy loss:-1855.794189453125
value loss:65.77288055419922
entropies:78.45929718017578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2074785232543945 seconds
policy loss:509.0899963378906
value loss:33.8613166809082
entropies:63.90856170654297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2438650131225586 seconds
policy loss:-23.917356491088867
value loss:25.132434844970703
entropies:60.132659912109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1927742958068848 seconds
policy loss:213.21856689453125
value loss:17.13559341430664
entropies:62.3594856262207
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1316.2157)
ToM Target loss= tensor(2155.8987)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1712279319763184 seconds
policy loss:-829.981689453125
value loss:14.945684432983398
entropies:69.10326385498047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2352612018585205 seconds
policy loss:-826.7481689453125
value loss:28.584178924560547
entropies:90.49322509765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2379300594329834 seconds
policy loss:-4397.05126953125
value loss:89.72806549072266
entropies:63.42011260986328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.242140293121338 seconds
policy loss:-1429.6661376953125
value loss:41.7282829284668
entropies:74.26173400878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1862194538116455 seconds
policy loss:-1869.5875244140625
value loss:24.711143493652344
entropies:81.74608612060547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1479.2020)
ToM Target loss= tensor(2243.7690)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2372157573699951 seconds
policy loss:-1103.417236328125
value loss:25.836084365844727
entropies:46.64885330200195
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2406108379364014 seconds
policy loss:-731.3516845703125
value loss:30.976543426513672
entropies:73.0246353149414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2503719329833984 seconds
policy loss:-807.1491088867188
value loss:31.101179122924805
entropies:61.55340576171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.242300271987915 seconds
policy loss:577.7699584960938
value loss:53.54482650756836
entropies:62.00868225097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1754348278045654 seconds
policy loss:741.7232055664062
value loss:29.541290283203125
entropies:54.117530822753906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1352.8062)
ToM Target loss= tensor(2190.7864)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2366714477539062 seconds
policy loss:-312.3252868652344
value loss:29.4534854888916
entropies:55.09033203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1801989078521729 seconds
policy loss:-97.48614501953125
value loss:30.63243865966797
entropies:70.76825714111328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1765508651733398 seconds
policy loss:-1643.8544921875
value loss:22.681291580200195
entropies:70.47441101074219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2401306629180908 seconds
policy loss:-843.21435546875
value loss:17.474693298339844
entropies:44.213043212890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2373900413513184 seconds
policy loss:-1199.343994140625
value loss:35.60871505737305
entropies:69.04846954345703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1342.2917)
ToM Target loss= tensor(2206.7285)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2418906688690186 seconds
policy loss:-923.4168701171875
value loss:33.16679763793945
entropies:75.65290832519531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2384302616119385 seconds
policy loss:-859.255615234375
value loss:28.12110137939453
entropies:61.034183502197266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.179389476776123 seconds
policy loss:-1057.8673095703125
value loss:31.89837074279785
entropies:87.45172119140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2385849952697754 seconds
policy loss:-889.0586547851562
value loss:39.949066162109375
entropies:73.31014251708984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2241642475128174 seconds
policy loss:-323.143310546875
value loss:21.416091918945312
entropies:68.28681945800781
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1376.0548)
ToM Target loss= tensor(2214.4736)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2355351448059082 seconds
policy loss:-395.7635803222656
value loss:21.791589736938477
entropies:70.37238311767578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2330474853515625 seconds
policy loss:-1162.020263671875
value loss:28.293703079223633
entropies:49.885337829589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1758699417114258 seconds
policy loss:-985.1089477539062
value loss:31.79391098022461
entropies:81.54718017578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.239173173904419 seconds
policy loss:-1594.9388427734375
value loss:35.18705749511719
entropies:101.12651062011719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2392199039459229 seconds
policy loss:-561.85888671875
value loss:20.219770431518555
entropies:54.94417190551758
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1369.4243)
ToM Target loss= tensor(2318.7361)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.183406114578247 seconds
policy loss:-1245.890380859375
value loss:22.191078186035156
entropies:71.28470611572266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.218956708908081 seconds
policy loss:-1132.9041748046875
value loss:27.205970764160156
entropies:71.74832916259766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2527105808258057 seconds
policy loss:146.2340545654297
value loss:26.334575653076172
entropies:74.29436492919922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.240915060043335 seconds
policy loss:-383.1454772949219
value loss:14.578448295593262
entropies:58.904266357421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2029926776885986 seconds
policy loss:334.7401123046875
value loss:21.13531494140625
entropies:85.07257843017578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1376.7079)
ToM Target loss= tensor(2234.0046)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2445123195648193 seconds
policy loss:-2384.67578125
value loss:33.74032211303711
entropies:76.97769165039062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.237361192703247 seconds
policy loss:-1821.2806396484375
value loss:102.61537170410156
entropies:63.349979400634766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2464642524719238 seconds
policy loss:-2177.10009765625
value loss:66.4610824584961
entropies:91.845458984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.205989122390747 seconds
policy loss:-804.1915893554688
value loss:16.7822208404541
entropies:52.79576873779297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1699905395507812 seconds
policy loss:-3415.974609375
value loss:49.342071533203125
entropies:55.341453552246094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1322.0144)
ToM Target loss= tensor(2133.0671)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2293696403503418 seconds
policy loss:31.6478214263916
value loss:26.506799697875977
entropies:63.478572845458984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1689341068267822 seconds
policy loss:-779.60400390625
value loss:11.326329231262207
entropies:48.84333038330078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2269690036773682 seconds
policy loss:52.700950622558594
value loss:17.925268173217773
entropies:89.65284729003906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1652405261993408 seconds
policy loss:-929.8004760742188
value loss:31.566728591918945
entropies:61.30475997924805
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2248363494873047 seconds
policy loss:-44.645179748535156
value loss:41.28657150268555
entropies:77.4765853881836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1302.7346)
ToM Target loss= tensor(2164.9458)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1701714992523193 seconds
policy loss:-1275.14208984375
value loss:17.714021682739258
entropies:61.83308410644531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1904575824737549 seconds
policy loss:-858.566650390625
value loss:14.685495376586914
entropies:36.96409225463867
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1877562999725342 seconds
policy loss:-1543.729736328125
value loss:24.774703979492188
entropies:62.61377716064453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.199357271194458 seconds
policy loss:-1247.3631591796875
value loss:28.552690505981445
entropies:73.02654266357422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2278227806091309 seconds
policy loss:-1290.40087890625
value loss:33.796024322509766
entropies:85.32704162597656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1353.0619)
ToM Target loss= tensor(2266.5637)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2320003509521484 seconds
policy loss:-645.9494018554688
value loss:27.059879302978516
entropies:51.31355285644531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1620213985443115 seconds
policy loss:-563.6732788085938
value loss:91.26611328125
entropies:61.23617172241211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2142534255981445 seconds
policy loss:-447.550537109375
value loss:32.3271598815918
entropies:67.62359619140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.209559679031372 seconds
policy loss:1192.027587890625
value loss:37.07033920288086
entropies:46.40122985839844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.162818193435669 seconds
policy loss:990.5828857421875
value loss:41.03640365600586
entropies:56.42372131347656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1275.5206)
ToM Target loss= tensor(2255.1250)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2353837490081787 seconds
policy loss:-241.33816528320312
value loss:38.99382400512695
entropies:42.513851165771484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228266954421997 seconds
policy loss:-705.086669921875
value loss:45.50895690917969
entropies:62.420677185058594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1953060626983643 seconds
policy loss:365.9808654785156
value loss:29.491947174072266
entropies:69.37045288085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2306768894195557 seconds
policy loss:-1011.2762451171875
value loss:38.37487030029297
entropies:64.77508544921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2175140380859375 seconds
policy loss:209.35211181640625
value loss:16.74341583251953
entropies:36.787540435791016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1295.3779)
ToM Target loss= tensor(2272.1643)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2143526077270508 seconds
policy loss:-3288.7314453125
value loss:61.20115280151367
entropies:77.77997589111328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1871452331542969 seconds
policy loss:-1500.917724609375
value loss:28.984413146972656
entropies:55.786659240722656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1811439990997314 seconds
policy loss:-4065.06201171875
value loss:109.53536224365234
entropies:78.0955810546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.225003957748413 seconds
policy loss:-3279.9150390625
value loss:81.83131408691406
entropies:75.40560913085938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1665983200073242 seconds
policy loss:-3264.35009765625
value loss:59.20969772338867
entropies:68.50292205810547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1324.5104)
ToM Target loss= tensor(2227.2256)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1985194683074951 seconds
policy loss:-2237.2802734375
value loss:40.9528694152832
entropies:93.56414794921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.194089651107788 seconds
policy loss:-1050.355712890625
value loss:17.762657165527344
entropies:68.02440643310547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1719048023223877 seconds
policy loss:-1451.793701171875
value loss:22.978477478027344
entropies:74.57158660888672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.161750316619873 seconds
policy loss:-447.2248229980469
value loss:16.76611328125
entropies:67.56863403320312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201509952545166 seconds
policy loss:-273.98956298828125
value loss:33.52839279174805
entropies:82.42498779296875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1386.4968)
ToM Target loss= tensor(2160.9299)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2209863662719727 seconds
policy loss:-13.996418952941895
value loss:39.704261779785156
entropies:49.76557922363281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.187286376953125 seconds
policy loss:637.1536865234375
value loss:62.59114456176758
entropies:64.72415161132812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2184929847717285 seconds
policy loss:247.60406494140625
value loss:66.62190246582031
entropies:68.47998046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.3126153945922852 seconds
policy loss:937.7413940429688
value loss:38.40901565551758
entropies:87.48148345947266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169001817703247 seconds
policy loss:157.154541015625
value loss:23.945911407470703
entropies:71.55716705322266
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1285.6733)
ToM Target loss= tensor(2116.5249)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.203536033630371 seconds
policy loss:-1342.2396240234375
value loss:29.685070037841797
entropies:86.74575805664062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1677119731903076 seconds
policy loss:-1270.9468994140625
value loss:31.016948699951172
entropies:81.08866882324219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2183210849761963 seconds
policy loss:-1732.444580078125
value loss:34.34866714477539
entropies:80.7868881225586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.199152946472168 seconds
policy loss:-1250.8779296875
value loss:22.308923721313477
entropies:60.19794464111328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1573879718780518 seconds
policy loss:-1316.4588623046875
value loss:59.8337516784668
entropies:65.70415496826172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1359.3544)
ToM Target loss= tensor(2216.8296)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2090635299682617 seconds
policy loss:-285.0094909667969
value loss:15.543349266052246
entropies:57.454307556152344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1750805377960205 seconds
policy loss:-195.97015380859375
value loss:22.40428352355957
entropies:67.99579620361328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2200970649719238 seconds
policy loss:-94.12879943847656
value loss:24.690444946289062
entropies:76.47293853759766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2019355297088623 seconds
policy loss:220.39195251464844
value loss:20.756956100463867
entropies:67.739013671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1576783657073975 seconds
policy loss:-774.3713989257812
value loss:47.42155456542969
entropies:49.286563873291016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1408.4028)
ToM Target loss= tensor(2308.1821)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2361290454864502 seconds
policy loss:490.9696350097656
value loss:36.21076202392578
entropies:51.29463195800781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1634771823883057 seconds
policy loss:-442.25384521484375
value loss:26.988523483276367
entropies:71.2077865600586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1604745388031006 seconds
policy loss:-368.1036071777344
value loss:23.220300674438477
entropies:62.85002136230469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2318856716156006 seconds
policy loss:-882.3637084960938
value loss:29.303306579589844
entropies:77.36522674560547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2204861640930176 seconds
policy loss:-745.8310546875
value loss:24.392925262451172
entropies:48.467437744140625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1287.6750)
ToM Target loss= tensor(2242.1790)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2266149520874023 seconds
policy loss:-948.8104858398438
value loss:15.557477951049805
entropies:63.393341064453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2365570068359375 seconds
policy loss:-437.10614013671875
value loss:8.520552635192871
entropies:33.82237243652344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2285032272338867 seconds
policy loss:-770.7589721679688
value loss:17.401123046875
entropies:50.84870910644531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1556742191314697 seconds
policy loss:-855.0213623046875
value loss:22.237451553344727
entropies:61.70661163330078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2274208068847656 seconds
policy loss:-230.90786743164062
value loss:19.58218002319336
entropies:78.90878295898438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1344.7341)
ToM Target loss= tensor(2245.1716)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.211359977722168 seconds
policy loss:-0.6706123352050781
value loss:17.64638900756836
entropies:69.8485107421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1673173904418945 seconds
policy loss:-1982.637939453125
value loss:50.325721740722656
entropies:93.68163299560547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227781057357788 seconds
policy loss:-548.6358642578125
value loss:15.951128959655762
entropies:31.125255584716797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2078227996826172 seconds
policy loss:-1806.21337890625
value loss:31.58146095275879
entropies:61.057228088378906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2379117012023926 seconds
policy loss:-554.97900390625
value loss:42.4698486328125
entropies:65.01016235351562
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1376.6512)
ToM Target loss= tensor(2142.5168)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2282130718231201 seconds
policy loss:-615.691650390625
value loss:24.048444747924805
entropies:56.895225524902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228760004043579 seconds
policy loss:-338.71868896484375
value loss:41.2718620300293
entropies:60.1202392578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.231778860092163 seconds
policy loss:-630.0419311523438
value loss:31.0861873626709
entropies:68.33399963378906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1882712841033936 seconds
policy loss:0.7355003356933594
value loss:29.058124542236328
entropies:80.9948959350586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2177865505218506 seconds
policy loss:-74.59768676757812
value loss:27.559946060180664
entropies:79.72367858886719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1394.4718)
ToM Target loss= tensor(2178.2852)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.200822353363037 seconds
policy loss:-901.9642944335938
value loss:29.263622283935547
entropies:53.38831329345703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2026500701904297 seconds
policy loss:-792.9580688476562
value loss:28.353364944458008
entropies:57.87278366088867
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.214430332183838 seconds
policy loss:-1126.572265625
value loss:28.040653228759766
entropies:53.124305725097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2050549983978271 seconds
policy loss:-2419.475830078125
value loss:42.318092346191406
entropies:68.61495971679688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1686396598815918 seconds
policy loss:-1853.423095703125
value loss:35.24235534667969
entropies:80.92536163330078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1437.4319)
ToM Target loss= tensor(2252.4673)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1913042068481445 seconds
policy loss:-97.92108154296875
value loss:16.691246032714844
entropies:60.67320251464844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1823058128356934 seconds
policy loss:-1454.4974365234375
value loss:42.180419921875
entropies:58.8139762878418
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2395739555358887 seconds
policy loss:-174.00611877441406
value loss:11.030077934265137
entropies:39.02626037597656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1783361434936523 seconds
policy loss:-774.9078979492188
value loss:41.78673553466797
entropies:59.5089225769043
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2019238471984863 seconds
policy loss:-56.845558166503906
value loss:33.230674743652344
entropies:45.34822082519531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1279.8606)
ToM Target loss= tensor(2244.7312)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1605303287506104 seconds
policy loss:-587.8297119140625
value loss:35.67697525024414
entropies:77.17548370361328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2142655849456787 seconds
policy loss:571.01171875
value loss:26.81092071533203
entropies:67.8604736328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.17037034034729 seconds
policy loss:324.9317321777344
value loss:16.88631248474121
entropies:49.699745178222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2038865089416504 seconds
policy loss:-903.57763671875
value loss:38.830810546875
entropies:76.79656982421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2095515727996826 seconds
policy loss:-261.8711853027344
value loss:58.563106536865234
entropies:41.5476188659668
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1352.8953)
ToM Target loss= tensor(2223.0955)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1679961681365967 seconds
policy loss:-880.9107666015625
value loss:42.12398147583008
entropies:66.17999267578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2391548156738281 seconds
policy loss:239.24351501464844
value loss:29.609519958496094
entropies:69.40370178222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1713924407958984 seconds
policy loss:-438.8040466308594
value loss:17.53290367126465
entropies:40.470420837402344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2353358268737793 seconds
policy loss:-804.5799560546875
value loss:22.02371597290039
entropies:73.28216552734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2022879123687744 seconds
policy loss:-712.2008666992188
value loss:25.883670806884766
entropies:68.57429504394531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1346.7903)
ToM Target loss= tensor(2211.8994)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2356038093566895 seconds
policy loss:-728.7539672851562
value loss:19.07898712158203
entropies:63.4095344543457
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2304067611694336 seconds
policy loss:-1778.3712158203125
value loss:27.42413330078125
entropies:84.30462646484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1726818084716797 seconds
policy loss:-308.57928466796875
value loss:19.669994354248047
entropies:65.984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1746699810028076 seconds
policy loss:-1451.9312744140625
value loss:22.743715286254883
entropies:72.64643859863281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1648612022399902 seconds
policy loss:-793.7265625
value loss:21.83099365234375
entropies:62.246826171875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1415.8569)
ToM Target loss= tensor(2398.9636)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2191162109375 seconds
policy loss:-1191.8719482421875
value loss:18.469440460205078
entropies:54.648399353027344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2271108627319336 seconds
policy loss:-215.06874084472656
value loss:13.362613677978516
entropies:50.45500183105469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1750407218933105 seconds
policy loss:-445.1119384765625
value loss:17.11090660095215
entropies:61.579742431640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.197089672088623 seconds
policy loss:-1629.3184814453125
value loss:44.81410217285156
entropies:55.69013977050781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.215611457824707 seconds
policy loss:-2080.48974609375
value loss:32.58766174316406
entropies:77.48410034179688
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1223.7480)
ToM Target loss= tensor(2297.0227)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1691865921020508 seconds
policy loss:-1064.6322021484375
value loss:18.081844329833984
entropies:59.4477653503418
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1761119365692139 seconds
policy loss:-1225.1141357421875
value loss:77.79117584228516
entropies:57.92298126220703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2318806648254395 seconds
policy loss:-134.12060546875
value loss:37.73281478881836
entropies:52.35730743408203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1995105743408203 seconds
policy loss:-1.4266959428787231
value loss:14.801344871520996
entropies:37.170257568359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2340023517608643 seconds
policy loss:-1068.6693115234375
value loss:69.05553436279297
entropies:46.822017669677734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1172.9525)
ToM Target loss= tensor(2384.4402)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1672818660736084 seconds
policy loss:-502.6152038574219
value loss:28.12623405456543
entropies:50.635684967041016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.166757583618164 seconds
policy loss:-590.6055908203125
value loss:25.24323272705078
entropies:32.52632141113281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2059767246246338 seconds
policy loss:-1250.825439453125
value loss:73.07889556884766
entropies:61.73033142089844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2437286376953125 seconds
policy loss:-414.4773864746094
value loss:35.49207305908203
entropies:65.22084045410156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2390422821044922 seconds
policy loss:86.60662078857422
value loss:31.565593719482422
entropies:40.16978073120117
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1199.9136)
ToM Target loss= tensor(2249.8184)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1688435077667236 seconds
policy loss:1491.423828125
value loss:44.20698165893555
entropies:40.92662811279297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2083406448364258 seconds
policy loss:677.718505859375
value loss:44.111671447753906
entropies:81.77064514160156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2275633811950684 seconds
policy loss:83.11432647705078
value loss:50.34830856323242
entropies:97.4898452758789
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177933931350708 seconds
policy loss:-369.05242919921875
value loss:34.125328063964844
entropies:45.669471740722656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.219024658203125 seconds
policy loss:-440.9096374511719
value loss:23.157604217529297
entropies:63.89271545410156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1347.9730)
ToM Target loss= tensor(2302.0159)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1787993907928467 seconds
policy loss:-139.52374267578125
value loss:41.419559478759766
entropies:57.58913803100586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1780660152435303 seconds
policy loss:-1239.823974609375
value loss:116.11515808105469
entropies:46.29450225830078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1656217575073242 seconds
policy loss:-2251.68603515625
value loss:27.856388092041016
entropies:59.65199279785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.180342674255371 seconds
policy loss:-1823.62060546875
value loss:33.450687408447266
entropies:56.69331359863281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2291607856750488 seconds
policy loss:-1017.635009765625
value loss:20.046676635742188
entropies:52.82299041748047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1249.6206)
ToM Target loss= tensor(2309.2905)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1729412078857422 seconds
policy loss:-686.162841796875
value loss:39.53105926513672
entropies:66.83377075195312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2099542617797852 seconds
policy loss:-421.21221923828125
value loss:59.560890197753906
entropies:84.07845306396484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1784567832946777 seconds
policy loss:-412.73095703125
value loss:30.84496307373047
entropies:81.24363708496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2325243949890137 seconds
policy loss:-9.183969497680664
value loss:49.92980194091797
entropies:67.11752319335938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178107738494873 seconds
policy loss:153.0513458251953
value loss:30.250141143798828
entropies:54.41218566894531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1350.4927)
ToM Target loss= tensor(2332.1125)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1846380233764648 seconds
policy loss:647.5784912109375
value loss:38.018218994140625
entropies:51.81476593017578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.175093412399292 seconds
policy loss:-563.3759155273438
value loss:51.0255012512207
entropies:56.78650665283203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2309455871582031 seconds
policy loss:751.2560424804688
value loss:24.04549789428711
entropies:77.39971923828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2391133308410645 seconds
policy loss:-259.6839599609375
value loss:31.44367218017578
entropies:110.1419677734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2269182205200195 seconds
policy loss:-1486.66064453125
value loss:25.60675621032715
entropies:57.17686462402344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1417.7645)
ToM Target loss= tensor(2290.9819)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.226743459701538 seconds
policy loss:-2298.30224609375
value loss:41.77436828613281
entropies:80.19083404541016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1745574474334717 seconds
policy loss:-2661.398681640625
value loss:62.25029373168945
entropies:62.32923126220703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2141013145446777 seconds
policy loss:-1304.627685546875
value loss:17.4157772064209
entropies:68.83845520019531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1702566146850586 seconds
policy loss:-1288.9500732421875
value loss:40.45026779174805
entropies:57.30975341796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2170860767364502 seconds
policy loss:-2122.90283203125
value loss:31.81018829345703
entropies:91.89200592041016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1423.8785)
ToM Target loss= tensor(2296.3494)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2012341022491455 seconds
policy loss:-499.443115234375
value loss:28.585908889770508
entropies:68.84793090820312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1817657947540283 seconds
policy loss:143.4059295654297
value loss:28.806774139404297
entropies:76.01446533203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1658835411071777 seconds
policy loss:-882.373046875
value loss:49.318023681640625
entropies:80.48690795898438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2338955402374268 seconds
policy loss:535.8688354492188
value loss:32.883453369140625
entropies:64.26179504394531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1638679504394531 seconds
policy loss:453.4700012207031
value loss:47.22325134277344
entropies:85.75924682617188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1450.7073)
ToM Target loss= tensor(2155.0488)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2311432361602783 seconds
policy loss:-31.503875732421875
value loss:36.93244171142578
entropies:61.579689025878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2404720783233643 seconds
policy loss:-2970.69873046875
value loss:63.96554183959961
entropies:69.6363525390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2410473823547363 seconds
policy loss:-1658.679443359375
value loss:22.563186645507812
entropies:56.904273986816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.202115774154663 seconds
policy loss:-1878.1807861328125
value loss:51.194007873535156
entropies:67.33053588867188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2045507431030273 seconds
policy loss:-848.22998046875
value loss:11.847036361694336
entropies:68.42195129394531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1419.9597)
ToM Target loss= tensor(2337.3142)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1769347190856934 seconds
policy loss:-608.8431396484375
value loss:11.775961875915527
entropies:38.51879119873047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1659998893737793 seconds
policy loss:-666.4403076171875
value loss:25.244224548339844
entropies:63.18798065185547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1756360530853271 seconds
policy loss:-932.6865844726562
value loss:32.4260139465332
entropies:70.16545867919922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1720025539398193 seconds
policy loss:-2408.414794921875
value loss:31.225746154785156
entropies:66.48072052001953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2360851764678955 seconds
policy loss:-935.5321044921875
value loss:44.057621002197266
entropies:80.80664825439453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1306.8701)
ToM Target loss= tensor(2303.1111)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2259292602539062 seconds
policy loss:-2.992774248123169
value loss:16.466035842895508
entropies:45.34281921386719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.240572452545166 seconds
policy loss:-643.2596435546875
value loss:35.77113723754883
entropies:31.372764587402344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2385342121124268 seconds
policy loss:-275.8079528808594
value loss:20.59781265258789
entropies:47.03730010986328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2216620445251465 seconds
policy loss:-485.4981689453125
value loss:30.189857482910156
entropies:74.66552734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176236629486084 seconds
policy loss:-107.78028106689453
value loss:13.770532608032227
entropies:42.36598205566406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1201.2031)
ToM Target loss= tensor(2426.3398)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1706976890563965 seconds
policy loss:-1744.39794921875
value loss:101.02198791503906
entropies:55.40105438232422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1779260635375977 seconds
policy loss:-1965.6065673828125
value loss:92.18208312988281
entropies:52.75002670288086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2364602088928223 seconds
policy loss:-2279.054443359375
value loss:66.61730194091797
entropies:64.5526123046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1700634956359863 seconds
policy loss:-967.2633056640625
value loss:17.677478790283203
entropies:52.415740966796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.183570384979248 seconds
policy loss:8.874052047729492
value loss:17.687744140625
entropies:42.6619873046875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1352.6906)
ToM Target loss= tensor(2322.5554)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1829440593719482 seconds
policy loss:-881.9754028320312
value loss:29.575153350830078
entropies:53.94090270996094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.219245433807373 seconds
policy loss:-1026.9063720703125
value loss:52.28144836425781
entropies:51.307743072509766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.203934669494629 seconds
policy loss:-1030.646240234375
value loss:61.64970397949219
entropies:78.5229263305664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177565574645996 seconds
policy loss:-343.76861572265625
value loss:31.251880645751953
entropies:66.84347534179688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1667206287384033 seconds
policy loss:-657.2316284179688
value loss:31.09222984313965
entropies:38.72785949707031
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1255.0277)
ToM Target loss= tensor(2166.1018)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.213247537612915 seconds
policy loss:-129.89321899414062
value loss:24.844074249267578
entropies:62.756813049316406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2130627632141113 seconds
policy loss:-774.4790649414062
value loss:45.30837631225586
entropies:43.69951248168945
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.181445598602295 seconds
policy loss:-534.548828125
value loss:48.075714111328125
entropies:34.85475158691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1656064987182617 seconds
policy loss:-436.12945556640625
value loss:18.613956451416016
entropies:58.664398193359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2022783756256104 seconds
policy loss:-851.5877075195312
value loss:18.678903579711914
entropies:36.79397201538086
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1223.1628)
ToM Target loss= tensor(2287.8684)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.181210994720459 seconds
policy loss:-1355.1768798828125
value loss:62.97230529785156
entropies:45.71000671386719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1729960441589355 seconds
policy loss:-1013.1311645507812
value loss:77.09740447998047
entropies:65.22618103027344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2236685752868652 seconds
policy loss:-406.3150939941406
value loss:43.7614631652832
entropies:59.662879943847656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1835582256317139 seconds
policy loss:-1340.7979736328125
value loss:23.640579223632812
entropies:67.11561584472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2073795795440674 seconds
policy loss:-1433.0137939453125
value loss:32.10516357421875
entropies:62.54300308227539
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1369.2534)
ToM Target loss= tensor(2195.5691)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2346100807189941 seconds
policy loss:345.0564270019531
value loss:29.84650993347168
entropies:77.90473175048828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2430453300476074 seconds
policy loss:340.0252990722656
value loss:33.254295349121094
entropies:61.199222564697266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2363848686218262 seconds
policy loss:-772.3348999023438
value loss:33.12181091308594
entropies:69.1859359741211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1742043495178223 seconds
policy loss:-182.23834228515625
value loss:35.183074951171875
entropies:72.41321563720703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.231252908706665 seconds
policy loss:-703.5947265625
value loss:27.780515670776367
entropies:58.26500701904297
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1410.3088)
ToM Target loss= tensor(2312.8198)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2347478866577148 seconds
policy loss:-1425.32666015625
value loss:25.13538360595703
entropies:81.8012466430664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2237489223480225 seconds
policy loss:-1780.20166015625
value loss:30.58786392211914
entropies:67.52716064453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1815640926361084 seconds
policy loss:-1668.3201904296875
value loss:34.9182014465332
entropies:50.855751037597656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1787819862365723 seconds
policy loss:-1976.4578857421875
value loss:39.00896072387695
entropies:49.04813766479492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2004647254943848 seconds
policy loss:-419.4142761230469
value loss:14.321989059448242
entropies:55.20248031616211
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1343.4546)
ToM Target loss= tensor(2322.4246)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2304301261901855 seconds
policy loss:-408.4629211425781
value loss:16.293527603149414
entropies:43.88949966430664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2422072887420654 seconds
policy loss:-221.57516479492188
value loss:43.49705505371094
entropies:61.35478591918945
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1707918643951416 seconds
policy loss:-844.5220947265625
value loss:27.976085662841797
entropies:70.82454681396484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1728525161743164 seconds
policy loss:-1166.044189453125
value loss:21.722440719604492
entropies:57.024295806884766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2137532234191895 seconds
policy loss:183.88592529296875
value loss:12.182623863220215
entropies:51.79814147949219
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1274.2058)
ToM Target loss= tensor(2278.9158)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1848244667053223 seconds
policy loss:-1244.1875
value loss:31.833942413330078
entropies:59.8773307800293
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2193591594696045 seconds
policy loss:-977.512451171875
value loss:29.2028865814209
entropies:68.5595474243164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1779701709747314 seconds
policy loss:-345.97869873046875
value loss:24.534589767456055
entropies:59.482749938964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.184870958328247 seconds
policy loss:143.23388671875
value loss:32.85348129272461
entropies:62.60080337524414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2371032238006592 seconds
policy loss:-65.47999572753906
value loss:13.315622329711914
entropies:59.42290115356445
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1351.4329)
ToM Target loss= tensor(2137.2681)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2311182022094727 seconds
policy loss:-328.78851318359375
value loss:24.29633140563965
entropies:51.05816650390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.233959436416626 seconds
policy loss:-3719.742431640625
value loss:62.80704116821289
entropies:66.01113891601562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1763052940368652 seconds
policy loss:-1041.2379150390625
value loss:20.96550750732422
entropies:61.98896408081055
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2216885089874268 seconds
policy loss:-2057.1171875
value loss:41.45417022705078
entropies:70.3621597290039
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2499995231628418 seconds
policy loss:-930.2531127929688
value loss:25.630725860595703
entropies:58.68944549560547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1400.1091)
ToM Target loss= tensor(2214.2056)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1682960987091064 seconds
policy loss:-378.2417907714844
value loss:20.57059669494629
entropies:73.83682250976562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.17390775680542 seconds
policy loss:-1155.258056640625
value loss:32.6187744140625
entropies:66.86431884765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2222726345062256 seconds
policy loss:-222.77488708496094
value loss:17.097551345825195
entropies:59.58808135986328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1748363971710205 seconds
policy loss:402.587158203125
value loss:20.126420974731445
entropies:49.687835693359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1691656112670898 seconds
policy loss:-546.0562133789062
value loss:26.742876052856445
entropies:70.43919372558594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1421.6804)
ToM Target loss= tensor(2266.2180)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.152198076248169 seconds
policy loss:-525.2758178710938
value loss:28.11475372314453
entropies:62.484901428222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.181753158569336 seconds
policy loss:-266.1855773925781
value loss:22.140954971313477
entropies:51.90473937988281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1651172637939453 seconds
policy loss:-1197.9249267578125
value loss:31.678274154663086
entropies:72.46106719970703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2958426475524902 seconds
policy loss:-328.85235595703125
value loss:19.230920791625977
entropies:67.93531036376953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1576035022735596 seconds
policy loss:-3158.431396484375
value loss:46.92246627807617
entropies:66.81690979003906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1397.6294)
ToM Target loss= tensor(2352.7297)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1987998485565186 seconds
policy loss:-1106.0618896484375
value loss:22.67726707458496
entropies:49.52099609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1770915985107422 seconds
policy loss:-2298.742919921875
value loss:79.46087646484375
entropies:70.21247863769531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1727986335754395 seconds
policy loss:-1328.70068359375
value loss:30.66986846923828
entropies:57.137943267822266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1752290725708008 seconds
policy loss:-842.4871215820312
value loss:35.843082427978516
entropies:45.776123046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2352590560913086 seconds
policy loss:-885.619873046875
value loss:15.47084903717041
entropies:44.221282958984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1206.4148)
ToM Target loss= tensor(2329.4788)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.233363389968872 seconds
policy loss:-243.092041015625
value loss:19.375232696533203
entropies:38.66701126098633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1696906089782715 seconds
policy loss:205.18893432617188
value loss:29.430971145629883
entropies:40.8065185546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1790621280670166 seconds
policy loss:-407.4400939941406
value loss:36.09119415283203
entropies:31.100563049316406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1853818893432617 seconds
policy loss:-90.56934356689453
value loss:21.412036895751953
entropies:42.962890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178853988647461 seconds
policy loss:-117.35649108886719
value loss:22.665630340576172
entropies:38.90384292602539
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1172.6665)
ToM Target loss= tensor(2269.7590)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2359280586242676 seconds
policy loss:-1550.7431640625
value loss:50.56645202636719
entropies:36.394264221191406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1757681369781494 seconds
policy loss:-1860.880615234375
value loss:64.64494323730469
entropies:61.39748764038086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2155864238739014 seconds
policy loss:-363.90655517578125
value loss:15.564005851745605
entropies:71.72062683105469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2322180271148682 seconds
policy loss:-458.22296142578125
value loss:34.59148406982422
entropies:43.78025817871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2367866039276123 seconds
policy loss:-723.63134765625
value loss:16.15375328063965
entropies:53.9027214050293
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1230.3503)
ToM Target loss= tensor(2245.0493)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2324469089508057 seconds
policy loss:-119.903076171875
value loss:19.194520950317383
entropies:49.898468017578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2395498752593994 seconds
policy loss:-854.4874267578125
value loss:58.87815475463867
entropies:45.103179931640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2124269008636475 seconds
policy loss:-6.572201728820801
value loss:20.566726684570312
entropies:54.57482147216797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.17218017578125 seconds
policy loss:-858.776611328125
value loss:30.863691329956055
entropies:52.99302673339844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.196852445602417 seconds
policy loss:-1393.174560546875
value loss:46.83538818359375
entropies:46.08910369873047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1231.4685)
ToM Target loss= tensor(2176.9355)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2441377639770508 seconds
policy loss:-568.1461181640625
value loss:27.205577850341797
entropies:50.37871551513672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1701133251190186 seconds
policy loss:-1089.010009765625
value loss:35.014686584472656
entropies:53.71697998046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2352635860443115 seconds
policy loss:-20.190092086791992
value loss:43.475318908691406
entropies:46.03782653808594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1698400974273682 seconds
policy loss:538.3704223632812
value loss:29.41248321533203
entropies:49.597084045410156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2324519157409668 seconds
policy loss:-1103.31884765625
value loss:41.66085433959961
entropies:56.045875549316406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1293.2592)
ToM Target loss= tensor(2247.7434)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1742722988128662 seconds
policy loss:-1033.785400390625
value loss:21.5851993560791
entropies:61.899635314941406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1724579334259033 seconds
policy loss:1013.57275390625
value loss:27.253339767456055
entropies:62.31331253051758
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2015750408172607 seconds
policy loss:-892.8177490234375
value loss:28.821504592895508
entropies:67.10904693603516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176469326019287 seconds
policy loss:-1943.2442626953125
value loss:54.40314483642578
entropies:54.10712432861328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.200181245803833 seconds
policy loss:-135.27273559570312
value loss:18.65192413330078
entropies:42.68588638305664
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1359.4508)
ToM Target loss= tensor(2309.8804)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2029247283935547 seconds
policy loss:639.3245849609375
value loss:21.49280548095703
entropies:60.116031646728516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.18161940574646 seconds
policy loss:127.30216217041016
value loss:16.64704132080078
entropies:48.74428176879883
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.232349157333374 seconds
policy loss:-372.6165771484375
value loss:15.317896842956543
entropies:45.30796813964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.166883945465088 seconds
policy loss:-240.1159210205078
value loss:23.963275909423828
entropies:51.91101837158203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2096984386444092 seconds
policy loss:-137.93067932128906
value loss:14.713258743286133
entropies:31.660263061523438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1281.8972)
ToM Target loss= tensor(2266.2598)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.199831247329712 seconds
policy loss:502.9703369140625
value loss:15.967615127563477
entropies:48.78323745727539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1726396083831787 seconds
policy loss:-770.8602294921875
value loss:19.68946647644043
entropies:49.58944320678711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2339119911193848 seconds
policy loss:-1116.038330078125
value loss:32.294044494628906
entropies:93.47898864746094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1704649925231934 seconds
policy loss:-702.849609375
value loss:25.660484313964844
entropies:58.82334899902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1950044631958008 seconds
policy loss:-578.6017456054688
value loss:15.410911560058594
entropies:45.1262092590332
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1312.3672)
ToM Target loss= tensor(2214.0703)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.174001693725586 seconds
policy loss:-878.1295776367188
value loss:25.766923904418945
entropies:52.13644790649414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2347896099090576 seconds
policy loss:-222.20547485351562
value loss:24.410362243652344
entropies:55.266998291015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2329084873199463 seconds
policy loss:-211.77398681640625
value loss:18.745357513427734
entropies:48.62063217163086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.231917142868042 seconds
policy loss:-433.3678283691406
value loss:19.66327667236328
entropies:49.149192810058594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.233417272567749 seconds
policy loss:-2458.063232421875
value loss:70.31670379638672
entropies:55.123931884765625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1224.0131)
ToM Target loss= tensor(2225.3948)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1681625843048096 seconds
policy loss:-1736.603271484375
value loss:23.487916946411133
entropies:50.261634826660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2452590465545654 seconds
policy loss:-1230.851806640625
value loss:22.549510955810547
entropies:50.13154602050781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1693732738494873 seconds
policy loss:-612.477294921875
value loss:19.97478675842285
entropies:39.972412109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1974000930786133 seconds
policy loss:-418.3629150390625
value loss:19.195133209228516
entropies:72.5447006225586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2290589809417725 seconds
policy loss:740.162841796875
value loss:20.690107345581055
entropies:53.241355895996094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1297.2216)
ToM Target loss= tensor(2347.5425)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2469556331634521 seconds
policy loss:454.1220703125
value loss:46.55615997314453
entropies:68.21025085449219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.194690227508545 seconds
policy loss:573.5335083007812
value loss:26.406482696533203
entropies:40.90031433105469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2042927742004395 seconds
policy loss:-131.4908447265625
value loss:30.243911743164062
entropies:55.23053741455078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2166929244995117 seconds
policy loss:-823.110595703125
value loss:25.253150939941406
entropies:67.36268615722656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2379660606384277 seconds
policy loss:-45.94060516357422
value loss:45.542022705078125
entropies:69.86479187011719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1421.3605)
ToM Target loss= tensor(2360.5610)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.170158863067627 seconds
policy loss:-1338.2930908203125
value loss:31.9134521484375
entropies:64.48086547851562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1743602752685547 seconds
policy loss:-1321.362060546875
value loss:24.984100341796875
entropies:44.959659576416016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1718320846557617 seconds
policy loss:-1710.1033935546875
value loss:46.43964767456055
entropies:72.78439331054688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2018234729766846 seconds
policy loss:-1125.7025146484375
value loss:36.96028137207031
entropies:55.8446044921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2228960990905762 seconds
policy loss:-2336.154541015625
value loss:44.14994812011719
entropies:76.20716857910156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1425.6813)
ToM Target loss= tensor(2313.8762)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2356486320495605 seconds
policy loss:-1274.7459716796875
value loss:44.33517074584961
entropies:78.73660278320312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.234447956085205 seconds
policy loss:84.48468017578125
value loss:42.932369232177734
entropies:59.424072265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.175051212310791 seconds
policy loss:454.0534362792969
value loss:35.857059478759766
entropies:64.80159759521484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1975407600402832 seconds
policy loss:-941.306884765625
value loss:25.067790985107422
entropies:69.30091094970703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227766752243042 seconds
policy loss:551.2606201171875
value loss:62.99298858642578
entropies:78.85842895507812
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1343.2607)
ToM Target loss= tensor(2239.8994)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2210264205932617 seconds
policy loss:565.0556640625
value loss:19.620460510253906
entropies:54.87238311767578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.23081636428833 seconds
policy loss:347.726318359375
value loss:21.529531478881836
entropies:45.18458938598633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1769485473632812 seconds
policy loss:-1097.5933837890625
value loss:30.367023468017578
entropies:56.149253845214844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2290480136871338 seconds
policy loss:-1537.3673095703125
value loss:30.003494262695312
entropies:69.36737060546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2417759895324707 seconds
policy loss:-818.3496704101562
value loss:28.766359329223633
entropies:78.20399475097656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1288.1681)
ToM Target loss= tensor(2279.3159)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2201404571533203 seconds
policy loss:256.8943786621094
value loss:11.620116233825684
entropies:35.00101089477539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1816751956939697 seconds
policy loss:-1499.166748046875
value loss:28.374181747436523
entropies:82.25701141357422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.22503662109375 seconds
policy loss:-1421.5076904296875
value loss:33.70376968383789
entropies:65.76934814453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1674859523773193 seconds
policy loss:-301.8327941894531
value loss:14.966059684753418
entropies:31.8828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.180609941482544 seconds
policy loss:-1267.122802734375
value loss:35.35435485839844
entropies:64.37065124511719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1307.2305)
ToM Target loss= tensor(2301.3491)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1987342834472656 seconds
policy loss:-782.6514892578125
value loss:48.705299377441406
entropies:65.24934387207031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2073948383331299 seconds
policy loss:-1015.9075317382812
value loss:37.89396286010742
entropies:69.0595703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2386975288391113 seconds
policy loss:-990.9309692382812
value loss:35.59656524658203
entropies:67.41835021972656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1769778728485107 seconds
policy loss:-712.0894775390625
value loss:24.603740692138672
entropies:46.465049743652344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2333626747131348 seconds
policy loss:-190.31436157226562
value loss:26.04975128173828
entropies:59.807884216308594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1334.1729)
ToM Target loss= tensor(2125.8225)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1771137714385986 seconds
policy loss:-349.3415832519531
value loss:26.812597274780273
entropies:60.76213073730469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1747322082519531 seconds
policy loss:-1340.805419921875
value loss:29.992305755615234
entropies:64.0255126953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.188068151473999 seconds
policy loss:-749.8402099609375
value loss:55.63991165161133
entropies:47.024871826171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2160077095031738 seconds
policy loss:-578.0284423828125
value loss:8.774398803710938
entropies:49.75437927246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.167154312133789 seconds
policy loss:-1842.91796875
value loss:49.025390625
entropies:68.45795440673828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1368.6741)
ToM Target loss= tensor(2291.4968)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2200889587402344 seconds
policy loss:-888.431884765625
value loss:22.183977127075195
entropies:70.33262634277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.206773281097412 seconds
policy loss:-215.2747802734375
value loss:24.799875259399414
entropies:58.1378059387207
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.241513729095459 seconds
policy loss:194.32559204101562
value loss:11.422181129455566
entropies:43.47480773925781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1757557392120361 seconds
policy loss:-1321.95849609375
value loss:27.390954971313477
entropies:68.33538818359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1771972179412842 seconds
policy loss:-1048.494384765625
value loss:23.74539566040039
entropies:79.2234878540039
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1385.1313)
ToM Target loss= tensor(2173.6384)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2396337985992432 seconds
policy loss:-985.7020874023438
value loss:20.083858489990234
entropies:63.3796272277832
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2247059345245361 seconds
policy loss:-1338.7215576171875
value loss:37.15503692626953
entropies:82.76481628417969
Policy training finished
---------------------
2024-04-07 04:01:57,588 : Time 02h 43m 39s, ave eps reward [-7.56 -7.56 -7.56], ave eps length 10.0, reward step [-0.76 -0.76 -0.76], FPS 6.53, mean reward -7.55739128732058, std reward 6.455387157082681, AG 0.0
2024-04-07 04:02:24,630 : Time 02h 44m 06s, ave eps reward [-6.5 -6.5 -6.5], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 8.94, mean reward -6.5028688023294805, std reward 6.6807000748755545, AG 0.0
2024-04-07 04:02:51,383 : Time 02h 44m 33s, ave eps reward [-7.27 -7.27 -7.27], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 8.36, mean reward -7.268780208706479, std reward 6.860015929009126, AG 0.0
2024-04-07 04:03:18,368 : Time 02h 45m 00s, ave eps reward [-5.55 -5.55 -5.55], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.45, mean reward -5.549026953105986, std reward 5.2886092055839, AG 0.0
2024-04-07 04:03:45,211 : Time 02h 45m 27s, ave eps reward [-8.97 -8.97 -8.97], ave eps length 10.0, reward step [-0.9 -0.9 -0.9], FPS 6.99, mean reward -8.966488577679367, std reward 6.142456921507548, AG 0.0
2024-04-07 04:04:12,191 : Time 02h 45m 54s, ave eps reward [-7.49 -7.49 -7.49], ave eps length 10.0, reward step [-0.75 -0.75 -0.75], FPS 6.69, mean reward -7.492936809400108, std reward 5.67098955323393, AG 0.0
2024-04-07 04:04:39,115 : Time 02h 46m 21s, ave eps reward [-3.49 -3.49 -3.49], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 6.68, mean reward -3.4903439296135352, std reward 1.5340383469737124, AG 0.0
2024-04-07 04:05:06,014 : Time 02h 46m 47s, ave eps reward [-6.33 -6.33 -6.33], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 7.41, mean reward -6.333042852140306, std reward 5.982194294418838, AG 0.0
2024-04-07 04:05:32,541 : Time 02h 47m 14s, ave eps reward [-4.79 -4.79 -4.79], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 10.11, mean reward -4.791212640864428, std reward 4.7327589318332866, AG 0.0
2024-04-07 04:05:59,435 : Time 02h 47m 41s, ave eps reward [-6.02 -6.02 -6.02], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 7.58, mean reward -6.022386407632331, std reward 5.459933178773875, AG 0.0
2024-04-07 04:06:26,810 : Time 02h 48m 08s, ave eps reward [-5.04 -5.04 -5.04], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 7.36, mean reward -5.044371224606707, std reward 4.977891439882702, AG 0.0
2024-04-07 04:06:53,455 : Time 02h 48m 35s, ave eps reward [-7.33 -7.33 -7.33], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 10.08, mean reward -7.334672758959927, std reward 6.079472877860602, AG 0.0
2024-04-07 04:07:20,355 : Time 02h 49m 02s, ave eps reward [-5.45 -5.45 -5.45], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 7.6, mean reward -5.449905644455589, std reward 4.565240389989611, AG 0.0
2024-04-07 04:07:47,275 : Time 02h 49m 29s, ave eps reward [-6.48 -6.48 -6.48], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 6.83, mean reward -6.477166220196255, std reward 5.627602578369247, AG 0.0
2024-04-07 04:08:14,208 : Time 02h 49m 56s, ave eps reward [-8.24 -8.24 -8.24], ave eps length 10.0, reward step [-0.82 -0.82 -0.82], FPS 6.39, mean reward -8.238332679584207, std reward 6.149874212195348, AG 0.0
2024-04-07 04:08:41,172 : Time 02h 50m 23s, ave eps reward [-5.42 -5.42 -5.42], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 10.44, mean reward -5.422762236879796, std reward 4.420764471870081, AG 0.0
2024-04-07 04:09:08,049 : Time 02h 50m 49s, ave eps reward [-7.81 -7.81 -7.81], ave eps length 10.0, reward step [-0.78 -0.78 -0.78], FPS 7.56, mean reward -7.814053534909954, std reward 7.693405792864364, AG 0.0
2024-04-07 04:09:34,988 : Time 02h 51m 16s, ave eps reward [-7.18 -7.18 -7.18], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 6.66, mean reward -7.180464487597651, std reward 6.106389190174893, AG 0.0
2024-04-07 04:10:01,742 : Time 02h 51m 43s, ave eps reward [-5.33 -5.33 -5.33], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 7.25, mean reward -5.329159260629314, std reward 4.925384589956096, AG 0.0
2024-04-07 04:10:28,804 : Time 02h 52m 10s, ave eps reward [-5.45 -5.45 -5.45], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 6.62, mean reward -5.446088626431039, std reward 5.024962054088519, AG 0.0
2024-04-07 04:10:55,694 : Time 02h 52m 37s, ave eps reward [-6.57 -6.57 -6.57], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 6.54, mean reward -6.572089966449548, std reward 5.342459534139981, AG 0.0
2024-04-07 04:11:22,428 : Time 02h 53m 04s, ave eps reward [-7.25 -7.25 -7.25], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 7.25, mean reward -7.248096303990444, std reward 6.713913015296547, AG 0.0
2024-04-07 04:11:49,033 : Time 02h 53m 30s, ave eps reward [-6.07 -6.07 -6.07], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 10.23, mean reward -6.072106462691908, std reward 5.518755382600507, AG 0.0
2024-04-07 04:12:15,891 : Time 02h 53m 57s, ave eps reward [-7.04 -7.04 -7.04], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 7.56, mean reward -7.043586115423035, std reward 4.834105990711692, AG 0.0
2024-04-07 04:12:43,151 : Time 02h 54m 25s, ave eps reward [-7.74 -7.74 -7.74], ave eps length 10.0, reward step [-0.77 -0.77 -0.77], FPS 7.58, mean reward -7.736994615011308, std reward 6.117701348056249, AG 0.0
2024-04-07 04:13:09,798 : Time 02h 54m 51s, ave eps reward [-7.4 -7.4 -7.4], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 9.72, mean reward -7.395711343537682, std reward 6.841330685206965, AG 0.0
2024-04-07 04:13:36,766 : Time 02h 55m 18s, ave eps reward [-6.92 -6.92 -6.92], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 7.24, mean reward -6.921449483675045, std reward 4.915952321964307, AG 0.0
2024-04-07 04:14:03,674 : Time 02h 55m 45s, ave eps reward [-6.85 -6.85 -6.85], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 6.63, mean reward -6.854269293099165, std reward 5.42826669160575, AG 0.0
2024-04-07 04:14:30,574 : Time 02h 56m 12s, ave eps reward [-5.61 -5.61 -5.61], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.78, mean reward -5.61288144825876, std reward 4.76304755584101, AG 0.0
2024-04-07 04:14:57,509 : Time 02h 56m 39s, ave eps reward [-4.79 -4.79 -4.79], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 7.8, mean reward -4.791679470225436, std reward 4.7218672164458235, AG 0.0
2024-04-07 04:15:24,426 : Time 02h 57m 06s, ave eps reward [-5.46 -5.46 -5.46], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.43, mean reward -5.460910464562587, std reward 4.703890599392052, AG 0.0
2024-04-07 04:15:51,227 : Time 02h 57m 33s, ave eps reward [-4.17 -4.17 -4.17], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 7.08, mean reward -4.167993443636495, std reward 3.3621633938938014, AG 0.0
2024-04-07 04:16:17,796 : Time 02h 57m 59s, ave eps reward [-8.43 -8.43 -8.43], ave eps length 10.0, reward step [-0.84 -0.84 -0.84], FPS 9.69, mean reward -8.427438203380488, std reward 5.621272414852834, AG 0.0
2024-04-07 04:16:45,188 : Time 02h 58m 27s, ave eps reward [-5.8 -5.8 -5.8], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 6.85, mean reward -5.795420541184242, std reward 5.9735011549152555, AG 0.0
2024-04-07 04:17:11,988 : Time 02h 58m 53s, ave eps reward [-6.58 -6.58 -6.58], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 7.25, mean reward -6.580228447606165, std reward 5.7680438601144735, AG 0.0
2024-04-07 04:17:38,514 : Time 02h 59m 20s, ave eps reward [-7.94 -7.94 -7.94], ave eps length 10.0, reward step [-0.79 -0.79 -0.79], FPS 10.35, mean reward -7.937107125856099, std reward 5.777070389164873, AG 0.0
2024-04-07 04:18:05,434 : Time 02h 59m 47s, ave eps reward [-6.33 -6.33 -6.33], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 7.46, mean reward -6.331830187593369, std reward 5.5224792373284775, AG 0.0
2024-04-07 04:18:32,233 : Time 03h 00m 14s, ave eps reward [-2.85 -2.85 -2.85], ave eps length 10.0, reward step [-0.29 -0.29 -0.29], FPS 6.6, mean reward -2.853155458227575, std reward 1.0190678366419128, AG 0.0
2024-04-07 04:18:59,113 : Time 03h 00m 41s, ave eps reward [-6.97 -6.97 -6.97], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 9.5, mean reward -6.9694048262431165, std reward 5.823895611993368, AG 0.0
2024-04-07 04:19:26,024 : Time 03h 01m 07s, ave eps reward [-5.43 -5.43 -5.43], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 6.71, mean reward -5.428377736292694, std reward 3.9537478062277835, AG 0.0
2024-04-07 04:19:53,050 : Time 03h 01m 34s, ave eps reward [-6.45 -6.45 -6.45], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 6.65, mean reward -6.4452124956338555, std reward 6.334913892501523, AG 0.0
2024-04-07 04:20:19,631 : Time 03h 02m 01s, ave eps reward [-4.78 -4.78 -4.78], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 8.76, mean reward -4.775699333660426, std reward 3.3422562848068234, AG 0.0
2024-04-07 04:20:46,809 : Time 03h 02m 28s, ave eps reward [-4.21 -4.21 -4.21], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.62, mean reward -4.210676868593617, std reward 5.076525690105927, AG 0.0
2024-04-07 04:21:13,671 : Time 03h 02m 55s, ave eps reward [-5.86 -5.86 -5.86], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 6.69, mean reward -5.864486300784018, std reward 4.912543000265877, AG 0.0
2024-04-07 04:21:40,385 : Time 03h 03m 22s, ave eps reward [-3.97 -3.97 -3.97], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 7.77, mean reward -3.9660496677993797, std reward 2.365398604601832, AG 0.0
2024-04-07 04:22:06,930 : Time 03h 03m 48s, ave eps reward [-6.73 -6.73 -6.73], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 9.56, mean reward -6.730755626241914, std reward 5.908589861408567, AG 0.0
2024-04-07 04:22:33,835 : Time 03h 04m 15s, ave eps reward [-6.15 -6.15 -6.15], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 6.95, mean reward -6.147277292291924, std reward 5.016990660272447, AG 0.0
2024-04-07 04:23:00,973 : Time 03h 04m 42s, ave eps reward [-8.23 -8.23 -8.23], ave eps length 10.0, reward step [-0.82 -0.82 -0.82], FPS 8.2, mean reward -8.231280970342056, std reward 6.8499504405876825, AG 0.0
2024-04-07 04:23:27,750 : Time 03h 05m 09s, ave eps reward [-8.63 -8.63 -8.63], ave eps length 10.0, reward step [-0.86 -0.86 -0.86], FPS 8.38, mean reward -8.634976071096549, std reward 7.533317437102365, AG 0.0
2024-04-07 04:23:54,639 : Time 03h 05m 36s, ave eps reward [-5.89 -5.89 -5.89], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 6.54, mean reward -5.885355419547484, std reward 5.3377919989850815, AG 0.0
2024-04-07 04:24:21,507 : Time 03h 06m 03s, ave eps reward [-5.15 -5.15 -5.15], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 6.73, mean reward -5.150769281211294, std reward 5.213317511253197, AG 0.0
2024-04-07 04:24:48,395 : Time 03h 06m 30s, ave eps reward [-6.24 -6.24 -6.24], ave eps length 10.0, reward step [-0.62 -0.62 -0.62], FPS 6.58, mean reward -6.240428573941211, std reward 6.448487223498524, AG 0.0
2024-04-07 04:25:15,331 : Time 03h 06m 57s, ave eps reward [-6.99 -6.99 -6.99], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 6.94, mean reward -6.993507222151718, std reward 5.345088350976662, AG 0.0
2024-04-07 04:25:42,299 : Time 03h 07m 24s, ave eps reward [-5.78 -5.78 -5.78], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 6.62, mean reward -5.784781698855054, std reward 4.805620541113694, AG 0.0
2024-04-07 04:26:08,921 : Time 03h 07m 50s, ave eps reward [-6.85 -6.85 -6.85], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 8.26, mean reward -6.851160608282295, std reward 6.235609708375401, AG 0.0
2024-04-07 04:26:35,539 : Time 03h 08m 17s, ave eps reward [-5.66 -5.66 -5.66], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 9.6, mean reward -5.655861898496506, std reward 5.530172827400902, AG 0.0
2024-04-07 04:27:02,999 : Time 03h 08m 44s, ave eps reward [-7.18 -7.18 -7.18], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 6.62, mean reward -7.179778953441276, std reward 7.651927317676345, AG 0.0
2024-04-07 04:27:29,798 : Time 03h 09m 11s, ave eps reward [-5.93 -5.93 -5.93], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 7.21, mean reward -5.925357558413545, std reward 5.732093799678493, AG 0.0
2024-04-07 04:27:56,358 : Time 03h 09m 38s, ave eps reward [-7.09 -7.09 -7.09], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 9.77, mean reward -7.089633969507657, std reward 5.997648559471222, AG 0.0
2024-04-07 04:28:23,163 : Time 03h 10m 05s, ave eps reward [-7.41 -7.41 -7.41], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 8.28, mean reward -7.406121945419557, std reward 6.632979664437111, AG 0.0
2024-04-07 04:28:50,118 : Time 03h 10m 32s, ave eps reward [-9.34 -9.34 -9.34], ave eps length 10.0, reward step [-0.93 -0.93 -0.93], FPS 7.08, mean reward -9.338034443939348, std reward 6.112335197483621, AG 0.0
2024-04-07 04:29:17,453 : Time 03h 10m 59s, ave eps reward [-6.82 -6.82 -6.82], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 7.27, mean reward -6.824689953722272, std reward 5.3797334605796525, AG 0.0
2024-04-07 04:29:44,016 : Time 03h 11m 25s, ave eps reward [-5.58 -5.58 -5.58], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 10.17, mean reward -5.584421096717298, std reward 4.621964771426571, AG 0.0
2024-04-07 04:30:10,892 : Time 03h 11m 52s, ave eps reward [-2.69 -2.69 -2.69], ave eps length 10.0, reward step [-0.27 -0.27 -0.27], FPS 7.93, mean reward -2.6897524309144094, std reward 1.2652036370746602, AG 0.0
2024-04-07 04:30:37,839 : Time 03h 12m 19s, ave eps reward [-6.24 -6.24 -6.24], ave eps length 10.0, reward step [-0.62 -0.62 -0.62], FPS 6.59, mean reward -6.237045595753283, std reward 5.106011270065517, AG 0.0
2024-04-07 04:31:04,693 : Time 03h 12m 46s, ave eps reward [-6.72 -6.72 -6.72], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 6.62, mean reward -6.7177775440719305, std reward 5.366474279502162, AG 0.0
2024-04-07 04:31:31,510 : Time 03h 13m 13s, ave eps reward [-7.07 -7.07 -7.07], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 8.54, mean reward -7.0723285977270205, std reward 6.456950876610071, AG 0.0
2024-04-07 04:31:58,360 : Time 03h 13m 40s, ave eps reward [-6.5 -6.5 -6.5], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 6.75, mean reward -6.504422752806275, std reward 5.4582826868622245, AG 0.0
2024-04-07 04:32:25,286 : Time 03h 14m 07s, ave eps reward [-6.74 -6.74 -6.74], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 6.59, mean reward -6.737010869977569, std reward 6.772777814088579, AG 0.0
2024-04-07 04:32:51,734 : Time 03h 14m 33s, ave eps reward [-5.53 -5.53 -5.53], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 8.97, mean reward -5.533144496317978, std reward 4.921673132436674, AG 0.0
2024-04-07 04:33:18,965 : Time 03h 15m 00s, ave eps reward [-5.32 -5.32 -5.32], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 6.72, mean reward -5.321651572420667, std reward 4.219220322813566, AG 0.0
2024-04-07 04:33:45,837 : Time 03h 15m 27s, ave eps reward [-4.75 -4.75 -4.75], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.68, mean reward -4.7473668302641325, std reward 4.824102605028244, AG 0.0
2024-04-07 04:34:12,557 : Time 03h 15m 54s, ave eps reward [-7.16 -7.16 -7.16], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 7.28, mean reward -7.1612728510681904, std reward 5.889120205093964, AG 0.0
2024-04-07 04:34:39,083 : Time 03h 16m 21s, ave eps reward [-6.94 -6.94 -6.94], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 9.96, mean reward -6.938442302489504, std reward 6.00216756238883, AG 0.0
2024-04-07 04:35:05,995 : Time 03h 16m 47s, ave eps reward [-6.89 -6.89 -6.89], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 7.42, mean reward -6.889399139577509, std reward 5.256844953454444, AG 0.0
2024-04-07 04:35:33,134 : Time 03h 17m 15s, ave eps reward [-6.3 -6.3 -6.3], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 7.78, mean reward -6.300750179485218, std reward 6.60063999279843, AG 0.0
2024-04-07 04:35:59,675 : Time 03h 17m 41s, ave eps reward [-6.07 -6.07 -6.07], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 10.33, mean reward -6.06856696474043, std reward 5.290630868675016, AG 0.0
2024-04-07 04:36:26,624 : Time 03h 18m 08s, ave eps reward [-4.58 -4.58 -4.58], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 7.37, mean reward -4.5832706165325785, std reward 4.215573223799693, AG 0.0
2024-04-07 04:36:53,584 : Time 03h 18m 35s, ave eps reward [-4.64 -4.64 -4.64], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 6.55, mean reward -4.639247795972787, std reward 4.9477115270118, AG 0.0
2024-04-07 04:37:20,504 : Time 03h 19m 02s, ave eps reward [-4.92 -4.92 -4.92], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.71, mean reward -4.9248029313987685, std reward 4.615981565274053, AG 0.0
2024-04-07 04:37:47,464 : Time 03h 19m 29s, ave eps reward [-6.9 -6.9 -6.9], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 6.98, mean reward -6.895957546189571, std reward 5.657035166525022, AG 0.0
2024-04-07 04:38:14,511 : Time 03h 19m 56s, ave eps reward [-5.53 -5.53 -5.53], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.76, mean reward -5.533947045287716, std reward 4.440634306197776, AG 0.0
2024-04-07 04:38:41,100 : Time 03h 20m 23s, ave eps reward [-5.22 -5.22 -5.22], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 8.49, mean reward -5.222751762622214, std reward 4.021431921608788, AG 0.0
2024-04-07 04:39:07,731 : Time 03h 20m 49s, ave eps reward [-4.34 -4.34 -4.34], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 9.18, mean reward -4.338113334028505, std reward 3.1813210346197476, AG 0.0
2024-04-07 04:39:35,276 : Time 03h 21m 17s, ave eps reward [-7.3 -7.3 -7.3], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 6.77, mean reward -7.295120044206688, std reward 6.733097346656106, AG 0.0
2024-04-07 04:40:01,969 : Time 03h 21m 43s, ave eps reward [-5.76 -5.76 -5.76], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 7.5, mean reward -5.76314532140336, std reward 5.238018955643277, AG 0.0
2024-04-07 04:40:28,492 : Time 03h 22m 10s, ave eps reward [-3.59 -3.59 -3.59], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 10.32, mean reward -3.591201812286525, std reward 1.7694520936723903, AG 0.0
2024-04-07 04:40:55,295 : Time 03h 22m 37s, ave eps reward [-5.46 -5.46 -5.46], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 7.8, mean reward -5.4619032895955275, std reward 4.878131970237076, AG 0.0
2024-04-07 04:41:22,189 : Time 03h 23m 04s, ave eps reward [-4.92 -4.92 -4.92], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.57, mean reward -4.921318515967958, std reward 3.1984731979648355, AG 0.0
2024-04-07 04:41:49,097 : Time 03h 23m 31s, ave eps reward [-4.74 -4.74 -4.74], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 10.14, mean reward -4.735578308358864, std reward 4.455168005849276, AG 0.0
2024-04-07 04:42:15,930 : Time 03h 23m 57s, ave eps reward [-5.88 -5.88 -5.88], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 8.08, mean reward -5.8846702554435355, std reward 4.90911548344499, AG 0.0
2024-04-07 04:42:42,763 : Time 03h 24m 24s, ave eps reward [-6.81 -6.81 -6.81], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 6.62, mean reward -6.810979594507893, std reward 5.989971300049463, AG 0.0
2024-04-07 04:43:09,605 : Time 03h 24m 51s, ave eps reward [-5.46 -5.46 -5.46], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.99, mean reward -5.458373086110256, std reward 4.371935134716018, AG 0.0
2024-04-07 04:43:36,572 : Time 03h 25m 18s, ave eps reward [-5.28 -5.28 -5.28], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 6.63, mean reward -5.280650543283463, std reward 5.8188332213324285, AG 0.0
2024-04-07 04:44:03,389 : Time 03h 25m 45s, ave eps reward [-4.86 -4.86 -4.86], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.75, mean reward -4.861827795761088, std reward 4.478243673185136, AG 0.0
2024-04-07 04:44:30,170 : Time 03h 26m 12s, ave eps reward [-5.37 -5.37 -5.37], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 7.1, mean reward -5.371162956048527, std reward 4.368745122551391, AG 0.0
2024-04-07 04:44:56,643 : Time 03h 26m 38s, ave eps reward [-6.67 -6.67 -6.67], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 9.94, mean reward -6.673479784338015, std reward 5.876963779685094, AG 0.0
2024-04-07 04:45:23,438 : Time 03h 27m 05s, ave eps reward [-6.05 -6.05 -6.05], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 8.11, mean reward -6.053329336722, std reward 5.7575011061162975, AG 0.0
2024-04-07 04:45:50,766 : Time 03h 27m 32s, ave eps reward [-5.44 -5.44 -5.44], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 7.68, mean reward -5.437293058901075, std reward 5.01866069324745, AG 0.0
2024-04-07 04:46:17,559 : Time 03h 27m 59s, ave eps reward [-6.99 -6.99 -6.99], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 8.71, mean reward -6.9914951476604115, std reward 6.3544336487725745, AG 0.0
2024-04-07 04:46:44,316 : Time 03h 28m 26s, ave eps reward [-3.68 -3.68 -3.68], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 8.48, mean reward -3.6796061808397105, std reward 2.464439828604024, AG 0.0
2024-04-07 04:47:11,175 : Time 03h 28m 53s, ave eps reward [-5.41 -5.41 -5.41], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 6.58, mean reward -5.412124172698787, std reward 4.573855844801833, AG 0.0
2024-04-07 04:47:38,103 : Time 03h 29m 20s, ave eps reward [-4.97 -4.97 -4.97], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 6.61, mean reward -4.973806824353662, std reward 3.1737427692313567, AG 0.0
2024-04-07 04:48:04,959 : Time 03h 29m 46s, ave eps reward [-5.28 -5.28 -5.28], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 8.96, mean reward -5.277695758808072, std reward 4.070056800450985, AG 0.0
2024-04-07 04:48:31,865 : Time 03h 30m 13s, ave eps reward [-5.52 -5.52 -5.52], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.78, mean reward -5.52215274499821, std reward 4.928620858860535, AG 0.0
2024-04-07 04:48:58,751 : Time 03h 30m 40s, ave eps reward [-5.85 -5.85 -5.85], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 6.69, mean reward -5.847893576455101, std reward 5.984578542524328, AG 0.0
2024-04-07 04:49:25,292 : Time 03h 31m 07s, ave eps reward [-5.14 -5.14 -5.14], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 8.21, mean reward -5.1413998113980295, std reward 4.51529019217554, AG 0.0
2024-04-07 04:49:52,484 : Time 03h 31m 34s, ave eps reward [-3.55 -3.55 -3.55], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 6.66, mean reward -3.5498442494229607, std reward 1.8488237271765853, AG 0.0
2024-04-07 04:50:19,437 : Time 03h 32m 01s, ave eps reward [-5.54 -5.54 -5.54], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.46, mean reward -5.539225930716874, std reward 5.044403955442068, AG 0.0
2024-04-07 04:50:46,354 : Time 03h 32m 28s, ave eps reward [-5.3 -5.3 -5.3], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 6.92, mean reward -5.295072438993874, std reward 3.754004973261129, AG 0.0
2024-04-07 04:51:12,857 : Time 03h 32m 54s, ave eps reward [-7.55 -7.55 -7.55], ave eps length 10.0, reward step [-0.75 -0.75 -0.75], FPS 9.96, mean reward -7.548636436660729, std reward 6.620459619418836, AG 0.0
2024-04-07 04:51:39,683 : Time 03h 33m 21s, ave eps reward [-5.61 -5.61 -5.61], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 7.94, mean reward -5.611598154568868, std reward 3.9772161103399557, AG 0.0
2024-04-07 04:52:07,304 : Time 03h 33m 49s, ave eps reward [-5.72 -5.72 -5.72], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 8.64, mean reward -5.7228208001282965, std reward 4.378873897164456, AG 0.0
2024-04-07 04:52:34,020 : Time 03h 34m 15s, ave eps reward [-5.37 -5.37 -5.37], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 9.63, mean reward -5.3670836807431215, std reward 4.9780304707174565, AG 0.0
2024-04-07 04:53:00,823 : Time 03h 34m 42s, ave eps reward [-4.26 -4.26 -4.26], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 8.07, mean reward -4.263875425682247, std reward 3.784104281274522, AG 0.0
2024-04-07 04:53:27,630 : Time 03h 35m 09s, ave eps reward [-4.7 -4.7 -4.7], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.65, mean reward -4.698022205219726, std reward 3.9924318292959398, AG 0.0
2024-04-07 04:53:54,483 : Time 03h 35m 36s, ave eps reward [-5.09 -5.09 -5.09], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.7, mean reward -5.086770469898478, std reward 4.226556797100562, AG 0.0
2024-04-07 04:54:21,300 : Time 03h 36m 03s, ave eps reward [-4.64 -4.64 -4.64], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 7.53, mean reward -4.638732893685052, std reward 4.894442047635072, AG 0.0
2024-04-07 04:54:48,182 : Time 03h 36m 30s, ave eps reward [-4.77 -4.77 -4.77], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.57, mean reward -4.773726743285534, std reward 3.3509553253651054, AG 0.0
2024-04-07 04:55:14,837 : Time 03h 36m 56s, ave eps reward [-4.88 -4.88 -4.88], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 7.41, mean reward -4.881359403107281, std reward 4.136573157916893, AG 0.0
2024-04-07 04:55:41,077 : Time 03h 37m 23s, ave eps reward [-5.73 -5.73 -5.73], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 10.51, mean reward -5.731600044802185, std reward 5.211221545260427, AG 0.0
2024-04-07 04:56:08,447 : Time 03h 37m 50s, ave eps reward [-6.52 -6.52 -6.52], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 6.74, mean reward -6.523512520539555, std reward 5.174028528021503, AG 0.0
2024-04-07 04:56:35,079 : Time 03h 38m 17s, ave eps reward [-6. -6. -6.], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 8.13, mean reward -6.001628998683189, std reward 4.535437593605499, AG 0.0
2024-04-07 04:57:01,685 : Time 03h 38m 43s, ave eps reward [-5.69 -5.69 -5.69], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 9.68, mean reward -5.6857452024465225, std reward 5.407805718059832, AG 0.0
2024-04-07 04:57:28,806 : Time 03h 39m 10s, ave eps reward [-4.65 -4.65 -4.65], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 7.03, mean reward -4.6454810227041445, std reward 3.617182634101191, AG 0.0
2024-04-07 04:57:55,600 : Time 03h 39m 37s, ave eps reward [-5.2 -5.2 -5.2], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 6.77, mean reward -5.199841104262396, std reward 4.979138493449118, AG 0.0
2024-04-07 04:58:22,402 : Time 03h 40m 04s, ave eps reward [-3.67 -3.67 -3.67], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 9.93, mean reward -3.674170336142833, std reward 3.600925089064951, AG 0.0
2024-04-07 04:58:49,275 : Time 03h 40m 31s, ave eps reward [-6.02 -6.02 -6.02], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 8.19, mean reward -6.024336854243047, std reward 6.284333455751742, AG 0.0
2024-04-07 04:59:16,187 : Time 03h 40m 58s, ave eps reward [-4.15 -4.15 -4.15], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.7, mean reward -4.151336931802378, std reward 2.860769044986028, AG 0.0
2024-04-07 04:59:42,941 : Time 03h 41m 24s, ave eps reward [-3.72 -3.72 -3.72], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 7.19, mean reward -3.7176991217384425, std reward 3.3780700204382854, AG 0.0
2024-04-07 05:00:09,987 : Time 03h 41m 51s, ave eps reward [-7.43 -7.43 -7.43], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 6.79, mean reward -7.434551455734019, std reward 5.191316837465603, AG 0.0
2024-04-07 05:00:36,974 : Time 03h 42m 18s, ave eps reward [-6.39 -6.39 -6.39], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 6.52, mean reward -6.386015706017027, std reward 4.799809779407825, AG 0.0
2024-04-07 05:01:03,819 : Time 03h 42m 45s, ave eps reward [-9.27 -9.27 -9.27], ave eps length 10.0, reward step [-0.93 -0.93 -0.93], FPS 6.69, mean reward -9.265461986192225, std reward 6.644670087247215, AG 0.0
2024-04-07 05:01:30,444 : Time 03h 43m 12s, ave eps reward [-5.9 -5.9 -5.9], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 9.04, mean reward -5.900092305897618, std reward 4.4532202753995, AG 0.0
2024-04-07 05:01:57,252 : Time 03h 43m 39s, ave eps reward [-4.99 -4.99 -4.99], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 9.1, mean reward -4.9851353230347115, std reward 4.06108848707004, AG 0.0
2024-04-07 05:02:24,739 : Time 03h 44m 06s, ave eps reward [-6.02 -6.02 -6.02], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 6.83, mean reward -6.022773105407523, std reward 5.490917481506783, AG 0.0
2024-04-07 05:02:51,356 : Time 03h 44m 33s, ave eps reward [-4.54 -4.54 -4.54], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 9.43, mean reward -4.535694712306328, std reward 3.881143947996798, AG 0.0
2024-04-07 05:03:18,322 : Time 03h 45m 00s, ave eps reward [-3.57 -3.57 -3.57], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 8.14, mean reward -3.5732191597518743, std reward 2.369537723307169, AG 0.0
2024-04-07 05:03:45,403 : Time 03h 45m 27s, ave eps reward [-6.57 -6.57 -6.57], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 6.53, mean reward -6.568856083051162, std reward 5.311713437447286, AG 0.0
2024-04-07 05:04:12,428 : Time 03h 45m 54s, ave eps reward [-3.38 -3.38 -3.38], ave eps length 10.0, reward step [-0.34 -0.34 -0.34], FPS 6.73, mean reward -3.3815228046761816, std reward 2.050161264207033, AG 0.0
2024-04-07 05:04:39,490 : Time 03h 46m 21s, ave eps reward [-6.29 -6.29 -6.29], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 7.8, mean reward -6.2851457056022735, std reward 5.616755475889909, AG 0.0
2024-04-07 05:05:06,490 : Time 03h 46m 48s, ave eps reward [-5.25 -5.25 -5.25], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 7.02, mean reward -5.2549373993839925, std reward 4.699499030848776, AG 0.0
2024-04-07 05:05:33,563 : Time 03h 47m 15s, ave eps reward [-5.11 -5.11 -5.11], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.61, mean reward -5.112813755813283, std reward 3.9486865246950424, AG 0.0
2024-04-07 05:06:00,415 : Time 03h 47m 42s, ave eps reward [-6.79 -6.79 -6.79], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 7.49, mean reward -6.78526397513069, std reward 5.495499472674384, AG 0.0
2024-04-07 05:06:27,478 : Time 03h 48m 09s, ave eps reward [-6.58 -6.58 -6.58], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 6.69, mean reward -6.582097503392895, std reward 5.642324505611471, AG 0.0
2024-04-07 05:06:54,538 : Time 03h 48m 36s, ave eps reward [-4.28 -4.28 -4.28], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 6.62, mean reward -4.283904731918696, std reward 2.8025214990795595, AG 0.0
2024-04-07 05:07:21,485 : Time 03h 49m 03s, ave eps reward [-5.08 -5.08 -5.08], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.67, mean reward -5.084479795645919, std reward 4.763543297627223, AG 0.0
2024-04-07 05:07:48,236 : Time 03h 49m 30s, ave eps reward [-4.26 -4.26 -4.26], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 8.54, mean reward -4.2588241368816355, std reward 2.329386906282182, AG 0.0
2024-04-07 05:08:14,932 : Time 03h 49m 56s, ave eps reward [-4.5 -4.5 -4.5], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 9.21, mean reward -4.503393596750686, std reward 2.714158158210608, AG 0.0
2024-04-07 05:08:42,598 : Time 03h 50m 24s, ave eps reward [-6.42 -6.42 -6.42], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 6.84, mean reward -6.418804944591192, std reward 5.81069635856407, AG 0.0
2024-04-07 05:09:09,285 : Time 03h 50m 51s, ave eps reward [-3.56 -3.56 -3.56], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 8.05, mean reward -3.5573693513740436, std reward 3.031216490532953, AG 0.0
2024-04-07 05:09:35,955 : Time 03h 51m 17s, ave eps reward [-5.69 -5.69 -5.69], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 9.46, mean reward -5.689894285527328, std reward 4.932211727928172, AG 0.0
2024-04-07 05:10:03,012 : Time 03h 51m 44s, ave eps reward [-6.27 -6.27 -6.27], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 6.87, mean reward -6.268616091436964, std reward 6.0211741672190024, AG 0.0
2024-04-07 05:10:30,051 : Time 03h 52m 11s, ave eps reward [-9.27 -9.27 -9.27], ave eps length 10.0, reward step [-0.93 -0.93 -0.93], FPS 6.57, mean reward -9.267654135676903, std reward 6.271850304744897, AG 0.0
2024-04-07 05:10:57,074 : Time 03h 52m 39s, ave eps reward [-7.86 -7.86 -7.86], ave eps length 10.0, reward step [-0.79 -0.79 -0.79], FPS 9.82, mean reward -7.864905320679135, std reward 5.562714501925742, AG 0.0
2024-04-07 05:11:24,106 : Time 03h 53m 06s, ave eps reward [-6.32 -6.32 -6.32], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 9.97, mean reward -6.316848906133705, std reward 6.085071306037668, AG 0.0
2024-04-07 05:11:51,090 : Time 03h 53m 33s, ave eps reward [-7.39 -7.39 -7.39], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 7.61, mean reward -7.3907817887760405, std reward 6.023135370270203, AG 0.0
2024-04-07 05:12:18,122 : Time 03h 54m 00s, ave eps reward [-4.19 -4.19 -4.19], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.73, mean reward -4.190574950166307, std reward 2.8116262129800904, AG 0.0
2024-04-07 05:12:45,196 : Time 03h 54m 27s, ave eps reward [-3.95 -3.95 -3.95], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 6.77, mean reward -3.9476441874192334, std reward 2.305876687009414, AG 0.0
2024-04-07 05:13:12,114 : Time 03h 54m 54s, ave eps reward [-4.04 -4.04 -4.04], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 6.99, mean reward -4.044730102615189, std reward 2.052974505012395, AG 0.0
2024-04-07 05:13:39,087 : Time 03h 55m 21s, ave eps reward [-6.84 -6.84 -6.84], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 6.65, mean reward -6.83789548892498, std reward 7.035637072027601, AG 0.0
2024-04-07 05:14:06,068 : Time 03h 55m 47s, ave eps reward [-6.18 -6.18 -6.18], ave eps length 10.0, reward step [-0.62 -0.62 -0.62], FPS 7.11, mean reward -6.184531726825784, std reward 4.705610030213093, AG 0.0
2024-04-07 05:14:32,635 : Time 03h 56m 14s, ave eps reward [-4.54 -4.54 -4.54], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 10.63, mean reward -4.541615219211399, std reward 5.067423161092454, AG 0.0
2024-04-07 05:15:00,239 : Time 03h 56m 42s, ave eps reward [-3.55 -3.55 -3.55], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 6.7, mean reward -3.5478309315673764, std reward 3.612790150755735, AG 0.0
2024-04-07 05:15:27,158 : Time 03h 57m 09s, ave eps reward [-4.9 -4.9 -4.9], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.96, mean reward -4.903439461457582, std reward 4.973623380805499, AG 0.0
2024-04-07 05:15:53,761 : Time 03h 57m 35s, ave eps reward [-5.5 -5.5 -5.5], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 10.28, mean reward -5.503885632499211, std reward 4.746874584196352, AG 0.0
2024-04-07 05:16:20,838 : Time 03h 58m 02s, ave eps reward [-5.36 -5.36 -5.36], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 7.39, mean reward -5.364270904619688, std reward 5.017762370424276, AG 0.0
2024-04-07 05:16:47,885 : Time 03h 58m 29s, ave eps reward [-4.93 -4.93 -4.93], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.6, mean reward -4.928621689471995, std reward 3.3827636760828175, AG 0.0
2024-04-07 05:17:14,882 : Time 03h 58m 56s, ave eps reward [-3.94 -3.94 -3.94], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 10.13, mean reward -3.9357773164858125, std reward 2.8775807817217887, AG 0.0
2024-04-07 05:17:41,938 : Time 03h 59m 23s, ave eps reward [-4.56 -4.56 -4.56], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 7.67, mean reward -4.558930642667746, std reward 4.098373053245546, AG 0.0
2024-04-07 05:18:08,892 : Time 03h 59m 50s, ave eps reward [-5.22 -5.22 -5.22], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 6.75, mean reward -5.218729699156361, std reward 5.231149607225124, AG 0.0
2024-04-07 05:18:35,903 : Time 04h 00m 17s, ave eps reward [-4.64 -4.64 -4.64], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 7.04, mean reward -4.635755199445869, std reward 4.432771164309009, AG 0.0
2024-04-07 05:19:02,987 : Time 04h 00m 44s, ave eps reward [-5.08 -5.08 -5.08], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.55, mean reward -5.0768182306602, std reward 3.582529677368916, AG 0.0
2024-04-07 05:19:30,025 : Time 04h 01m 11s, ave eps reward [-4.86 -4.86 -4.86], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 7.07, mean reward -4.864734575464499, std reward 5.173931906174215, AG 0.0
2024-04-07 05:19:57,055 : Time 04h 01m 38s, ave eps reward [-5.15 -5.15 -5.15], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.63, mean reward -5.147242895130328, std reward 4.398239030126279, AG 0.0
2024-04-07 05:20:23,851 : Time 04h 02m 05s, ave eps reward [-5.78 -5.78 -5.78], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 7.95, mean reward -5.780699246303398, std reward 3.7645390831464858, AG 0.0
2024-04-07 05:20:50,518 : Time 04h 02m 32s, ave eps reward [-5.7 -5.7 -5.7], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 10.34, mean reward -5.703512147416818, std reward 4.707522798185485, AG 0.0
2024-04-07 05:21:18,165 : Time 04h 03m 00s, ave eps reward [-5.04 -5.04 -5.04], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 6.7, mean reward -5.0366427148835875, std reward 4.073447109213658, AG 0.0
2024-04-07 05:21:45,122 : Time 04h 03m 27s, ave eps reward [-4.91 -4.91 -4.91], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 7.01, mean reward -4.909829445733989, std reward 3.3673369682130487, AG 0.0
2024-04-07 05:22:11,806 : Time 04h 03m 53s, ave eps reward [-6.44 -6.44 -6.44], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 9.43, mean reward -6.440159718124299, std reward 4.747648402917873, AG 0.0
2024-04-07 05:22:38,751 : Time 04h 04m 20s, ave eps reward [-5.6 -5.6 -5.6], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 8.29, mean reward -5.601226789877758, std reward 4.983489843899992, AG 0.0
2024-04-07 05:23:05,762 : Time 04h 04m 47s, ave eps reward [-6.64 -6.64 -6.64], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 6.53, mean reward -6.638286152432235, std reward 4.779380713662113, AG 0.0
2024-04-07 05:23:32,932 : Time 04h 05m 14s, ave eps reward [-4.32 -4.32 -4.32], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 9.14, mean reward -4.321091573053787, std reward 2.822236388040792, AG 0.0
2024-04-07 05:23:59,861 : Time 04h 05m 41s, ave eps reward [-3.49 -3.49 -3.49], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 8.78, mean reward -3.4923753859969, std reward 2.9955764576721875, AG 0.0
2024-04-07 05:24:26,905 : Time 04h 06m 08s, ave eps reward [-5.18 -5.18 -5.18], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 6.71, mean reward -5.182739148644972, std reward 3.7952281651935444, AG 0.0
2024-04-07 05:24:53,895 : Time 04h 06m 35s, ave eps reward [-4.96 -4.96 -4.96], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 6.79, mean reward -4.963211989022483, std reward 5.698523788819399, AG 0.0
2024-04-07 05:25:20,921 : Time 04h 07m 02s, ave eps reward [-4.81 -4.81 -4.81], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.67, mean reward -4.805733654074631, std reward 3.7532654263083423, AG 0.0
2024-04-07 05:25:47,981 : Time 04h 07m 29s, ave eps reward [-5.12 -5.12 -5.12], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 7.73, mean reward -5.116224326966976, std reward 4.209712722312768, AG 0.0
2024-04-07 05:26:15,027 : Time 04h 07m 56s, ave eps reward [-7.7 -7.7 -7.7], ave eps length 10.0, reward step [-0.77 -0.77 -0.77], FPS 6.68, mean reward -7.701225066391407, std reward 6.015189960380127, AG 0.0
2024-04-07 05:26:41,981 : Time 04h 08m 23s, ave eps reward [-6.38 -6.38 -6.38], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 7.19, mean reward -6.3848825575241746, std reward 4.978126470999094, AG 0.0
2024-04-07 05:27:08,619 : Time 04h 08m 50s, ave eps reward [-6.59 -6.59 -6.59], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 10.0, mean reward -6.593760550283585, std reward 5.44057225391278, AG 0.0
2024-04-07 05:27:36,139 : Time 04h 09m 18s, ave eps reward [-4.68 -4.68 -4.68], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.73, mean reward -4.678555047491296, std reward 4.696143308624766, AG 0.0
2024-04-07 05:28:03,102 : Time 04h 09m 45s, ave eps reward [-7.82 -7.82 -7.82], ave eps length 10.0, reward step [-0.78 -0.78 -0.78], FPS 7.55, mean reward -7.821182712208634, std reward 6.242771324493544, AG 0.0
2024-04-07 05:28:29,824 : Time 04h 10m 11s, ave eps reward [-7.59 -7.59 -7.59], ave eps length 10.0, reward step [-0.76 -0.76 -0.76], FPS 9.83, mean reward -7.594782506168433, std reward 6.287367224505736, AG 0.0
2024-04-07 05:28:56,860 : Time 04h 10m 38s, ave eps reward [-5.07 -5.07 -5.07], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.93, mean reward -5.073270934032335, std reward 3.24614011459435, AG 0.0
2024-04-07 05:29:23,912 : Time 04h 11m 05s, ave eps reward [-3.61 -3.61 -3.61], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 6.73, mean reward -3.612376521462702, std reward 2.4176204007786564, AG 0.0
2024-04-07 05:29:50,943 : Time 04h 11m 32s, ave eps reward [-4.83 -4.83 -4.83], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 9.64, mean reward -4.831012831602329, std reward 3.633496890021104, AG 0.0
2024-04-07 05:30:17,946 : Time 04h 11m 59s, ave eps reward [-5.97 -5.97 -5.97], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 6.99, mean reward -5.973940115854808, std reward 4.448690969177913, AG 0.0
2024-04-07 05:30:45,011 : Time 04h 12m 26s, ave eps reward [-5.63 -5.63 -5.63], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.63, mean reward -5.628300372914672, std reward 4.388705513116805, AG 0.0
2024-04-07 05:31:11,984 : Time 04h 12m 53s, ave eps reward [-3.76 -3.76 -3.76], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 7.19, mean reward -3.7573476229184677, std reward 3.74997797759542, AG 0.0
2024-04-07 05:31:38,992 : Time 04h 13m 20s, ave eps reward [-3.94 -3.94 -3.94], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 6.68, mean reward -3.9399294506404514, std reward 4.327183250690189, AG 0.0
2024-04-07 05:32:06,076 : Time 04h 13m 48s, ave eps reward [-4.36 -4.36 -4.36], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 6.51, mean reward -4.36122213031638, std reward 3.531781739094869, AG 0.0
2024-04-07 05:32:33,060 : Time 04h 14m 14s, ave eps reward [-6.05 -6.05 -6.05], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 6.54, mean reward -6.0502360337262395, std reward 5.461480557844773, AG 0.0
2024-04-07 05:33:00,027 : Time 04h 14m 41s, ave eps reward [-4.24 -4.24 -4.24], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.64, mean reward -4.2402673876701105, std reward 3.0979281414488695, AG 0.0
2024-04-07 05:33:26,700 : Time 04h 15m 08s, ave eps reward [-4.29 -4.29 -4.29], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 8.61, mean reward -4.288856268514428, std reward 4.601690633659238, AG 0.0
2024-04-07 05:33:53,696 : Time 04h 15m 35s, ave eps reward [-5.9 -5.9 -5.9], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 6.75, mean reward -5.904932095610489, std reward 5.312040600812617, AG 0.0
2024-04-07 05:34:20,501 : Time 04h 16m 02s, ave eps reward [-5.25 -5.25 -5.25], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 6.97, mean reward -5.249994971224785, std reward 4.579714757860279, AG 0.0
2024-04-07 05:34:46,913 : Time 04h 16m 28s, ave eps reward [-3.49 -3.49 -3.49], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 8.83, mean reward -3.492729042013167, std reward 2.294936573677454, AG 0.0
2024-04-07 05:35:13,529 : Time 04h 16m 55s, ave eps reward [-4.47 -4.47 -4.47], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 9.29, mean reward -4.472635244915836, std reward 3.046056254429003, AG 0.0
2024-04-07 05:35:40,536 : Time 04h 17m 22s, ave eps reward [-3.96 -3.96 -3.96], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 6.7, mean reward -3.9581753546510847, std reward 2.5934224920345876, AG 0.0
2024-04-07 05:36:07,758 : Time 04h 17m 49s, ave eps reward [-6.9 -6.9 -6.9], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 9.33, mean reward -6.896780257164323, std reward 5.458714735613269, AG 0.0
2024-04-07 05:36:34,700 : Time 04h 18m 16s, ave eps reward [-6.2 -6.2 -6.2], ave eps length 10.0, reward step [-0.62 -0.62 -0.62], FPS 7.83, mean reward -6.195215699960175, std reward 5.7094311617772115, AG 0.0
2024-04-07 05:37:01,794 : Time 04h 18m 43s, ave eps reward [-6.77 -6.77 -6.77], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 6.55, mean reward -6.768774839214063, std reward 5.463680992862152, AG 0.0
2024-04-07 05:37:28,811 : Time 04h 19m 10s, ave eps reward [-6.68 -6.68 -6.68], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 6.7, mean reward -6.675173232996784, std reward 6.494934903874759, AG 0.0
2024-04-07 05:37:55,956 : Time 04h 19m 37s, ave eps reward [-3.52 -3.52 -3.52], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 6.77, mean reward -3.524219208446818, std reward 2.045088331210845, AG 0.0
2024-04-07 05:38:23,042 : Time 04h 20m 04s, ave eps reward [-5.29 -5.29 -5.29], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 7.05, mean reward -5.291548218137988, std reward 4.352116082301703, AG 0.0
2024-04-07 05:38:50,188 : Time 04h 20m 32s, ave eps reward [-5.32 -5.32 -5.32], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 6.73, mean reward -5.316736063684199, std reward 3.727266911159646, AG 0.0
2024-04-07 05:39:17,045 : Time 04h 20m 58s, ave eps reward [-4.55 -4.55 -4.55], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 7.66, mean reward -4.5467709266871355, std reward 2.823729957926767, AG 0.0
2024-04-07 05:39:43,829 : Time 04h 21m 25s, ave eps reward [-4.3 -4.3 -4.3], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 9.88, mean reward -4.299165369485412, std reward 3.5835405136354423, AG 0.0
2024-04-07 05:40:11,491 : Time 04h 21m 53s, ave eps reward [-4.82 -4.82 -4.82], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.73, mean reward -4.820377461129267, std reward 4.476008375238633, AG 0.0
2024-04-07 05:40:38,529 : Time 04h 22m 20s, ave eps reward [-5.45 -5.45 -5.45], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 7.51, mean reward -5.44918082439937, std reward 4.233959469875639, AG 0.0
2024-04-07 05:41:05,236 : Time 04h 22m 47s, ave eps reward [-3.2 -3.2 -3.2], ave eps length 10.0, reward step [-0.32 -0.32 -0.32], FPS 10.09, mean reward -3.2001994978445247, std reward 1.5220048735807021, AG 0.0
2024-04-07 05:41:32,499 : Time 04h 23m 14s, ave eps reward [-6.99 -6.99 -6.99], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 6.99, mean reward -6.992116601236755, std reward 5.37109328633173, AG 0.0
2024-04-07 05:41:59,611 : Time 04h 23m 41s, ave eps reward [-4.24 -4.24 -4.24], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.67, mean reward -4.244047317299005, std reward 3.400099797808097, AG 0.0
2024-04-07 05:42:26,682 : Time 04h 24m 08s, ave eps reward [-4.77 -4.77 -4.77], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 10.45, mean reward -4.773131426789391, std reward 3.671792828111551, AG 0.0
2024-04-07 05:42:53,702 : Time 04h 24m 35s, ave eps reward [-5.75 -5.75 -5.75], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 7.53, mean reward -5.748014714175062, std reward 4.386672343584713, AG 0.0
2024-04-07 05:43:20,897 : Time 04h 25m 02s, ave eps reward [-5.78 -5.78 -5.78], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 6.49, mean reward -5.778458452155067, std reward 6.043698085663203, AG 0.0
2024-04-07 05:43:47,986 : Time 04h 25m 29s, ave eps reward [-5.05 -5.05 -5.05], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 6.88, mean reward -5.046150252542821, std reward 4.075931750092332, AG 0.0
2024-04-07 05:44:15,237 : Time 04h 25m 57s, ave eps reward [-4.19 -4.19 -4.19], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.65, mean reward -4.188525821164485, std reward 4.464293037483448, AG 0.0
2024-04-07 05:44:42,268 : Time 04h 26m 24s, ave eps reward [-4.49 -4.49 -4.49], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.65, mean reward -4.494125467493216, std reward 4.177723789943979, AG 0.0
2024-04-07 05:45:09,322 : Time 04h 26m 51s, ave eps reward [-4.49 -4.49 -4.49], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.89, mean reward -4.487809590693904, std reward 4.024297757671677, AG 0.0
2024-04-07 05:45:36,125 : Time 04h 27m 18s, ave eps reward [-4.08 -4.08 -4.08], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 8.81, mean reward -4.083633262208208, std reward 2.614734529350593, AG 0.0
2024-04-07 05:46:03,048 : Time 04h 27m 44s, ave eps reward [-4.11 -4.11 -4.11], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 9.0, mean reward -4.114471386377944, std reward 3.875939710256309, AG 0.0
2024-04-07 05:46:30,753 : Time 04h 28m 12s, ave eps reward [-3.71 -3.71 -3.71], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.7, mean reward -3.7056666918373606, std reward 1.8026517228075565, AG 0.0
2024-04-07 05:46:57,468 : Time 04h 28m 39s, ave eps reward [-4.68 -4.68 -4.68], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 8.41, mean reward -4.683047201691293, std reward 2.3108459046577625, AG 0.0
2024-04-07 05:47:24,393 : Time 04h 29m 06s, ave eps reward [-3.23 -3.23 -3.23], ave eps length 10.0, reward step [-0.32 -0.32 -0.32], FPS 8.46, mean reward -3.2289501805806777, std reward 2.084801911675392, AG 0.0
2024-04-07 05:47:51,522 : Time 04h 29m 33s, ave eps reward [-7.84 -7.84 -7.84], ave eps length 10.0, reward step [-0.78 -0.78 -0.78], FPS 6.53, mean reward -7.842780907336477, std reward 6.226245544256314, AG 0.0
2024-04-07 05:48:18,573 : Time 04h 30m 00s, ave eps reward [-5.66 -5.66 -5.66], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 6.66, mean reward -5.660524161553576, std reward 4.778953840561227, AG 0.0
2024-04-07 05:48:45,653 : Time 04h 30m 27s, ave eps reward [-3.88 -3.88 -3.88], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 8.62, mean reward -3.87639531934621, std reward 3.4027151335701804, AG 0.0
2024-04-07 05:49:12,754 : Time 04h 30m 54s, ave eps reward [-3.89 -3.89 -3.89], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 6.77, mean reward -3.8887248160862358, std reward 2.3548123897184876, AG 0.0
2024-04-07 05:49:39,855 : Time 04h 31m 21s, ave eps reward [-5.77 -5.77 -5.77], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 6.77, mean reward -5.77113116649951, std reward 4.654502883884326, AG 0.0
2024-04-07 05:50:06,858 : Time 04h 31m 48s, ave eps reward [-5.09 -5.09 -5.09], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 7.13, mean reward -5.0851389012865145, std reward 3.53618385096881, AG 0.0
2024-04-07 05:50:34,113 : Time 04h 32m 16s, ave eps reward [-4.31 -4.31 -4.31], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 6.57, mean reward -4.31404313722818, std reward 2.208753520469521, AG 0.0
2024-04-07 05:51:01,266 : Time 04h 32m 43s, ave eps reward [-4.76 -4.76 -4.76], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.7, mean reward -4.761552158898191, std reward 3.2378248130043312, AG 0.0
2024-04-07 05:51:28,332 : Time 04h 33m 10s, ave eps reward [-4.99 -4.99 -4.99], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 7.33, mean reward -4.985827747585607, std reward 4.459979371759353, AG 0.0
2024-04-07 05:51:55,141 : Time 04h 33m 37s, ave eps reward [-5. -5. -5.], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 10.22, mean reward -4.997750117270089, std reward 4.494053538075625, AG 0.0
2024-04-07 05:52:22,118 : Time 04h 34m 04s, ave eps reward [-4.74 -4.74 -4.74], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 8.09, mean reward -4.743037276243049, std reward 4.042823396242142, AG 0.0
2024-04-07 05:52:49,676 : Time 04h 34m 31s, ave eps reward [-6.18 -6.18 -6.18], ave eps length 10.0, reward step [-0.62 -0.62 -0.62], FPS 7.45, mean reward -6.180671180475325, std reward 7.009844523994952, AG 0.0
2024-04-07 05:53:16,479 : Time 04h 34m 58s, ave eps reward [-5.22 -5.22 -5.22], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 10.46, mean reward -5.219042246621302, std reward 5.024608334152463, AG 0.0
2024-04-07 05:53:43,506 : Time 04h 35m 25s, ave eps reward [-4.46 -4.46 -4.46], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 9.16, mean reward -4.461119704427081, std reward 3.058596263778506, AG 0.0
2024-04-07 05:54:10,610 : Time 04h 35m 52s, ave eps reward [-5.76 -5.76 -5.76], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 7.19, mean reward -5.757587701404171, std reward 4.712490977242886, AG 0.0
2024-04-07 05:54:37,699 : Time 04h 36m 19s, ave eps reward [-4.21 -4.21 -4.21], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.56, mean reward -4.21245065834541, std reward 2.79552158915531, AG 0.0
2024-04-07 05:55:04,840 : Time 04h 36m 46s, ave eps reward [-5.22 -5.22 -5.22], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 9.85, mean reward -5.218655593770329, std reward 4.248150209140698, AG 0.0
2024-04-07 05:55:31,852 : Time 04h 37m 13s, ave eps reward [-5.41 -5.41 -5.41], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 7.87, mean reward -5.409163518849316, std reward 4.411226767438906, AG 0.0
2024-04-07 05:55:58,968 : Time 04h 37m 40s, ave eps reward [-5.23 -5.23 -5.23], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 6.56, mean reward -5.225496250362773, std reward 3.6276684559040233, AG 0.0
2024-04-07 05:56:25,954 : Time 04h 38m 07s, ave eps reward [-5. -5. -5.], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 6.7, mean reward -5.000427548197477, std reward 4.501155571667422, AG 0.0
2024-04-07 05:56:53,157 : Time 04h 38m 35s, ave eps reward [-5.35 -5.35 -5.35], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 6.63, mean reward -5.351885145638186, std reward 4.7639178561548094, AG 0.0
2024-04-07 05:57:20,326 : Time 04h 39m 02s, ave eps reward [-4.47 -4.47 -4.47], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.64, mean reward -4.471539319445268, std reward 3.1627671393597585, AG 0.0
2024-04-07 05:57:47,440 : Time 04h 39m 29s, ave eps reward [-5.06 -5.06 -5.06], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.74, mean reward -5.05688611960738, std reward 4.5678157790639835, AG 0.0
2024-04-07 05:58:14,184 : Time 04h 39m 56s, ave eps reward [-6.36 -6.36 -6.36], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 9.01, mean reward -6.3553482192038615, std reward 4.007093375216269, AG 0.0
2024-04-07 05:58:41,136 : Time 04h 40m 23s, ave eps reward [-4.54 -4.54 -4.54], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 8.81, mean reward -4.54495078339748, std reward 4.02665155630686, AG 0.0
2024-04-07 05:59:08,758 : Time 04h 40m 50s, ave eps reward [-6.52 -6.52 -6.52], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 6.77, mean reward -6.516108277266492, std reward 4.467178717786116, AG 0.0
2024-04-07 05:59:35,452 : Time 04h 41m 17s, ave eps reward [-5. -5. -5.], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 9.2, mean reward -5.0045720947757015, std reward 3.2117725874834155, AG 0.0
2024-04-07 06:00:02,492 : Time 04h 41m 44s, ave eps reward [-5.26 -5.26 -5.26], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 8.14, mean reward -5.264167958514074, std reward 4.553692224543066, AG 0.0
2024-04-07 06:00:29,601 : Time 04h 42m 11s, ave eps reward [-5.71 -5.71 -5.71], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 6.57, mean reward -5.708740838070672, std reward 5.570980227384278, AG 0.0
2024-04-07 06:00:56,813 : Time 04h 42m 38s, ave eps reward [-4.2 -4.2 -4.2], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.75, mean reward -4.200531987821052, std reward 2.530040765307285, AG 0.0
2024-04-07 06:01:23,878 : Time 04h 43m 05s, ave eps reward [-3.85 -3.85 -3.85], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 8.47, mean reward -3.8477577704230987, std reward 1.8370733704260456, AG 0.0
2024-04-07 06:01:50,958 : Time 04h 43m 32s, ave eps reward [-6.72 -6.72 -6.72], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 6.68, mean reward -6.719878316355972, std reward 5.012667799923841, AG 0.0
2024-04-07 06:02:18,014 : Time 04h 43m 59s, ave eps reward [-4.78 -4.78 -4.78], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 7.04, mean reward -4.7798928589302525, std reward 3.8856055193550563, AG 0.0
2024-04-07 06:02:44,948 : Time 04h 44m 26s, ave eps reward [-3.6 -3.6 -3.6], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 7.96, mean reward -3.5989133716272326, std reward 1.699908538589369, AG 0.0
2024-04-07 06:03:12,370 : Time 04h 44m 54s, ave eps reward [-3.36 -3.36 -3.36], ave eps length 10.0, reward step [-0.34 -0.34 -0.34], FPS 6.66, mean reward -3.359790897858043, std reward 1.9042089252522167, AG 0.0
2024-04-07 06:03:39,453 : Time 04h 45m 21s, ave eps reward [-4.87 -4.87 -4.87], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.66, mean reward -4.869222925470141, std reward 4.370996376922686, AG 0.0
2024-04-07 06:04:06,262 : Time 04h 45m 48s, ave eps reward [-5.22 -5.22 -5.22], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 8.02, mean reward -5.220527135410058, std reward 3.9565993644903954, AG 0.0
2024-04-07 06:04:33,107 : Time 04h 46m 15s, ave eps reward [-4.18 -4.18 -4.18], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 8.64, mean reward -4.184578183410751, std reward 2.9759794554478436, AG 0.0
2024-04-07 06:05:00,230 : Time 04h 46m 42s, ave eps reward [-5.92 -5.92 -5.92], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 6.48, mean reward -5.920042993751748, std reward 5.356030838298975, AG 0.0
2024-04-07 06:05:27,572 : Time 04h 47m 09s, ave eps reward [-5.46 -5.46 -5.46], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 8.83, mean reward -5.4646426176110765, std reward 5.695767740509448, AG 0.0
2024-04-07 06:05:54,583 : Time 04h 47m 36s, ave eps reward [-7.68 -7.68 -7.68], ave eps length 10.0, reward step [-0.77 -0.77 -0.77], FPS 8.15, mean reward -7.6824909918717825, std reward 7.057689379439072, AG 0.0
2024-04-07 06:06:21,716 : Time 04h 48m 03s, ave eps reward [-6.53 -6.53 -6.53], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 6.64, mean reward -6.52981292108809, std reward 5.156870550127415, AG 0.0
2024-04-07 06:06:48,720 : Time 04h 48m 30s, ave eps reward [-8.11 -8.11 -8.11], ave eps length 10.0, reward step [-0.81 -0.81 -0.81], FPS 7.14, mean reward -8.112893691491076, std reward 5.151480042241812, AG 0.0
2024-04-07 06:07:15,960 : Time 04h 48m 57s, ave eps reward [-6.97 -6.97 -6.97], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 6.75, mean reward -6.968457580448147, std reward 4.246006288206582, AG 0.0
2024-04-07 06:07:43,079 : Time 04h 49m 25s, ave eps reward [-7.12 -7.12 -7.12], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 6.75, mean reward -7.118611701170437, std reward 5.000164954429199, AG 0.0
2024-04-07 06:08:10,041 : Time 04h 49m 51s, ave eps reward [-5.09 -5.09 -5.09], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 7.15, mean reward -5.08690079089966, std reward 5.886126991239159, AG 0.0
2024-04-07 06:08:36,744 : Time 04h 50m 18s, ave eps reward [-3.79 -3.79 -3.79], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 10.2, mean reward -3.7854194393334866, std reward 2.6902962157622285, AG 0.0
2024-04-07 06:09:03,880 : Time 04h 50m 45s, ave eps reward [-3.9 -3.9 -3.9], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 8.6, mean reward -3.9021077563776077, std reward 2.515804389803179, AG 0.0
2024-04-07 06:09:31,643 : Time 04h 51m 13s, ave eps reward [-7.22 -7.22 -7.22], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 6.71, mean reward -7.217621843158729, std reward 5.505306294924369, AG 0.0
2024-04-07 06:09:58,411 : Time 04h 51m 40s, ave eps reward [-6.57 -6.57 -6.57], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 8.91, mean reward -6.567824202264985, std reward 5.734454181558535, AG 0.0
2024-04-07 06:10:25,400 : Time 04h 52m 07s, ave eps reward [-6.57 -6.57 -6.57], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 8.23, mean reward -6.569962797772634, std reward 4.694805612084621, AG 0.0
2024-04-07 06:10:52,599 : Time 04h 52m 34s, ave eps reward [-6.12 -6.12 -6.12], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 6.51, mean reward -6.122218587639848, std reward 4.900947974649419, AG 0.0
2024-04-07 06:11:19,701 : Time 04h 53m 01s, ave eps reward [-4.53 -4.53 -4.53], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.82, mean reward -4.533263056577305, std reward 3.3215382957598747, AG 0.0
2024-04-07 06:11:46,842 : Time 04h 53m 28s, ave eps reward [-3.64 -3.64 -3.64], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 7.42, mean reward -3.6404124764524446, std reward 2.0055568181617818, AG 0.0
2024-04-07 06:12:14,025 : Time 04h 53m 55s, ave eps reward [-4.39 -4.39 -4.39], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 6.58, mean reward -4.394343922782319, std reward 4.776183086877092, AG 0.0
2024-04-07 06:12:40,940 : Time 04h 54m 22s, ave eps reward [-3.5 -3.5 -3.5], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 7.55, mean reward -3.497862832904667, std reward 1.3257712288434067, AG 0.0
2024-04-07 06:13:07,584 : Time 04h 54m 49s, ave eps reward [-3.6 -3.6 -3.6], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 10.13, mean reward -3.6027953103045443, std reward 1.9364106107431163, AG 0.0
2024-04-07 06:13:34,913 : Time 04h 55m 16s, ave eps reward [-5.4 -5.4 -5.4], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 6.59, mean reward -5.397291720581277, std reward 5.270434672809327, AG 0.0
2024-04-07 06:14:01,301 : Time 04h 55m 43s, ave eps reward [-7.59 -7.59 -7.59], ave eps length 10.0, reward step [-0.76 -0.76 -0.76], FPS 8.58, mean reward -7.593453792932346, std reward 5.017846170386645, AG 0.0
2024-04-07 06:14:27,750 : Time 04h 56m 09s, ave eps reward [-3.79 -3.79 -3.79], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 8.94, mean reward -3.785632227441907, std reward 2.6782335228353387, AG 0.0
2024-04-07 06:14:54,516 : Time 04h 56m 36s, ave eps reward [-4.22 -4.22 -4.22], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.73, mean reward -4.22447502986781, std reward 2.7956134130023877, AG 0.0
2024-04-07 06:15:21,231 : Time 04h 57m 03s, ave eps reward [-6.17 -6.17 -6.17], ave eps length 10.0, reward step [-0.62 -0.62 -0.62], FPS 6.59, mean reward -6.171646210549189, std reward 5.1786599263226405, AG 0.0
2024-04-07 06:15:47,953 : Time 04h 57m 29s, ave eps reward [-5.1 -5.1 -5.1], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 10.4, mean reward -5.103889602661354, std reward 5.207800801788622, AG 0.0
2024-04-07 06:16:14,727 : Time 04h 57m 56s, ave eps reward [-3.67 -3.67 -3.67], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.78, mean reward -3.67149328674049, std reward 2.325397963968863, AG 0.0
2024-04-07 06:16:41,444 : Time 04h 58m 23s, ave eps reward [-5.81 -5.81 -5.81], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 6.61, mean reward -5.811324313476378, std reward 4.684234288364355, AG 0.0
2024-04-07 06:17:07,834 : Time 04h 58m 49s, ave eps reward [-4.55 -4.55 -4.55], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 8.33, mean reward -4.551627600245259, std reward 3.850360516017, AG 0.0
2024-04-07 06:17:34,808 : Time 04h 59m 16s, ave eps reward [-5.08 -5.08 -5.08], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.8, mean reward -5.080700200398434, std reward 4.352589307533857, AG 0.0
2024-04-07 06:18:01,430 : Time 04h 59m 43s, ave eps reward [-4.11 -4.11 -4.11], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 7.19, mean reward -4.109881954257029, std reward 3.570948913674498, AG 0.0
2024-04-07 06:18:27,646 : Time 05h 00m 09s, ave eps reward [-5.3 -5.3 -5.3], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 10.6, mean reward -5.303818350960621, std reward 3.8401700049109424, AG 0.0
2024-04-07 06:18:54,242 : Time 05h 00m 36s, ave eps reward [-4.41 -4.41 -4.41], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 7.3, mean reward -4.413399998776674, std reward 3.1654664043059135, AG 0.0
2024-04-07 06:19:20,947 : Time 05h 01m 02s, ave eps reward [-3.18 -3.18 -3.18], ave eps length 10.0, reward step [-0.32 -0.32 -0.32], FPS 6.7, mean reward -3.1844841220575324, std reward 1.7740254685097576, AG 0.0
2024-04-07 06:19:47,563 : Time 05h 01m 29s, ave eps reward [-4.87 -4.87 -4.87], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 10.39, mean reward -4.874790554839814, std reward 3.516510477095619, AG 0.0
2024-04-07 06:20:14,266 : Time 05h 01m 56s, ave eps reward [-4.09 -4.09 -4.09], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 6.94, mean reward -4.093373245975655, std reward 3.1526388533637326, AG 0.0
2024-04-07 06:20:40,967 : Time 05h 02m 22s, ave eps reward [-5.55 -5.55 -5.55], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.67, mean reward -5.5500514474716685, std reward 5.446412420844576, AG 0.0
2024-04-07 06:21:07,331 : Time 05h 02m 49s, ave eps reward [-3.09 -3.09 -3.09], ave eps length 10.0, reward step [-0.31 -0.31 -0.31], FPS 8.59, mean reward -3.08933643408086, std reward 1.3337247603008109, AG 0.0
2024-04-07 06:21:34,303 : Time 05h 03m 16s, ave eps reward [-5.68 -5.68 -5.68], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 6.87, mean reward -5.677462680886203, std reward 4.551460232370031, AG 0.0
2024-04-07 06:22:00,982 : Time 05h 03m 42s, ave eps reward [-4.78 -4.78 -4.78], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 7.14, mean reward -4.783139691365472, std reward 3.410012194623632, AG 0.0
2024-04-07 06:22:27,256 : Time 05h 04m 09s, ave eps reward [-3.63 -3.63 -3.63], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 9.57, mean reward -3.6286930553554733, std reward 1.3674760401734318, AG 0.0
2024-04-07 06:22:53,738 : Time 05h 04m 35s, ave eps reward [-3.46 -3.46 -3.46], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 9.53, mean reward -3.462817810721513, std reward 2.2934435395984347, AG 0.0
2024-04-07 06:23:20,478 : Time 05h 05m 02s, ave eps reward [-4.71 -4.71 -4.71], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.67, mean reward -4.712836131131263, std reward 3.787839931922883, AG 0.0
2024-04-07 06:23:47,383 : Time 05h 05m 29s, ave eps reward [-4.27 -4.27 -4.27], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 9.28, mean reward -4.268779483407211, std reward 3.485639308242647, AG 0.0
2024-04-07 06:24:14,478 : Time 05h 05m 56s, ave eps reward [-4.78 -4.78 -4.78], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 7.18, mean reward -4.77611245660883, std reward 4.803211068879183, AG 0.0
2024-04-07 06:24:41,077 : Time 05h 06m 23s, ave eps reward [-5.97 -5.97 -5.97], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 6.79, mean reward -5.970300163572289, std reward 4.8300780577780005, AG 0.0
2024-04-07 06:25:07,093 : Time 05h 06m 49s, ave eps reward [-7.39 -7.39 -7.39], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 9.01, mean reward -7.391754593729289, std reward 4.7998884208448676, AG 0.0
2024-04-07 06:25:33,878 : Time 05h 07m 15s, ave eps reward [-6.83 -6.83 -6.83], ave eps length 10.0, reward step [-0.68 -0.68 -0.68], FPS 6.98, mean reward -6.826215128395613, std reward 5.131644055257324, AG 0.0
2024-04-07 06:26:00,045 : Time 05h 07m 41s, ave eps reward [-4.87 -4.87 -4.87], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 8.02, mean reward -4.871779720001165, std reward 4.0469536108973845, AG 0.0
2024-04-07 06:26:26,154 : Time 05h 08m 08s, ave eps reward [-4.96 -4.96 -4.96], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 8.94, mean reward -4.958117815393793, std reward 3.8058996776926577, AG 0.0
2024-04-07 06:26:52,563 : Time 05h 08m 34s, ave eps reward [-5.01 -5.01 -5.01], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 6.66, mean reward -5.0103818534549704, std reward 5.443102819031135, AG 0.0
2024-04-07 06:27:18,983 : Time 05h 09m 00s, ave eps reward [-3.98 -3.98 -3.98], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 6.92, mean reward -3.979440617488783, std reward 1.9097833529938508, AG 0.0
2024-04-07 06:27:45,386 : Time 05h 09m 27s, ave eps reward [-6.54 -6.54 -6.54], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 6.84, mean reward -6.537797483608953, std reward 5.175549340859088, AG 0.0
2024-04-07 06:28:11,739 : Time 05h 09m 53s, ave eps reward [-5.22 -5.22 -5.22], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 6.94, mean reward -5.223724754190887, std reward 4.111700148327046, AG 0.0
2024-04-07 06:28:37,634 : Time 05h 10m 19s, ave eps reward [-5.83 -5.83 -5.83], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 10.59, mean reward -5.833280896236061, std reward 5.271593681688552, AG 0.0
2024-04-07 06:29:04,047 : Time 05h 10m 45s, ave eps reward [-5.72 -5.72 -5.72], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 8.38, mean reward -5.719284525985428, std reward 5.103863387984351, AG 0.0
2024-04-07 06:29:30,853 : Time 05h 11m 12s, ave eps reward [-6.29 -6.29 -6.29], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 7.62, mean reward -6.291983776977561, std reward 5.4868147669021585, AG 0.0
2024-04-07 06:29:56,807 : Time 05h 11m 38s, ave eps reward [-6.99 -6.99 -6.99], ave eps length 10.0, reward step [-0.7 -0.7 -0.7], FPS 10.1, mean reward -6.986803847967782, std reward 4.630354304693663, AG 0.0
2024-04-07 06:30:23,194 : Time 05h 12m 05s, ave eps reward [-4.86 -4.86 -4.86], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.89, mean reward -4.861708081874259, std reward 4.338144703081495, AG 0.0
2024-04-07 06:30:49,652 : Time 05h 12m 31s, ave eps reward [-4.32 -4.32 -4.32], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 6.68, mean reward -4.319629109708492, std reward 2.821814871990639, AG 0.0
2024-04-07 06:31:16,166 : Time 05h 12m 58s, ave eps reward [-4.63 -4.63 -4.63], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 6.61, mean reward -4.632949337739122, std reward 3.5040804362515123, AG 0.0
2024-04-07 06:31:42,512 : Time 05h 13m 24s, ave eps reward [-4.59 -4.59 -4.59], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 6.67, mean reward -4.5908168372743825, std reward 3.4595820670845154, AG 0.0
2024-04-07 06:32:08,689 : Time 05h 13m 50s, ave eps reward [-3.25 -3.25 -3.25], ave eps length 10.0, reward step [-0.32 -0.32 -0.32], FPS 8.34, mean reward -3.249857225034366, std reward 2.0194914560907447, AG 0.0
2024-04-07 06:32:34,745 : Time 05h 14m 16s, ave eps reward [-4.8 -4.8 -4.8], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 10.4, mean reward -4.795151755273594, std reward 5.087496914818995, AG 0.0
2024-04-07 06:33:01,142 : Time 05h 14m 43s, ave eps reward [-5.49 -5.49 -5.49], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 7.14, mean reward -5.486827384775784, std reward 4.7121655019837645, AG 0.0
2024-04-07 06:33:27,715 : Time 05h 15m 09s, ave eps reward [-6.23 -6.23 -6.23], ave eps length 10.0, reward step [-0.62 -0.62 -0.62], FPS 8.61, mean reward -6.22628143122215, std reward 5.721765536622072, AG 0.0
2024-04-07 06:33:53,957 : Time 05h 15m 35s, ave eps reward [-3.48 -3.48 -3.48], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 7.99, mean reward -3.476804701798085, std reward 1.8739763352927172, AG 0.0
2024-04-07 06:34:20,382 : Time 05h 16m 02s, ave eps reward [-4.86 -4.86 -4.86], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.8, mean reward -4.859135194125918, std reward 2.041284244957823, AG 0.0
2024-04-07 06:34:46,659 : Time 05h 16m 28s, ave eps reward [-4.49 -4.49 -4.49], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 7.71, mean reward -4.4895073342524645, std reward 2.8004289383453433, AG 0.0
2024-04-07 06:35:13,283 : Time 05h 16m 55s, ave eps reward [-3.5 -3.5 -3.5], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 6.66, mean reward -3.501106550477207, std reward 1.961684320504339, AG 0.0
2024-04-07 06:35:39,662 : Time 05h 17m 21s, ave eps reward [-4.51 -4.51 -4.51], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.88, mean reward -4.508689005905703, std reward 2.67469313530448, AG 0.0
2024-04-07 06:36:06,129 : Time 05h 17m 48s, ave eps reward [-3.95 -3.95 -3.95], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 6.68, mean reward -3.9519570155766752, std reward 3.0086037835425588, AG 0.0
2024-04-07 06:36:32,274 : Time 05h 18m 14s, ave eps reward [-4.63 -4.63 -4.63], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 8.56, mean reward -4.625171178358327, std reward 3.9351281469596113, AG 0.0
2024-04-07 06:36:58,381 : Time 05h 18m 40s, ave eps reward [-4.43 -4.43 -4.43], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 9.36, mean reward -4.42766456268181, std reward 3.725034749275896, AG 0.0
2024-04-07 06:37:25,305 : Time 05h 19m 07s, ave eps reward [-5.55 -5.55 -5.55], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.94, mean reward -5.545345470580537, std reward 5.174759767133563, AG 0.0
2024-04-07 06:37:51,245 : Time 05h 19m 33s, ave eps reward [-5.18 -5.18 -5.18], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 10.28, mean reward -5.182046751630596, std reward 3.6742915384177217, AG 0.0
2024-04-07 06:38:17,559 : Time 05h 19m 59s, ave eps reward [-4.94 -4.94 -4.94], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 7.06, mean reward -4.938564404769467, std reward 4.220061565893434, AG 0.0
2024-04-07 06:38:44,015 : Time 05h 20m 25s, ave eps reward [-3.62 -3.62 -3.62], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 7.01, mean reward -3.618115026231572, std reward 2.0455478604597466, AG 0.0
2024-04-07 06:39:10,443 : Time 05h 20m 52s, ave eps reward [-4.12 -4.12 -4.12], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 6.89, mean reward -4.119164317400786, std reward 2.1718755906731753, AG 0.0
2024-04-07 06:39:36,829 : Time 05h 21m 18s, ave eps reward [-3.68 -3.68 -3.68], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.79, mean reward -3.6811236934035128, std reward 1.6166307075323283, AG 0.0
2024-04-07 06:40:03,002 : Time 05h 21m 44s, ave eps reward [-3.2 -3.2 -3.2], ave eps length 10.0, reward step [-0.32 -0.32 -0.32], FPS 8.14, mean reward -3.1951061058019197, std reward 1.2842493924116178, AG 0.0
gamma: 0.1
training start after waiting for 1.2024378776550293 seconds
policy loss:621.8361206054688
value loss:32.99641799926758
entropies:68.43096160888672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1663086414337158 seconds
policy loss:302.3257141113281
value loss:19.63732147216797
entropies:42.0621223449707
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2358331680297852 seconds
policy loss:-1141.0400390625
value loss:43.855262756347656
entropies:61.497276306152344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1455.3615)
ToM Target loss= tensor(2124.8533)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2146623134613037 seconds
policy loss:-135.73959350585938
value loss:33.860294342041016
entropies:62.55986785888672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1684138774871826 seconds
policy loss:-135.6156005859375
value loss:21.4138240814209
entropies:48.50869369506836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1731219291687012 seconds
policy loss:-785.4225463867188
value loss:16.737857818603516
entropies:55.021934509277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2177436351776123 seconds
policy loss:-244.249267578125
value loss:28.058595657348633
entropies:61.5453987121582
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.17360520362854 seconds
policy loss:-3629.7900390625
value loss:71.92333984375
entropies:65.95101928710938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1296.4835)
ToM Target loss= tensor(2195.3867)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1714823246002197 seconds
policy loss:-1656.989990234375
value loss:28.300540924072266
entropies:67.34269714355469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1749069690704346 seconds
policy loss:-2693.00244140625
value loss:36.0203857421875
entropies:73.7787857055664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2232534885406494 seconds
policy loss:-1545.4642333984375
value loss:32.10165786743164
entropies:70.53193664550781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2356185913085938 seconds
policy loss:-1918.6446533203125
value loss:38.78890609741211
entropies:58.7012825012207
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2069718837738037 seconds
policy loss:-69.46686553955078
value loss:8.267040252685547
entropies:56.884857177734375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1304.7126)
ToM Target loss= tensor(2303.0164)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1694972515106201 seconds
policy loss:-768.1712646484375
value loss:26.657955169677734
entropies:46.86371612548828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1652188301086426 seconds
policy loss:-628.1298828125
value loss:35.28871154785156
entropies:80.19189453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1766140460968018 seconds
policy loss:407.4554748535156
value loss:32.1469612121582
entropies:53.08053970336914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.205366611480713 seconds
policy loss:-266.69232177734375
value loss:22.631629943847656
entropies:53.58002853393555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2281875610351562 seconds
policy loss:-81.4368896484375
value loss:18.137561798095703
entropies:53.0706901550293
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1313.5042)
ToM Target loss= tensor(2325.6821)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2326467037200928 seconds
policy loss:-923.9166259765625
value loss:29.67367172241211
entropies:51.79827880859375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2394452095031738 seconds
policy loss:-214.2228546142578
value loss:18.594438552856445
entropies:43.958335876464844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1724350452423096 seconds
policy loss:-1653.84326171875
value loss:29.55321502685547
entropies:73.40052795410156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.236668586730957 seconds
policy loss:-1493.2725830078125
value loss:34.03578567504883
entropies:54.612518310546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.167013168334961 seconds
policy loss:-1956.15283203125
value loss:31.92070770263672
entropies:67.04822540283203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1404.7161)
ToM Target loss= tensor(2314.7961)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2061562538146973 seconds
policy loss:-1919.9346923828125
value loss:64.41183471679688
entropies:66.5179672241211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173034429550171 seconds
policy loss:-362.55035400390625
value loss:22.872482299804688
entropies:50.265926361083984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2094495296478271 seconds
policy loss:904.7476196289062
value loss:33.225830078125
entropies:67.22447204589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.241633415222168 seconds
policy loss:-163.71124267578125
value loss:24.3265438079834
entropies:70.48309326171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2349982261657715 seconds
policy loss:-358.1730651855469
value loss:30.78201675415039
entropies:55.039207458496094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1424.2347)
ToM Target loss= tensor(2355.9902)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2159597873687744 seconds
policy loss:-298.5498046875
value loss:17.32379913330078
entropies:54.962947845458984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176551342010498 seconds
policy loss:-1057.4027099609375
value loss:13.4560546875
entropies:44.50286865234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2269599437713623 seconds
policy loss:-1684.534912109375
value loss:27.80479621887207
entropies:65.61923217773438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2402007579803467 seconds
policy loss:-548.3787841796875
value loss:16.87680435180664
entropies:36.646671295166016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1689016819000244 seconds
policy loss:-1072.870849609375
value loss:31.259971618652344
entropies:71.60201263427734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1483.1815)
ToM Target loss= tensor(2359.0598)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1976087093353271 seconds
policy loss:-539.2232055664062
value loss:15.875579833984375
entropies:50.429664611816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1804232597351074 seconds
policy loss:-366.4876708984375
value loss:27.568471908569336
entropies:59.10552978515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1669425964355469 seconds
policy loss:-350.57275390625
value loss:19.93351936340332
entropies:47.40552520751953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2552545070648193 seconds
policy loss:-246.30247497558594
value loss:29.7647647857666
entropies:58.49452209472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201915979385376 seconds
policy loss:-486.27850341796875
value loss:25.24144172668457
entropies:38.50431823730469
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1329.2899)
ToM Target loss= tensor(2356.4329)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.179640769958496 seconds
policy loss:-472.6771240234375
value loss:26.16027069091797
entropies:31.551284790039062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2402684688568115 seconds
policy loss:-620.851318359375
value loss:13.323866844177246
entropies:53.33858108520508
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1707062721252441 seconds
policy loss:-1769.5218505859375
value loss:55.38075637817383
entropies:59.485443115234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1708095073699951 seconds
policy loss:-216.2653045654297
value loss:5.735926151275635
entropies:35.21802520751953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1780741214752197 seconds
policy loss:-2309.485107421875
value loss:38.59606170654297
entropies:44.76370620727539
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1258.2476)
ToM Target loss= tensor(2288.3401)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.171142339706421 seconds
policy loss:-1554.8675537109375
value loss:24.92479133605957
entropies:53.224151611328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1700797080993652 seconds
policy loss:-1702.0877685546875
value loss:60.263675689697266
entropies:49.54414367675781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1702508926391602 seconds
policy loss:-669.703125
value loss:25.62348747253418
entropies:51.514278411865234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1712243556976318 seconds
policy loss:283.5638732910156
value loss:29.180065155029297
entropies:31.4542293548584
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.165579080581665 seconds
policy loss:240.34664916992188
value loss:25.6182804107666
entropies:37.46611785888672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1214.3020)
ToM Target loss= tensor(2313.3127)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2226402759552002 seconds
policy loss:-561.1866455078125
value loss:30.650251388549805
entropies:51.16108703613281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.234288215637207 seconds
policy loss:-1729.9102783203125
value loss:30.9808406829834
entropies:47.47129440307617
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1662766933441162 seconds
policy loss:-1149.5062255859375
value loss:21.62702178955078
entropies:49.82174301147461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173732042312622 seconds
policy loss:-2738.276611328125
value loss:54.13330841064453
entropies:79.06314086914062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.199357509613037 seconds
policy loss:380.3905334472656
value loss:11.705406188964844
entropies:49.2916374206543
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1259.1282)
ToM Target loss= tensor(2245.6821)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2083027362823486 seconds
policy loss:-427.77349853515625
value loss:14.216416358947754
entropies:70.41658020019531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1706063747406006 seconds
policy loss:-1273.353759765625
value loss:19.772640228271484
entropies:50.23151779174805
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169694185256958 seconds
policy loss:-374.248046875
value loss:15.504626274108887
entropies:60.70370101928711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2352919578552246 seconds
policy loss:-1884.3516845703125
value loss:52.665977478027344
entropies:44.00116729736328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2367298603057861 seconds
policy loss:-1392.61376953125
value loss:20.109249114990234
entropies:60.45958709716797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1318.1628)
ToM Target loss= tensor(2334.4502)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1963589191436768 seconds
policy loss:410.1319274902344
value loss:22.66196632385254
entropies:38.01032257080078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2186927795410156 seconds
policy loss:-526.0767211914062
value loss:19.862995147705078
entropies:47.69984436035156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1751420497894287 seconds
policy loss:-373.3153991699219
value loss:29.41521453857422
entropies:36.4811897277832
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2521042823791504 seconds
policy loss:-528.1083374023438
value loss:46.82509994506836
entropies:57.1352424621582
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2382729053497314 seconds
policy loss:-241.8415069580078
value loss:19.112884521484375
entropies:52.14338302612305
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1211.1832)
ToM Target loss= tensor(2299.5928)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1735825538635254 seconds
policy loss:287.2496032714844
value loss:17.88311004638672
entropies:49.52629852294922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2303900718688965 seconds
policy loss:-74.50391387939453
value loss:12.430854797363281
entropies:43.82828140258789
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.190401554107666 seconds
policy loss:29.56829833984375
value loss:10.721441268920898
entropies:50.36885452270508
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1979858875274658 seconds
policy loss:-1132.340087890625
value loss:22.290937423706055
entropies:59.69731903076172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2326560020446777 seconds
policy loss:-831.6451416015625
value loss:18.249147415161133
entropies:52.99848175048828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1168.4689)
ToM Target loss= tensor(2121.9458)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.174983024597168 seconds
policy loss:-2678.504150390625
value loss:48.86494827270508
entropies:62.940643310546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227858304977417 seconds
policy loss:-1853.5968017578125
value loss:33.43865966796875
entropies:58.57111358642578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2123496532440186 seconds
policy loss:-1480.87890625
value loss:38.837154388427734
entropies:34.5690803527832
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.156785488128662 seconds
policy loss:-64.05537414550781
value loss:13.651845932006836
entropies:64.53876495361328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1768217086791992 seconds
policy loss:-119.15811920166016
value loss:21.509185791015625
entropies:48.49147033691406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1251.2209)
ToM Target loss= tensor(2219.5371)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2383153438568115 seconds
policy loss:-790.6420288085938
value loss:19.713603973388672
entropies:41.24848937988281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1795117855072021 seconds
policy loss:-1106.4462890625
value loss:27.379121780395508
entropies:62.989646911621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2012522220611572 seconds
policy loss:-1667.61962890625
value loss:57.14613723754883
entropies:60.20696258544922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2655675411224365 seconds
policy loss:-139.5749969482422
value loss:38.25593948364258
entropies:59.92277526855469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2382080554962158 seconds
policy loss:180.45138549804688
value loss:25.714351654052734
entropies:45.8872184753418
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1253.1450)
ToM Target loss= tensor(2178.4211)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1891124248504639 seconds
policy loss:-84.0841293334961
value loss:18.433507919311523
entropies:40.525840759277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2306981086730957 seconds
policy loss:-233.218017578125
value loss:61.86396408081055
entropies:49.99758529663086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.184363842010498 seconds
policy loss:-734.1455078125
value loss:29.257362365722656
entropies:67.71100616455078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2374358177185059 seconds
policy loss:580.0438842773438
value loss:31.13138198852539
entropies:50.78228759765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1943111419677734 seconds
policy loss:-573.4373779296875
value loss:19.669898986816406
entropies:57.546714782714844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1380.0486)
ToM Target loss= tensor(2254.6406)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2494964599609375 seconds
policy loss:-1846.2449951171875
value loss:33.524566650390625
entropies:45.90205001831055
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1630465984344482 seconds
policy loss:-1084.5555419921875
value loss:23.336345672607422
entropies:43.683250427246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228369951248169 seconds
policy loss:-1304.19873046875
value loss:18.082128524780273
entropies:57.19826126098633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2347838878631592 seconds
policy loss:-151.0355224609375
value loss:11.861793518066406
entropies:52.00505065917969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1724622249603271 seconds
policy loss:-729.501220703125
value loss:29.200170516967773
entropies:67.453857421875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1312.8679)
ToM Target loss= tensor(2298.3799)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1702828407287598 seconds
policy loss:176.20887756347656
value loss:25.546279907226562
entropies:50.871498107910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2343931198120117 seconds
policy loss:-734.078369140625
value loss:16.52376937866211
entropies:76.98339080810547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2325286865234375 seconds
policy loss:-277.5408935546875
value loss:17.999391555786133
entropies:37.815185546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2173528671264648 seconds
policy loss:-1076.1527099609375
value loss:24.477943420410156
entropies:64.85696411132812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2402844429016113 seconds
policy loss:149.87986755371094
value loss:10.154047966003418
entropies:25.16824722290039
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1319.8221)
ToM Target loss= tensor(2266.9961)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2295899391174316 seconds
policy loss:683.0206298828125
value loss:23.276111602783203
entropies:39.79774475097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.219783067703247 seconds
policy loss:-480.8581848144531
value loss:29.97444725036621
entropies:48.95846176147461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.237356424331665 seconds
policy loss:-1072.779052734375
value loss:41.8758430480957
entropies:70.94459533691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2119553089141846 seconds
policy loss:-1291.5789794921875
value loss:23.242454528808594
entropies:64.04170227050781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2245185375213623 seconds
policy loss:-642.9297485351562
value loss:17.036434173583984
entropies:37.246421813964844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1318.1682)
ToM Target loss= tensor(2312.9082)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2383084297180176 seconds
policy loss:-720.218017578125
value loss:36.62211608886719
entropies:49.72444152832031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2377088069915771 seconds
policy loss:-730.2293090820312
value loss:38.44416809082031
entropies:60.45954895019531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.215486764907837 seconds
policy loss:-56.87106704711914
value loss:24.163677215576172
entropies:39.96351623535156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2889394760131836 seconds
policy loss:-32.27863693237305
value loss:34.056880950927734
entropies:60.6319580078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.184593677520752 seconds
policy loss:-439.5880126953125
value loss:37.22071838378906
entropies:53.94833755493164
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1194.0253)
ToM Target loss= tensor(2299.3101)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2294681072235107 seconds
policy loss:-20.250308990478516
value loss:46.657859802246094
entropies:38.761329650878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.214738130569458 seconds
policy loss:-230.7167510986328
value loss:30.263519287109375
entropies:31.327939987182617
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2306456565856934 seconds
policy loss:-1237.08740234375
value loss:40.72957992553711
entropies:46.70972442626953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1793711185455322 seconds
policy loss:-55.2698860168457
value loss:14.736688613891602
entropies:42.2929573059082
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1725692749023438 seconds
policy loss:-885.8495483398438
value loss:17.498483657836914
entropies:36.94040298461914
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1162.3616)
ToM Target loss= tensor(2218.9446)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1920289993286133 seconds
policy loss:-1476.73486328125
value loss:26.19296646118164
entropies:66.17144775390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1814873218536377 seconds
policy loss:-1332.243408203125
value loss:23.38158416748047
entropies:62.90525817871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2375555038452148 seconds
policy loss:-1943.059326171875
value loss:40.52064514160156
entropies:46.616058349609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2379639148712158 seconds
policy loss:-939.09375
value loss:31.02555274963379
entropies:39.02771759033203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.165992259979248 seconds
policy loss:-259.4951477050781
value loss:26.112102508544922
entropies:43.19367218017578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1269.5083)
ToM Target loss= tensor(2241.8054)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1670513153076172 seconds
policy loss:-144.30938720703125
value loss:19.92837142944336
entropies:52.134071350097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1927428245544434 seconds
policy loss:-699.7468872070312
value loss:23.026187896728516
entropies:63.18534851074219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2294182777404785 seconds
policy loss:-1062.35546875
value loss:21.785194396972656
entropies:51.497459411621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.170424461364746 seconds
policy loss:-86.60064697265625
value loss:10.793078422546387
entropies:49.92240524291992
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1846177577972412 seconds
policy loss:-806.4896240234375
value loss:35.23350524902344
entropies:55.44171905517578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1404.0197)
ToM Target loss= tensor(2265.9282)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1888751983642578 seconds
policy loss:-842.8128051757812
value loss:31.79202651977539
entropies:67.44181060791016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2400131225585938 seconds
policy loss:-360.98797607421875
value loss:27.547109603881836
entropies:37.34823226928711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.216090202331543 seconds
policy loss:-978.928466796875
value loss:27.28249740600586
entropies:55.09652328491211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2065706253051758 seconds
policy loss:-386.3399353027344
value loss:24.951120376586914
entropies:66.92561340332031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2204852104187012 seconds
policy loss:-1180.925537109375
value loss:29.218578338623047
entropies:60.114505767822266
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1320.3641)
ToM Target loss= tensor(2171.7166)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2331609725952148 seconds
policy loss:-507.3398742675781
value loss:43.31111526489258
entropies:65.97332763671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176215648651123 seconds
policy loss:-648.1389770507812
value loss:32.58896255493164
entropies:69.42903900146484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2080943584442139 seconds
policy loss:-1227.3775634765625
value loss:31.36683464050293
entropies:59.17306900024414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1732196807861328 seconds
policy loss:-1010.4197998046875
value loss:30.409635543823242
entropies:54.97382736206055
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2339138984680176 seconds
policy loss:-198.70855712890625
value loss:23.337291717529297
entropies:50.68929672241211
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1405.1178)
ToM Target loss= tensor(2225.1904)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1942260265350342 seconds
policy loss:-197.1306915283203
value loss:23.55109405517578
entropies:69.18372344970703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2295212745666504 seconds
policy loss:-1495.0218505859375
value loss:29.898029327392578
entropies:66.7325668334961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1678276062011719 seconds
policy loss:-1706.461181640625
value loss:42.24910354614258
entropies:71.5581283569336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2253212928771973 seconds
policy loss:-201.23365783691406
value loss:25.727571487426758
entropies:50.877742767333984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2316350936889648 seconds
policy loss:-515.809814453125
value loss:26.356979370117188
entropies:71.48829650878906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1373.3344)
ToM Target loss= tensor(2182.0381)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1650965213775635 seconds
policy loss:-803.095703125
value loss:25.216548919677734
entropies:69.26773834228516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2129158973693848 seconds
policy loss:-908.01513671875
value loss:14.631610870361328
entropies:51.97401809692383
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178638219833374 seconds
policy loss:-563.08837890625
value loss:17.595901489257812
entropies:75.8076400756836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.23026442527771 seconds
policy loss:-1299.130126953125
value loss:24.49223518371582
entropies:62.058349609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1902401447296143 seconds
policy loss:-138.01417541503906
value loss:15.466825485229492
entropies:53.859962463378906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1387.6257)
ToM Target loss= tensor(2272.9163)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2364320755004883 seconds
policy loss:-317.6811218261719
value loss:21.152103424072266
entropies:53.732421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1753032207489014 seconds
policy loss:-672.5280151367188
value loss:12.528939247131348
entropies:44.67668151855469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1778316497802734 seconds
policy loss:-1100.1158447265625
value loss:23.1018009185791
entropies:71.00835418701172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2352526187896729 seconds
policy loss:-496.9846496582031
value loss:22.393701553344727
entropies:55.47095489501953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1758418083190918 seconds
policy loss:-1334.881591796875
value loss:35.83221435546875
entropies:45.201171875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1440.9492)
ToM Target loss= tensor(2313.3315)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2155776023864746 seconds
policy loss:357.0438232421875
value loss:25.42694664001465
entropies:70.53787231445312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.207841157913208 seconds
policy loss:1049.2757568359375
value loss:22.398134231567383
entropies:72.0255126953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2471089363098145 seconds
policy loss:-393.1984558105469
value loss:18.734607696533203
entropies:40.36391067504883
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1742441654205322 seconds
policy loss:-315.5437927246094
value loss:23.541732788085938
entropies:52.82279968261719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2204391956329346 seconds
policy loss:-239.64260864257812
value loss:31.527212142944336
entropies:63.15065002441406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1633.7021)
ToM Target loss= tensor(2293.3787)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2182581424713135 seconds
policy loss:-1197.697021484375
value loss:34.59962844848633
entropies:52.49938201904297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1716418266296387 seconds
policy loss:-1959.9522705078125
value loss:31.580829620361328
entropies:55.19886016845703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2106235027313232 seconds
policy loss:-426.18646240234375
value loss:16.42621421813965
entropies:41.406517028808594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2172396183013916 seconds
policy loss:-627.86279296875
value loss:15.984604835510254
entropies:41.926856994628906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2931654453277588 seconds
policy loss:-1396.7513427734375
value loss:28.94799041748047
entropies:52.84831237792969
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1400.6711)
ToM Target loss= tensor(2473.3582)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2326126098632812 seconds
policy loss:-849.3954467773438
value loss:31.501354217529297
entropies:50.25072479248047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2249820232391357 seconds
policy loss:-199.1121063232422
value loss:26.93448257446289
entropies:43.649269104003906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2053649425506592 seconds
policy loss:240.04782104492188
value loss:17.549118041992188
entropies:42.24284744262695
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2037341594696045 seconds
policy loss:-23.409969329833984
value loss:26.200698852539062
entropies:47.70408630371094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2249338626861572 seconds
policy loss:-228.6572265625
value loss:38.83729553222656
entropies:59.322715759277344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1478.4778)
ToM Target loss= tensor(2478.9780)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2248222827911377 seconds
policy loss:-228.06918334960938
value loss:15.653145790100098
entropies:50.032684326171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1753642559051514 seconds
policy loss:-392.24981689453125
value loss:7.668258190155029
entropies:31.671859741210938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1844868659973145 seconds
policy loss:-297.9197998046875
value loss:16.510494232177734
entropies:31.045936584472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.170318603515625 seconds
policy loss:-1205.96826171875
value loss:29.361351013183594
entropies:42.98473358154297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1687672138214111 seconds
policy loss:-806.2086791992188
value loss:19.66790199279785
entropies:48.63256072998047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1279.2732)
ToM Target loss= tensor(2686.9861)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2050960063934326 seconds
policy loss:-1742.049560546875
value loss:34.09657669067383
entropies:58.730934143066406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2111821174621582 seconds
policy loss:-943.9265747070312
value loss:29.590744018554688
entropies:59.4991455078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1808984279632568 seconds
policy loss:-122.03641510009766
value loss:17.326719284057617
entropies:54.464759826660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2379851341247559 seconds
policy loss:403.0065612792969
value loss:23.152177810668945
entropies:46.853458404541016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.214651107788086 seconds
policy loss:-131.9142303466797
value loss:25.43061637878418
entropies:48.86751937866211
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1244.1727)
ToM Target loss= tensor(2477.0723)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2404875755310059 seconds
policy loss:-89.31031036376953
value loss:16.30808448791504
entropies:37.135215759277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2255332469940186 seconds
policy loss:-1575.818359375
value loss:36.62456512451172
entropies:42.73907470703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2384889125823975 seconds
policy loss:132.2341766357422
value loss:14.045709609985352
entropies:46.03963088989258
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.210359811782837 seconds
policy loss:-928.11572265625
value loss:92.97193908691406
entropies:47.01002502441406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1790235042572021 seconds
policy loss:-2701.771240234375
value loss:44.970802307128906
entropies:70.34889221191406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1267.0166)
ToM Target loss= tensor(2330.4719)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.235713005065918 seconds
policy loss:8.274202346801758
value loss:13.423489570617676
entropies:46.362709045410156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1746196746826172 seconds
policy loss:-748.75634765625
value loss:16.57270050048828
entropies:60.501834869384766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1729600429534912 seconds
policy loss:-1791.1085205078125
value loss:31.092769622802734
entropies:66.99645233154297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2383763790130615 seconds
policy loss:-1224.5474853515625
value loss:20.13294792175293
entropies:71.901123046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1823859214782715 seconds
policy loss:-377.3858947753906
value loss:37.731204986572266
entropies:59.74739074707031
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1320.2032)
ToM Target loss= tensor(2279.5015)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2337567806243896 seconds
policy loss:-155.63108825683594
value loss:29.06211280822754
entropies:54.28877639770508
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1917307376861572 seconds
policy loss:-219.05116271972656
value loss:27.86205291748047
entropies:62.74443054199219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2183575630187988 seconds
policy loss:317.6065368652344
value loss:35.785526275634766
entropies:61.530757904052734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.237457036972046 seconds
policy loss:421.23309326171875
value loss:33.8798713684082
entropies:71.07630157470703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2185449600219727 seconds
policy loss:302.84613037109375
value loss:16.270511627197266
entropies:55.71392822265625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1418.3469)
ToM Target loss= tensor(2283.1826)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1730475425720215 seconds
policy loss:34.36869812011719
value loss:17.8660945892334
entropies:42.08740234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2293307781219482 seconds
policy loss:-1061.29150390625
value loss:76.37793731689453
entropies:57.41159439086914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2250328063964844 seconds
policy loss:-3482.62451171875
value loss:98.55345916748047
entropies:59.818077087402344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2232961654663086 seconds
policy loss:-755.22412109375
value loss:20.465417861938477
entropies:62.47147750854492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2402708530426025 seconds
policy loss:-1723.896484375
value loss:32.1630859375
entropies:51.55529022216797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1348.6611)
ToM Target loss= tensor(2503.6111)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2327797412872314 seconds
policy loss:-2603.210693359375
value loss:44.40694808959961
entropies:75.52843475341797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2288243770599365 seconds
policy loss:-2182.851318359375
value loss:46.83680725097656
entropies:58.8349494934082
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2383010387420654 seconds
policy loss:-1767.74755859375
value loss:50.28761291503906
entropies:76.6913833618164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1778686046600342 seconds
policy loss:-293.3916320800781
value loss:43.95821762084961
entropies:76.29229736328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1884584426879883 seconds
policy loss:-121.5788345336914
value loss:32.90388870239258
entropies:57.61993408203125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1452.9042)
ToM Target loss= tensor(2404.1389)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.246150016784668 seconds
policy loss:113.53030395507812
value loss:25.000652313232422
entropies:67.86925506591797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1758925914764404 seconds
policy loss:-201.30804443359375
value loss:46.26006317138672
entropies:57.07916259765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.23305344581604 seconds
policy loss:-330.49530029296875
value loss:25.9283447265625
entropies:73.18943786621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1714928150177002 seconds
policy loss:-1524.7913818359375
value loss:43.91578674316406
entropies:75.45755004882812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.196810007095337 seconds
policy loss:-334.0813293457031
value loss:26.309131622314453
entropies:40.81749725341797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1405.2875)
ToM Target loss= tensor(2501.0056)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1821651458740234 seconds
policy loss:735.9437255859375
value loss:29.655561447143555
entropies:96.01802825927734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1792049407958984 seconds
policy loss:-1294.2113037109375
value loss:24.884552001953125
entropies:91.59374237060547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1650052070617676 seconds
policy loss:-1271.0738525390625
value loss:22.56655502319336
entropies:60.511409759521484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2306029796600342 seconds
policy loss:-2519.248779296875
value loss:31.538328170776367
entropies:69.74555969238281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.172515869140625 seconds
policy loss:-1227.7664794921875
value loss:20.489959716796875
entropies:45.4278564453125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1427.0769)
ToM Target loss= tensor(2484.2053)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.175644874572754 seconds
policy loss:-1839.4814453125
value loss:28.916460037231445
entropies:81.51719665527344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2339928150177002 seconds
policy loss:-2819.377685546875
value loss:49.4072380065918
entropies:88.05522155761719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2266464233398438 seconds
policy loss:-1510.94384765625
value loss:23.765304565429688
entropies:44.44630432128906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2346124649047852 seconds
policy loss:-575.7435302734375
value loss:27.559297561645508
entropies:63.437721252441406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1797986030578613 seconds
policy loss:-448.88037109375
value loss:19.78830337524414
entropies:64.3997573852539
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1364.4264)
ToM Target loss= tensor(2294.6746)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2070107460021973 seconds
policy loss:-626.2598876953125
value loss:32.11348342895508
entropies:77.77495574951172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2297532558441162 seconds
policy loss:-238.5996856689453
value loss:28.025135040283203
entropies:63.901283264160156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2486040592193604 seconds
policy loss:-2473.562744140625
value loss:41.703365325927734
entropies:51.16287612915039
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2252779006958008 seconds
policy loss:-1321.781494140625
value loss:25.44023895263672
entropies:64.28025817871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1900458335876465 seconds
policy loss:482.4925842285156
value loss:13.829294204711914
entropies:59.18319320678711
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1372.6940)
ToM Target loss= tensor(2273.0776)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1715972423553467 seconds
policy loss:-1403.7032470703125
value loss:24.728857040405273
entropies:60.91107177734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1720356941223145 seconds
policy loss:-831.8562622070312
value loss:23.87965965270996
entropies:53.106136322021484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2218620777130127 seconds
policy loss:-470.6235046386719
value loss:23.315128326416016
entropies:59.92974853515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.168879747390747 seconds
policy loss:-157.80821228027344
value loss:19.91900062561035
entropies:57.19371032714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1624832153320312 seconds
policy loss:-1333.288818359375
value loss:15.479538917541504
entropies:49.57575988769531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1283.7819)
ToM Target loss= tensor(2281.6768)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2283718585968018 seconds
policy loss:-1632.7593994140625
value loss:25.94732093811035
entropies:73.67945861816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1886749267578125 seconds
policy loss:-1038.783203125
value loss:19.79424285888672
entropies:62.58573532104492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1820859909057617 seconds
policy loss:-972.389404296875
value loss:24.141517639160156
entropies:45.4337272644043
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1782424449920654 seconds
policy loss:-443.40789794921875
value loss:17.436553955078125
entropies:48.704349517822266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2144520282745361 seconds
policy loss:-314.8731384277344
value loss:11.158137321472168
entropies:62.366573333740234
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1357.0675)
ToM Target loss= tensor(2268.1516)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.201918125152588 seconds
policy loss:47.51941680908203
value loss:6.31472635269165
entropies:24.583621978759766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2421343326568604 seconds
policy loss:-690.7099609375
value loss:13.090834617614746
entropies:54.73585510253906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.222998857498169 seconds
policy loss:-1479.3056640625
value loss:33.150760650634766
entropies:71.82119750976562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.213125467300415 seconds
policy loss:-678.6697998046875
value loss:25.886995315551758
entropies:75.48638916015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.218017339706421 seconds
policy loss:-538.1526489257812
value loss:22.831451416015625
entropies:60.5973014831543
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1345.8450)
ToM Target loss= tensor(2291.9534)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2045342922210693 seconds
policy loss:-990.1823120117188
value loss:20.99188995361328
entropies:52.11794662475586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1872460842132568 seconds
policy loss:-412.07110595703125
value loss:35.7021484375
entropies:34.032325744628906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2205719947814941 seconds
policy loss:-508.8608703613281
value loss:17.382368087768555
entropies:43.95862579345703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1702287197113037 seconds
policy loss:-83.81999206542969
value loss:14.133947372436523
entropies:39.073204040527344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2002477645874023 seconds
policy loss:-668.62109375
value loss:25.9528865814209
entropies:46.5389518737793
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1213.6147)
ToM Target loss= tensor(2227.9316)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1916499137878418 seconds
policy loss:239.6543731689453
value loss:6.32319450378418
entropies:32.966026306152344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.215705156326294 seconds
policy loss:-1800.304443359375
value loss:54.42453384399414
entropies:71.10794067382812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176387071609497 seconds
policy loss:-178.71832275390625
value loss:15.55433464050293
entropies:42.324066162109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227041482925415 seconds
policy loss:-828.9476318359375
value loss:29.797075271606445
entropies:61.914947509765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2425956726074219 seconds
policy loss:-1117.7928466796875
value loss:30.00298309326172
entropies:44.00396728515625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1197.9995)
ToM Target loss= tensor(2201.2427)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1968824863433838 seconds
policy loss:-40.136348724365234
value loss:15.637595176696777
entropies:57.610469818115234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2272462844848633 seconds
policy loss:-1302.7554931640625
value loss:48.33655548095703
entropies:74.67620086669922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1740481853485107 seconds
policy loss:60.80448913574219
value loss:15.343477249145508
entropies:35.05302429199219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1765868663787842 seconds
policy loss:-374.0550842285156
value loss:32.6906852722168
entropies:40.9677848815918
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2399353981018066 seconds
policy loss:235.96006774902344
value loss:24.276155471801758
entropies:73.51179504394531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1317.9863)
ToM Target loss= tensor(2260.1470)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2382194995880127 seconds
policy loss:-425.253173828125
value loss:20.09113121032715
entropies:42.39690017700195
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2385931015014648 seconds
policy loss:-1550.286865234375
value loss:36.99654006958008
entropies:76.69574737548828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2308964729309082 seconds
policy loss:-169.9186553955078
value loss:17.329917907714844
entropies:61.70325469970703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2274794578552246 seconds
policy loss:-1295.91357421875
value loss:17.907535552978516
entropies:48.549835205078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2023169994354248 seconds
policy loss:-1274.51806640625
value loss:21.312374114990234
entropies:67.75706481933594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1311.6328)
ToM Target loss= tensor(2243.2578)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1765272617340088 seconds
policy loss:-418.6051330566406
value loss:14.156576156616211
entropies:52.30183410644531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.166149616241455 seconds
policy loss:-55.44607925415039
value loss:9.649618148803711
entropies:40.12833786010742
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1781811714172363 seconds
policy loss:-827.0228881835938
value loss:10.316924095153809
entropies:44.02509307861328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.24226713180542 seconds
policy loss:-1881.8577880859375
value loss:27.593294143676758
entropies:72.45033264160156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1730084419250488 seconds
policy loss:-1672.171630859375
value loss:37.65568542480469
entropies:53.431373596191406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1350.9803)
ToM Target loss= tensor(2374.4634)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2366578578948975 seconds
policy loss:-471.0308837890625
value loss:14.398031234741211
entropies:49.481021881103516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228334665298462 seconds
policy loss:-533.1778564453125
value loss:29.60257339477539
entropies:62.611351013183594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2407481670379639 seconds
policy loss:-463.14459228515625
value loss:22.07575225830078
entropies:56.57672882080078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2129721641540527 seconds
policy loss:-402.85498046875
value loss:21.791095733642578
entropies:65.99494171142578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2269384860992432 seconds
policy loss:39.423736572265625
value loss:15.17163372039795
entropies:49.80483627319336
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1448.3386)
ToM Target loss= tensor(2341.9661)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2106449604034424 seconds
policy loss:-1029.294921875
value loss:19.4437198638916
entropies:69.61436462402344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2153103351593018 seconds
policy loss:-653.4654541015625
value loss:17.171113967895508
entropies:68.04582214355469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1642272472381592 seconds
policy loss:-1230.71533203125
value loss:30.04436683654785
entropies:65.69266510009766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2360169887542725 seconds
policy loss:-783.6631469726562
value loss:13.13745403289795
entropies:51.496185302734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1777188777923584 seconds
policy loss:-1126.224365234375
value loss:28.258996963500977
entropies:57.06336975097656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1347.5284)
ToM Target loss= tensor(2252.4998)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2021799087524414 seconds
policy loss:853.6553955078125
value loss:19.931367874145508
entropies:58.62899398803711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2036406993865967 seconds
policy loss:-308.79693603515625
value loss:25.07072639465332
entropies:69.34019470214844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.172231674194336 seconds
policy loss:-465.4105224609375
value loss:17.664993286132812
entropies:74.81980895996094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2001063823699951 seconds
policy loss:193.31161499023438
value loss:24.45117950439453
entropies:73.76577758789062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2286458015441895 seconds
policy loss:-807.8253173828125
value loss:21.16097640991211
entropies:76.23004913330078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1380.1931)
ToM Target loss= tensor(2204.1768)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2304704189300537 seconds
policy loss:-1169.7603759765625
value loss:21.324310302734375
entropies:64.24854278564453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2243258953094482 seconds
policy loss:-1588.9078369140625
value loss:35.12360382080078
entropies:83.55360412597656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2143616676330566 seconds
policy loss:-1127.00048828125
value loss:27.261383056640625
entropies:64.80964660644531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1722314357757568 seconds
policy loss:-1641.1044921875
value loss:30.28539276123047
entropies:57.19347381591797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1754424571990967 seconds
policy loss:-1626.2379150390625
value loss:30.826744079589844
entropies:59.12483596801758
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1327.2283)
ToM Target loss= tensor(2247.4839)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2226989269256592 seconds
policy loss:-556.9194946289062
value loss:33.532440185546875
entropies:46.04949188232422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1769287586212158 seconds
policy loss:-326.21673583984375
value loss:32.91424560546875
entropies:69.78996276855469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2518796920776367 seconds
policy loss:-306.8288269042969
value loss:33.782470703125
entropies:54.9453010559082
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1885318756103516 seconds
policy loss:389.6949157714844
value loss:28.82549285888672
entropies:30.02499771118164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2019975185394287 seconds
policy loss:350.31793212890625
value loss:20.349899291992188
entropies:43.058162689208984
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1211.5170)
ToM Target loss= tensor(2265.3105)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2492456436157227 seconds
policy loss:-1065.03125
value loss:31.909196853637695
entropies:57.58403015136719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2296695709228516 seconds
policy loss:-340.99664306640625
value loss:10.453693389892578
entropies:40.78900909423828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2275075912475586 seconds
policy loss:-1437.53173828125
value loss:23.1995906829834
entropies:96.34266662597656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1666362285614014 seconds
policy loss:-260.24591064453125
value loss:9.568014144897461
entropies:38.315696716308594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1758205890655518 seconds
policy loss:-1685.1820068359375
value loss:29.273733139038086
entropies:53.08274459838867
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1324.1790)
ToM Target loss= tensor(2202.5938)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2117164134979248 seconds
policy loss:-652.529052734375
value loss:40.678916931152344
entropies:58.310890197753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1660823822021484 seconds
policy loss:-1354.96923828125
value loss:26.71287727355957
entropies:62.304630279541016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1637043952941895 seconds
policy loss:-1248.235107421875
value loss:22.668716430664062
entropies:45.5482292175293
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1723802089691162 seconds
policy loss:-223.28436279296875
value loss:7.591865539550781
entropies:40.84120178222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2100646495819092 seconds
policy loss:419.12579345703125
value loss:18.617996215820312
entropies:67.21920013427734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1286.6897)
ToM Target loss= tensor(2243.6172)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.217684030532837 seconds
policy loss:28.285432815551758
value loss:26.91446876525879
entropies:52.352500915527344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1640005111694336 seconds
policy loss:56.98335266113281
value loss:25.48562240600586
entropies:53.25658416748047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1982097625732422 seconds
policy loss:66.7391357421875
value loss:38.63408660888672
entropies:56.23773193359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.168285846710205 seconds
policy loss:-273.46966552734375
value loss:15.863688468933105
entropies:63.09962463378906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2233798503875732 seconds
policy loss:642.8323974609375
value loss:15.186126708984375
entropies:46.23025131225586
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1260.1162)
ToM Target loss= tensor(2314.1860)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2339155673980713 seconds
policy loss:-2453.50537109375
value loss:40.802879333496094
entropies:86.49922180175781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1682252883911133 seconds
policy loss:-222.4616241455078
value loss:19.463096618652344
entropies:53.13094711303711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2329840660095215 seconds
policy loss:-1342.4300537109375
value loss:28.62328338623047
entropies:74.98310852050781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1671149730682373 seconds
policy loss:-161.99978637695312
value loss:14.540949821472168
entropies:46.4721565246582
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2315220832824707 seconds
policy loss:-1103.6881103515625
value loss:36.08964538574219
entropies:67.1722183227539
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1344.9341)
ToM Target loss= tensor(2273.6096)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.227482557296753 seconds
policy loss:-307.8714904785156
value loss:29.231151580810547
entropies:55.062583923339844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.242292881011963 seconds
policy loss:-727.2683715820312
value loss:24.020320892333984
entropies:51.86791229248047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1680221557617188 seconds
policy loss:454.493896484375
value loss:29.946096420288086
entropies:66.7379150390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2320239543914795 seconds
policy loss:112.6180419921875
value loss:17.00214195251465
entropies:35.08918762207031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2194311618804932 seconds
policy loss:-796.480712890625
value loss:28.43763542175293
entropies:54.44309616088867
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1306.0364)
ToM Target loss= tensor(2276.9509)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1720051765441895 seconds
policy loss:-1614.2313232421875
value loss:21.07097053527832
entropies:78.55328369140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1770868301391602 seconds
policy loss:179.6239776611328
value loss:16.184038162231445
entropies:51.002403259277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2361505031585693 seconds
policy loss:-523.02783203125
value loss:21.875839233398438
entropies:63.73741149902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1782612800598145 seconds
policy loss:-590.2430419921875
value loss:14.594402313232422
entropies:57.46487045288086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1674187183380127 seconds
policy loss:-1865.539794921875
value loss:29.996212005615234
entropies:53.17933654785156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1297.9508)
ToM Target loss= tensor(2154.5547)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2061445713043213 seconds
policy loss:470.53314208984375
value loss:9.618318557739258
entropies:46.749122619628906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1942615509033203 seconds
policy loss:-755.4457397460938
value loss:12.349609375
entropies:63.36376953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2448365688323975 seconds
policy loss:-174.0967559814453
value loss:10.949176788330078
entropies:33.87409591674805
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.231494665145874 seconds
policy loss:-658.3837890625
value loss:17.39935874938965
entropies:44.070892333984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1719081401824951 seconds
policy loss:-199.035400390625
value loss:9.720888137817383
entropies:62.90385437011719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1228.4972)
ToM Target loss= tensor(2317.3833)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1738505363464355 seconds
policy loss:118.77071380615234
value loss:14.486089706420898
entropies:58.73915100097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2382278442382812 seconds
policy loss:-1759.6439208984375
value loss:21.549243927001953
entropies:45.31181716918945
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2330055236816406 seconds
policy loss:-1891.87451171875
value loss:29.100040435791016
entropies:64.93269348144531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2285025119781494 seconds
policy loss:-1310.7572021484375
value loss:21.17035484313965
entropies:53.74440002441406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2300376892089844 seconds
policy loss:-953.1177368164062
value loss:44.80455017089844
entropies:75.14289093017578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1313.7947)
ToM Target loss= tensor(2262.1118)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.176018238067627 seconds
policy loss:131.60482788085938
value loss:25.480457305908203
entropies:70.43637084960938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1813464164733887 seconds
policy loss:-336.1173400878906
value loss:29.364757537841797
entropies:62.557647705078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2000401020050049 seconds
policy loss:65.122802734375
value loss:14.665383338928223
entropies:46.65703582763672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2129778861999512 seconds
policy loss:242.15724182128906
value loss:15.885717391967773
entropies:33.729736328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2103681564331055 seconds
policy loss:-440.3193359375
value loss:17.47942543029785
entropies:61.722740173339844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1206.2515)
ToM Target loss= tensor(2179.7061)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2237579822540283 seconds
policy loss:-3417.741455078125
value loss:57.8116455078125
entropies:78.17633056640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2355964183807373 seconds
policy loss:-2046.0771484375
value loss:26.64484977722168
entropies:39.300758361816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.174818992614746 seconds
policy loss:-1980.74365234375
value loss:20.017009735107422
entropies:50.622650146484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1812326908111572 seconds
policy loss:-339.8733215332031
value loss:13.63319206237793
entropies:33.794212341308594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1717627048492432 seconds
policy loss:-1259.5626220703125
value loss:45.66929244995117
entropies:56.25129699707031
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1207.7031)
ToM Target loss= tensor(2301.8042)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1671819686889648 seconds
policy loss:-684.1076049804688
value loss:13.514383316040039
entropies:42.414424896240234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173701286315918 seconds
policy loss:-395.6608581542969
value loss:14.567723274230957
entropies:54.21563720703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.209892988204956 seconds
policy loss:-608.9865112304688
value loss:34.437294006347656
entropies:57.0362548828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1867311000823975 seconds
policy loss:147.4735870361328
value loss:22.450836181640625
entropies:45.52330780029297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1828229427337646 seconds
policy loss:294.203125
value loss:26.990074157714844
entropies:39.62570571899414
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1130.0717)
ToM Target loss= tensor(2094.7676)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1810002326965332 seconds
policy loss:153.8852081298828
value loss:20.106121063232422
entropies:43.953372955322266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173410177230835 seconds
policy loss:-734.493408203125
value loss:26.288719177246094
entropies:55.776363372802734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.237966537475586 seconds
policy loss:-182.9800567626953
value loss:16.518529891967773
entropies:69.11952209472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1764662265777588 seconds
policy loss:140.58766174316406
value loss:17.852046966552734
entropies:44.194129943847656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2347996234893799 seconds
policy loss:-2562.35693359375
value loss:49.45723342895508
entropies:64.12905883789062
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1320.7527)
ToM Target loss= tensor(2221.4016)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.203444242477417 seconds
policy loss:-400.45947265625
value loss:13.835458755493164
entropies:49.54389572143555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227656602859497 seconds
policy loss:-877.5609130859375
value loss:12.147504806518555
entropies:35.03114318847656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2393271923065186 seconds
policy loss:-1711.49169921875
value loss:25.911985397338867
entropies:46.33165740966797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1816189289093018 seconds
policy loss:-1603.66796875
value loss:33.35661315917969
entropies:76.92138671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2174358367919922 seconds
policy loss:-611.6669311523438
value loss:17.939550399780273
entropies:56.23088073730469
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1290.1337)
ToM Target loss= tensor(2228.3296)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2247068881988525 seconds
policy loss:409.2774963378906
value loss:29.45716094970703
entropies:75.5120849609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2238705158233643 seconds
policy loss:-61.819034576416016
value loss:31.540966033935547
entropies:73.85555267333984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2189011573791504 seconds
policy loss:332.3746337890625
value loss:34.69676971435547
entropies:32.22624206542969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1306400299072266 seconds
policy loss:-1065.9764404296875
value loss:46.36416244506836
entropies:68.540771484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2325565814971924 seconds
policy loss:197.7397918701172
value loss:26.7308349609375
entropies:37.837581634521484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1287.7333)
ToM Target loss= tensor(2336.8513)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2317345142364502 seconds
policy loss:134.5054931640625
value loss:27.502643585205078
entropies:47.80432891845703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2377550601959229 seconds
policy loss:86.85232543945312
value loss:15.801421165466309
entropies:50.426979064941406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2367405891418457 seconds
policy loss:-423.2930603027344
value loss:16.92205810546875
entropies:53.813026428222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2392551898956299 seconds
policy loss:-2064.41552734375
value loss:30.610517501831055
entropies:51.41717529296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2326314449310303 seconds
policy loss:-364.1498107910156
value loss:13.674098014831543
entropies:50.072967529296875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1189.3584)
ToM Target loss= tensor(2181.0737)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2053074836730957 seconds
policy loss:93.75691986083984
value loss:4.195075035095215
entropies:39.330848693847656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1801280975341797 seconds
policy loss:-1343.7225341796875
value loss:16.23533821105957
entropies:41.20348358154297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.170860767364502 seconds
policy loss:-742.2759399414062
value loss:20.698408126831055
entropies:48.891849517822266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1757769584655762 seconds
policy loss:-1095.3994140625
value loss:21.01835060119629
entropies:36.08774948120117
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2269344329833984 seconds
policy loss:0.4995603561401367
value loss:6.875506401062012
entropies:32.07456970214844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1208.6823)
ToM Target loss= tensor(2326.4199)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2345166206359863 seconds
policy loss:-1943.6593017578125
value loss:31.611955642700195
entropies:65.44585418701172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.222553014755249 seconds
policy loss:-155.45254516601562
value loss:15.548362731933594
entropies:51.49108123779297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2265534400939941 seconds
policy loss:114.41670989990234
value loss:24.13623809814453
entropies:49.335323333740234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2062263488769531 seconds
policy loss:-496.827880859375
value loss:20.860149383544922
entropies:48.37127685546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177022933959961 seconds
policy loss:-319.4467468261719
value loss:18.794363021850586
entropies:41.38387680053711
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1235.5681)
ToM Target loss= tensor(2142.8066)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.229017972946167 seconds
policy loss:-1394.9603271484375
value loss:24.577558517456055
entropies:63.42914581298828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2216038703918457 seconds
policy loss:-321.0828552246094
value loss:32.06584930419922
entropies:57.74140167236328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1896486282348633 seconds
policy loss:-83.71342468261719
value loss:16.288496017456055
entropies:43.23610305786133
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1715667247772217 seconds
policy loss:-98.14093780517578
value loss:9.085030555725098
entropies:37.199058532714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2156357765197754 seconds
policy loss:-772.90966796875
value loss:12.920333862304688
entropies:32.90572738647461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1229.5903)
ToM Target loss= tensor(2149.8369)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2264690399169922 seconds
policy loss:-1751.2054443359375
value loss:39.827449798583984
entropies:56.785621643066406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228236436843872 seconds
policy loss:-1343.7080078125
value loss:23.01175308227539
entropies:42.9068603515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1815006732940674 seconds
policy loss:-1418.66162109375
value loss:22.999109268188477
entropies:53.537269592285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2518196105957031 seconds
policy loss:-972.5167846679688
value loss:49.728904724121094
entropies:42.13325500488281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2517447471618652 seconds
policy loss:-440.2441101074219
value loss:10.130989074707031
entropies:50.500267028808594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1241.3475)
ToM Target loss= tensor(2223.5122)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1621272563934326 seconds
policy loss:-1329.0340576171875
value loss:35.9970703125
entropies:42.37678909301758
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2369322776794434 seconds
policy loss:-722.8540649414062
value loss:31.969890594482422
entropies:51.29293441772461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1980218887329102 seconds
policy loss:89.94197082519531
value loss:34.68741226196289
entropies:50.861698150634766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2270338535308838 seconds
policy loss:-338.9644470214844
value loss:34.62302017211914
entropies:58.836212158203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1783199310302734 seconds
policy loss:493.4722900390625
value loss:16.24700355529785
entropies:37.19541931152344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1267.8883)
ToM Target loss= tensor(2212.3867)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2420151233673096 seconds
policy loss:-590.0599365234375
value loss:25.689395904541016
entropies:59.16765594482422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1781206130981445 seconds
policy loss:-865.4006958007812
value loss:39.343360900878906
entropies:63.139610290527344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1700706481933594 seconds
policy loss:-1125.5555419921875
value loss:29.303632736206055
entropies:70.12418365478516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.171466588973999 seconds
policy loss:-847.99609375
value loss:25.127643585205078
entropies:56.27954864501953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228097915649414 seconds
policy loss:-2369.096435546875
value loss:47.68354415893555
entropies:72.77914428710938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1430.9207)
ToM Target loss= tensor(2317.0779)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2325010299682617 seconds
policy loss:-28.62425994873047
value loss:9.850689888000488
entropies:37.366397857666016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1803603172302246 seconds
policy loss:-493.468994140625
value loss:31.107271194458008
entropies:54.873756408691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1842777729034424 seconds
policy loss:-379.2010803222656
value loss:22.54761505126953
entropies:53.90901184082031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.167983055114746 seconds
policy loss:-140.6940155029297
value loss:8.622831344604492
entropies:52.66102600097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2244133949279785 seconds
policy loss:-633.6790771484375
value loss:16.75347137451172
entropies:65.74190521240234
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1285.8130)
ToM Target loss= tensor(2182.2852)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2297861576080322 seconds
policy loss:-632.8154296875
value loss:7.279355049133301
entropies:40.93094253540039
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2291862964630127 seconds
policy loss:-212.06124877929688
value loss:20.23237419128418
entropies:28.886459350585938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2347404956817627 seconds
policy loss:-1289.5367431640625
value loss:29.071269989013672
entropies:52.24781799316406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2283341884613037 seconds
policy loss:-215.6179656982422
value loss:8.14039421081543
entropies:38.02250671386719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2379586696624756 seconds
policy loss:-215.39573669433594
value loss:9.49057674407959
entropies:53.40568923950195
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1280.5060)
ToM Target loss= tensor(2183.8872)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2372527122497559 seconds
policy loss:-570.8837280273438
value loss:28.14029884338379
entropies:37.62248992919922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173119068145752 seconds
policy loss:360.0429382324219
value loss:13.330423355102539
entropies:52.01412582397461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.213289737701416 seconds
policy loss:-55.1067008972168
value loss:14.333552360534668
entropies:33.32398986816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2140400409698486 seconds
policy loss:-394.1960754394531
value loss:23.26498031616211
entropies:60.22271728515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2297141551971436 seconds
policy loss:-273.4654846191406
value loss:8.202777862548828
entropies:36.480045318603516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1228.0747)
ToM Target loss= tensor(2153.9370)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2157721519470215 seconds
policy loss:-210.034912109375
value loss:10.234673500061035
entropies:36.29719161987305
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2399959564208984 seconds
policy loss:-1037.9398193359375
value loss:18.728824615478516
entropies:44.549991607666016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2405133247375488 seconds
policy loss:-658.7730102539062
value loss:14.905609130859375
entropies:50.26443099975586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1657507419586182 seconds
policy loss:-2239.148681640625
value loss:26.28111457824707
entropies:50.245323181152344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2351431846618652 seconds
policy loss:-1981.677734375
value loss:40.561893463134766
entropies:67.64740753173828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1253.5414)
ToM Target loss= tensor(2168.6621)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1787004470825195 seconds
policy loss:-167.98187255859375
value loss:17.277814865112305
entropies:30.773271560668945
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.233626365661621 seconds
policy loss:-953.818603515625
value loss:18.866619110107422
entropies:59.84989929199219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2156410217285156 seconds
policy loss:-353.47503662109375
value loss:17.62823486328125
entropies:33.464534759521484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.185819149017334 seconds
policy loss:336.2374267578125
value loss:16.78961181640625
entropies:49.53157043457031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1714274883270264 seconds
policy loss:640.8687744140625
value loss:13.134725570678711
entropies:45.575889587402344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1246.1323)
ToM Target loss= tensor(2248.5845)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1771295070648193 seconds
policy loss:-1022.8809204101562
value loss:22.053625106811523
entropies:51.021339416503906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.245577335357666 seconds
policy loss:-138.5370330810547
value loss:7.191944122314453
entropies:31.97085952758789
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2218220233917236 seconds
policy loss:-1231.99072265625
value loss:18.018524169921875
entropies:61.18110275268555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1800801753997803 seconds
policy loss:-1943.3980712890625
value loss:32.00059127807617
entropies:55.73038864135742
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1735336780548096 seconds
policy loss:-1228.0826416015625
value loss:28.986759185791016
entropies:52.91394805908203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1333.2025)
ToM Target loss= tensor(2250.5278)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2241096496582031 seconds
policy loss:-355.4400329589844
value loss:16.150484085083008
entropies:58.948997497558594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2927968502044678 seconds
policy loss:-1336.981201171875
value loss:36.5330924987793
entropies:88.05988311767578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2351086139678955 seconds
policy loss:-680.8134765625
value loss:21.79055404663086
entropies:38.66734313964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1728243827819824 seconds
policy loss:71.1417007446289
value loss:14.24650764465332
entropies:36.65448760986328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2305335998535156 seconds
policy loss:485.3804016113281
value loss:20.33530044555664
entropies:46.84535217285156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1277.3070)
ToM Target loss= tensor(2181.4539)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2406210899353027 seconds
policy loss:-691.9332275390625
value loss:25.297927856445312
entropies:63.59022521972656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1668429374694824 seconds
policy loss:-1606.8441162109375
value loss:29.635000228881836
entropies:58.95975875854492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1772589683532715 seconds
policy loss:-669.057861328125
value loss:15.24384880065918
entropies:67.04344177246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1734380722045898 seconds
policy loss:-1785.45751953125
value loss:29.698627471923828
entropies:71.14168548583984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2248365879058838 seconds
policy loss:-839.3385009765625
value loss:26.450239181518555
entropies:64.82710266113281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1401.5632)
ToM Target loss= tensor(2219.1829)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.186767578125 seconds
policy loss:-823.6149291992188
value loss:13.85179615020752
entropies:47.005958557128906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2177674770355225 seconds
policy loss:-271.1661071777344
value loss:16.934295654296875
entropies:44.19544982910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1908376216888428 seconds
policy loss:-965.1395263671875
value loss:14.01541805267334
entropies:34.61029815673828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2236850261688232 seconds
policy loss:-806.922607421875
value loss:19.611454010009766
entropies:67.01408386230469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.206373691558838 seconds
policy loss:205.04783630371094
value loss:12.786145210266113
entropies:53.46240997314453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1218.3827)
ToM Target loss= tensor(2097.6819)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.171550989151001 seconds
policy loss:-299.015625
value loss:20.606067657470703
entropies:47.412689208984375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2368874549865723 seconds
policy loss:-124.5634765625
value loss:12.591980934143066
entropies:51.38731002807617
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2227356433868408 seconds
policy loss:6.099298477172852
value loss:13.360428810119629
entropies:37.85456848144531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1922011375427246 seconds
policy loss:-1649.5928955078125
value loss:50.263484954833984
entropies:43.134857177734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.207489013671875 seconds
policy loss:-1062.5206298828125
value loss:19.049673080444336
entropies:28.155719757080078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1200.8644)
ToM Target loss= tensor(2203.3684)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2743918895721436 seconds
policy loss:-1935.0770263671875
value loss:29.82855224609375
entropies:40.18260955810547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.196366310119629 seconds
policy loss:-2335.416748046875
value loss:32.929786682128906
entropies:75.95962524414062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.204679250717163 seconds
policy loss:-654.625244140625
value loss:25.272844314575195
entropies:36.03581237792969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.229231357574463 seconds
policy loss:-826.9607543945312
value loss:26.771251678466797
entropies:40.60432434082031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2229983806610107 seconds
policy loss:171.89111328125
value loss:30.42325782775879
entropies:47.181400299072266
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1153.5428)
ToM Target loss= tensor(2197.5906)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2388358116149902 seconds
policy loss:554.9668579101562
value loss:20.298831939697266
entropies:32.87183380126953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1749193668365479 seconds
policy loss:3.096966505050659
value loss:14.55960750579834
entropies:33.934112548828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2379014492034912 seconds
policy loss:-680.3777465820312
value loss:22.92624282836914
entropies:49.26561737060547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177459955215454 seconds
policy loss:-280.2760314941406
value loss:17.268667221069336
entropies:34.766319274902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1798484325408936 seconds
policy loss:-799.7958374023438
value loss:23.971149444580078
entropies:41.06769943237305
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1097.7394)
ToM Target loss= tensor(2176.8499)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2394845485687256 seconds
policy loss:-1370.2086181640625
value loss:21.31941795349121
entropies:45.55816650390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2431704998016357 seconds
policy loss:-456.4007568359375
value loss:8.675508499145508
entropies:34.42276382446289
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173400640487671 seconds
policy loss:-934.2827758789062
value loss:29.830121994018555
entropies:49.88197326660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2101414203643799 seconds
policy loss:-1031.9228515625
value loss:23.752395629882812
entropies:52.822792053222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1832125186920166 seconds
policy loss:-422.6404724121094
value loss:17.559350967407227
entropies:29.967487335205078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1141.7853)
ToM Target loss= tensor(2149.7524)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1709651947021484 seconds
policy loss:-805.3435668945312
value loss:15.321992874145508
entropies:43.813072204589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.222093105316162 seconds
policy loss:-676.6610717773438
value loss:24.74794578552246
entropies:49.736846923828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.222787857055664 seconds
policy loss:-660.6266479492188
value loss:41.9813346862793
entropies:24.18166732788086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1831724643707275 seconds
policy loss:-590.7896118164062
value loss:18.203556060791016
entropies:49.55442428588867
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1650676727294922 seconds
policy loss:-515.034423828125
value loss:18.073074340820312
entropies:39.49214172363281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1243.4679)
ToM Target loss= tensor(2250.3438)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1857774257659912 seconds
policy loss:-586.2706298828125
value loss:17.84728240966797
entropies:66.39496612548828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1778182983398438 seconds
policy loss:-458.0367736816406
value loss:11.445550918579102
entropies:23.369709014892578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2037022113800049 seconds
policy loss:-389.52838134765625
value loss:14.988462448120117
entropies:50.35561752319336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177854299545288 seconds
policy loss:-1537.265380859375
value loss:28.658626556396484
entropies:49.66028594970703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1683292388916016 seconds
policy loss:-826.4486694335938
value loss:28.94112777709961
entropies:63.37324905395508
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1297.8572)
ToM Target loss= tensor(2299.6709)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.243208885192871 seconds
policy loss:-243.6842041015625
value loss:23.138511657714844
entropies:28.13303565979004
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2090773582458496 seconds
policy loss:-205.15074157714844
value loss:14.17821979522705
entropies:33.126220703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1746790409088135 seconds
policy loss:-69.40924835205078
value loss:17.094514846801758
entropies:30.10572624206543
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2238163948059082 seconds
policy loss:-1261.900634765625
value loss:19.188196182250977
entropies:33.50841522216797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2213919162750244 seconds
policy loss:-427.5373840332031
value loss:13.01722526550293
entropies:39.1513557434082
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1169.3665)
ToM Target loss= tensor(2394.3943)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2297968864440918 seconds
policy loss:-1087.8876953125
value loss:22.735013961791992
entropies:44.933349609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2401320934295654 seconds
policy loss:449.47601318359375
value loss:13.075948715209961
entropies:33.895668029785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2581145763397217 seconds
policy loss:-20.280332565307617
value loss:24.654438018798828
entropies:40.129051208496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1993951797485352 seconds
policy loss:-29.318267822265625
value loss:26.54243278503418
entropies:45.38639831542969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1716363430023193 seconds
policy loss:-368.115234375
value loss:13.238040924072266
entropies:31.83734130859375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1115.2754)
ToM Target loss= tensor(2360.7817)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2643005847930908 seconds
policy loss:-223.78750610351562
value loss:11.68483829498291
entropies:37.28705596923828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2261962890625 seconds
policy loss:-485.79595947265625
value loss:22.538244247436523
entropies:50.74274444580078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1748356819152832 seconds
policy loss:-161.0584259033203
value loss:15.448888778686523
entropies:47.46222686767578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1796469688415527 seconds
policy loss:-507.794921875
value loss:15.620426177978516
entropies:47.863670349121094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.239830493927002 seconds
policy loss:-976.6455688476562
value loss:20.94099235534668
entropies:66.77008056640625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1284.8529)
ToM Target loss= tensor(2235.4106)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2400460243225098 seconds
policy loss:-1045.1844482421875
value loss:24.172569274902344
entropies:44.2495002746582
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2267587184906006 seconds
policy loss:-186.05076599121094
value loss:17.938440322875977
entropies:43.34816360473633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2317843437194824 seconds
policy loss:-33.92259216308594
value loss:11.036604881286621
entropies:26.880569458007812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.167111873626709 seconds
policy loss:-349.302978515625
value loss:22.15656280517578
entropies:42.64226150512695
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2228329181671143 seconds
policy loss:-707.8811645507812
value loss:29.496334075927734
entropies:33.953956604003906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1247.9475)
ToM Target loss= tensor(2298.3711)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2408545017242432 seconds
policy loss:-512.2301635742188
value loss:14.914329528808594
entropies:33.259090423583984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2341034412384033 seconds
policy loss:-1185.0494384765625
value loss:20.151268005371094
entropies:57.87654113769531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.175443410873413 seconds
policy loss:-1751.15380859375
value loss:39.27224349975586
entropies:35.82429504394531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2042961120605469 seconds
policy loss:-717.7604370117188
value loss:28.104259490966797
entropies:46.72016143798828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1752467155456543 seconds
policy loss:-529.7166137695312
value loss:15.182373046875
entropies:39.08782958984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1173.1028)
ToM Target loss= tensor(2320.0625)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1660559177398682 seconds
policy loss:-36.350852966308594
value loss:23.14236831665039
entropies:57.35837936401367
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1712567806243896 seconds
policy loss:-702.924560546875
value loss:11.841852188110352
entropies:45.68768310546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.195894479751587 seconds
policy loss:-148.53623962402344
value loss:19.821475982666016
entropies:47.76722717285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.199148178100586 seconds
policy loss:-243.66978454589844
value loss:19.777326583862305
entropies:53.204261779785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.139247179031372 seconds
policy loss:660.68310546875
value loss:9.838685035705566
entropies:29.302762985229492
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1226.7427)
ToM Target loss= tensor(2282.3252)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.238732099533081 seconds
policy loss:-887.1305541992188
value loss:20.662933349609375
entropies:59.8408203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1987292766571045 seconds
policy loss:-260.5359802246094
value loss:16.913467407226562
entropies:42.329803466796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2275571823120117 seconds
policy loss:-438.51025390625
value loss:13.07400131225586
entropies:50.49066162109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1736092567443848 seconds
policy loss:-621.5833129882812
value loss:20.23243522644043
entropies:45.540931701660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2347784042358398 seconds
policy loss:-1360.310302734375
value loss:19.166330337524414
entropies:55.233741760253906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1175.1614)
ToM Target loss= tensor(2212.4824)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.230285406112671 seconds
policy loss:-882.5631103515625
value loss:18.777891159057617
entropies:50.20294952392578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2191777229309082 seconds
policy loss:-1444.6053466796875
value loss:19.91288948059082
entropies:68.88525390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177800178527832 seconds
policy loss:-1253.5850830078125
value loss:25.505210876464844
entropies:58.78063201904297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2218632698059082 seconds
policy loss:262.7913513183594
value loss:15.344578742980957
entropies:53.24729537963867
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1737825870513916 seconds
policy loss:-633.3096923828125
value loss:34.247581481933594
entropies:41.6477165222168
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1275.2238)
ToM Target loss= tensor(2142.3535)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.181656837463379 seconds
policy loss:-264.514892578125
value loss:28.365036010742188
entropies:57.609710693359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.202148199081421 seconds
policy loss:-220.13101196289062
value loss:25.81946563720703
entropies:76.4749755859375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2320876121520996 seconds
policy loss:-146.8106689453125
value loss:22.905345916748047
entropies:60.76364517211914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2441697120666504 seconds
policy loss:-233.80084228515625
value loss:13.602651596069336
entropies:35.855926513671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2354023456573486 seconds
policy loss:-776.5657348632812
value loss:12.613654136657715
entropies:36.15654754638672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1277.3456)
ToM Target loss= tensor(2195.8965)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.223374605178833 seconds
policy loss:-1386.33935546875
value loss:35.729129791259766
entropies:54.92201232910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2309787273406982 seconds
policy loss:-1294.562255859375
value loss:23.684297561645508
entropies:51.17188262939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1834759712219238 seconds
policy loss:-1272.846435546875
value loss:28.319385528564453
entropies:53.20549774169922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2196619510650635 seconds
policy loss:-1674.8018798828125
value loss:29.4176025390625
entropies:53.53205871582031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2174012660980225 seconds
policy loss:-38.75513458251953
value loss:11.553339004516602
entropies:40.447750091552734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1210.0781)
ToM Target loss= tensor(2279.2434)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2342941761016846 seconds
policy loss:-232.8631591796875
value loss:18.0093994140625
entropies:48.57509231567383
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1741292476654053 seconds
policy loss:173.6210174560547
value loss:21.7926025390625
entropies:35.5035400390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2142539024353027 seconds
policy loss:-286.709228515625
value loss:14.715734481811523
entropies:57.875701904296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201314926147461 seconds
policy loss:-268.0707702636719
value loss:25.929279327392578
entropies:67.70252227783203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2482051849365234 seconds
policy loss:-141.7292022705078
value loss:11.43227481842041
entropies:35.59090805053711
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1239.2079)
ToM Target loss= tensor(2259.0105)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1971917152404785 seconds
policy loss:-236.0589599609375
value loss:10.610344886779785
entropies:34.580448150634766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2300934791564941 seconds
policy loss:-690.00341796875
value loss:14.162147521972656
entropies:50.75358200073242
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2111828327178955 seconds
policy loss:-994.0220336914062
value loss:21.115692138671875
entropies:61.613582611083984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1676836013793945 seconds
policy loss:-1974.63671875
value loss:60.70751953125
entropies:50.137550354003906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2244234085083008 seconds
policy loss:-413.4735412597656
value loss:13.032628059387207
entropies:43.34510040283203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1293.8441)
ToM Target loss= tensor(2245.2742)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.17564058303833 seconds
policy loss:-316.63714599609375
value loss:15.544795989990234
entropies:32.48003005981445
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2094357013702393 seconds
policy loss:-460.5996398925781
value loss:14.955842971801758
entropies:40.2369384765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1706347465515137 seconds
policy loss:-417.846435546875
value loss:17.793418884277344
entropies:46.043060302734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2258691787719727 seconds
policy loss:-1761.4444580078125
value loss:65.743896484375
entropies:61.15645980834961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1754024028778076 seconds
policy loss:254.98101806640625
value loss:14.676565170288086
entropies:43.32711410522461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1241.2860)
ToM Target loss= tensor(2168.6750)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2380754947662354 seconds
policy loss:-338.84112548828125
value loss:16.940387725830078
entropies:41.141807556152344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1687753200531006 seconds
policy loss:-189.2620849609375
value loss:15.096242904663086
entropies:32.34721755981445
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1950395107269287 seconds
policy loss:-223.6356201171875
value loss:14.09967041015625
entropies:39.208717346191406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1790807247161865 seconds
policy loss:-886.1906127929688
value loss:9.058297157287598
entropies:32.6553840637207
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2358262538909912 seconds
policy loss:-1191.75732421875
value loss:18.731781005859375
entropies:55.349937438964844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1211.0599)
ToM Target loss= tensor(2362.2979)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1746251583099365 seconds
policy loss:-1328.62841796875
value loss:17.821504592895508
entropies:40.93245315551758
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1783082485198975 seconds
policy loss:-428.9267883300781
value loss:18.665376663208008
entropies:53.804412841796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2071807384490967 seconds
policy loss:-293.9748229980469
value loss:23.832439422607422
entropies:46.701416015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2159733772277832 seconds
policy loss:10.98660659790039
value loss:26.06261444091797
entropies:35.79106903076172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2053306102752686 seconds
policy loss:101.02897644042969
value loss:27.896352767944336
entropies:47.796512603759766
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1218.4792)
ToM Target loss= tensor(2366.3469)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2388255596160889 seconds
policy loss:-52.56412124633789
value loss:30.482257843017578
entropies:61.113250732421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.211205005645752 seconds
policy loss:-1724.2479248046875
value loss:31.95254898071289
entropies:61.54339599609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1761689186096191 seconds
policy loss:-570.0072021484375
value loss:15.211777687072754
entropies:47.839195251464844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.179504632949829 seconds
policy loss:-733.5521240234375
value loss:23.986919403076172
entropies:48.96637725830078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1897528171539307 seconds
policy loss:-345.2130432128906
value loss:25.465835571289062
entropies:40.84806823730469
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1253.7563)
ToM Target loss= tensor(2196.0366)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2512006759643555 seconds
policy loss:-452.76971435546875
value loss:12.083694458007812
entropies:44.377830505371094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1700692176818848 seconds
policy loss:-433.91082763671875
value loss:11.147115707397461
entropies:43.34478759765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.23293137550354 seconds
policy loss:-280.23553466796875
value loss:16.944931030273438
entropies:45.978675842285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2028865814208984 seconds
policy loss:-1653.9727783203125
value loss:25.385040283203125
entropies:77.85541534423828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2266430854797363 seconds
policy loss:-841.3869018554688
value loss:18.689496994018555
entropies:57.36677551269531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1236.7290)
ToM Target loss= tensor(2105.4937)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2041149139404297 seconds
policy loss:-1000.2315063476562
value loss:13.99230670928955
entropies:41.538822174072266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2256669998168945 seconds
policy loss:-619.4048461914062
value loss:28.29001235961914
entropies:47.301658630371094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2057929039001465 seconds
policy loss:-120.34019470214844
value loss:8.443840026855469
entropies:48.30916213989258
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2295117378234863 seconds
policy loss:-1062.4114990234375
value loss:15.44680404663086
entropies:49.484031677246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.233325719833374 seconds
policy loss:-132.0911102294922
value loss:12.198233604431152
entropies:42.348548889160156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1164.7291)
ToM Target loss= tensor(2206.0334)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.168717861175537 seconds
policy loss:-993.283935546875
value loss:18.381296157836914
entropies:46.97209167480469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1746039390563965 seconds
policy loss:-1126.1995849609375
value loss:40.40736389160156
entropies:57.46520233154297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2154779434204102 seconds
policy loss:-1094.4976806640625
value loss:25.654035568237305
entropies:44.357276916503906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1764564514160156 seconds
policy loss:-958.177490234375
value loss:42.66592788696289
entropies:51.9886474609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1758713722229004 seconds
policy loss:-145.69125366210938
value loss:8.411116600036621
entropies:41.09742736816406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1323.1526)
ToM Target loss= tensor(2448.8054)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.222895622253418 seconds
policy loss:166.01116943359375
value loss:14.185226440429688
entropies:34.59244918823242
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2291173934936523 seconds
policy loss:-448.9441833496094
value loss:26.947099685668945
entropies:50.33380889892578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2271344661712646 seconds
policy loss:-455.524169921875
value loss:15.288152694702148
entropies:53.34379196166992
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2112526893615723 seconds
policy loss:490.28277587890625
value loss:23.49566650390625
entropies:55.908363342285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1703789234161377 seconds
policy loss:-199.6092071533203
value loss:10.064306259155273
entropies:24.999723434448242
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1136.7980)
ToM Target loss= tensor(2276.5591)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2360694408416748 seconds
policy loss:-1250.01416015625
value loss:27.365110397338867
entropies:40.773353576660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1720681190490723 seconds
policy loss:-1482.55224609375
value loss:33.090415954589844
entropies:65.59642791748047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2363955974578857 seconds
policy loss:-162.45443725585938
value loss:22.009143829345703
entropies:42.359840393066406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1897063255310059 seconds
policy loss:-547.9822998046875
value loss:15.249479293823242
entropies:32.77256774902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.243969440460205 seconds
policy loss:248.95460510253906
value loss:17.760194778442383
entropies:48.58183288574219
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1256.8918)
ToM Target loss= tensor(2346.5676)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2136800289154053 seconds
policy loss:-241.4571075439453
value loss:11.164623260498047
entropies:41.1827507019043
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1683239936828613 seconds
policy loss:-148.39474487304688
value loss:16.58030128479004
entropies:35.31229019165039
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2261779308319092 seconds
policy loss:-1684.2686767578125
value loss:32.25819778442383
entropies:64.36861419677734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2115240097045898 seconds
policy loss:333.25189208984375
value loss:19.66916847229004
entropies:49.36716079711914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2384724617004395 seconds
policy loss:-1095.1807861328125
value loss:30.27495765686035
entropies:55.17701721191406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1237.6375)
ToM Target loss= tensor(2079.6121)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2439351081848145 seconds
policy loss:442.9865417480469
value loss:19.895069122314453
entropies:51.18266296386719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.160451889038086 seconds
policy loss:-1310.0074462890625
value loss:24.241586685180664
entropies:56.733238220214844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1633880138397217 seconds
policy loss:-1537.942138671875
value loss:38.38179397583008
entropies:47.82806396484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2083535194396973 seconds
policy loss:-1237.76904296875
value loss:11.901001930236816
entropies:42.97051239013672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1774640083312988 seconds
policy loss:-611.2589111328125
value loss:10.494218826293945
entropies:23.148523330688477
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1256.5055)
ToM Target loss= tensor(2185.4270)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2084555625915527 seconds
policy loss:-1156.6802978515625
value loss:15.97542667388916
entropies:23.895267486572266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.23403000831604 seconds
policy loss:-129.05715942382812
value loss:13.461352348327637
entropies:53.83525848388672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1652791500091553 seconds
policy loss:-45.48702621459961
value loss:28.299480438232422
entropies:71.19696807861328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.231745719909668 seconds
policy loss:276.16497802734375
value loss:16.071054458618164
entropies:36.477352142333984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.230116605758667 seconds
policy loss:53.008453369140625
value loss:16.230241775512695
entropies:53.94529342651367
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1195.5015)
ToM Target loss= tensor(2272.7095)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2531781196594238 seconds
policy loss:-437.42913818359375
value loss:24.1158447265625
entropies:37.5178337097168
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1749353408813477 seconds
policy loss:-13.783609390258789
value loss:13.816088676452637
entropies:45.75321578979492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.174635410308838 seconds
policy loss:-1254.0191650390625
value loss:16.456218719482422
entropies:50.0811882019043
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2136342525482178 seconds
policy loss:-680.506103515625
value loss:11.829753875732422
entropies:29.85219383239746
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2356648445129395 seconds
policy loss:-2694.21826171875
value loss:84.46347045898438
entropies:52.132652282714844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1193.7297)
ToM Target loss= tensor(2252.4302)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2757275104522705 seconds
policy loss:-1442.543212890625
value loss:31.58025550842285
entropies:29.11465072631836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2444963455200195 seconds
policy loss:-954.1126708984375
value loss:20.184011459350586
entropies:44.170135498046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1668312549591064 seconds
policy loss:-577.8644409179688
value loss:16.997831344604492
entropies:44.263153076171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2128875255584717 seconds
policy loss:-935.4177856445312
value loss:26.92753791809082
entropies:50.91807174682617
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2533488273620605 seconds
policy loss:271.5345458984375
value loss:18.57095718383789
entropies:53.416202545166016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1272.8955)
ToM Target loss= tensor(2206.4919)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1703596115112305 seconds
policy loss:-957.143310546875
value loss:27.707277297973633
entropies:48.32768249511719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.157238483428955 seconds
policy loss:-124.69398498535156
value loss:15.054874420166016
entropies:29.68696403503418
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.223219871520996 seconds
policy loss:-309.8309020996094
value loss:28.93927764892578
entropies:42.601036071777344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2332637310028076 seconds
policy loss:-1373.80908203125
value loss:50.30629348754883
entropies:65.59761810302734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1669130325317383 seconds
policy loss:-937.3424072265625
value loss:20.297542572021484
entropies:51.68872833251953
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1251.4460)
ToM Target loss= tensor(2200.0908)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2328147888183594 seconds
policy loss:36.31834030151367
value loss:12.171013832092285
entropies:40.155113220214844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2161567211151123 seconds
policy loss:-292.78106689453125
value loss:19.74358558654785
entropies:36.72724533081055
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1753530502319336 seconds
policy loss:-485.8719787597656
value loss:18.534276962280273
entropies:43.31146240234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.234938383102417 seconds
policy loss:-1051.350830078125
value loss:17.809823989868164
entropies:48.667503356933594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2326223850250244 seconds
policy loss:-231.15025329589844
value loss:17.336288452148438
entropies:49.64086151123047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1249.0957)
ToM Target loss= tensor(2163.6436)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2003283500671387 seconds
policy loss:-115.00818634033203
value loss:10.46656608581543
entropies:43.269187927246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2321093082427979 seconds
policy loss:-930.6914672851562
value loss:31.925785064697266
entropies:54.8188362121582
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1732511520385742 seconds
policy loss:-1154.0960693359375
value loss:32.876930236816406
entropies:52.90248107910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173198938369751 seconds
policy loss:-1554.0478515625
value loss:34.58279037475586
entropies:47.57685089111328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.231675386428833 seconds
policy loss:-453.9061584472656
value loss:12.26466178894043
entropies:43.1007080078125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1258.0417)
ToM Target loss= tensor(2169.2373)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2216956615447998 seconds
policy loss:-583.7048950195312
value loss:18.19076156616211
entropies:49.05124282836914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1785838603973389 seconds
policy loss:-1082.757080078125
value loss:26.221355438232422
entropies:47.81367111206055
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1725609302520752 seconds
policy loss:-1188.8397216796875
value loss:16.206260681152344
entropies:41.67894744873047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1825737953186035 seconds
policy loss:-895.665283203125
value loss:29.887039184570312
entropies:59.76400375366211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1964166164398193 seconds
policy loss:-2326.25146484375
value loss:32.70701599121094
entropies:52.84251022338867
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1316.1191)
ToM Target loss= tensor(2279.4573)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1829700469970703 seconds
policy loss:-940.959228515625
value loss:25.582311630249023
entropies:56.78352737426758
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1651313304901123 seconds
policy loss:-161.42762756347656
value loss:15.972305297851562
entropies:33.02764129638672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2348055839538574 seconds
policy loss:-325.1788635253906
value loss:13.445457458496094
entropies:34.68865966796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.172318696975708 seconds
policy loss:-65.9049301147461
value loss:8.177492141723633
entropies:40.164588928222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.180574893951416 seconds
policy loss:-148.3919677734375
value loss:11.662829399108887
entropies:49.61545181274414
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1148.2032)
ToM Target loss= tensor(2200.6057)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.195298433303833 seconds
policy loss:231.6044158935547
value loss:13.766351699829102
entropies:50.339447021484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.209378957748413 seconds
policy loss:61.21098327636719
value loss:6.761849880218506
entropies:23.11086654663086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2004141807556152 seconds
policy loss:-1421.73291015625
value loss:26.56747817993164
entropies:56.207740783691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176255226135254 seconds
policy loss:-1473.8533935546875
value loss:30.89308738708496
entropies:42.26706314086914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1722087860107422 seconds
policy loss:-1003.549560546875
value loss:16.529762268066406
entropies:58.09321594238281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1293.8925)
ToM Target loss= tensor(2229.0654)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.211127758026123 seconds
policy loss:-1.7040824890136719
value loss:16.638383865356445
entropies:59.311065673828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1731746196746826 seconds
policy loss:-455.8697509765625
value loss:16.946613311767578
entropies:47.08586883544922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2166874408721924 seconds
policy loss:-270.50164794921875
value loss:24.301904678344727
entropies:64.7943344116211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2377815246582031 seconds
policy loss:-241.64981079101562
value loss:12.612863540649414
entropies:48.01080322265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1657829284667969 seconds
policy loss:-369.447509765625
value loss:20.13198471069336
entropies:38.08989715576172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1315.5847)
ToM Target loss= tensor(2356.7866)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2115705013275146 seconds
policy loss:-592.2041015625
value loss:14.30005931854248
entropies:46.6084098815918
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2248296737670898 seconds
policy loss:-230.80459594726562
value loss:13.443329811096191
entropies:51.39894104003906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2347831726074219 seconds
policy loss:-720.0264282226562
value loss:16.945646286010742
entropies:57.87343215942383
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.231412649154663 seconds
policy loss:-842.576171875
value loss:32.45956039428711
entropies:54.11073303222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1699373722076416 seconds
policy loss:-1223.564453125
value loss:20.153484344482422
entropies:32.82419967651367
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1279.4009)
ToM Target loss= tensor(2243.9658)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2168710231781006 seconds
policy loss:104.07301330566406
value loss:4.005340576171875
entropies:25.495830535888672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1710963249206543 seconds
policy loss:-444.7254943847656
value loss:17.97123146057129
entropies:55.376190185546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1748881340026855 seconds
policy loss:-26.178224563598633
value loss:30.40237045288086
entropies:42.079410552978516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1846694946289062 seconds
policy loss:-106.89449310302734
value loss:6.183366298675537
entropies:34.26158905029297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2198388576507568 seconds
policy loss:-1259.6395263671875
value loss:31.6502628326416
entropies:64.02761840820312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1280.7024)
ToM Target loss= tensor(2316.6738)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2127771377563477 seconds
policy loss:-1688.4732666015625
value loss:28.32430076599121
entropies:51.96837615966797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.233699083328247 seconds
policy loss:-1386.7664794921875
value loss:60.503604888916016
entropies:53.95245361328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2345318794250488 seconds
policy loss:-1274.4356689453125
value loss:36.046409606933594
entropies:41.90532302856445
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1717534065246582 seconds
policy loss:-220.43276977539062
value loss:18.28730583190918
entropies:60.91389083862305
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2349424362182617 seconds
policy loss:-215.05776977539062
value loss:14.484033584594727
entropies:42.67544937133789
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1262.0587)
ToM Target loss= tensor(2218.2832)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1732492446899414 seconds
policy loss:-79.91924285888672
value loss:14.338569641113281
entropies:32.11005401611328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1956446170806885 seconds
policy loss:162.83978271484375
value loss:14.400165557861328
entropies:42.6845703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2338862419128418 seconds
policy loss:-372.0823974609375
value loss:17.92304801940918
entropies:40.89720153808594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1791799068450928 seconds
policy loss:-1628.6812744140625
value loss:27.98251724243164
entropies:57.71820831298828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.212512493133545 seconds
policy loss:157.35012817382812
value loss:8.289819717407227
entropies:27.250957489013672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1154.8591)
ToM Target loss= tensor(2174.5449)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1772270202636719 seconds
policy loss:-333.4093322753906
value loss:6.996294975280762
entropies:20.340116500854492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1827983856201172 seconds
policy loss:-1522.5137939453125
value loss:52.79806137084961
entropies:58.55339813232422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2333998680114746 seconds
policy loss:-1535.6956787109375
value loss:52.16475296020508
entropies:51.3255729675293
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2046422958374023 seconds
policy loss:-647.9430541992188
value loss:18.36357307434082
entropies:42.42397689819336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1774563789367676 seconds
policy loss:-856.2320556640625
value loss:26.676774978637695
entropies:45.36549377441406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1241.4235)
ToM Target loss= tensor(2302.6572)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2742321491241455 seconds
policy loss:114.61124420166016
value loss:27.02704429626465
entropies:49.39216232299805
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2274773120880127 seconds
policy loss:-1501.8072509765625
value loss:32.31856918334961
entropies:58.511810302734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2219209671020508 seconds
policy loss:164.9011688232422
value loss:28.8920841217041
entropies:46.0155143737793
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.23740816116333 seconds
policy loss:-1255.99951171875
value loss:29.405139923095703
entropies:50.22637176513672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1803090572357178 seconds
policy loss:291.0866394042969
value loss:15.369383811950684
entropies:37.07061004638672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1250.0133)
ToM Target loss= tensor(2284.9590)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1756973266601562 seconds
policy loss:-100.15348815917969
value loss:23.286518096923828
entropies:41.692291259765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2132394313812256 seconds
policy loss:-336.0126953125
value loss:16.618309020996094
entropies:60.977264404296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2112746238708496 seconds
policy loss:-390.7476806640625
value loss:20.61658477783203
entropies:58.27729797363281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.220414638519287 seconds
policy loss:-1106.98193359375
value loss:30.384233474731445
entropies:55.64191818237305
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2390234470367432 seconds
policy loss:-1207.429931640625
value loss:35.069236755371094
entropies:44.3826904296875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1278.6268)
ToM Target loss= tensor(2248.5237)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2213904857635498 seconds
policy loss:375.3684997558594
value loss:11.227178573608398
entropies:28.91207504272461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2250301837921143 seconds
policy loss:-748.3133544921875
value loss:21.01645278930664
entropies:49.0634765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.233163833618164 seconds
policy loss:-523.9884643554688
value loss:26.123153686523438
entropies:49.58586502075195
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1830203533172607 seconds
policy loss:-145.7036895751953
value loss:30.98395538330078
entropies:44.190025329589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1876797676086426 seconds
policy loss:-722.1505737304688
value loss:18.132055282592773
entropies:65.55043029785156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1248.4991)
ToM Target loss= tensor(2215.8560)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2345705032348633 seconds
policy loss:569.9301147460938
value loss:21.00730323791504
entropies:35.346954345703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2511584758758545 seconds
policy loss:-664.1461181640625
value loss:36.788822174072266
entropies:67.04776763916016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228950023651123 seconds
policy loss:-1021.26708984375
value loss:30.531803131103516
entropies:57.16974639892578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2338755130767822 seconds
policy loss:-1213.9454345703125
value loss:39.32567596435547
entropies:54.00471496582031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2401621341705322 seconds
policy loss:-490.8724670410156
value loss:23.591279983520508
entropies:34.08543395996094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1293.0909)
ToM Target loss= tensor(2301.5991)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2313666343688965 seconds
policy loss:-638.4199829101562
value loss:15.515583992004395
entropies:38.541473388671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2067649364471436 seconds
policy loss:-1074.6038818359375
value loss:35.207786560058594
entropies:58.6959114074707
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2444870471954346 seconds
policy loss:-945.3443603515625
value loss:16.68883514404297
entropies:36.96685791015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2365903854370117 seconds
policy loss:-986.1076049804688
value loss:27.43053436279297
entropies:58.912742614746094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1729133129119873 seconds
policy loss:-954.6151123046875
value loss:17.903839111328125
entropies:51.91948699951172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1299.9938)
ToM Target loss= tensor(2349.4233)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1621184349060059 seconds
policy loss:-229.84454345703125
value loss:18.884361267089844
entropies:55.39923095703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2342250347137451 seconds
policy loss:-1281.697998046875
value loss:36.1520881652832
entropies:63.01194763183594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1771605014801025 seconds
policy loss:-337.0638732910156
value loss:26.353055953979492
entropies:42.11870193481445
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2455694675445557 seconds
policy loss:-295.90972900390625
value loss:19.53402328491211
entropies:39.64986801147461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1738178730010986 seconds
policy loss:-1411.26953125
value loss:26.840099334716797
entropies:62.17528533935547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1359.6438)
ToM Target loss= tensor(2259.9656)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2288262844085693 seconds
policy loss:-439.56219482421875
value loss:37.84226989746094
entropies:56.4776496887207
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.223015308380127 seconds
policy loss:-2789.551513671875
value loss:54.518882751464844
entropies:66.54023742675781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.213249921798706 seconds
policy loss:-224.20814514160156
value loss:21.62346649169922
entropies:54.03620147705078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.22544264793396 seconds
policy loss:296.0570983886719
value loss:14.49664306640625
entropies:49.62506103515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.222928762435913 seconds
policy loss:-522.083251953125
value loss:29.344890594482422
entropies:46.37364196777344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1354.6021)
ToM Target loss= tensor(2218.9382)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2074098587036133 seconds
policy loss:-665.9878540039062
value loss:11.12624740600586
entropies:49.069278717041016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2185237407684326 seconds
policy loss:9.354584693908691
value loss:16.19660186767578
entropies:44.57395935058594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1717441082000732 seconds
policy loss:-347.6997985839844
value loss:14.869010925292969
entropies:45.992103576660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1780807971954346 seconds
policy loss:-349.0070495605469
value loss:20.716747283935547
entropies:48.37138366699219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2338838577270508 seconds
policy loss:307.4638366699219
value loss:4.112741947174072
entropies:25.331363677978516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1244.8741)
ToM Target loss= tensor(2421.8708)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2161130905151367 seconds
policy loss:-438.534423828125
value loss:12.454432487487793
entropies:32.04106521606445
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2355749607086182 seconds
policy loss:-325.54052734375
value loss:13.03222370147705
entropies:42.56291580200195
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2202081680297852 seconds
policy loss:-1415.850341796875
value loss:22.882993698120117
entropies:46.63795852661133
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.21370530128479 seconds
policy loss:-1078.8353271484375
value loss:18.59747886657715
entropies:49.528961181640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2378900051116943 seconds
policy loss:-1051.5213623046875
value loss:16.065643310546875
entropies:36.6562614440918
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1223.1305)
ToM Target loss= tensor(2409.2532)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1589891910552979 seconds
policy loss:-149.8183135986328
value loss:6.403022289276123
entropies:34.12802505493164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1633579730987549 seconds
policy loss:-369.6578369140625
value loss:14.645980834960938
entropies:26.99919891357422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2192883491516113 seconds
policy loss:-1289.4764404296875
value loss:48.24000549316406
entropies:59.908260345458984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.238497018814087 seconds
policy loss:217.44912719726562
value loss:11.657682418823242
entropies:33.485809326171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2250316143035889 seconds
policy loss:44.77495574951172
value loss:34.30622863769531
entropies:54.05575180053711
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1186.4216)
ToM Target loss= tensor(2308.2214)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2315943241119385 seconds
policy loss:79.5301742553711
value loss:16.209394454956055
entropies:32.217647552490234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2345733642578125 seconds
policy loss:142.1859588623047
value loss:10.604214668273926
entropies:30.015899658203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2368872165679932 seconds
policy loss:-348.68096923828125
value loss:17.472293853759766
entropies:46.42732620239258
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.182349681854248 seconds
policy loss:-1356.219482421875
value loss:30.682098388671875
entropies:64.43762969970703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1754310131072998 seconds
policy loss:-1266.865234375
value loss:28.843942642211914
entropies:49.0072135925293
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1295.0596)
ToM Target loss= tensor(2206.1362)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1306238174438477 seconds
policy loss:-1341.1787109375
value loss:20.257543563842773
entropies:53.37208557128906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.203852653503418 seconds
policy loss:-852.5084228515625
value loss:18.974712371826172
entropies:37.143856048583984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.203906774520874 seconds
policy loss:-1397.123046875
value loss:21.019115447998047
entropies:45.575782775878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1776211261749268 seconds
policy loss:-704.713134765625
value loss:13.909638404846191
entropies:58.444374084472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2510488033294678 seconds
policy loss:305.54608154296875
value loss:13.17038631439209
entropies:31.67376708984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1279.6309)
ToM Target loss= tensor(2339.5688)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.240490436553955 seconds
policy loss:-276.7814025878906
value loss:27.933834075927734
entropies:44.00350570678711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169752836227417 seconds
policy loss:-115.94133758544922
value loss:29.50960922241211
entropies:63.58690643310547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1648187637329102 seconds
policy loss:267.23828125
value loss:25.22805404663086
entropies:34.082374572753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.171492338180542 seconds
policy loss:-147.37863159179688
value loss:20.64597511291504
entropies:42.02740478515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1806433200836182 seconds
policy loss:-893.90576171875
value loss:19.965892791748047
entropies:53.463218688964844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1276.1306)
ToM Target loss= tensor(2378.7622)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.182736873626709 seconds
policy loss:-1027.945068359375
value loss:21.27020263671875
entropies:48.776336669921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1956987380981445 seconds
policy loss:-1370.9708251953125
value loss:23.2515811920166
entropies:48.49571990966797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2293477058410645 seconds
policy loss:-1254.466552734375
value loss:26.209108352661133
entropies:58.122901916503906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1785707473754883 seconds
policy loss:-1124.1363525390625
value loss:15.514103889465332
entropies:56.48006057739258
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2154879570007324 seconds
policy loss:-1444.8297119140625
value loss:48.28870391845703
entropies:60.023582458496094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1271.6293)
ToM Target loss= tensor(2180.2312)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2087273597717285 seconds
policy loss:112.67460632324219
value loss:13.91971206665039
entropies:45.039794921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1895339488983154 seconds
policy loss:-810.6163330078125
value loss:30.62549591064453
entropies:62.267791748046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1699786186218262 seconds
policy loss:-282.9010009765625
value loss:16.729934692382812
entropies:38.19738006591797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.189091444015503 seconds
policy loss:-771.9345092773438
value loss:23.12191390991211
entropies:58.83418655395508
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1891441345214844 seconds
policy loss:445.1665954589844
value loss:21.10020637512207
entropies:63.93549346923828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1335.2540)
ToM Target loss= tensor(2182.7122)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2325215339660645 seconds
policy loss:-696.87158203125
value loss:19.6608829498291
entropies:40.534950256347656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1847996711730957 seconds
policy loss:-898.375244140625
value loss:37.99152755737305
entropies:63.4581184387207
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2204952239990234 seconds
policy loss:-208.02545166015625
value loss:15.13868522644043
entropies:42.79390335083008
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1778852939605713 seconds
policy loss:-560.5612182617188
value loss:18.246816635131836
entropies:56.99299240112305
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2259719371795654 seconds
policy loss:-1039.04443359375
value loss:23.688730239868164
entropies:47.053062438964844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1287.7754)
ToM Target loss= tensor(2314.9028)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.196861982345581 seconds
policy loss:-1156.5570068359375
value loss:24.6028995513916
entropies:55.908668518066406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.205026388168335 seconds
policy loss:341.3524475097656
value loss:23.901153564453125
entropies:67.52684020996094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1999824047088623 seconds
policy loss:-1074.377197265625
value loss:28.34925651550293
entropies:45.97643280029297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1989343166351318 seconds
policy loss:-729.6180419921875
value loss:23.37601661682129
entropies:63.580467224121094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2415881156921387 seconds
policy loss:42.930694580078125
value loss:17.49166488647461
entropies:45.84831619262695
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1425.2202)
ToM Target loss= tensor(2292.0640)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.176793098449707 seconds
policy loss:-18.857858657836914
value loss:8.437559127807617
entropies:36.53029251098633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1701133251190186 seconds
policy loss:-1812.0650634765625
value loss:19.37390899658203
entropies:58.804527282714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2299792766571045 seconds
policy loss:-1231.9364013671875
value loss:14.515990257263184
entropies:50.55900192260742
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1856448650360107 seconds
policy loss:-217.8561248779297
value loss:9.45780086517334
entropies:36.1412239074707
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1838560104370117 seconds
policy loss:-386.1015625
value loss:18.644832611083984
entropies:53.09561538696289
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1312.4786)
ToM Target loss= tensor(2278.9810)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.240929365158081 seconds
policy loss:-718.4910888671875
value loss:14.952587127685547
entropies:37.52570724487305
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1697967052459717 seconds
policy loss:-1107.2965087890625
value loss:11.055627822875977
entropies:35.536705017089844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1720962524414062 seconds
policy loss:83.58319091796875
value loss:2.1999239921569824
entropies:27.94640350341797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.179246425628662 seconds
policy loss:-158.55233764648438
value loss:18.198692321777344
entropies:53.053611755371094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1845014095306396 seconds
policy loss:-1340.9979248046875
value loss:32.715518951416016
entropies:48.172428131103516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1251.1687)
ToM Target loss= tensor(2344.2144)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.166576623916626 seconds
policy loss:-473.699951171875
value loss:17.677318572998047
entropies:47.20185089111328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2155003547668457 seconds
policy loss:-407.19622802734375
value loss:19.163433074951172
entropies:54.116729736328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1779401302337646 seconds
policy loss:-408.5328674316406
value loss:7.619235992431641
entropies:32.73041534423828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.174501895904541 seconds
policy loss:-1321.4365234375
value loss:20.492650985717773
entropies:53.267730712890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1766321659088135 seconds
policy loss:-2490.2158203125
value loss:58.29941940307617
entropies:72.27101135253906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1342.2136)
ToM Target loss= tensor(2247.3208)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.229522943496704 seconds
policy loss:-745.95458984375
value loss:15.784069061279297
entropies:48.33344650268555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1821088790893555 seconds
policy loss:3.4021267890930176
value loss:30.241439819335938
entropies:40.35285186767578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2209429740905762 seconds
policy loss:-538.2991333007812
value loss:17.62173843383789
entropies:45.68696212768555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2446837425231934 seconds
policy loss:-1388.180908203125
value loss:23.26476287841797
entropies:75.3943862915039
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.172048568725586 seconds
policy loss:-982.1744384765625
value loss:28.491676330566406
entropies:68.71044921875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1409.3937)
ToM Target loss= tensor(2334.5845)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2165770530700684 seconds
policy loss:9.05636978149414
value loss:17.55670738220215
entropies:32.11294174194336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.248894453048706 seconds
policy loss:-469.4344787597656
value loss:18.00304412841797
entropies:60.18994140625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2319302558898926 seconds
policy loss:-136.6140899658203
value loss:13.191852569580078
entropies:21.82648468017578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1759064197540283 seconds
policy loss:-504.9571533203125
value loss:10.865091323852539
entropies:35.790504455566406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1660594940185547 seconds
policy loss:-128.76280212402344
value loss:6.730130195617676
entropies:31.397262573242188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1200.7931)
ToM Target loss= tensor(2409.9448)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2248237133026123 seconds
policy loss:-1069.0003662109375
value loss:20.623151779174805
entropies:47.40990447998047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2221508026123047 seconds
policy loss:-1503.6448974609375
value loss:35.673702239990234
entropies:44.984649658203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.236274242401123 seconds
policy loss:-3221.04638671875
value loss:65.46688842773438
entropies:60.9796257019043
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2149534225463867 seconds
policy loss:-1655.234130859375
value loss:32.09123229980469
entropies:52.560462951660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.14878249168396 seconds
policy loss:-419.50775146484375
value loss:21.68741226196289
entropies:39.11092758178711
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1276.4330)
ToM Target loss= tensor(2366.9097)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2284765243530273 seconds
policy loss:-721.3836669921875
value loss:18.001972198486328
entropies:29.63575553894043
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.153428077697754 seconds
policy loss:-439.89862060546875
value loss:26.458900451660156
entropies:30.266693115234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.161578893661499 seconds
policy loss:-521.8978881835938
value loss:24.38517951965332
entropies:32.55910873413086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1382544040679932 seconds
policy loss:-75.66353607177734
value loss:13.015600204467773
entropies:25.871667861938477
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.204394817352295 seconds
policy loss:-44.394100189208984
value loss:21.553146362304688
entropies:42.513145446777344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1257.7745)
ToM Target loss= tensor(2388.2368)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1882679462432861 seconds
policy loss:3.1906356811523438
value loss:15.276333808898926
entropies:60.185585021972656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1952455043792725 seconds
policy loss:-1167.992431640625
value loss:22.534257888793945
entropies:51.28280258178711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2208292484283447 seconds
policy loss:-929.7476196289062
value loss:17.524768829345703
entropies:49.121795654296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2186508178710938 seconds
policy loss:-131.69927978515625
value loss:9.01767349243164
entropies:30.576934814453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2495648860931396 seconds
policy loss:-227.84988403320312
value loss:17.88612174987793
entropies:48.25828552246094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1303.1516)
ToM Target loss= tensor(2313.9048)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1674182415008545 seconds
policy loss:-1351.2314453125
value loss:28.47970199584961
entropies:56.841644287109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2275054454803467 seconds
policy loss:-133.55633544921875
value loss:10.9556884765625
entropies:37.665714263916016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2393670082092285 seconds
policy loss:-468.6470642089844
value loss:16.550016403198242
entropies:55.531578063964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2324748039245605 seconds
policy loss:-138.71949768066406
value loss:14.10170841217041
entropies:44.30982971191406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2175345420837402 seconds
policy loss:-1848.841796875
value loss:56.15359878540039
entropies:56.21989059448242
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1327.0409)
ToM Target loss= tensor(2240.7231)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2042689323425293 seconds
policy loss:-590.7213134765625
value loss:13.014654159545898
entropies:49.0115966796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1706593036651611 seconds
policy loss:-1757.2279052734375
value loss:35.34492492675781
entropies:43.2150764465332
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2359685897827148 seconds
policy loss:-1265.971435546875
value loss:30.42346954345703
entropies:30.341712951660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1881811618804932 seconds
policy loss:-176.4278106689453
value loss:20.941307067871094
entropies:41.63085174560547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1723816394805908 seconds
policy loss:-324.4115295410156
value loss:29.964576721191406
entropies:51.72254180908203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1233.4646)
ToM Target loss= tensor(2256.1716)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2412872314453125 seconds
policy loss:128.6205291748047
value loss:14.188871383666992
entropies:38.34473419189453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2151341438293457 seconds
policy loss:-142.60369873046875
value loss:15.402556419372559
entropies:36.31749725341797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2321257591247559 seconds
policy loss:-1235.6802978515625
value loss:24.654525756835938
entropies:52.27760696411133
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.207641839981079 seconds
policy loss:309.7973327636719
value loss:16.225791931152344
entropies:60.74635314941406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2262132167816162 seconds
policy loss:-1904.639404296875
value loss:29.26506996154785
entropies:52.809669494628906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1293.6277)
ToM Target loss= tensor(2195.1323)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1945433616638184 seconds
policy loss:-1183.648193359375
value loss:16.545015335083008
entropies:57.44793701171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2427139282226562 seconds
policy loss:-896.9535522460938
value loss:12.177851676940918
entropies:37.31391906738281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2097208499908447 seconds
policy loss:-80.73735809326172
value loss:15.867327690124512
entropies:58.72690200805664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2320570945739746 seconds
policy loss:4.073421478271484
value loss:13.238901138305664
entropies:43.32571792602539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2334234714508057 seconds
policy loss:-1198.9127197265625
value loss:17.105344772338867
entropies:41.0235710144043
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1216.1262)
ToM Target loss= tensor(2220.6008)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.227071762084961 seconds
policy loss:45.30675506591797
value loss:8.353012084960938
entropies:40.87208557128906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2171292304992676 seconds
policy loss:-644.3020629882812
value loss:11.88483715057373
entropies:39.324546813964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2102959156036377 seconds
policy loss:-53.76972961425781
value loss:8.861294746398926
entropies:44.54497528076172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2030467987060547 seconds
policy loss:-608.6749877929688
value loss:12.57795524597168
entropies:40.204830169677734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2255942821502686 seconds
policy loss:-1614.823486328125
value loss:39.69223403930664
entropies:46.92267608642578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1263.2225)
ToM Target loss= tensor(2282.6870)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2324035167694092 seconds
policy loss:-959.6668090820312
value loss:18.516239166259766
entropies:43.294410705566406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2240190505981445 seconds
policy loss:-500.4020080566406
value loss:20.625791549682617
entropies:47.95154571533203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2157599925994873 seconds
policy loss:-1473.026123046875
value loss:13.842564582824707
entropies:42.193199157714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.160351276397705 seconds
policy loss:354.0750427246094
value loss:15.66946029663086
entropies:51.83170700073242
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2248263359069824 seconds
policy loss:-860.92333984375
value loss:19.127525329589844
entropies:38.016937255859375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1216.9321)
ToM Target loss= tensor(2174.1658)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1690046787261963 seconds
policy loss:-218.19700622558594
value loss:18.963071823120117
entropies:38.14814758300781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2181079387664795 seconds
policy loss:-732.8541870117188
value loss:12.117506980895996
entropies:45.353599548339844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1736741065979004 seconds
policy loss:-1148.505126953125
value loss:22.53050994873047
entropies:46.87952423095703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1857309341430664 seconds
policy loss:-374.2393493652344
value loss:9.613736152648926
entropies:29.73124885559082
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1958942413330078 seconds
policy loss:-929.859130859375
value loss:20.247957229614258
entropies:43.97385787963867
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1239.7269)
ToM Target loss= tensor(2197.5195)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.168649673461914 seconds
policy loss:-367.8194885253906
value loss:9.489752769470215
entropies:31.49053955078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2048535346984863 seconds
policy loss:-1115.8221435546875
value loss:24.478281021118164
entropies:43.48168182373047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2062146663665771 seconds
policy loss:-589.8637084960938
value loss:22.30889892578125
entropies:50.288230895996094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2078769207000732 seconds
policy loss:176.45445251464844
value loss:3.3329286575317383
entropies:17.73324966430664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.185269832611084 seconds
policy loss:265.2720031738281
value loss:12.852803230285645
entropies:47.6181640625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1409.9480)
ToM Target loss= tensor(2375.5315)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1825730800628662 seconds
policy loss:-570.2664184570312
value loss:9.768193244934082
entropies:24.624061584472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.229130506515503 seconds
policy loss:-496.9884948730469
value loss:17.814109802246094
entropies:37.62099838256836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2342703342437744 seconds
policy loss:-301.0374450683594
value loss:9.896129608154297
entropies:34.217830657958984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2289016246795654 seconds
policy loss:-40.80562973022461
value loss:3.981287956237793
entropies:10.689675331115723
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2157986164093018 seconds
policy loss:-1122.634765625
value loss:32.19200897216797
entropies:47.11689758300781
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1121.9722)
ToM Target loss= tensor(2339.2917)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2300570011138916 seconds
policy loss:-205.72479248046875
value loss:9.698174476623535
entropies:19.480199813842773
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1927525997161865 seconds
policy loss:-283.84112548828125
value loss:17.79819679260254
entropies:54.11812973022461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1704809665679932 seconds
policy loss:-529.3109130859375
value loss:18.926979064941406
entropies:48.79379653930664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.180795431137085 seconds
policy loss:-326.10626220703125
value loss:27.098806381225586
entropies:37.86846923828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1665644645690918 seconds
policy loss:-236.05841064453125
value loss:22.658262252807617
entropies:33.7303352355957
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1263.5741)
ToM Target loss= tensor(2174.1133)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.185159683227539 seconds
policy loss:-30.74207878112793
value loss:3.8445916175842285
entropies:25.37757682800293
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228133201599121 seconds
policy loss:-2072.782470703125
value loss:40.05441665649414
entropies:50.51469039916992
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2317492961883545 seconds
policy loss:-1986.80615234375
value loss:23.800159454345703
entropies:42.84431457519531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2235138416290283 seconds
policy loss:-1501.39501953125
value loss:23.12989044189453
entropies:27.595088958740234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1702704429626465 seconds
policy loss:-1530.8592529296875
value loss:28.678701400756836
entropies:49.48018264770508
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1188.7861)
ToM Target loss= tensor(2252.7861)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2437055110931396 seconds
policy loss:-304.8847961425781
value loss:17.810033798217773
entropies:22.91600799560547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.241896390914917 seconds
policy loss:-301.07183837890625
value loss:11.247932434082031
entropies:24.40524673461914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2359881401062012 seconds
policy loss:-1815.9300537109375
value loss:28.29530906677246
entropies:37.34940719604492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.24562406539917 seconds
policy loss:-24.735990524291992
value loss:21.711149215698242
entropies:31.862537384033203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1866958141326904 seconds
policy loss:-696.2905883789062
value loss:26.453235626220703
entropies:39.90521240234375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1140.7882)
ToM Target loss= tensor(2194.7961)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2087197303771973 seconds
policy loss:20.18476676940918
value loss:9.995000839233398
entropies:49.962074279785156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.231938362121582 seconds
policy loss:157.66445922851562
value loss:28.30259895324707
entropies:45.8707389831543
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1653449535369873 seconds
policy loss:-935.41455078125
value loss:29.363876342773438
entropies:47.711570739746094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.234760046005249 seconds
policy loss:103.43017578125
value loss:6.368734359741211
entropies:23.811269760131836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2157220840454102 seconds
policy loss:-316.9654846191406
value loss:14.531829833984375
entropies:43.059356689453125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1333.9241)
ToM Target loss= tensor(2353.8342)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.248175859451294 seconds
policy loss:-1066.3846435546875
value loss:28.287925720214844
entropies:39.24409103393555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228076696395874 seconds
policy loss:-1157.094970703125
value loss:35.670413970947266
entropies:59.68769454956055
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2368206977844238 seconds
policy loss:-190.46974182128906
value loss:22.66473388671875
entropies:56.60764694213867
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1437194347381592 seconds
policy loss:137.72817993164062
value loss:33.84156799316406
entropies:33.50266647338867
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2395963668823242 seconds
policy loss:-902.9290161132812
value loss:38.54785919189453
entropies:60.03084945678711
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1367.7528)
ToM Target loss= tensor(2374.7151)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2090015411376953 seconds
policy loss:354.2862548828125
value loss:21.290756225585938
entropies:34.383522033691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2381765842437744 seconds
policy loss:540.1697998046875
value loss:34.309993743896484
entropies:40.25226593017578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2431225776672363 seconds
policy loss:-58.636741638183594
value loss:14.227646827697754
entropies:45.31645202636719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.17421555519104 seconds
policy loss:-221.88031005859375
value loss:22.657936096191406
entropies:54.782012939453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.182511568069458 seconds
policy loss:-1450.595947265625
value loss:25.76022720336914
entropies:36.6416015625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1312.3345)
ToM Target loss= tensor(2403.3411)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2188668251037598 seconds
policy loss:-605.8410034179688
value loss:17.238245010375977
entropies:28.810548782348633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2346200942993164 seconds
policy loss:-874.1331787109375
value loss:30.688732147216797
entropies:37.53329849243164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.237776279449463 seconds
policy loss:-98.67589569091797
value loss:12.609001159667969
entropies:24.870328903198242
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2416656017303467 seconds
policy loss:-367.0575866699219
value loss:15.566291809082031
entropies:43.29975891113281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176093339920044 seconds
policy loss:-2622.2451171875
value loss:54.54127883911133
entropies:62.70935821533203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1264.8296)
ToM Target loss= tensor(2402.0186)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1952824592590332 seconds
policy loss:-655.0560302734375
value loss:12.314949989318848
entropies:33.893226623535156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1818311214447021 seconds
policy loss:-1179.1451416015625
value loss:33.30805969238281
entropies:44.96089172363281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2163174152374268 seconds
policy loss:-1163.1007080078125
value loss:26.353025436401367
entropies:60.97304153442383
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.17958402633667 seconds
policy loss:-204.81478881835938
value loss:21.611467361450195
entropies:48.918426513671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2330610752105713 seconds
policy loss:-988.2734375
value loss:27.905651092529297
entropies:61.31866455078125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1289.8237)
ToM Target loss= tensor(2318.7947)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.175170660018921 seconds
policy loss:-178.68896484375
value loss:12.51791763305664
entropies:45.3202018737793
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.182889461517334 seconds
policy loss:5.202807426452637
value loss:14.577001571655273
entropies:28.752498626708984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2373051643371582 seconds
policy loss:-820.5936889648438
value loss:25.830360412597656
entropies:44.110572814941406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1348187923431396 seconds
policy loss:-397.40875244140625
value loss:10.524284362792969
entropies:34.863426208496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2384111881256104 seconds
policy loss:-1997.937255859375
value loss:39.133541107177734
entropies:45.844703674316406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1306.9705)
ToM Target loss= tensor(2375.0449)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1852138042449951 seconds
policy loss:-827.2325439453125
value loss:12.206049919128418
entropies:33.26447296142578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2343032360076904 seconds
policy loss:-738.9090576171875
value loss:24.847370147705078
entropies:45.79808044433594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2228028774261475 seconds
policy loss:81.11065673828125
value loss:9.476696968078613
entropies:46.589195251464844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2520349025726318 seconds
policy loss:46.383174896240234
value loss:9.330084800720215
entropies:37.698631286621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2016301155090332 seconds
policy loss:-668.23291015625
value loss:21.322460174560547
entropies:36.76115798950195
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1283.6945)
ToM Target loss= tensor(2391.8281)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1802008152008057 seconds
policy loss:398.45977783203125
value loss:10.277134895324707
entropies:26.672622680664062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1748435497283936 seconds
policy loss:-853.6699829101562
value loss:16.162397384643555
entropies:27.854459762573242
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1820054054260254 seconds
policy loss:-430.3963317871094
value loss:6.227153778076172
entropies:23.10509490966797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2398464679718018 seconds
policy loss:-379.5649108886719
value loss:10.027374267578125
entropies:33.021141052246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.182196855545044 seconds
policy loss:-267.9917297363281
value loss:19.888755798339844
entropies:48.78373718261719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1105.9596)
ToM Target loss= tensor(2455.9131)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2310960292816162 seconds
policy loss:-362.0008544921875
value loss:19.78388214111328
entropies:52.17312240600586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2116222381591797 seconds
policy loss:-551.3721313476562
value loss:17.178037643432617
entropies:35.30853271484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2348535060882568 seconds
policy loss:-310.18963623046875
value loss:20.284425735473633
entropies:49.98017120361328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.188237190246582 seconds
policy loss:82.14924621582031
value loss:10.110919952392578
entropies:24.04245948791504
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2036678791046143 seconds
policy loss:-307.5122375488281
value loss:23.7216796875
entropies:43.84469223022461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1239.8092)
ToM Target loss= tensor(2410.2756)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.204350471496582 seconds
policy loss:-643.9461059570312
value loss:23.658985137939453
entropies:38.98353576660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1779613494873047 seconds
policy loss:-3.7247257232666016
value loss:6.787208080291748
entropies:33.10496520996094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1744351387023926 seconds
policy loss:97.85466766357422
value loss:5.033825397491455
entropies:20.634822845458984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2353432178497314 seconds
policy loss:-1085.995361328125
value loss:23.3410587310791
entropies:43.85920333862305
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2271108627319336 seconds
policy loss:-102.47569274902344
value loss:5.382260322570801
entropies:21.69715118408203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1044.3505)
ToM Target loss= tensor(2375.6526)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2241404056549072 seconds
policy loss:-649.205810546875
value loss:33.358116149902344
entropies:32.68156433105469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2435855865478516 seconds
policy loss:-1360.98486328125
value loss:30.91090965270996
entropies:35.189453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1830625534057617 seconds
policy loss:-1183.4697265625
value loss:24.247285842895508
entropies:37.161231994628906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1956350803375244 seconds
policy loss:-508.8779602050781
value loss:28.40593910217285
entropies:51.994728088378906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2425692081451416 seconds
policy loss:-184.9101104736328
value loss:15.43940258026123
entropies:40.06875228881836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1270.9277)
ToM Target loss= tensor(2400.2786)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1803834438323975 seconds
policy loss:-214.81307983398438
value loss:27.513809204101562
entropies:37.45790481567383
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2474069595336914 seconds
policy loss:-660.7560424804688
value loss:21.886268615722656
entropies:67.95105743408203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2324881553649902 seconds
policy loss:-341.1060791015625
value loss:28.86482810974121
entropies:34.1087760925293
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1818594932556152 seconds
policy loss:295.5085754394531
value loss:21.913406372070312
entropies:40.98314666748047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2383627891540527 seconds
policy loss:52.97275161743164
value loss:7.152288436889648
entropies:35.2105598449707
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1304.3569)
ToM Target loss= tensor(2434.9805)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1723418235778809 seconds
policy loss:-1027.4271240234375
value loss:20.639753341674805
entropies:47.02045440673828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.231724739074707 seconds
policy loss:-1352.2366943359375
value loss:16.235849380493164
entropies:44.40320587158203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1786506175994873 seconds
policy loss:-685.3125
value loss:29.384891510009766
entropies:29.739953994750977
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2224335670471191 seconds
policy loss:-1614.4508056640625
value loss:55.3177375793457
entropies:48.09962844848633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1883692741394043 seconds
policy loss:-587.967041015625
value loss:24.6077880859375
entropies:29.719837188720703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1176.0710)
ToM Target loss= tensor(2339.6162)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1838152408599854 seconds
policy loss:-1944.895263671875
value loss:26.50141143798828
entropies:43.48777770996094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.180168628692627 seconds
policy loss:-464.49505615234375
value loss:10.532278060913086
entropies:29.640628814697266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2524874210357666 seconds
policy loss:-504.2689208984375
value loss:7.496415138244629
entropies:21.980363845825195
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2296807765960693 seconds
policy loss:206.940185546875
value loss:16.449771881103516
entropies:33.94232940673828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1761753559112549 seconds
policy loss:-310.0551452636719
value loss:12.454730987548828
entropies:35.881065368652344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1177.6456)
ToM Target loss= tensor(2415.4800)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2339913845062256 seconds
policy loss:-396.9747009277344
value loss:34.82572937011719
entropies:42.338401794433594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1756157875061035 seconds
policy loss:-1035.5162353515625
value loss:17.860048294067383
entropies:41.875858306884766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.239546537399292 seconds
policy loss:-1516.972900390625
value loss:28.058815002441406
entropies:47.36661911010742
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.240705966949463 seconds
policy loss:-17.387693405151367
value loss:5.499573707580566
entropies:22.29766845703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2422633171081543 seconds
policy loss:-1486.0301513671875
value loss:112.69046020507812
entropies:41.829376220703125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1237.0726)
ToM Target loss= tensor(2398.2446)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2158665657043457 seconds
policy loss:310.9285583496094
value loss:9.389127731323242
entropies:40.97726821899414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1780657768249512 seconds
policy loss:-1261.6942138671875
value loss:20.968107223510742
entropies:50.621665954589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2389659881591797 seconds
policy loss:-1565.2362060546875
value loss:44.393165588378906
entropies:43.02099609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2425761222839355 seconds
policy loss:-716.577392578125
value loss:17.76004409790039
entropies:40.997039794921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1699960231781006 seconds
policy loss:-160.9483184814453
value loss:16.664270401000977
entropies:39.522342681884766
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1282.7407)
ToM Target loss= tensor(2232.2307)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2407159805297852 seconds
policy loss:-1265.3118896484375
value loss:45.44748306274414
entropies:52.79697799682617
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.180102825164795 seconds
policy loss:-425.15228271484375
value loss:18.4796142578125
entropies:43.72770309448242
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2291491031646729 seconds
policy loss:107.74674987792969
value loss:14.37109375
entropies:45.98516845703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2287116050720215 seconds
policy loss:-2105.632080078125
value loss:79.63829803466797
entropies:54.399803161621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2334771156311035 seconds
policy loss:-551.3705444335938
value loss:23.675682067871094
entropies:55.87837219238281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1292.7030)
ToM Target loss= tensor(2199.6624)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2714893817901611 seconds
policy loss:-675.9417114257812
value loss:16.575084686279297
entropies:50.87548828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2466816902160645 seconds
policy loss:-1457.72216796875
value loss:24.116117477416992
entropies:61.30156707763672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.208918809890747 seconds
policy loss:-1529.851806640625
value loss:35.727508544921875
entropies:48.32646560668945
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2231369018554688 seconds
policy loss:-444.2521667480469
value loss:18.014625549316406
entropies:32.996395111083984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1959056854248047 seconds
policy loss:-563.0823364257812
value loss:15.9495210647583
entropies:47.06373596191406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1276.1146)
ToM Target loss= tensor(2213.2720)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.3190174102783203 seconds
policy loss:-967.306640625
value loss:22.771570205688477
entropies:49.156593322753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2431707382202148 seconds
policy loss:252.25711059570312
value loss:33.310699462890625
entropies:58.45166015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2213084697723389 seconds
policy loss:849.1021118164062
value loss:31.786027908325195
entropies:49.03218078613281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2085661888122559 seconds
policy loss:-67.73777770996094
value loss:22.540403366088867
entropies:47.6955451965332
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2088000774383545 seconds
policy loss:-1158.2908935546875
value loss:25.241046905517578
entropies:49.502967834472656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1323.5645)
ToM Target loss= tensor(2229.2200)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1769492626190186 seconds
policy loss:-388.6770324707031
value loss:16.230606079101562
entropies:55.58960723876953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2363288402557373 seconds
policy loss:137.77584838867188
value loss:10.391359329223633
entropies:25.22412109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1680593490600586 seconds
policy loss:-1369.1424560546875
value loss:25.83424949645996
entropies:61.42352294921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2125380039215088 seconds
policy loss:-258.2432861328125
value loss:11.671335220336914
entropies:46.769752502441406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2334182262420654 seconds
policy loss:-493.0334167480469
value loss:11.44355297088623
entropies:32.0079345703125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1316.5383)
ToM Target loss= tensor(2281.2124)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2138950824737549 seconds
policy loss:-221.0552520751953
value loss:9.776966094970703
entropies:36.939083099365234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2422223091125488 seconds
policy loss:-563.7832641601562
value loss:23.391157150268555
entropies:41.828433990478516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1284549236297607 seconds
policy loss:-77.45758819580078
value loss:9.583257675170898
entropies:29.995922088623047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.218235731124878 seconds
policy loss:-1698.026611328125
value loss:25.153377532958984
entropies:53.429908752441406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2329344749450684 seconds
policy loss:-475.4905090332031
value loss:11.111891746520996
entropies:31.43242073059082
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1208.4999)
ToM Target loss= tensor(2262.8174)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.226963758468628 seconds
policy loss:-187.83998107910156
value loss:11.828259468078613
entropies:40.446083068847656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2418174743652344 seconds
policy loss:-340.53900146484375
value loss:25.498756408691406
entropies:46.408573150634766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1795308589935303 seconds
policy loss:-476.4852600097656
value loss:12.088970184326172
entropies:30.751998901367188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2475934028625488 seconds
policy loss:-237.61846923828125
value loss:7.505516052246094
entropies:42.08871841430664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2415168285369873 seconds
policy loss:-1455.9267578125
value loss:38.296180725097656
entropies:47.59654998779297
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1251.2012)
ToM Target loss= tensor(2237.0776)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1701862812042236 seconds
policy loss:-450.2897644042969
value loss:15.921768188476562
entropies:36.774383544921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2314105033874512 seconds
policy loss:-329.2032165527344
value loss:7.8677473068237305
entropies:28.629905700683594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2218046188354492 seconds
policy loss:-582.8016357421875
value loss:13.28495979309082
entropies:28.673124313354492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.211423635482788 seconds
policy loss:-189.13429260253906
value loss:15.999114036560059
entropies:26.593692779541016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176910400390625 seconds
policy loss:-185.2012481689453
value loss:20.3348331451416
entropies:35.5500373840332
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1169.5170)
ToM Target loss= tensor(2323.6851)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2009844779968262 seconds
policy loss:-563.2726440429688
value loss:16.420061111450195
entropies:53.737247467041016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1806893348693848 seconds
policy loss:-178.65899658203125
value loss:16.834320068359375
entropies:39.68398666381836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2424826622009277 seconds
policy loss:95.49119567871094
value loss:8.174885749816895
entropies:24.030845642089844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2089290618896484 seconds
policy loss:17.866653442382812
value loss:15.132161140441895
entropies:22.039249420166016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2246427536010742 seconds
policy loss:-147.51718139648438
value loss:13.77184009552002
entropies:30.01673698425293
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1048.1127)
ToM Target loss= tensor(2192.0391)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2116491794586182 seconds
policy loss:-637.5052490234375
value loss:33.168052673339844
entropies:28.430641174316406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1738669872283936 seconds
policy loss:277.7624206542969
value loss:6.624921798706055
entropies:20.660114288330078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2306244373321533 seconds
policy loss:-478.1184387207031
value loss:11.609689712524414
entropies:35.81206512451172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1837005615234375 seconds
policy loss:83.12708282470703
value loss:5.322760581970215
entropies:24.23019027709961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1758432388305664 seconds
policy loss:-853.9750366210938
value loss:17.873056411743164
entropies:38.28994369506836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1078.9630)
ToM Target loss= tensor(2289.0881)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2396619319915771 seconds
policy loss:-1692.237060546875
value loss:56.435340881347656
entropies:50.514198303222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1927027702331543 seconds
policy loss:-377.72662353515625
value loss:15.634979248046875
entropies:26.98967933654785
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2344698905944824 seconds
policy loss:-168.14271545410156
value loss:12.510735511779785
entropies:36.949588775634766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1696147918701172 seconds
policy loss:-185.5604705810547
value loss:20.933460235595703
entropies:37.82106399536133
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1877672672271729 seconds
policy loss:-1448.851806640625
value loss:27.44498634338379
entropies:42.64780807495117
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1125.0771)
ToM Target loss= tensor(2165.5615)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1916940212249756 seconds
policy loss:-192.4137420654297
value loss:27.425195693969727
entropies:41.65924835205078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.204707145690918 seconds
policy loss:78.44560241699219
value loss:22.966983795166016
entropies:26.887670516967773
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1714518070220947 seconds
policy loss:-403.0048522949219
value loss:28.459218978881836
entropies:36.270198822021484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1714377403259277 seconds
policy loss:-144.08529663085938
value loss:25.318605422973633
entropies:25.384763717651367
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2277512550354004 seconds
policy loss:-660.7727661132812
value loss:13.395069122314453
entropies:45.648712158203125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1253.1262)
ToM Target loss= tensor(2270.6260)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2306334972381592 seconds
policy loss:-1424.2987060546875
value loss:82.2125244140625
entropies:61.92363739013672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1781284809112549 seconds
policy loss:-279.7369384765625
value loss:10.641011238098145
entropies:38.58708953857422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1807470321655273 seconds
policy loss:-131.88816833496094
value loss:28.008319854736328
entropies:46.856719970703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2120435237884521 seconds
policy loss:-142.88153076171875
value loss:6.138237953186035
entropies:36.92591094970703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1901979446411133 seconds
policy loss:-580.4114990234375
value loss:29.461828231811523
entropies:41.7142333984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1216.2136)
ToM Target loss= tensor(2342.9675)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.207829475402832 seconds
policy loss:-641.265869140625
value loss:35.00925827026367
entropies:33.35858154296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.202042579650879 seconds
policy loss:-335.60791015625
value loss:15.47275161743164
entropies:36.05004119873047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2258813381195068 seconds
policy loss:-945.111083984375
value loss:25.787410736083984
entropies:54.13201141357422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2334284782409668 seconds
policy loss:644.0404663085938
value loss:19.112083435058594
entropies:50.157230377197266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1745753288269043 seconds
policy loss:-22.913291931152344
value loss:22.885848999023438
entropies:59.42056655883789
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1271.8239)
ToM Target loss= tensor(2300.4023)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2451095581054688 seconds
policy loss:-657.4359130859375
value loss:25.216167449951172
entropies:40.482460021972656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2395927906036377 seconds
policy loss:28.48126983642578
value loss:19.919544219970703
entropies:26.193614959716797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2299411296844482 seconds
policy loss:-231.3306121826172
value loss:13.420217514038086
entropies:49.14678955078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2258317470550537 seconds
policy loss:-153.15243530273438
value loss:15.195904731750488
entropies:43.125823974609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2266056537628174 seconds
policy loss:-1146.2789306640625
value loss:19.143354415893555
entropies:57.97026062011719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1266.5610)
ToM Target loss= tensor(2342.1970)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1797850131988525 seconds
policy loss:-937.8687133789062
value loss:20.223148345947266
entropies:45.44965744018555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1727063655853271 seconds
policy loss:-2857.539794921875
value loss:42.04966735839844
entropies:63.7347412109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.17557954788208 seconds
policy loss:-1849.3636474609375
value loss:34.6003532409668
entropies:45.02815628051758
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178525686264038 seconds
policy loss:163.7957305908203
value loss:6.3907036781311035
entropies:28.62488555908203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1732983589172363 seconds
policy loss:-620.6088256835938
value loss:8.476211547851562
entropies:39.38833999633789
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1254.3206)
ToM Target loss= tensor(2275.0947)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1763522624969482 seconds
policy loss:-1004.9698486328125
value loss:18.257246017456055
entropies:42.71522521972656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.238872766494751 seconds
policy loss:-196.76817321777344
value loss:19.52515983581543
entropies:45.9349365234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178454875946045 seconds
policy loss:-230.40390014648438
value loss:12.17823314666748
entropies:27.987018585205078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1770656108856201 seconds
policy loss:-155.4059600830078
value loss:10.017315864562988
entropies:37.19633865356445
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1797161102294922 seconds
policy loss:-27.44252586364746
value loss:13.40011215209961
entropies:42.39006423950195
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1215.4158)
ToM Target loss= tensor(2261.0776)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.189366102218628 seconds
policy loss:-1023.9791870117188
value loss:22.802249908447266
entropies:42.57516860961914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178304672241211 seconds
policy loss:-1264.6756591796875
value loss:24.126781463623047
entropies:43.298831939697266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2074933052062988 seconds
policy loss:-379.86383056640625
value loss:10.034123420715332
entropies:32.8720703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2199797630310059 seconds
policy loss:68.44351196289062
value loss:11.497451782226562
entropies:24.278284072875977
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2296075820922852 seconds
policy loss:-858.930419921875
value loss:15.95645523071289
entropies:53.71422576904297
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1199.9451)
ToM Target loss= tensor(2185.2793)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1823058128356934 seconds
policy loss:-31.51917266845703
value loss:14.15401554107666
entropies:48.76873016357422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1781680583953857 seconds
policy loss:-569.6124877929688
value loss:14.77155590057373
entropies:35.61375427246094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1754529476165771 seconds
policy loss:-779.80029296875
value loss:16.343862533569336
entropies:37.445655822753906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.173698902130127 seconds
policy loss:-85.48682403564453
value loss:13.716598510742188
entropies:26.912851333618164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2353370189666748 seconds
policy loss:215.39971923828125
value loss:10.293900489807129
entropies:29.103851318359375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1254.1176)
ToM Target loss= tensor(2304.8635)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2392969131469727 seconds
policy loss:-1289.989990234375
value loss:11.996223449707031
entropies:46.5787239074707
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1751115322113037 seconds
policy loss:-888.7200927734375
value loss:24.346284866333008
entropies:41.080848693847656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2301509380340576 seconds
policy loss:-962.2864990234375
value loss:17.9420166015625
entropies:28.831296920776367
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1814241409301758 seconds
policy loss:-188.40538024902344
value loss:9.383747100830078
entropies:27.68328857421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2057583332061768 seconds
policy loss:-540.0591430664062
value loss:13.121328353881836
entropies:44.4058837890625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1232.6538)
ToM Target loss= tensor(2265.7124)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2309365272521973 seconds
policy loss:-379.8545227050781
value loss:13.343815803527832
entropies:34.78093338012695
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.225752592086792 seconds
policy loss:-560.2975463867188
value loss:18.495031356811523
entropies:36.861915588378906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.240657091140747 seconds
policy loss:-612.9746704101562
value loss:29.876859664916992
entropies:50.62409210205078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2314138412475586 seconds
policy loss:-707.2391357421875
value loss:20.852157592773438
entropies:56.6081428527832
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2372775077819824 seconds
policy loss:-279.01129150390625
value loss:12.458345413208008
entropies:51.13097381591797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1245.2573)
ToM Target loss= tensor(2181.5747)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.177635908126831 seconds
policy loss:-85.57766723632812
value loss:14.160152435302734
entropies:27.43408966064453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1786556243896484 seconds
policy loss:177.56170654296875
value loss:11.02259349822998
entropies:27.7325382232666
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2422423362731934 seconds
policy loss:25.183650970458984
value loss:13.763790130615234
entropies:37.46794891357422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2142243385314941 seconds
policy loss:-464.4969482421875
value loss:19.80649185180664
entropies:54.07139205932617
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178720474243164 seconds
policy loss:-2274.428955078125
value loss:39.868629455566406
entropies:57.57865905761719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1178.8983)
ToM Target loss= tensor(2225.7988)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2182588577270508 seconds
policy loss:-129.18368530273438
value loss:6.586800575256348
entropies:25.15195083618164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1790568828582764 seconds
policy loss:-1095.107421875
value loss:24.264528274536133
entropies:33.278289794921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2504711151123047 seconds
policy loss:-1970.0
value loss:29.067968368530273
entropies:37.85514831542969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1782665252685547 seconds
policy loss:-214.2688446044922
value loss:5.3492631912231445
entropies:25.570518493652344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2188198566436768 seconds
policy loss:-8.88827896118164
value loss:10.635993957519531
entropies:36.32310485839844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1079.8234)
ToM Target loss= tensor(2236.4453)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2492663860321045 seconds
policy loss:-432.86248779296875
value loss:20.090513229370117
entropies:44.52349853515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2410955429077148 seconds
policy loss:-1607.6624755859375
value loss:18.145366668701172
entropies:41.85329818725586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2337446212768555 seconds
policy loss:-1120.467529296875
value loss:24.774961471557617
entropies:42.04896545410156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1800482273101807 seconds
policy loss:-570.25244140625
value loss:31.88619613647461
entropies:28.690853118896484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1767878532409668 seconds
policy loss:-519.4462890625
value loss:32.84049606323242
entropies:50.60456085205078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1233.5400)
ToM Target loss= tensor(2207.0908)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1890208721160889 seconds
policy loss:460.51239013671875
value loss:16.51926040649414
entropies:39.70050811767578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201401948928833 seconds
policy loss:681.9130859375
value loss:27.917766571044922
entropies:30.730220794677734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.207749366760254 seconds
policy loss:594.8281860351562
value loss:23.658432006835938
entropies:47.04362487792969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2416408061981201 seconds
policy loss:123.6216049194336
value loss:27.32927703857422
entropies:49.134765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2350265979766846 seconds
policy loss:64.3038101196289
value loss:18.034345626831055
entropies:20.937480926513672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1182.8776)
ToM Target loss= tensor(2247.1440)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2360475063323975 seconds
policy loss:-1677.571533203125
value loss:26.812929153442383
entropies:34.37702560424805
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1761162281036377 seconds
policy loss:-1201.6435546875
value loss:31.726533889770508
entropies:59.25885009765625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.134089469909668 seconds
policy loss:-541.2100830078125
value loss:27.977115631103516
entropies:48.44767379760742
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1746585369110107 seconds
policy loss:-448.8469543457031
value loss:50.535945892333984
entropies:40.13829803466797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2329154014587402 seconds
policy loss:309.0123596191406
value loss:10.127664566040039
entropies:28.31053924560547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1296.7837)
ToM Target loss= tensor(2311.1350)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2380826473236084 seconds
policy loss:190.17181396484375
value loss:7.4205546379089355
entropies:54.38466262817383
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2299718856811523 seconds
policy loss:-199.70848083496094
value loss:20.862873077392578
entropies:50.307945251464844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227649450302124 seconds
policy loss:-775.1227416992188
value loss:61.208736419677734
entropies:56.16865158081055
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2424712181091309 seconds
policy loss:-121.21539306640625
value loss:26.361597061157227
entropies:35.35175323486328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2200016975402832 seconds
policy loss:441.5338134765625
value loss:14.874019622802734
entropies:39.52145004272461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1332.3848)
ToM Target loss= tensor(2160.2026)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1731553077697754 seconds
policy loss:-673.5651245117188
value loss:23.46116065979004
entropies:51.932979583740234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2075014114379883 seconds
policy loss:-734.6694946289062
value loss:34.899169921875
entropies:59.40660858154297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2384305000305176 seconds
policy loss:-856.2737426757812
value loss:33.322044372558594
entropies:52.04194259643555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1997606754302979 seconds
policy loss:256.35308837890625
value loss:12.029356956481934
entropies:34.07453536987305
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1974446773529053 seconds
policy loss:-1404.021240234375
value loss:26.629798889160156
entropies:72.52891540527344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1342.7186)
ToM Target loss= tensor(2168.0742)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1855051517486572 seconds
policy loss:-252.31419372558594
value loss:23.367603302001953
entropies:54.71842575073242
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2289025783538818 seconds
policy loss:-142.5535888671875
value loss:16.77644157409668
entropies:43.64041519165039
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2363104820251465 seconds
policy loss:-278.05706787109375
value loss:5.814696311950684
entropies:28.508228302001953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1805520057678223 seconds
policy loss:-956.5443725585938
value loss:22.258342742919922
entropies:45.141780853271484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2539246082305908 seconds
policy loss:14.028708457946777
value loss:5.84300422668457
entropies:43.1489372253418
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1165.9238)
ToM Target loss= tensor(2252.9727)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2314469814300537 seconds
policy loss:-515.8168334960938
value loss:10.720396995544434
entropies:36.004180908203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.181485652923584 seconds
policy loss:-824.7470092773438
value loss:8.49435806274414
entropies:30.794300079345703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2302308082580566 seconds
policy loss:333.0423889160156
value loss:15.697436332702637
entropies:35.57898712158203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2426187992095947 seconds
policy loss:-855.0907592773438
value loss:30.18675994873047
entropies:48.14408874511719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.254664659500122 seconds
policy loss:-1321.0750732421875
value loss:34.52524948120117
entropies:49.2974853515625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1281.1489)
ToM Target loss= tensor(2313.3191)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1782755851745605 seconds
policy loss:110.15387725830078
value loss:11.64281177520752
entropies:48.343284606933594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2532918453216553 seconds
policy loss:-349.3982238769531
value loss:19.00098991394043
entropies:38.96938705444336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1795432567596436 seconds
policy loss:-296.6673583984375
value loss:23.366439819335938
entropies:36.634403228759766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.208655834197998 seconds
policy loss:-233.74789428710938
value loss:12.227165222167969
entropies:53.172176361083984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1813364028930664 seconds
policy loss:-45.421974182128906
value loss:7.236093997955322
entropies:28.767925262451172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1144.1669)
ToM Target loss= tensor(2146.0298)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1710450649261475 seconds
policy loss:-808.3064575195312
value loss:14.363409042358398
entropies:27.990737915039062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1762948036193848 seconds
policy loss:-1780.19287109375
value loss:24.14541244506836
entropies:61.50423812866211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.200772762298584 seconds
policy loss:-77.77255249023438
value loss:6.259207248687744
entropies:32.86834716796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2437858581542969 seconds
policy loss:-300.54388427734375
value loss:5.2250189781188965
entropies:42.32393264770508
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.241457223892212 seconds
policy loss:-710.8042602539062
value loss:14.333879470825195
entropies:42.63337326049805
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1112.8624)
ToM Target loss= tensor(2094.1162)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2473721504211426 seconds
policy loss:-1343.33056640625
value loss:19.159221649169922
entropies:47.83769226074219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2189829349517822 seconds
policy loss:-1342.207763671875
value loss:20.74352264404297
entropies:55.549652099609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1737868785858154 seconds
policy loss:-634.337646484375
value loss:12.547335624694824
entropies:39.490196228027344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.188572883605957 seconds
policy loss:-1412.730712890625
value loss:53.28347396850586
entropies:47.38666534423828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2197561264038086 seconds
policy loss:-428.2490234375
value loss:38.670108795166016
entropies:30.443225860595703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1252.1868)
ToM Target loss= tensor(2272.1665)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2413058280944824 seconds
policy loss:542.2310180664062
value loss:17.89756202697754
entropies:33.890602111816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1399264335632324 seconds
policy loss:-588.6917724609375
value loss:35.156219482421875
entropies:56.16425323486328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1759226322174072 seconds
policy loss:-160.01255798339844
value loss:10.539895057678223
entropies:30.92740249633789
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.238161563873291 seconds
policy loss:-270.6684875488281
value loss:11.973278999328613
entropies:42.296730041503906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1731367111206055 seconds
policy loss:-240.4296112060547
value loss:10.292887687683105
entropies:41.05382537841797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1233.5182)
ToM Target loss= tensor(2262.9822)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2348079681396484 seconds
policy loss:-380.06304931640625
value loss:11.375667572021484
entropies:53.230262756347656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2358672618865967 seconds
policy loss:-874.8731079101562
value loss:14.426982879638672
entropies:44.01164627075195
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2315199375152588 seconds
policy loss:-875.1429443359375
value loss:13.052447319030762
entropies:47.618099212646484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178551197052002 seconds
policy loss:-1621.42724609375
value loss:31.99640655517578
entropies:50.13399887084961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2366089820861816 seconds
policy loss:268.2314453125
value loss:26.459989547729492
entropies:33.681602478027344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1264.0261)
ToM Target loss= tensor(2207.2727)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1775352954864502 seconds
policy loss:247.7257843017578
value loss:19.963836669921875
entropies:42.13441467285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2492234706878662 seconds
policy loss:-406.9476318359375
value loss:10.916147232055664
entropies:34.001014709472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1814422607421875 seconds
policy loss:-231.80947875976562
value loss:20.620271682739258
entropies:51.25476837158203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1815059185028076 seconds
policy loss:19.77703857421875
value loss:11.324753761291504
entropies:34.36362075805664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2082345485687256 seconds
policy loss:-474.1488037109375
value loss:6.002213954925537
entropies:32.937583923339844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1156.0598)
ToM Target loss= tensor(2293.2107)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2125186920166016 seconds
policy loss:-2512.03466796875
value loss:38.67584228515625
entropies:48.09330749511719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2285938262939453 seconds
policy loss:-948.6171875
value loss:18.137989044189453
entropies:42.23599624633789
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.184117317199707 seconds
policy loss:-665.3665771484375
value loss:28.358688354492188
entropies:43.64402770996094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1765923500061035 seconds
policy loss:-1348.0242919921875
value loss:22.177743911743164
entropies:40.26884078979492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2467286586761475 seconds
policy loss:-373.50921630859375
value loss:15.734375
entropies:37.81947326660156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1252.7910)
ToM Target loss= tensor(2197.5906)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.237825870513916 seconds
policy loss:239.26344299316406
value loss:10.11882209777832
entropies:24.781782150268555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1776134967803955 seconds
policy loss:104.2618408203125
value loss:16.285261154174805
entropies:38.127281188964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2011308670043945 seconds
policy loss:-487.3746337890625
value loss:23.520904541015625
entropies:56.35218048095703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2393300533294678 seconds
policy loss:-138.71665954589844
value loss:8.849273681640625
entropies:45.839271545410156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1763789653778076 seconds
policy loss:-762.5130615234375
value loss:32.76469802856445
entropies:53.47578430175781
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1143.2653)
ToM Target loss= tensor(2151.2202)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2352137565612793 seconds
policy loss:-611.1336669921875
value loss:9.497949600219727
entropies:38.18030548095703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2209434509277344 seconds
policy loss:-992.2533569335938
value loss:15.187742233276367
entropies:46.197669982910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2402334213256836 seconds
policy loss:-760.7044067382812
value loss:22.400867462158203
entropies:50.99271774291992
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2128520011901855 seconds
policy loss:69.8785629272461
value loss:5.856185436248779
entropies:30.878719329833984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.267547845840454 seconds
policy loss:-832.3922729492188
value loss:28.94459342956543
entropies:44.82749938964844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1178.7418)
ToM Target loss= tensor(2193.4543)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2326135635375977 seconds
policy loss:-104.0435791015625
value loss:16.271577835083008
entropies:29.997896194458008
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1747424602508545 seconds
policy loss:-2422.534423828125
value loss:61.73838806152344
entropies:55.6492919921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2186758518218994 seconds
policy loss:-799.853515625
value loss:29.33890151977539
entropies:42.55723190307617
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2377958297729492 seconds
policy loss:-693.2625732421875
value loss:15.550296783447266
entropies:37.19697952270508
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2407410144805908 seconds
policy loss:-410.7972412109375
value loss:23.06902503967285
entropies:38.247764587402344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1148.8275)
ToM Target loss= tensor(2096.0928)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1761023998260498 seconds
policy loss:169.96177673339844
value loss:8.74372673034668
entropies:23.449024200439453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2386119365692139 seconds
policy loss:-148.1984100341797
value loss:15.522724151611328
entropies:37.625953674316406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1778039932250977 seconds
policy loss:-16.63075828552246
value loss:24.60248565673828
entropies:41.538429260253906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1720082759857178 seconds
policy loss:53.87207794189453
value loss:7.624893665313721
entropies:28.64693832397461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.247284173965454 seconds
policy loss:-38.16997528076172
value loss:16.755075454711914
entropies:49.15898132324219
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1172.1843)
ToM Target loss= tensor(2290.5430)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2488963603973389 seconds
policy loss:-120.44395446777344
value loss:15.813613891601562
entropies:39.3180046081543
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2019283771514893 seconds
policy loss:-501.89752197265625
value loss:17.182371139526367
entropies:41.96635055541992
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2407588958740234 seconds
policy loss:-545.790283203125
value loss:16.78777313232422
entropies:32.93549728393555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1803936958312988 seconds
policy loss:-490.95489501953125
value loss:30.715618133544922
entropies:65.87932586669922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2423841953277588 seconds
policy loss:455.4810485839844
value loss:14.213354110717773
entropies:34.9932975769043
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1249.6506)
ToM Target loss= tensor(2228.1187)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.252485752105713 seconds
policy loss:452.3862609863281
value loss:9.501282691955566
entropies:40.20085144042969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2525994777679443 seconds
policy loss:179.23097229003906
value loss:18.842538833618164
entropies:46.68110656738281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2068605422973633 seconds
policy loss:-1348.119873046875
value loss:22.65336799621582
entropies:62.311805725097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1757209300994873 seconds
policy loss:-782.3150634765625
value loss:15.879983901977539
entropies:53.69471740722656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2383568286895752 seconds
policy loss:47.86308288574219
value loss:9.414262771606445
entropies:31.031322479248047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1204.6763)
ToM Target loss= tensor(2123.8911)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.241288185119629 seconds
policy loss:-2101.52099609375
value loss:42.40555191040039
entropies:44.50106430053711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2266945838928223 seconds
policy loss:-1281.7042236328125
value loss:23.701757431030273
entropies:40.7529182434082
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1727674007415771 seconds
policy loss:-1071.4019775390625
value loss:19.29543113708496
entropies:30.25839614868164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1896274089813232 seconds
policy loss:-1160.7109375
value loss:27.61904525756836
entropies:56.25432586669922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1838388442993164 seconds
policy loss:-657.9778442382812
value loss:25.547739028930664
entropies:45.09497833251953
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1221.4034)
ToM Target loss= tensor(2197.4128)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1776461601257324 seconds
policy loss:-2521.025390625
value loss:37.84434127807617
entropies:48.9456672668457
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2426254749298096 seconds
policy loss:-503.4585876464844
value loss:14.156188011169434
entropies:50.15334701538086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1741576194763184 seconds
policy loss:-1068.124755859375
value loss:38.309879302978516
entropies:56.01019287109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2368857860565186 seconds
policy loss:210.3568878173828
value loss:16.331195831298828
entropies:56.249752044677734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1704528331756592 seconds
policy loss:-45.201759338378906
value loss:30.03690528869629
entropies:52.951072692871094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1360.3738)
ToM Target loss= tensor(2174.7007)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1639580726623535 seconds
policy loss:116.71035766601562
value loss:16.687875747680664
entropies:58.12055206298828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2395315170288086 seconds
policy loss:-672.0079345703125
value loss:19.641191482543945
entropies:36.091339111328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2359905242919922 seconds
policy loss:-111.99022674560547
value loss:15.601700782775879
entropies:44.07707977294922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2437140941619873 seconds
policy loss:-748.863037109375
value loss:13.700322151184082
entropies:32.4276008605957
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1765108108520508 seconds
policy loss:-644.8441162109375
value loss:18.365324020385742
entropies:46.03544616699219
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1357.8586)
ToM Target loss= tensor(2193.2339)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1980111598968506 seconds
policy loss:-697.8994140625
value loss:13.86732292175293
entropies:35.20085525512695
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2087199687957764 seconds
policy loss:-925.3981323242188
value loss:15.35461711883545
entropies:33.40153884887695
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.170318841934204 seconds
policy loss:-97.2541275024414
value loss:15.03591537475586
entropies:40.8203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1902356147766113 seconds
policy loss:-1166.9918212890625
value loss:21.29311180114746
entropies:37.20804214477539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2399160861968994 seconds
policy loss:-238.71014404296875
value loss:18.00310516357422
entropies:39.701995849609375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1203.5223)
ToM Target loss= tensor(2289.2202)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.245041847229004 seconds
policy loss:-606.9187622070312
value loss:30.360116958618164
entropies:51.30470275878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1737046241760254 seconds
policy loss:233.3895721435547
value loss:17.242950439453125
entropies:25.409130096435547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2013130187988281 seconds
policy loss:373.511962890625
value loss:18.913311004638672
entropies:31.457080841064453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1761560440063477 seconds
policy loss:-386.3056945800781
value loss:27.616302490234375
entropies:42.1854133605957
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2558746337890625 seconds
policy loss:-150.747314453125
value loss:12.578277587890625
entropies:30.444110870361328
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1206.7927)
ToM Target loss= tensor(2264.1628)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2057316303253174 seconds
policy loss:-3.2074241638183594
value loss:14.632102966308594
entropies:28.406295776367188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1776123046875 seconds
policy loss:-394.20513916015625
value loss:19.896081924438477
entropies:42.0896110534668
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1681625843048096 seconds
policy loss:-848.521240234375
value loss:11.4736328125
entropies:34.779998779296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.239243984222412 seconds
policy loss:-715.1461791992188
value loss:16.735431671142578
entropies:34.83574676513672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2279152870178223 seconds
policy loss:-1606.4954833984375
value loss:38.857704162597656
entropies:53.66009521484375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1210.8025)
ToM Target loss= tensor(2377.1323)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2143135070800781 seconds
policy loss:-171.52145385742188
value loss:13.95704174041748
entropies:32.76885986328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2198140621185303 seconds
policy loss:-76.54463958740234
value loss:19.694143295288086
entropies:28.19284439086914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2370960712432861 seconds
policy loss:484.84759521484375
value loss:18.79616928100586
entropies:42.75922775268555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1754772663116455 seconds
policy loss:-278.69427490234375
value loss:14.37300968170166
entropies:37.08463668823242
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2301714420318604 seconds
policy loss:-334.2502746582031
value loss:10.868820190429688
entropies:27.246105194091797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1153.3799)
ToM Target loss= tensor(2359.4976)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2299675941467285 seconds
policy loss:-594.4454345703125
value loss:13.148910522460938
entropies:30.796968460083008
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169497013092041 seconds
policy loss:-797.27783203125
value loss:12.62213134765625
entropies:45.75956726074219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.237861156463623 seconds
policy loss:-1989.4439697265625
value loss:30.907079696655273
entropies:37.443687438964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2350702285766602 seconds
policy loss:-1081.0712890625
value loss:16.458738327026367
entropies:51.12267303466797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.232269525527954 seconds
policy loss:-2692.385009765625
value loss:59.50225830078125
entropies:39.16391372680664
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1201.2678)
ToM Target loss= tensor(2174.4231)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1970243453979492 seconds
policy loss:-1988.43994140625
value loss:38.167686462402344
entropies:32.844207763671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.238797664642334 seconds
policy loss:205.34722900390625
value loss:12.879806518554688
entropies:32.81303405761719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2120287418365479 seconds
policy loss:485.6961975097656
value loss:31.80681800842285
entropies:45.76203918457031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2392692565917969 seconds
policy loss:61.29808807373047
value loss:16.509143829345703
entropies:35.73127746582031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1812362670898438 seconds
policy loss:-648.8377685546875
value loss:18.394250869750977
entropies:41.22052001953125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1164.4534)
ToM Target loss= tensor(2155.0989)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2161006927490234 seconds
policy loss:-515.699951171875
value loss:16.843669891357422
entropies:50.12118911743164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1967005729675293 seconds
policy loss:-416.2840881347656
value loss:9.98987865447998
entropies:45.616554260253906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2328622341156006 seconds
policy loss:-817.0466918945312
value loss:16.992788314819336
entropies:38.55528259277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2356617450714111 seconds
policy loss:-1909.9501953125
value loss:28.296724319458008
entropies:67.1346664428711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2319018840789795 seconds
policy loss:-857.059326171875
value loss:23.513700485229492
entropies:28.320091247558594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1294.0830)
ToM Target loss= tensor(2238.5671)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1811907291412354 seconds
policy loss:-1665.869140625
value loss:22.677797317504883
entropies:46.12117004394531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2413396835327148 seconds
policy loss:-690.170166015625
value loss:13.62787914276123
entropies:32.53604507446289
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1855504512786865 seconds
policy loss:-1112.751220703125
value loss:19.463632583618164
entropies:36.78556823730469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2401518821716309 seconds
policy loss:-537.0147705078125
value loss:16.551382064819336
entropies:41.30518341064453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2342545986175537 seconds
policy loss:-1130.9456787109375
value loss:34.83736801147461
entropies:48.94361114501953
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1173.4856)
ToM Target loss= tensor(2252.0889)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1804823875427246 seconds
policy loss:-288.2353515625
value loss:31.759315490722656
entropies:37.51540756225586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2260828018188477 seconds
policy loss:136.71612548828125
value loss:11.426712989807129
entropies:27.198776245117188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2278006076812744 seconds
policy loss:187.886962890625
value loss:23.728654861450195
entropies:31.417627334594727
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1729719638824463 seconds
policy loss:366.6533508300781
value loss:8.1515531539917
entropies:30.434654235839844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2441332340240479 seconds
policy loss:-512.8734130859375
value loss:19.9875545501709
entropies:52.73272705078125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1193.3444)
ToM Target loss= tensor(2247.9807)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1797206401824951 seconds
policy loss:-2077.53857421875
value loss:36.480369567871094
entropies:42.37488555908203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2369842529296875 seconds
policy loss:-1875.4520263671875
value loss:21.39875030517578
entropies:30.665340423583984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2374582290649414 seconds
policy loss:-522.7229614257812
value loss:10.550968170166016
entropies:26.836544036865234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2061736583709717 seconds
policy loss:-951.0628051757812
value loss:19.669158935546875
entropies:38.296409606933594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2065393924713135 seconds
policy loss:-1363.709228515625
value loss:31.39732551574707
entropies:48.39000701904297
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1124.6011)
ToM Target loss= tensor(2136.1838)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.243328332901001 seconds
policy loss:-292.7449035644531
value loss:26.715608596801758
entropies:28.82323455810547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2345106601715088 seconds
policy loss:98.87413024902344
value loss:14.824455261230469
entropies:27.667415618896484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2029201984405518 seconds
policy loss:628.2036743164062
value loss:21.634767532348633
entropies:33.51830291748047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1887671947479248 seconds
policy loss:-174.09661865234375
value loss:16.03074073791504
entropies:37.25624084472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1812705993652344 seconds
policy loss:56.61920928955078
value loss:15.533939361572266
entropies:39.770633697509766
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1120.3652)
ToM Target loss= tensor(2164.5959)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.213841438293457 seconds
policy loss:-523.7391967773438
value loss:13.747815132141113
entropies:39.63609313964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.214693307876587 seconds
policy loss:-593.9962158203125
value loss:32.31840896606445
entropies:40.48463821411133
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2395508289337158 seconds
policy loss:323.5
value loss:7.726725101470947
entropies:36.17901611328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2494423389434814 seconds
policy loss:-693.868408203125
value loss:15.02998161315918
entropies:35.257835388183594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2727773189544678 seconds
policy loss:-663.8873901367188
value loss:10.690079689025879
entropies:37.90761184692383
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1112.3716)
ToM Target loss= tensor(2267.3105)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1770341396331787 seconds
policy loss:-709.1043701171875
value loss:23.194427490234375
entropies:41.33047866821289
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2469453811645508 seconds
policy loss:-1125.7772216796875
value loss:32.327537536621094
entropies:58.528316497802734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.220909595489502 seconds
policy loss:142.50772094726562
value loss:12.89941120147705
entropies:35.30462646484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1692867279052734 seconds
policy loss:-461.0840759277344
value loss:30.519603729248047
entropies:21.158475875854492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2012052536010742 seconds
policy loss:-311.7727355957031
value loss:11.57339859008789
entropies:35.78623962402344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1253.5197)
ToM Target loss= tensor(2228.5132)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1730217933654785 seconds
policy loss:86.11309814453125
value loss:12.519797325134277
entropies:28.631380081176758
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2353699207305908 seconds
policy loss:-224.62094116210938
value loss:10.344322204589844
entropies:29.614429473876953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2243452072143555 seconds
policy loss:-844.2476196289062
value loss:23.555727005004883
entropies:41.88700485229492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1829214096069336 seconds
policy loss:-851.9947509765625
value loss:29.058387756347656
entropies:44.82992935180664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1859824657440186 seconds
policy loss:-944.1781616210938
value loss:11.315855979919434
entropies:27.55445098876953
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1155.8184)
ToM Target loss= tensor(2323.3171)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1810195446014404 seconds
policy loss:-1251.9029541015625
value loss:17.053939819335938
entropies:41.51881790161133
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.237630844116211 seconds
policy loss:-1102.71044921875
value loss:19.362945556640625
entropies:54.297271728515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1763741970062256 seconds
policy loss:49.37523651123047
value loss:7.317699432373047
entropies:35.78517150878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1793978214263916 seconds
policy loss:-745.7562866210938
value loss:20.55608367919922
entropies:39.78778839111328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2354152202606201 seconds
policy loss:106.7974624633789
value loss:14.620720863342285
entropies:34.4285888671875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1215.3733)
ToM Target loss= tensor(2300.2756)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2316064834594727 seconds
policy loss:-1091.98388671875
value loss:23.83611297607422
entropies:38.5594596862793
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1962692737579346 seconds
policy loss:-1128.7821044921875
value loss:17.31684684753418
entropies:40.13282012939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.219513177871704 seconds
policy loss:-450.3619384765625
value loss:13.868674278259277
entropies:58.591575622558594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1675572395324707 seconds
policy loss:-709.28466796875
value loss:19.07817268371582
entropies:44.7281608581543
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1803877353668213 seconds
policy loss:-323.2853698730469
value loss:10.063811302185059
entropies:39.65032958984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1242.0070)
ToM Target loss= tensor(2184.8560)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1774871349334717 seconds
policy loss:-724.3638305664062
value loss:20.482547760009766
entropies:41.58116149902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.256622314453125 seconds
policy loss:-1812.4212646484375
value loss:39.541133880615234
entropies:61.20612335205078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177645206451416 seconds
policy loss:-652.1243286132812
value loss:19.923236846923828
entropies:44.69963073730469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.229201316833496 seconds
policy loss:97.80403137207031
value loss:26.343814849853516
entropies:45.95580291748047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2250041961669922 seconds
policy loss:-370.6231384277344
value loss:15.461628913879395
entropies:43.24318313598633
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1265.4723)
ToM Target loss= tensor(2178.4946)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1948463916778564 seconds
policy loss:-654.791748046875
value loss:28.515310287475586
entropies:41.903907775878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2374639511108398 seconds
policy loss:-277.2410583496094
value loss:11.774746894836426
entropies:30.381956100463867
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1743249893188477 seconds
policy loss:448.5951843261719
value loss:11.500480651855469
entropies:22.181018829345703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2325890064239502 seconds
policy loss:-1376.9410400390625
value loss:23.127782821655273
entropies:53.82879638671875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.248284101486206 seconds
policy loss:-482.6008605957031
value loss:12.64474868774414
entropies:26.75635528564453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1174.1809)
ToM Target loss= tensor(2245.4941)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2129290103912354 seconds
policy loss:-672.409912109375
value loss:13.093647956848145
entropies:38.70187759399414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1889104843139648 seconds
policy loss:-1422.9776611328125
value loss:22.359073638916016
entropies:40.04490280151367
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1715362071990967 seconds
policy loss:-1353.035400390625
value loss:28.954221725463867
entropies:47.64608383178711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2307536602020264 seconds
policy loss:-63.317298889160156
value loss:9.290594100952148
entropies:30.089569091796875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2278926372528076 seconds
policy loss:93.27767944335938
value loss:5.950192928314209
entropies:20.650516510009766
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1118.8065)
ToM Target loss= tensor(2178.5229)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2396042346954346 seconds
policy loss:-678.49755859375
value loss:14.143253326416016
entropies:25.765464782714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2125029563903809 seconds
policy loss:-1431.9713134765625
value loss:18.602981567382812
entropies:28.854816436767578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1842916011810303 seconds
policy loss:96.4585952758789
value loss:18.714466094970703
entropies:27.70138168334961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1764802932739258 seconds
policy loss:-305.8764343261719
value loss:7.377012252807617
entropies:25.53624725341797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1371665000915527 seconds
policy loss:-305.806884765625
value loss:22.65381622314453
entropies:43.6051025390625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1190.6416)
ToM Target loss= tensor(2311.5503)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2383418083190918 seconds
policy loss:-497.368408203125
value loss:9.619421005249023
entropies:29.389780044555664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2109148502349854 seconds
policy loss:-1479.7672119140625
value loss:36.97755813598633
entropies:55.448280334472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2165896892547607 seconds
policy loss:-887.3711547851562
value loss:20.948963165283203
entropies:39.08839416503906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.240117073059082 seconds
policy loss:66.50316619873047
value loss:11.751344680786133
entropies:31.579069137573242
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2039306163787842 seconds
policy loss:-224.34909057617188
value loss:10.904935836791992
entropies:39.728004455566406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1191.5006)
ToM Target loss= tensor(2252.7205)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2406959533691406 seconds
policy loss:85.25384521484375
value loss:7.7819390296936035
entropies:29.070693969726562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2455523014068604 seconds
policy loss:-407.0426330566406
value loss:13.392414093017578
entropies:32.280853271484375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.187208890914917 seconds
policy loss:172.94808959960938
value loss:10.182360649108887
entropies:23.297697067260742
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2003812789916992 seconds
policy loss:-188.3027801513672
value loss:13.889870643615723
entropies:43.74055862426758
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2269980907440186 seconds
policy loss:-284.23870849609375
value loss:8.313577651977539
entropies:38.86881637573242
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1206.2152)
ToM Target loss= tensor(2275.7554)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2384564876556396 seconds
policy loss:452.0758361816406
value loss:19.14708709716797
entropies:34.6576042175293
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1749494075775146 seconds
policy loss:492.21160888671875
value loss:13.266069412231445
entropies:36.3470344543457
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2079336643218994 seconds
policy loss:-942.3298950195312
value loss:30.416996002197266
entropies:55.58876419067383
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1961889266967773 seconds
policy loss:-75.15483093261719
value loss:22.10923194885254
entropies:39.84568786621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.270956039428711 seconds
policy loss:-445.9109191894531
value loss:46.19291687011719
entropies:44.52997589111328
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1238.9343)
ToM Target loss= tensor(2164.6335)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1665961742401123 seconds
policy loss:5.157900810241699
value loss:10.479522705078125
entropies:22.687576293945312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2308509349822998 seconds
policy loss:-603.0970458984375
value loss:11.142560958862305
entropies:28.220983505249023
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2341907024383545 seconds
policy loss:-349.779052734375
value loss:10.528656005859375
entropies:20.007038116455078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2404227256774902 seconds
policy loss:-479.56500244140625
value loss:10.955204963684082
entropies:39.047733306884766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2282984256744385 seconds
policy loss:-951.2809448242188
value loss:16.39887809753418
entropies:27.58133316040039
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1176.4485)
ToM Target loss= tensor(2316.7600)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1822054386138916 seconds
policy loss:-460.3696594238281
value loss:15.77309513092041
entropies:32.22854995727539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.172893762588501 seconds
policy loss:-759.06591796875
value loss:16.83633804321289
entropies:26.767013549804688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.3079984188079834 seconds
policy loss:-643.873779296875
value loss:26.950748443603516
entropies:31.769617080688477
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2324426174163818 seconds
policy loss:-628.2625122070312
value loss:40.63847351074219
entropies:39.58273696899414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2068901062011719 seconds
policy loss:439.60723876953125
value loss:20.756967544555664
entropies:32.03116989135742
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1176.7546)
ToM Target loss= tensor(2143.6331)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2168095111846924 seconds
policy loss:-193.35321044921875
value loss:25.11862564086914
entropies:41.85097885131836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.186434268951416 seconds
policy loss:-359.7965087890625
value loss:25.206544876098633
entropies:40.95186996459961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2183306217193604 seconds
policy loss:-14.649650573730469
value loss:26.153961181640625
entropies:32.25672912597656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1682970523834229 seconds
policy loss:90.54374694824219
value loss:29.730012893676758
entropies:43.14656066894531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.228147029876709 seconds
policy loss:247.0869140625
value loss:23.25106430053711
entropies:37.845611572265625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1248.5117)
ToM Target loss= tensor(2232.6221)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1901922225952148 seconds
policy loss:-1104.30078125
value loss:22.78533935546875
entropies:49.87853240966797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.164536714553833 seconds
policy loss:-752.4182739257812
value loss:14.844953536987305
entropies:46.61663818359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.233335018157959 seconds
policy loss:-1020.2848510742188
value loss:16.532657623291016
entropies:48.66047668457031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2402923107147217 seconds
policy loss:-1819.4613037109375
value loss:37.255401611328125
entropies:51.074520111083984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2453887462615967 seconds
policy loss:-795.1964111328125
value loss:18.64702033996582
entropies:35.15088653564453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1198.2075)
ToM Target loss= tensor(2226.5215)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1962440013885498 seconds
policy loss:-247.81918334960938
value loss:15.175878524780273
entropies:37.49668884277344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177384614944458 seconds
policy loss:-648.93212890625
value loss:13.880850791931152
entropies:43.00583267211914
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1921427249908447 seconds
policy loss:-316.77911376953125
value loss:16.118635177612305
entropies:37.188228607177734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2031230926513672 seconds
policy loss:-157.9850616455078
value loss:10.763117790222168
entropies:45.81282043457031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1853809356689453 seconds
policy loss:-598.5029296875
value loss:21.06199836730957
entropies:61.72471618652344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1255.2638)
ToM Target loss= tensor(2232.1641)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2242207527160645 seconds
policy loss:-1867.861572265625
value loss:26.679075241088867
entropies:61.887290954589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2172319889068604 seconds
policy loss:-370.06591796875
value loss:8.26093578338623
entropies:41.623836517333984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2063908576965332 seconds
policy loss:-1194.634521484375
value loss:21.66605567932129
entropies:47.26125717163086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1946687698364258 seconds
policy loss:-672.3467407226562
value loss:19.27695083618164
entropies:63.80876159667969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1749846935272217 seconds
policy loss:-659.102294921875
value loss:18.85356330871582
entropies:42.4826545715332
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1304.2355)
ToM Target loss= tensor(2209.0208)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2231459617614746 seconds
policy loss:-369.206787109375
value loss:13.90855884552002
entropies:27.88595962524414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.164074420928955 seconds
policy loss:492.6818542480469
value loss:19.50612449645996
entropies:29.564496994018555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1692380905151367 seconds
policy loss:-498.5938720703125
value loss:15.191003799438477
entropies:25.658405303955078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1653540134429932 seconds
policy loss:-665.033935546875
value loss:10.69093132019043
entropies:32.553489685058594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1673192977905273 seconds
policy loss:73.43788146972656
value loss:9.788239479064941
entropies:33.502899169921875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1145.2552)
ToM Target loss= tensor(2276.8809)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1706624031066895 seconds
policy loss:-437.36981201171875
value loss:18.439638137817383
entropies:33.26808166503906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.200202226638794 seconds
policy loss:-384.3747863769531
value loss:5.807865142822266
entropies:26.012182235717773
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1692612171173096 seconds
policy loss:-748.6864624023438
value loss:14.706589698791504
entropies:60.79828643798828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.235280990600586 seconds
policy loss:-409.70245361328125
value loss:16.603858947753906
entropies:47.07673645019531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.165740728378296 seconds
policy loss:-301.3180847167969
value loss:11.76206111907959
entropies:41.06877136230469
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1214.2655)
ToM Target loss= tensor(2335.9702)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2538928985595703 seconds
policy loss:-766.7294311523438
value loss:15.063393592834473
entropies:46.33237838745117
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1729977130889893 seconds
policy loss:-482.330322265625
value loss:12.383655548095703
entropies:34.97858428955078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1742339134216309 seconds
policy loss:-642.6326293945312
value loss:13.86894416809082
entropies:41.988037109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1955101490020752 seconds
policy loss:-1339.684814453125
value loss:34.85108947753906
entropies:57.81855010986328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2226524353027344 seconds
policy loss:-1826.8658447265625
value loss:48.02231216430664
entropies:62.966163635253906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1380.4879)
ToM Target loss= tensor(2269.3877)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2325987815856934 seconds
policy loss:182.70086669921875
value loss:14.03956413269043
entropies:33.382286071777344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1751179695129395 seconds
policy loss:45.71289825439453
value loss:11.381810188293457
entropies:34.56310272216797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2312214374542236 seconds
policy loss:-135.2697296142578
value loss:12.488813400268555
entropies:23.597633361816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.174325942993164 seconds
policy loss:-192.23965454101562
value loss:10.604044914245605
entropies:29.914588928222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.167271375656128 seconds
policy loss:169.71221923828125
value loss:17.625728607177734
entropies:43.500396728515625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1069.1294)
ToM Target loss= tensor(2158.0457)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2415666580200195 seconds
policy loss:-1131.397216796875
value loss:17.664268493652344
entropies:30.994861602783203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2248013019561768 seconds
policy loss:-830.2328491210938
value loss:11.755266189575195
entropies:40.106475830078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1700494289398193 seconds
policy loss:-1157.254638671875
value loss:26.990182876586914
entropies:48.018733978271484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.16843581199646 seconds
policy loss:-1302.46533203125
value loss:16.74410057067871
entropies:48.34654235839844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2371423244476318 seconds
policy loss:55.20921325683594
value loss:10.76700210571289
entropies:22.333763122558594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1259.2867)
ToM Target loss= tensor(2301.6174)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2197024822235107 seconds
policy loss:188.59222412109375
value loss:17.9898681640625
entropies:54.64790725708008
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.174391508102417 seconds
policy loss:-80.84273529052734
value loss:18.331928253173828
entropies:26.50130844116211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1701953411102295 seconds
policy loss:119.97289276123047
value loss:17.403358459472656
entropies:31.873550415039062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1792166233062744 seconds
policy loss:-461.359375
value loss:17.647817611694336
entropies:36.92284393310547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1711595058441162 seconds
policy loss:-509.17431640625
value loss:18.65729522705078
entropies:26.538597106933594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1245.2987)
ToM Target loss= tensor(2425.0776)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1840739250183105 seconds
policy loss:-1233.55712890625
value loss:13.891268730163574
entropies:33.51449966430664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2233386039733887 seconds
policy loss:-894.0841064453125
value loss:16.88238525390625
entropies:34.923824310302734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.241933822631836 seconds
policy loss:-1407.2279052734375
value loss:20.682369232177734
entropies:40.35084533691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1759700775146484 seconds
policy loss:-1103.8836669921875
value loss:21.85028839111328
entropies:35.412269592285156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.198516607284546 seconds
policy loss:-858.2551879882812
value loss:23.941513061523438
entropies:59.213600158691406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1206.9517)
ToM Target loss= tensor(2282.8918)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1655468940734863 seconds
policy loss:335.3686218261719
value loss:11.442183494567871
entropies:28.69019317626953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.236741065979004 seconds
policy loss:-608.7272338867188
value loss:33.50935745239258
entropies:54.37670135498047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2210304737091064 seconds
policy loss:-1947.325439453125
value loss:49.29446792602539
entropies:44.00148391723633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1661052703857422 seconds
policy loss:535.9118041992188
value loss:20.38626480102539
entropies:32.04097366333008
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1704471111297607 seconds
policy loss:250.85263061523438
value loss:21.798755645751953
entropies:34.81610107421875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1190.9620)
ToM Target loss= tensor(2225.9570)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1660311222076416 seconds
policy loss:-1636.7451171875
value loss:48.887001037597656
entropies:60.424598693847656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2194228172302246 seconds
policy loss:570.9061279296875
value loss:22.91048240661621
entropies:54.47154235839844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2368335723876953 seconds
policy loss:-155.61459350585938
value loss:17.630399703979492
entropies:36.41925048828125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.236567497253418 seconds
policy loss:-1300.7855224609375
value loss:24.896909713745117
entropies:51.356624603271484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.251354455947876 seconds
policy loss:-1049.25244140625
value loss:30.663740158081055
entropies:43.50612258911133
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1312.0924)
ToM Target loss= tensor(2230.6750)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2361643314361572 seconds
policy loss:-125.35137939453125
value loss:12.0261869430542
entropies:29.186243057250977
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.260261058807373 seconds
policy loss:-1042.109130859375
value loss:20.02132225036621
entropies:38.00325012207031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1770424842834473 seconds
policy loss:-1227.6451416015625
value loss:50.81012725830078
entropies:51.598628997802734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2425951957702637 seconds
policy loss:-179.0307159423828
value loss:10.664971351623535
entropies:30.942691802978516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2208213806152344 seconds
policy loss:596.4400024414062
value loss:16.46723175048828
entropies:45.39625930786133
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1231.7109)
ToM Target loss= tensor(2333.3516)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1547596454620361 seconds
policy loss:-897.4137573242188
value loss:21.693742752075195
entropies:58.343902587890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2275035381317139 seconds
policy loss:-116.73036193847656
value loss:20.52751350402832
entropies:34.157752990722656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1742284297943115 seconds
policy loss:-809.89501953125
value loss:23.41793441772461
entropies:44.87067413330078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1644136905670166 seconds
policy loss:-705.9583129882812
value loss:10.569846153259277
entropies:42.230377197265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2262239456176758 seconds
policy loss:-652.2434692382812
value loss:24.54753875732422
entropies:46.05458068847656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1256.6561)
ToM Target loss= tensor(2280.5815)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.220750093460083 seconds
policy loss:-1594.7896728515625
value loss:22.570133209228516
entropies:40.716346740722656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2404248714447021 seconds
policy loss:-363.90533447265625
value loss:14.817989349365234
entropies:34.090675354003906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1685938835144043 seconds
policy loss:333.697265625
value loss:27.252241134643555
entropies:45.8912467956543
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1679084300994873 seconds
policy loss:240.59617614746094
value loss:17.53680419921875
entropies:43.146095275878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2371916770935059 seconds
policy loss:-157.30343627929688
value loss:18.262849807739258
entropies:45.37540054321289
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1232.8970)
ToM Target loss= tensor(2374.0635)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2287371158599854 seconds
policy loss:-536.6776733398438
value loss:28.594629287719727
entropies:46.793113708496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1856865882873535 seconds
policy loss:-1161.150634765625
value loss:17.053569793701172
entropies:38.65087890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2315258979797363 seconds
policy loss:-486.1610107421875
value loss:11.073318481445312
entropies:40.24848937988281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2218310832977295 seconds
policy loss:-117.2793197631836
value loss:9.604859352111816
entropies:29.251113891601562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2110133171081543 seconds
policy loss:-198.7296142578125
value loss:9.84561538696289
entropies:30.212881088256836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1222.0104)
ToM Target loss= tensor(2343.2214)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1727807521820068 seconds
policy loss:-773.0891723632812
value loss:22.03068733215332
entropies:45.50197982788086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2409491539001465 seconds
policy loss:286.4991760253906
value loss:8.795439720153809
entropies:35.804107666015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2057209014892578 seconds
policy loss:-1281.081298828125
value loss:18.694486618041992
entropies:53.13562774658203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1790292263031006 seconds
policy loss:-1189.1451416015625
value loss:36.688087463378906
entropies:46.17913818359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2342395782470703 seconds
policy loss:94.87826538085938
value loss:16.657392501831055
entropies:16.961339950561523
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1204.9686)
ToM Target loss= tensor(2195.9426)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2019541263580322 seconds
policy loss:-452.6669006347656
value loss:18.68169403076172
entropies:24.699432373046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.193021535873413 seconds
policy loss:152.3162384033203
value loss:16.4274845123291
entropies:26.01198959350586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.185086965560913 seconds
policy loss:-943.2706909179688
value loss:23.833635330200195
entropies:42.08879089355469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2248427867889404 seconds
policy loss:-98.45658111572266
value loss:7.663527965545654
entropies:31.93000030517578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2259399890899658 seconds
policy loss:-142.75750732421875
value loss:19.674896240234375
entropies:26.308958053588867
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1242.7638)
ToM Target loss= tensor(2372.5681)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1851105690002441 seconds
policy loss:-162.34495544433594
value loss:10.197918891906738
entropies:18.917556762695312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1827800273895264 seconds
policy loss:-125.6959457397461
value loss:8.002967834472656
entropies:20.100387573242188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1801836490631104 seconds
policy loss:-2241.306640625
value loss:63.24795150756836
entropies:55.59046936035156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2346956729888916 seconds
policy loss:-1004.25244140625
value loss:12.571537017822266
entropies:41.48135757446289
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1653993129730225 seconds
policy loss:-1096.69873046875
value loss:18.63182258605957
entropies:24.063615798950195
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1126.6368)
ToM Target loss= tensor(2198.0322)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2250196933746338 seconds
policy loss:-956.3878173828125
value loss:18.92243194580078
entropies:32.986061096191406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.179323673248291 seconds
policy loss:97.952392578125
value loss:9.853058815002441
entropies:26.92323112487793
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.207627296447754 seconds
policy loss:111.6521224975586
value loss:11.396595001220703
entropies:24.524621963500977
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2147672176361084 seconds
policy loss:133.9847869873047
value loss:15.570182800292969
entropies:21.666898727416992
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2282240390777588 seconds
policy loss:-486.59466552734375
value loss:17.536155700683594
entropies:46.14686965942383
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1132.0685)
ToM Target loss= tensor(2217.9268)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2290759086608887 seconds
policy loss:-163.48333740234375
value loss:14.651453018188477
entropies:35.05483627319336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2329816818237305 seconds
policy loss:253.13787841796875
value loss:8.061836242675781
entropies:24.75185775756836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2406041622161865 seconds
policy loss:-861.3331909179688
value loss:18.878467559814453
entropies:37.0203971862793
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2193434238433838 seconds
policy loss:-978.7356567382812
value loss:22.643640518188477
entropies:28.945106506347656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.230942726135254 seconds
policy loss:-292.4219055175781
value loss:10.073490142822266
entropies:16.661680221557617
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1119.4507)
ToM Target loss= tensor(2244.6409)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.179452657699585 seconds
policy loss:-740.19775390625
value loss:49.03049087524414
entropies:36.14604187011719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2445971965789795 seconds
policy loss:-844.3587036132812
value loss:12.449745178222656
entropies:39.52354049682617
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.172520637512207 seconds
policy loss:135.06068420410156
value loss:7.644166469573975
entropies:20.035362243652344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.166733741760254 seconds
policy loss:-494.0452575683594
value loss:12.241326332092285
entropies:26.79373550415039
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1754493713378906 seconds
policy loss:321.6269226074219
value loss:16.242061614990234
entropies:30.39029312133789
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1206.1274)
ToM Target loss= tensor(2321.7363)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.231269359588623 seconds
policy loss:111.8233871459961
value loss:12.770499229431152
entropies:34.07206344604492
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2341527938842773 seconds
policy loss:-750.0953369140625
value loss:37.15361404418945
entropies:48.24543380737305
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2336711883544922 seconds
policy loss:109.28424072265625
value loss:11.987939834594727
entropies:35.248779296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2209930419921875 seconds
policy loss:-876.705078125
value loss:19.293437957763672
entropies:30.21595573425293
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.221357822418213 seconds
policy loss:-503.86785888671875
value loss:9.278160095214844
entropies:31.025142669677734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1222.1381)
ToM Target loss= tensor(2260.5464)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2484326362609863 seconds
policy loss:-617.8358764648438
value loss:13.026050567626953
entropies:24.411014556884766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2389183044433594 seconds
policy loss:-363.8205871582031
value loss:18.009014129638672
entropies:19.89344024658203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1737165451049805 seconds
policy loss:-1278.56640625
value loss:46.57063293457031
entropies:43.04865264892578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2273852825164795 seconds
policy loss:223.26129150390625
value loss:5.25173282623291
entropies:18.126968383789062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2377774715423584 seconds
policy loss:-113.3297348022461
value loss:15.283196449279785
entropies:33.54833984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1100.6772)
ToM Target loss= tensor(2220.3633)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1583261489868164 seconds
policy loss:-396.5408630371094
value loss:19.013763427734375
entropies:40.8704719543457
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1709787845611572 seconds
policy loss:-55.02983856201172
value loss:11.790521621704102
entropies:29.753536224365234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2468962669372559 seconds
policy loss:-453.3047180175781
value loss:14.968587875366211
entropies:28.051097869873047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2030487060546875 seconds
policy loss:-587.9598999023438
value loss:28.196361541748047
entropies:41.013545989990234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2349231243133545 seconds
policy loss:-203.46653747558594
value loss:13.694061279296875
entropies:17.097347259521484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1098.4222)
ToM Target loss= tensor(2224.9219)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2010061740875244 seconds
policy loss:82.34709167480469
value loss:10.0410737991333
entropies:49.39795684814453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1716365814208984 seconds
policy loss:51.37479019165039
value loss:6.698857307434082
entropies:26.801734924316406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.168569803237915 seconds
policy loss:19.92772102355957
value loss:9.537527084350586
entropies:30.924665451049805
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1880486011505127 seconds
policy loss:-958.454833984375
value loss:13.483804702758789
entropies:34.609344482421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.231766700744629 seconds
policy loss:-610.192138671875
value loss:10.523120880126953
entropies:22.316207885742188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1192.5728)
ToM Target loss= tensor(2332.9885)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.197824478149414 seconds
policy loss:-881.5901489257812
value loss:10.735196113586426
entropies:37.51430130004883
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2294807434082031 seconds
policy loss:175.2021484375
value loss:7.893671989440918
entropies:23.488258361816406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2278194427490234 seconds
policy loss:-873.9326782226562
value loss:16.98381805419922
entropies:31.295223236083984
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.161912441253662 seconds
policy loss:-1085.1988525390625
value loss:29.696020126342773
entropies:36.28207778930664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1742510795593262 seconds
policy loss:-1145.068115234375
value loss:30.741445541381836
entropies:41.6268310546875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1195.8938)
ToM Target loss= tensor(2248.7461)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2306175231933594 seconds
policy loss:-447.6920166015625
value loss:18.76752471923828
entropies:42.57594299316406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1752710342407227 seconds
policy loss:-745.8922119140625
value loss:34.84596252441406
entropies:34.09528350830078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2262773513793945 seconds
policy loss:-131.8708953857422
value loss:15.83713436126709
entropies:47.67377471923828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.244760513305664 seconds
policy loss:416.4837646484375
value loss:26.473472595214844
entropies:44.206207275390625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1794567108154297 seconds
policy loss:99.07978057861328
value loss:17.675914764404297
entropies:40.89949035644531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1173.5244)
ToM Target loss= tensor(2072.7803)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1835203170776367 seconds
policy loss:-531.416015625
value loss:20.443161010742188
entropies:40.92852020263672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1689832210540771 seconds
policy loss:358.6540832519531
value loss:13.466062545776367
entropies:39.343536376953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1715710163116455 seconds
policy loss:-731.6104125976562
value loss:27.965578079223633
entropies:43.26268005371094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1698925495147705 seconds
policy loss:-1475.3497314453125
value loss:34.38554000854492
entropies:39.939422607421875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1758182048797607 seconds
policy loss:-1261.906494140625
value loss:21.265823364257812
entropies:50.8132438659668
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1227.3025)
ToM Target loss= tensor(2169.6130)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1859021186828613 seconds
policy loss:-218.21206665039062
value loss:22.654319763183594
entropies:34.46302032470703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.25091552734375 seconds
policy loss:97.07051086425781
value loss:10.053190231323242
entropies:43.49510192871094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.20721435546875 seconds
policy loss:96.65118408203125
value loss:18.99053192138672
entropies:40.37742233276367
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2388439178466797 seconds
policy loss:396.2004699707031
value loss:15.794981956481934
entropies:45.71424102783203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2348029613494873 seconds
policy loss:201.52389526367188
value loss:15.026029586791992
entropies:31.816272735595703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1196.9777)
ToM Target loss= tensor(2262.1506)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1774218082427979 seconds
policy loss:264.1536560058594
value loss:10.765345573425293
entropies:26.841632843017578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1795778274536133 seconds
policy loss:-261.83538818359375
value loss:13.417667388916016
entropies:38.93206787109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2349441051483154 seconds
policy loss:-754.1068725585938
value loss:18.855337142944336
entropies:50.18696594238281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2296559810638428 seconds
policy loss:-272.6753234863281
value loss:13.52995491027832
entropies:31.67105484008789
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.239696979522705 seconds
policy loss:-727.4173583984375
value loss:14.750604629516602
entropies:31.65970230102539
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1228.2439)
ToM Target loss= tensor(2338.3584)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1637179851531982 seconds
policy loss:-621.1382446289062
value loss:11.99876880645752
entropies:32.06953430175781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1966683864593506 seconds
policy loss:-764.3463745117188
value loss:17.47270965576172
entropies:47.39693832397461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1723194122314453 seconds
policy loss:-480.5491027832031
value loss:21.400850296020508
entropies:44.21429443359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.16849946975708 seconds
policy loss:-406.37030029296875
value loss:16.288766860961914
entropies:41.77120590209961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.235701560974121 seconds
policy loss:87.6321792602539
value loss:12.027320861816406
entropies:32.947601318359375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1252.7985)
ToM Target loss= tensor(2327.6938)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1701998710632324 seconds
policy loss:59.598114013671875
value loss:16.457368850708008
entropies:33.3614501953125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.261399745941162 seconds
policy loss:113.86906433105469
value loss:13.24791145324707
entropies:40.69940948486328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2335724830627441 seconds
policy loss:-155.2428741455078
value loss:10.916296005249023
entropies:25.530765533447266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1691653728485107 seconds
policy loss:179.13742065429688
value loss:7.215986251831055
entropies:26.125667572021484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2452545166015625 seconds
policy loss:-1596.3004150390625
value loss:22.47652244567871
entropies:35.189395904541016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1284.1128)
ToM Target loss= tensor(2352.7957)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1744139194488525 seconds
policy loss:-939.9254760742188
value loss:23.221817016601562
entropies:36.762779235839844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1907131671905518 seconds
policy loss:-363.409912109375
value loss:10.388592720031738
entropies:21.695003509521484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1907191276550293 seconds
policy loss:-681.6319580078125
value loss:23.695796966552734
entropies:42.58986282348633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2362008094787598 seconds
policy loss:-1134.806884765625
value loss:24.101255416870117
entropies:49.036399841308594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2492334842681885 seconds
policy loss:-563.0230102539062
value loss:10.1383638381958
entropies:32.09431838989258
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1273.1060)
ToM Target loss= tensor(2373.7754)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1761839389801025 seconds
policy loss:-513.2012939453125
value loss:19.860118865966797
entropies:62.53596496582031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.242234230041504 seconds
policy loss:-387.90667724609375
value loss:22.17995262145996
entropies:48.95256805419922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2196145057678223 seconds
policy loss:-465.4522399902344
value loss:16.572914123535156
entropies:34.36344909667969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1740813255310059 seconds
policy loss:-845.7677612304688
value loss:20.778512954711914
entropies:38.44132995605469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1919608116149902 seconds
policy loss:-110.62141418457031
value loss:7.9950127601623535
entropies:26.486373901367188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1296.6759)
ToM Target loss= tensor(2304.3774)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1762933731079102 seconds
policy loss:-183.54489135742188
value loss:11.263477325439453
entropies:28.78907012939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.221956729888916 seconds
policy loss:-1563.8160400390625
value loss:40.6783561706543
entropies:46.026702880859375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.169614315032959 seconds
policy loss:-816.8609008789062
value loss:25.526592254638672
entropies:44.12566375732422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.240431785583496 seconds
policy loss:-426.1898498535156
value loss:23.548778533935547
entropies:31.249887466430664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2298226356506348 seconds
policy loss:-480.42913818359375
value loss:13.02957820892334
entropies:35.104881286621094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1247.5720)
ToM Target loss= tensor(2240.4294)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.212430477142334 seconds
policy loss:-529.9783325195312
value loss:14.074210166931152
entropies:46.23777389526367
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1917402744293213 seconds
policy loss:-41.63629913330078
value loss:23.627042770385742
entropies:27.005474090576172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1726148128509521 seconds
policy loss:-4.998435974121094
value loss:20.12982749938965
entropies:31.152042388916016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.16864013671875 seconds
policy loss:-85.30413818359375
value loss:29.72116470336914
entropies:42.50101089477539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2237911224365234 seconds
policy loss:-105.37330627441406
value loss:28.9271183013916
entropies:48.151573181152344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1271.0551)
ToM Target loss= tensor(2294.2781)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1724743843078613 seconds
policy loss:-61.19806671142578
value loss:18.596364974975586
entropies:31.40062713623047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2124559879302979 seconds
policy loss:510.90606689453125
value loss:21.127803802490234
entropies:47.3060302734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1753294467926025 seconds
policy loss:-671.43212890625
value loss:14.793678283691406
entropies:32.384246826171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.171065092086792 seconds
policy loss:-1006.393310546875
value loss:26.632282257080078
entropies:40.718467712402344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1805906295776367 seconds
policy loss:-1540.6578369140625
value loss:72.10104370117188
entropies:38.38711929321289
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1264.3265)
ToM Target loss= tensor(2183.2246)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.199674367904663 seconds
policy loss:-1059.9383544921875
value loss:22.707246780395508
entropies:37.083465576171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2119216918945312 seconds
policy loss:-580.65966796875
value loss:17.941797256469727
entropies:33.4024658203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1728615760803223 seconds
policy loss:-521.4846801757812
value loss:13.712309837341309
entropies:38.84281921386719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1945180892944336 seconds
policy loss:86.97557830810547
value loss:13.290241241455078
entropies:29.312484741210938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2426683902740479 seconds
policy loss:-109.19558715820312
value loss:19.914033889770508
entropies:37.18159484863281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1176.3818)
ToM Target loss= tensor(2271.5435)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1699166297912598 seconds
policy loss:526.7233276367188
value loss:13.201007843017578
entropies:29.062602996826172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1923747062683105 seconds
policy loss:-495.7781066894531
value loss:25.84250831604004
entropies:29.64828872680664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.18951416015625 seconds
policy loss:-540.5391845703125
value loss:37.97252655029297
entropies:29.765270233154297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1771907806396484 seconds
policy loss:-249.8095703125
value loss:8.094313621520996
entropies:33.05731964111328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2325036525726318 seconds
policy loss:-273.79864501953125
value loss:12.175941467285156
entropies:19.54526138305664
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1122.6417)
ToM Target loss= tensor(2457.4028)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2342839241027832 seconds
policy loss:-1711.7742919921875
value loss:43.94804000854492
entropies:48.41071701049805
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1602139472961426 seconds
policy loss:-256.2863464355469
value loss:12.636866569519043
entropies:26.762039184570312
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1776974201202393 seconds
policy loss:-2525.23388671875
value loss:32.80927658081055
entropies:51.83429718017578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.167673110961914 seconds
policy loss:-1378.08642578125
value loss:50.848876953125
entropies:38.2685546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2352416515350342 seconds
policy loss:-1086.798828125
value loss:39.96220397949219
entropies:44.11376953125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1235.4563)
ToM Target loss= tensor(2312.3767)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2671318054199219 seconds
policy loss:-1147.668212890625
value loss:25.005830764770508
entropies:43.78337097167969
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2206368446350098 seconds
policy loss:238.93203735351562
value loss:11.910840034484863
entropies:21.631797790527344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.162816047668457 seconds
policy loss:-1003.2389526367188
value loss:24.491031646728516
entropies:42.94371032714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1731452941894531 seconds
policy loss:284.5574645996094
value loss:16.34600067138672
entropies:22.494205474853516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227442741394043 seconds
policy loss:541.000244140625
value loss:19.986141204833984
entropies:34.129493713378906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1187.7979)
ToM Target loss= tensor(2260.9893)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2308976650238037 seconds
policy loss:-415.0955810546875
value loss:29.22459602355957
entropies:47.98516082763672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2529335021972656 seconds
policy loss:-1413.4617919921875
value loss:46.238731384277344
entropies:42.551002502441406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1812007427215576 seconds
policy loss:-380.7433166503906
value loss:21.110532760620117
entropies:51.94499969482422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.175311803817749 seconds
policy loss:-381.4068298339844
value loss:13.172623634338379
entropies:35.0259895324707
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.197082281112671 seconds
policy loss:-523.0676879882812
value loss:21.678939819335938
entropies:43.98255157470703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1174.2178)
ToM Target loss= tensor(2175.9321)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.199127435684204 seconds
policy loss:-170.41485595703125
value loss:7.002199649810791
entropies:22.987060546875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1977331638336182 seconds
policy loss:-669.7247314453125
value loss:31.295459747314453
entropies:37.2147102355957
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1861798763275146 seconds
policy loss:-527.9461059570312
value loss:16.153074264526367
entropies:40.712730407714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2175014019012451 seconds
policy loss:-73.72017669677734
value loss:14.624504089355469
entropies:36.54290008544922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2278196811676025 seconds
policy loss:-674.5172119140625
value loss:15.321529388427734
entropies:45.81024932861328
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1182.7014)
ToM Target loss= tensor(2198.9851)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2359068393707275 seconds
policy loss:-463.1127624511719
value loss:9.125414848327637
entropies:28.833566665649414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2104339599609375 seconds
policy loss:-440.70611572265625
value loss:13.019598960876465
entropies:30.335731506347656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2238645553588867 seconds
policy loss:-306.1141357421875
value loss:10.238734245300293
entropies:28.008827209472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2005040645599365 seconds
policy loss:-2556.399658203125
value loss:18.16162109375
entropies:35.82844161987305
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1781787872314453 seconds
policy loss:-716.2931518554688
value loss:15.431833267211914
entropies:41.623512268066406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1168.9670)
ToM Target loss= tensor(2348.4263)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.17622971534729 seconds
policy loss:-348.02734375
value loss:11.445026397705078
entropies:33.956512451171875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1721839904785156 seconds
policy loss:-878.5092163085938
value loss:17.031982421875
entropies:56.47123336791992
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1685099601745605 seconds
policy loss:-1179.7232666015625
value loss:35.70647048950195
entropies:60.42599105834961
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1789581775665283 seconds
policy loss:7.942806243896484
value loss:19.72517967224121
entropies:52.07337188720703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2236497402191162 seconds
policy loss:-269.9297180175781
value loss:19.343570709228516
entropies:27.978988647460938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1308.6273)
ToM Target loss= tensor(2232.7800)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2203912734985352 seconds
policy loss:97.25419616699219
value loss:23.845626831054688
entropies:53.0545539855957
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2036101818084717 seconds
policy loss:53.062286376953125
value loss:22.875125885009766
entropies:34.41273498535156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1930255889892578 seconds
policy loss:-62.9617805480957
value loss:26.856447219848633
entropies:52.60725402832031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2350523471832275 seconds
policy loss:-856.757568359375
value loss:23.31534194946289
entropies:44.41108703613281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.165097951889038 seconds
policy loss:-1366.81494140625
value loss:26.95817756652832
entropies:38.893428802490234
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1252.4143)
ToM Target loss= tensor(2278.2068)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2277412414550781 seconds
policy loss:-807.6895141601562
value loss:17.05105972290039
entropies:50.258174896240234
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2166943550109863 seconds
policy loss:-761.13330078125
value loss:16.7958984375
entropies:39.28033447265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2087419033050537 seconds
policy loss:-1656.27587890625
value loss:26.507061004638672
entropies:39.09728240966797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2294139862060547 seconds
policy loss:-1391.779296875
value loss:47.245567321777344
entropies:49.17060089111328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1675732135772705 seconds
policy loss:-72.25936889648438
value loss:10.622417449951172
entropies:30.817794799804688
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1242.0490)
ToM Target loss= tensor(2310.2686)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1627697944641113 seconds
policy loss:-456.89532470703125
value loss:16.877790451049805
entropies:36.487281799316406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2262020111083984 seconds
policy loss:-337.0638427734375
value loss:22.044445037841797
entropies:59.353755950927734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1952555179595947 seconds
policy loss:-272.3572998046875
value loss:14.771303176879883
entropies:31.111745834350586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.239347219467163 seconds
policy loss:-755.9746704101562
value loss:19.099933624267578
entropies:37.09260559082031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1737773418426514 seconds
policy loss:246.39053344726562
value loss:9.779480934143066
entropies:21.007192611694336
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1258.4016)
ToM Target loss= tensor(2367.7681)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2333266735076904 seconds
policy loss:-389.67486572265625
value loss:12.165543556213379
entropies:32.59066390991211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2198200225830078 seconds
policy loss:-1056.34033203125
value loss:19.822467803955078
entropies:36.26458740234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2458386421203613 seconds
policy loss:-863.6736450195312
value loss:11.636672973632812
entropies:45.5422248840332
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1780543327331543 seconds
policy loss:-368.67559814453125
value loss:19.185991287231445
entropies:47.73765182495117
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2253315448760986 seconds
policy loss:-1018.5714721679688
value loss:27.572742462158203
entropies:55.1314697265625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1258.6572)
ToM Target loss= tensor(2241.9214)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2414846420288086 seconds
policy loss:-259.3991394042969
value loss:6.964807510375977
entropies:24.950651168823242
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2346985340118408 seconds
policy loss:-249.511474609375
value loss:12.75307846069336
entropies:37.970455169677734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.215043067932129 seconds
policy loss:360.05755615234375
value loss:9.652168273925781
entropies:35.57615661621094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2432796955108643 seconds
policy loss:307.62127685546875
value loss:15.327254295349121
entropies:37.44282531738281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2248923778533936 seconds
policy loss:-370.8747863769531
value loss:28.41596031188965
entropies:39.62049865722656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1183.8927)
ToM Target loss= tensor(2336.9341)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1991808414459229 seconds
policy loss:19.464906692504883
value loss:2.457296133041382
entropies:21.7044677734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2381463050842285 seconds
policy loss:-1041.56884765625
value loss:17.657865524291992
entropies:42.32667922973633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2361814975738525 seconds
policy loss:-1791.4296875
value loss:20.470815658569336
entropies:31.221534729003906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1792447566986084 seconds
policy loss:-1886.869873046875
value loss:23.693870544433594
entropies:51.955322265625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1990551948547363 seconds
policy loss:-637.578369140625
value loss:19.781421661376953
entropies:27.440357208251953
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1265.3060)
ToM Target loss= tensor(2335.3433)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.240356683731079 seconds
policy loss:-10.978302955627441
value loss:8.176448822021484
entropies:35.02521896362305
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2321491241455078 seconds
policy loss:-85.267822265625
value loss:15.124481201171875
entropies:32.026100158691406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2454962730407715 seconds
policy loss:-2010.934326171875
value loss:22.2187442779541
entropies:36.54042053222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2273499965667725 seconds
policy loss:-685.1629638671875
value loss:18.10262680053711
entropies:44.54227066040039
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2190186977386475 seconds
policy loss:-514.5194091796875
value loss:15.383707046508789
entropies:32.846290588378906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1260.9427)
ToM Target loss= tensor(2294.8855)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.235234022140503 seconds
policy loss:-307.1991882324219
value loss:11.621562957763672
entropies:29.949905395507812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.185765266418457 seconds
policy loss:-663.9179077148438
value loss:18.566194534301758
entropies:37.70646667480469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1974713802337646 seconds
policy loss:-6.726619720458984
value loss:8.509204864501953
entropies:39.787574768066406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1682558059692383 seconds
policy loss:-521.9195556640625
value loss:13.342964172363281
entropies:60.175575256347656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1696524620056152 seconds
policy loss:-345.13262939453125
value loss:8.624258041381836
entropies:25.41754150390625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1235.8625)
ToM Target loss= tensor(2275.0825)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1921112537384033 seconds
policy loss:-656.7343139648438
value loss:15.164266586303711
entropies:43.18459701538086
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1947317123413086 seconds
policy loss:-592.5665283203125
value loss:13.939140319824219
entropies:34.88928985595703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2098116874694824 seconds
policy loss:-979.8157958984375
value loss:22.781810760498047
entropies:39.628868103027344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2067070007324219 seconds
policy loss:-1197.22314453125
value loss:13.411152839660645
entropies:36.6134033203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2110874652862549 seconds
policy loss:10.996875762939453
value loss:3.974623203277588
entropies:22.088722229003906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1223.3972)
ToM Target loss= tensor(2359.3958)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2197003364562988 seconds
policy loss:-1449.375732421875
value loss:26.144306182861328
entropies:51.19922637939453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2412657737731934 seconds
policy loss:-604.8225708007812
value loss:24.547218322753906
entropies:39.09699249267578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1682016849517822 seconds
policy loss:252.8135223388672
value loss:4.792990684509277
entropies:25.966690063476562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2219531536102295 seconds
policy loss:-568.0667114257812
value loss:13.576373100280762
entropies:49.66051483154297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.208744764328003 seconds
policy loss:-1593.2462158203125
value loss:33.7835693359375
entropies:38.560482025146484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1277.1982)
ToM Target loss= tensor(2343.9114)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1751434803009033 seconds
policy loss:-581.6486206054688
value loss:20.098796844482422
entropies:35.23707580566406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1656463146209717 seconds
policy loss:-1329.4884033203125
value loss:43.918052673339844
entropies:37.33890151977539
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1303167343139648 seconds
policy loss:419.365966796875
value loss:8.066415786743164
entropies:29.612659454345703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.175666093826294 seconds
policy loss:-24.614320755004883
value loss:8.528353691101074
entropies:21.44040870666504
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.20719313621521 seconds
policy loss:-79.65238189697266
value loss:8.780257225036621
entropies:32.59445571899414
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1116.3630)
ToM Target loss= tensor(2267.5505)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1667888164520264 seconds
policy loss:-1073.3660888671875
value loss:20.361534118652344
entropies:43.5690803527832
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2089834213256836 seconds
policy loss:-481.4631652832031
value loss:15.812743186950684
entropies:37.920475006103516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1810743808746338 seconds
policy loss:-146.47567749023438
value loss:7.117624759674072
entropies:26.665660858154297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1924729347229004 seconds
policy loss:-262.9445495605469
value loss:15.642824172973633
entropies:37.93836212158203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2210705280303955 seconds
policy loss:-547.5965576171875
value loss:12.475056648254395
entropies:39.73082733154297
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1190.8417)
ToM Target loss= tensor(2320.0056)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2132556438446045 seconds
policy loss:121.69458770751953
value loss:23.401941299438477
entropies:44.911170959472656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2262616157531738 seconds
policy loss:-21.15506935119629
value loss:11.087005615234375
entropies:32.16426467895508
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2254064083099365 seconds
policy loss:70.11123657226562
value loss:17.723541259765625
entropies:31.502227783203125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2200963497161865 seconds
policy loss:149.85545349121094
value loss:8.833602905273438
entropies:32.460628509521484
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1696624755859375 seconds
policy loss:-167.013671875
value loss:10.543994903564453
entropies:35.67452621459961
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1281.9402)
ToM Target loss= tensor(2279.9734)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2374470233917236 seconds
policy loss:-701.5423583984375
value loss:21.695066452026367
entropies:34.122161865234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2297415733337402 seconds
policy loss:-982.9851684570312
value loss:17.1751651763916
entropies:36.45707702636719
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.224586009979248 seconds
policy loss:-352.5265808105469
value loss:10.503409385681152
entropies:31.465057373046875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.167191743850708 seconds
policy loss:-476.4119873046875
value loss:22.893352508544922
entropies:30.743165969848633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2306313514709473 seconds
policy loss:-1877.8372802734375
value loss:47.51235580444336
entropies:50.717384338378906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1157.8157)
ToM Target loss= tensor(2201.3196)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2275099754333496 seconds
policy loss:-174.6710662841797
value loss:41.09725570678711
entropies:42.51251983642578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2418789863586426 seconds
policy loss:28.139366149902344
value loss:18.72145652770996
entropies:21.430727005004883
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1971170902252197 seconds
policy loss:693.3160400390625
value loss:22.70082664489746
entropies:53.747520446777344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.174311876296997 seconds
policy loss:-82.07392120361328
value loss:23.03716468811035
entropies:34.92516326904297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2029945850372314 seconds
policy loss:-778.0743408203125
value loss:39.168487548828125
entropies:37.74214172363281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1264.7819)
ToM Target loss= tensor(2237.0481)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.225569486618042 seconds
policy loss:-276.8818054199219
value loss:23.130983352661133
entropies:31.02277374267578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2134110927581787 seconds
policy loss:102.73248291015625
value loss:8.918731689453125
entropies:18.371532440185547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.221168041229248 seconds
policy loss:-315.7146911621094
value loss:16.2608585357666
entropies:44.77437973022461
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.241800308227539 seconds
policy loss:-1133.169921875
value loss:12.505311012268066
entropies:21.19556427001953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1988041400909424 seconds
policy loss:-1598.9136962890625
value loss:31.043930053710938
entropies:47.79264831542969
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1278.1357)
ToM Target loss= tensor(2327.7363)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1653611660003662 seconds
policy loss:-966.93798828125
value loss:24.904666900634766
entropies:48.71653747558594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1704943180084229 seconds
policy loss:-700.0377807617188
value loss:17.663225173950195
entropies:45.901153564453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2324092388153076 seconds
policy loss:68.34698486328125
value loss:7.868791103363037
entropies:12.586761474609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.208653211593628 seconds
policy loss:-626.2586669921875
value loss:6.97139310836792
entropies:21.8638973236084
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1904850006103516 seconds
policy loss:118.86582946777344
value loss:12.31204605102539
entropies:31.70838165283203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1194.5930)
ToM Target loss= tensor(2313.4578)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2283892631530762 seconds
policy loss:-1222.617919921875
value loss:29.47056007385254
entropies:37.875946044921875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.19631028175354 seconds
policy loss:-179.3456268310547
value loss:12.634413719177246
entropies:30.637557983398438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1385514736175537 seconds
policy loss:-163.60594177246094
value loss:7.687893867492676
entropies:22.794281005859375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.164092779159546 seconds
policy loss:95.45966339111328
value loss:5.836544036865234
entropies:23.05771255493164
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1746065616607666 seconds
policy loss:-322.90924072265625
value loss:11.901968955993652
entropies:39.32368469238281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1131.0825)
ToM Target loss= tensor(2231.5535)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2350552082061768 seconds
policy loss:-200.06227111816406
value loss:18.235742568969727
entropies:30.91189956665039
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1794371604919434 seconds
policy loss:-505.86376953125
value loss:10.066008567810059
entropies:20.68732452392578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.227013349533081 seconds
policy loss:-775.499755859375
value loss:14.179059028625488
entropies:38.09729766845703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2428438663482666 seconds
policy loss:-2275.50390625
value loss:33.88441467285156
entropies:46.998382568359375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2304356098175049 seconds
policy loss:-984.2122802734375
value loss:19.301044464111328
entropies:42.10714340209961
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1114.9368)
ToM Target loss= tensor(2197.9922)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2307696342468262 seconds
policy loss:-7.926661491394043
value loss:19.643037796020508
entropies:46.583839416503906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2318358421325684 seconds
policy loss:8.350269317626953
value loss:8.88547420501709
entropies:16.971927642822266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1745927333831787 seconds
policy loss:227.57713317871094
value loss:13.986533164978027
entropies:33.51103973388672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2306225299835205 seconds
policy loss:-47.04743957519531
value loss:16.204936981201172
entropies:26.381450653076172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.252668857574463 seconds
policy loss:-525.827392578125
value loss:16.299114227294922
entropies:37.93128204345703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1148.2815)
ToM Target loss= tensor(2283.7900)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2333643436431885 seconds
policy loss:-177.39556884765625
value loss:6.813450813293457
entropies:18.355911254882812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1765902042388916 seconds
policy loss:-948.7814331054688
value loss:19.295398712158203
entropies:45.415557861328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1763331890106201 seconds
policy loss:-939.443115234375
value loss:20.733470916748047
entropies:34.631622314453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.225952386856079 seconds
policy loss:-1318.4234619140625
value loss:33.60676574707031
entropies:43.101173400878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2127220630645752 seconds
policy loss:-1903.2415771484375
value loss:24.85845947265625
entropies:43.02694320678711
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1223.6178)
ToM Target loss= tensor(2277.3423)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2380237579345703 seconds
policy loss:-169.21737670898438
value loss:14.715347290039062
entropies:52.617881774902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1748175621032715 seconds
policy loss:83.52908325195312
value loss:13.3945951461792
entropies:39.67974853515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1681923866271973 seconds
policy loss:364.73931884765625
value loss:16.3320255279541
entropies:30.755300521850586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2138328552246094 seconds
policy loss:387.9552917480469
value loss:17.47295379638672
entropies:23.354782104492188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1711575984954834 seconds
policy loss:-543.9888305664062
value loss:10.624208450317383
entropies:38.869544982910156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1218.2687)
ToM Target loss= tensor(2297.7351)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2047522068023682 seconds
policy loss:-628.7283325195312
value loss:12.410614013671875
entropies:26.224132537841797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.177750825881958 seconds
policy loss:-558.6940307617188
value loss:19.854156494140625
entropies:40.593223571777344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2422220706939697 seconds
policy loss:-61.93734359741211
value loss:6.542865753173828
entropies:28.001422882080078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2355530261993408 seconds
policy loss:-383.4485778808594
value loss:9.768110275268555
entropies:25.647043228149414
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2478697299957275 seconds
policy loss:-522.8258666992188
value loss:9.962377548217773
entropies:41.80389404296875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1185.3762)
ToM Target loss= tensor(2285.3098)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.231490135192871 seconds
policy loss:-343.4674072265625
value loss:12.56043815612793
entropies:27.638446807861328
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2308130264282227 seconds
policy loss:197.08335876464844
value loss:14.67465591430664
entropies:42.76055145263672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2337677478790283 seconds
policy loss:-836.5011596679688
value loss:25.207443237304688
entropies:53.889469146728516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.249807596206665 seconds
policy loss:-243.73550415039062
value loss:25.52113151550293
entropies:48.60832977294922
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2006785869598389 seconds
policy loss:-454.7894592285156
value loss:7.283926963806152
entropies:36.915626525878906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1238.7338)
ToM Target loss= tensor(2238.6677)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1840486526489258 seconds
policy loss:-790.932373046875
value loss:21.5045108795166
entropies:37.37583541870117
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1789476871490479 seconds
policy loss:-798.0745849609375
value loss:30.018272399902344
entropies:42.770118713378906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1697657108306885 seconds
policy loss:-1529.7564697265625
value loss:30.350318908691406
entropies:47.4609489440918
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2332572937011719 seconds
policy loss:-679.6088256835938
value loss:18.787202835083008
entropies:32.29053497314453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2317636013031006 seconds
policy loss:103.08704376220703
value loss:12.874527931213379
entropies:24.20574188232422
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1246.5380)
ToM Target loss= tensor(2212.9189)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2089006900787354 seconds
policy loss:140.8848876953125
value loss:23.194625854492188
entropies:32.20166015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.217308759689331 seconds
policy loss:-562.7940673828125
value loss:41.01164245605469
entropies:30.912670135498047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1720004081726074 seconds
policy loss:341.66082763671875
value loss:9.725617408752441
entropies:34.57832336425781
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1637144088745117 seconds
policy loss:23.771949768066406
value loss:9.939964294433594
entropies:25.648639678955078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1705687046051025 seconds
policy loss:-1126.80078125
value loss:23.31938362121582
entropies:39.76762390136719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1185.4618)
ToM Target loss= tensor(2312.9646)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2446651458740234 seconds
policy loss:-1285.891357421875
value loss:9.179523468017578
entropies:35.51945877075195
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2276105880737305 seconds
policy loss:-159.43551635742188
value loss:6.721407890319824
entropies:20.298564910888672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2175219058990479 seconds
policy loss:-272.92578125
value loss:10.273025512695312
entropies:32.96574783325195
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1671268939971924 seconds
policy loss:131.3929901123047
value loss:12.021673202514648
entropies:36.593814849853516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.178818702697754 seconds
policy loss:-1005.044189453125
value loss:13.291160583496094
entropies:49.517295837402344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1225.4064)
ToM Target loss= tensor(2277.8516)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1798365116119385 seconds
policy loss:-661.52880859375
value loss:31.512147903442383
entropies:37.281471252441406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2504332065582275 seconds
policy loss:-461.09808349609375
value loss:14.494396209716797
entropies:42.92036437988281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2188820838928223 seconds
policy loss:-79.2201919555664
value loss:14.928657531738281
entropies:32.60835647583008
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1658363342285156 seconds
policy loss:-1440.5299072265625
value loss:46.11453628540039
entropies:42.80939483642578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2443666458129883 seconds
policy loss:-3.916924238204956
value loss:14.346677780151367
entropies:32.62639617919922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1212.3655)
ToM Target loss= tensor(2179.3135)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2225744724273682 seconds
policy loss:-1498.2230224609375
value loss:69.0741958618164
entropies:42.1236457824707
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2297577857971191 seconds
policy loss:-510.5289306640625
value loss:19.62537384033203
entropies:59.73038864135742
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1604840755462646 seconds
policy loss:-114.22972106933594
value loss:16.456588745117188
entropies:32.76698303222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2115528583526611 seconds
policy loss:-631.4439697265625
value loss:26.118064880371094
entropies:39.010807037353516
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2321422100067139 seconds
policy loss:135.86209106445312
value loss:16.075336456298828
entropies:38.381553649902344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1290.6565)
ToM Target loss= tensor(2265.7769)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1727559566497803 seconds
policy loss:-813.855712890625
value loss:55.389015197753906
entropies:31.197742462158203
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2075822353363037 seconds
policy loss:-517.871826171875
value loss:24.146526336669922
entropies:48.342620849609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.176217794418335 seconds
policy loss:-435.24725341796875
value loss:17.5250244140625
entropies:33.95930862426758
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2410640716552734 seconds
policy loss:-117.30769348144531
value loss:13.53711986541748
entropies:31.56110191345215
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2545900344848633 seconds
policy loss:-12.084064483642578
value loss:10.832653999328613
entropies:29.107250213623047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1262.6843)
ToM Target loss= tensor(2276.8237)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2545676231384277 seconds
policy loss:-977.3810424804688
value loss:26.641132354736328
entropies:37.50724792480469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1765081882476807 seconds
policy loss:-866.7805786132812
value loss:14.07206916809082
entropies:32.50392150878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2233781814575195 seconds
policy loss:170.57965087890625
value loss:13.742300033569336
entropies:47.594947814941406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1718230247497559 seconds
policy loss:-347.9285888671875
value loss:14.213808059692383
entropies:45.55260467529297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1670055389404297 seconds
policy loss:33.42818832397461
value loss:10.266524314880371
entropies:29.93526268005371
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1242.3486)
ToM Target loss= tensor(2297.3208)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1754825115203857 seconds
policy loss:-308.40087890625
value loss:12.663437843322754
entropies:34.53076934814453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.208817958831787 seconds
policy loss:-355.69775390625
value loss:10.669118881225586
entropies:37.25404357910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1717982292175293 seconds
policy loss:-845.3859252929688
value loss:13.2728271484375
entropies:30.21413803100586
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2282509803771973 seconds
policy loss:-219.0426483154297
value loss:5.613039493560791
entropies:27.087383270263672
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1992204189300537 seconds
policy loss:-1050.18896484375
value loss:8.766507148742676
entropies:31.83639907836914
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1192.1271)
ToM Target loss= tensor(2250.9316)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1805613040924072 seconds
policy loss:-2005.4488525390625
value loss:33.089500427246094
entropies:49.71269607543945
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1688523292541504 seconds
policy loss:-797.9781494140625
value loss:7.909181118011475
entropies:34.78921127319336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2364656925201416 seconds
policy loss:-407.7901916503906
value loss:11.407249450683594
entropies:23.610794067382812
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2010014057159424 seconds
policy loss:-634.3297729492188
value loss:16.66407012939453
entropies:31.38612174987793
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1705784797668457 seconds
policy loss:-377.26043701171875
value loss:18.864805221557617
entropies:33.33997344970703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1248.5955)
ToM Target loss= tensor(2373.1614)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2360336780548096 seconds
policy loss:-147.4229278564453
value loss:50.87291717529297
entropies:33.20619201660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1776387691497803 seconds
policy loss:244.075439453125
value loss:7.6043572425842285
entropies:24.490039825439453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.215867280960083 seconds
policy loss:-578.3698120117188
value loss:21.59745979309082
entropies:49.28337860107422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2468249797821045 seconds
policy loss:-241.71142578125
value loss:16.229440689086914
entropies:32.293460845947266
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1820876598358154 seconds
policy loss:32.056541442871094
value loss:13.373519897460938
entropies:36.463138580322266
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1202.4497)
ToM Target loss= tensor(2304.6863)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1943905353546143 seconds
policy loss:-597.1783447265625
value loss:9.800992965698242
entropies:30.574111938476562
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1778545379638672 seconds
policy loss:-760.622802734375
value loss:22.893856048583984
entropies:37.9636116027832
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1589131355285645 seconds
policy loss:-566.7217407226562
value loss:12.201879501342773
entropies:40.32737350463867
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2427687644958496 seconds
policy loss:-824.0016479492188
value loss:19.3308048248291
entropies:44.257999420166016
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2311286926269531 seconds
policy loss:-127.55560302734375
value loss:8.694894790649414
entropies:19.637287139892578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1240.4044)
ToM Target loss= tensor(2306.8057)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2426164150238037 seconds
policy loss:-1591.8792724609375
value loss:55.60174560546875
entropies:41.07453918457031
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2339231967926025 seconds
policy loss:68.71509552001953
value loss:13.841300964355469
entropies:32.5167350769043
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2382690906524658 seconds
policy loss:-300.3249206542969
value loss:18.692033767700195
entropies:38.156002044677734
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1828527450561523 seconds
policy loss:115.01399993896484
value loss:11.571660041809082
entropies:26.134855270385742
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1873993873596191 seconds
policy loss:35.57752227783203
value loss:4.142763137817383
entropies:30.59365463256836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1187.2528)
ToM Target loss= tensor(2255.2012)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.181511402130127 seconds
policy loss:-813.1064453125
value loss:13.937620162963867
entropies:32.03988265991211
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2023499011993408 seconds
policy loss:-423.61273193359375
value loss:17.162551879882812
entropies:40.35850524902344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2388253211975098 seconds
policy loss:-1498.756103515625
value loss:30.47943115234375
entropies:43.049232482910156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2660412788391113 seconds
policy loss:-363.57464599609375
value loss:9.437602043151855
entropies:32.01220703125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1707837581634521 seconds
policy loss:-690.1138305664062
value loss:21.390554428100586
entropies:37.58610153198242
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1240.0166)
ToM Target loss= tensor(2304.2700)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2130095958709717 seconds
policy loss:-165.83729553222656
value loss:15.051284790039062
entropies:33.95956039428711
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2027313709259033 seconds
policy loss:-1658.5333251953125
value loss:29.278636932373047
entropies:49.16065216064453
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1658141613006592 seconds
policy loss:-476.15081787109375
value loss:25.572498321533203
entropies:39.4371337890625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2177419662475586 seconds
policy loss:11.786861419677734
value loss:17.63490104675293
entropies:26.831039428710938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1798832416534424 seconds
policy loss:-864.1362915039062
value loss:21.790069580078125
entropies:45.4697151184082
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1263.7781)
ToM Target loss= tensor(2363.8013)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.184852123260498 seconds
policy loss:-917.655517578125
value loss:12.309685707092285
entropies:41.820228576660156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1948888301849365 seconds
policy loss:-1012.7587890625
value loss:16.139554977416992
entropies:30.715797424316406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1746737957000732 seconds
policy loss:-566.4386596679688
value loss:13.124856948852539
entropies:44.46996307373047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1441130638122559 seconds
policy loss:381.96221923828125
value loss:12.766324996948242
entropies:42.59968566894531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1674418449401855 seconds
policy loss:86.20984649658203
value loss:14.94758415222168
entropies:40.88179016113281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1223.6063)
ToM Target loss= tensor(2363.4705)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1693127155303955 seconds
policy loss:-235.96795654296875
value loss:12.66502571105957
entropies:41.840171813964844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1662161350250244 seconds
policy loss:-1141.3206787109375
value loss:21.968727111816406
entropies:43.69537353515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1791749000549316 seconds
policy loss:15.9028902053833
value loss:10.993850708007812
entropies:24.059104919433594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2356045246124268 seconds
policy loss:-58.22425079345703
value loss:13.615349769592285
entropies:31.21466064453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2037913799285889 seconds
policy loss:-438.6576843261719
value loss:10.700000762939453
entropies:34.164546966552734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1258.4495)
ToM Target loss= tensor(2385.3032)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1728827953338623 seconds
policy loss:-1006.2316284179688
value loss:15.624208450317383
entropies:35.759315490722656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.175410509109497 seconds
policy loss:-511.06011962890625
value loss:10.182538986206055
entropies:30.480331420898438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1820342540740967 seconds
policy loss:-214.48744201660156
value loss:8.299577713012695
entropies:33.494789123535156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1804249286651611 seconds
policy loss:-732.2381591796875
value loss:19.714576721191406
entropies:30.645057678222656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1621224880218506 seconds
policy loss:-299.1697692871094
value loss:6.799923419952393
entropies:19.54018783569336
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1243.1680)
ToM Target loss= tensor(2412.3271)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1989281177520752 seconds
policy loss:-434.1800231933594
value loss:15.258443832397461
entropies:37.96906280517578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1719915866851807 seconds
policy loss:-663.7296142578125
value loss:20.58568000793457
entropies:25.977436065673828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2195487022399902 seconds
policy loss:-582.161376953125
value loss:21.9505615234375
entropies:31.620952606201172
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.239253044128418 seconds
policy loss:-265.99505615234375
value loss:14.336264610290527
entropies:39.7334098815918
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2203726768493652 seconds
policy loss:-708.014404296875
value loss:31.40260124206543
entropies:22.761812210083008
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1261.0256)
ToM Target loss= tensor(2249.9058)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.231309413909912 seconds
policy loss:82.91157531738281
value loss:19.3978328704834
entropies:42.91102600097656
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.18355131149292 seconds
policy loss:-179.5824737548828
value loss:11.615582466125488
entropies:30.130823135375977
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1750288009643555 seconds
policy loss:-461.7455139160156
value loss:18.712987899780273
entropies:38.17134475708008
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2342939376831055 seconds
policy loss:-1093.591064453125
value loss:19.39818572998047
entropies:32.708866119384766
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2327353954315186 seconds
policy loss:131.5852508544922
value loss:8.341714859008789
entropies:32.20269012451172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1239.3313)
ToM Target loss= tensor(2262.9795)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2089080810546875 seconds
policy loss:-597.5250244140625
value loss:37.371917724609375
entropies:29.63835906982422
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1711339950561523 seconds
policy loss:-1125.592041015625
value loss:20.694957733154297
entropies:39.732269287109375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2033517360687256 seconds
policy loss:-292.9043273925781
value loss:9.160409927368164
entropies:23.642894744873047
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.201845645904541 seconds
policy loss:-16.529909133911133
value loss:15.220849990844727
entropies:32.02825927734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2340061664581299 seconds
policy loss:-678.8732299804688
value loss:31.957889556884766
entropies:35.45854949951172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1165.0604)
ToM Target loss= tensor(2333.6931)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1748771667480469 seconds
policy loss:321.4593200683594
value loss:14.785560607910156
entropies:26.371187210083008
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1732585430145264 seconds
policy loss:335.70404052734375
value loss:11.39172077178955
entropies:23.630701065063477
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2276990413665771 seconds
policy loss:298.2442321777344
value loss:13.429945945739746
entropies:31.6185359954834
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1688222885131836 seconds
policy loss:-387.5168762207031
value loss:9.618668556213379
entropies:29.081544876098633
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.166093349456787 seconds
policy loss:-85.79365539550781
value loss:4.007590293884277
entropies:19.544532775878906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1154.6310)
ToM Target loss= tensor(2287.8506)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1813030242919922 seconds
policy loss:-498.3404541015625
value loss:11.483587265014648
entropies:30.90165901184082
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2097365856170654 seconds
policy loss:-1131.5631103515625
value loss:32.47846603393555
entropies:23.99057388305664
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2132232189178467 seconds
policy loss:-1444.21826171875
value loss:64.42695617675781
entropies:25.12124252319336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2475676536560059 seconds
policy loss:-699.5214233398438
value loss:12.086480140686035
entropies:28.707443237304688
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.198348045349121 seconds
policy loss:-918.587158203125
value loss:17.19839859008789
entropies:33.1055908203125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1155.2854)
ToM Target loss= tensor(2215.6628)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1964874267578125 seconds
policy loss:-1220.37158203125
value loss:33.18125915527344
entropies:32.60350799560547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1990017890930176 seconds
policy loss:-219.70169067382812
value loss:10.541199684143066
entropies:30.475112915039062
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.170583963394165 seconds
policy loss:-349.75872802734375
value loss:28.579822540283203
entropies:41.58087921142578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.232112169265747 seconds
policy loss:-54.2629508972168
value loss:30.423778533935547
entropies:29.24335289001465
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1858081817626953 seconds
policy loss:-38.66167449951172
value loss:39.306278228759766
entropies:25.989076614379883
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1160.7258)
ToM Target loss= tensor(2234.6887)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1658787727355957 seconds
policy loss:-249.33827209472656
value loss:28.76913070678711
entropies:32.7613639831543
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1724236011505127 seconds
policy loss:823.4649047851562
value loss:31.73590087890625
entropies:34.58790588378906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2172937393188477 seconds
policy loss:394.72918701171875
value loss:16.23341941833496
entropies:20.459186553955078
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.219057559967041 seconds
policy loss:-11.705769538879395
value loss:9.123412132263184
entropies:24.11925506591797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.171276569366455 seconds
policy loss:-184.31640625
value loss:19.610469818115234
entropies:30.065731048583984
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1111.2874)
ToM Target loss= tensor(2268.0728)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2056171894073486 seconds
policy loss:-1422.888671875
value loss:26.04193687438965
entropies:44.567413330078125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2397613525390625 seconds
policy loss:-894.547607421875
value loss:22.10365867614746
entropies:36.13959884643555
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.21708083152771 seconds
policy loss:-858.6519775390625
value loss:40.0998649597168
entropies:41.972686767578125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.200526237487793 seconds
policy loss:-801.8163452148438
value loss:16.369892120361328
entropies:25.12024688720703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1525228023529053 seconds
policy loss:-452.77899169921875
value loss:10.670066833496094
entropies:27.22994041442871
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1259.1456)
ToM Target loss= tensor(2357.0947)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.175828218460083 seconds
policy loss:-147.846923828125
value loss:13.41036605834961
entropies:35.52716064453125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2171308994293213 seconds
policy loss:-779.6608276367188
value loss:14.044925689697266
entropies:50.84862518310547
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2140450477600098 seconds
policy loss:-21.94373321533203
value loss:13.999743461608887
entropies:38.24275207519531
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1665399074554443 seconds
policy loss:-620.3968505859375
value loss:16.735624313354492
entropies:33.69453048706055
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.175234079360962 seconds
policy loss:-505.6168518066406
value loss:14.182130813598633
entropies:38.64741897583008
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1283.6984)
ToM Target loss= tensor(2279.4702)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1886706352233887 seconds
policy loss:-372.65753173828125
value loss:32.0899658203125
entropies:43.732818603515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1769890785217285 seconds
policy loss:-204.87770080566406
value loss:13.346419334411621
entropies:25.253158569335938
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1702227592468262 seconds
policy loss:233.64938354492188
value loss:17.157155990600586
entropies:33.27061462402344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1790339946746826 seconds
policy loss:-1251.890869140625
value loss:20.61592674255371
entropies:34.08411407470703
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1818232536315918 seconds
policy loss:-699.2960815429688
value loss:17.553852081298828
entropies:38.10719680786133
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1252.3224)
ToM Target loss= tensor(2243.9282)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2260489463806152 seconds
policy loss:261.32379150390625
value loss:17.954814910888672
entropies:33.032798767089844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2067112922668457 seconds
policy loss:-618.857177734375
value loss:8.249858856201172
entropies:34.9698486328125
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2064142227172852 seconds
policy loss:303.4324035644531
value loss:6.372707843780518
entropies:16.98834228515625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1722180843353271 seconds
policy loss:47.065494537353516
value loss:2.916163921356201
entropies:12.537242889404297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2328038215637207 seconds
policy loss:-489.3919677734375
value loss:26.630687713623047
entropies:31.65332794189453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1056.0244)
ToM Target loss= tensor(2189.9966)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2263751029968262 seconds
policy loss:-1069.834228515625
value loss:31.881093978881836
entropies:36.70972442626953
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1807403564453125 seconds
policy loss:-812.0379028320312
value loss:10.041882514953613
entropies:26.821613311767578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2590765953063965 seconds
policy loss:60.90230178833008
value loss:3.9458088874816895
entropies:15.988205909729004
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1631312370300293 seconds
policy loss:-552.4307250976562
value loss:13.272934913635254
entropies:29.313142776489258
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2427971363067627 seconds
policy loss:-760.4692993164062
value loss:29.88801383972168
entropies:25.0306396484375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1051.8046)
ToM Target loss= tensor(2204.0911)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2207896709442139 seconds
policy loss:-323.54705810546875
value loss:12.776559829711914
entropies:26.218639373779297
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2390518188476562 seconds
policy loss:-623.4839477539062
value loss:9.948996543884277
entropies:29.105241775512695
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1734821796417236 seconds
policy loss:-142.13450622558594
value loss:12.415345191955566
entropies:25.937131881713867
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2663400173187256 seconds
policy loss:-86.64273834228516
value loss:4.243340492248535
entropies:21.900741577148438
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2300612926483154 seconds
policy loss:-693.464111328125
value loss:45.466773986816406
entropies:41.286556243896484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1131.5720)
ToM Target loss= tensor(2171.3938)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2297005653381348 seconds
policy loss:-962.513916015625
value loss:57.05339813232422
entropies:33.50642013549805
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2309467792510986 seconds
policy loss:-360.6671142578125
value loss:20.96196746826172
entropies:39.88893127441406
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2507028579711914 seconds
policy loss:-129.22491455078125
value loss:11.566149711608887
entropies:37.610992431640625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.190000057220459 seconds
policy loss:-491.2454528808594
value loss:23.41382598876953
entropies:35.34757995605469
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1886403560638428 seconds
policy loss:175.73110961914062
value loss:9.385130882263184
entropies:12.817182540893555
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1164.8302)
ToM Target loss= tensor(2134.9583)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2068641185760498 seconds
policy loss:-1116.160888671875
value loss:45.246360778808594
entropies:40.76720428466797
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2094485759735107 seconds
policy loss:-1161.5394287109375
value loss:22.28192710876465
entropies:41.839805603027344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.237598180770874 seconds
policy loss:-1070.1666259765625
value loss:39.828285217285156
entropies:41.81474304199219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2355780601501465 seconds
policy loss:-520.3224487304688
value loss:18.34640884399414
entropies:30.221027374267578
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1648366451263428 seconds
policy loss:-143.5050506591797
value loss:11.144468307495117
entropies:33.50562286376953
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1167.2812)
ToM Target loss= tensor(2154.7102)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1722259521484375 seconds
policy loss:87.48777770996094
value loss:17.03896141052246
entropies:24.036762237548828
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1865057945251465 seconds
policy loss:-285.6582946777344
value loss:9.291918754577637
entropies:20.907630920410156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1684534549713135 seconds
policy loss:-458.5206604003906
value loss:24.417766571044922
entropies:43.30699157714844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1749913692474365 seconds
policy loss:-398.6073913574219
value loss:14.455286979675293
entropies:37.45066833496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1974148750305176 seconds
policy loss:-322.24017333984375
value loss:8.130640983581543
entropies:18.014663696289062
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1119.4996)
ToM Target loss= tensor(2236.7498)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2392702102661133 seconds
policy loss:-358.5591735839844
value loss:12.997447967529297
entropies:24.28481674194336
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2142786979675293 seconds
policy loss:18.37185287475586
value loss:9.821554183959961
entropies:32.04423522949219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2337863445281982 seconds
policy loss:-1063.3997802734375
value loss:31.33255958557129
entropies:42.433738708496094
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2360975742340088 seconds
policy loss:-607.7406005859375
value loss:15.556009292602539
entropies:34.183204650878906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1651618480682373 seconds
policy loss:-240.07916259765625
value loss:9.932740211486816
entropies:40.81028747558594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1239.2159)
ToM Target loss= tensor(2187.4163)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2415087223052979 seconds
policy loss:-776.779052734375
value loss:18.918106079101562
entropies:41.625572204589844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1796367168426514 seconds
policy loss:285.0463562011719
value loss:8.882367134094238
entropies:27.914596557617188
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2332196235656738 seconds
policy loss:-214.87466430664062
value loss:8.753978729248047
entropies:32.16081619262695
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2198951244354248 seconds
policy loss:-220.00299072265625
value loss:7.548781394958496
entropies:33.96565628051758
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.171428918838501 seconds
policy loss:-711.9761962890625
value loss:18.717540740966797
entropies:41.96369934082031
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1207.4521)
ToM Target loss= tensor(2233.9587)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1788856983184814 seconds
policy loss:-326.0788879394531
value loss:10.341442108154297
entropies:36.31455612182617
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.287757158279419 seconds
policy loss:-783.98828125
value loss:21.96251678466797
entropies:33.99427795410156
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2125904560089111 seconds
policy loss:-131.93836975097656
value loss:6.440523624420166
entropies:24.254417419433594
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2336540222167969 seconds
policy loss:-1011.2337646484375
value loss:10.713314056396484
entropies:33.32325744628906
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2090578079223633 seconds
policy loss:-314.2332763671875
value loss:8.634520530700684
entropies:32.7175178527832
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1112.6055)
ToM Target loss= tensor(2189.6870)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.1857764720916748 seconds
policy loss:-396.8338928222656
value loss:31.268512725830078
entropies:54.96528625488281
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2575209140777588 seconds
policy loss:242.37765502929688
value loss:11.689352035522461
entropies:32.176719665527344
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1845104694366455 seconds
policy loss:-734.4422607421875
value loss:18.546611785888672
entropies:38.54283142089844
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2237358093261719 seconds
policy loss:-803.3253784179688
value loss:22.359071731567383
entropies:43.654541015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.164994716644287 seconds
policy loss:-242.74362182617188
value loss:15.287534713745117
entropies:26.8917236328125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1319.2754)
ToM Target loss= tensor(2272.8318)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2167479991912842 seconds
policy loss:-1133.540283203125
value loss:18.248239517211914
entropies:35.35551834106445
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1784210205078125 seconds
policy loss:-971.0465087890625
value loss:12.697101593017578
entropies:33.23857116699219
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1807382106781006 seconds
policy loss:-675.7913818359375
value loss:8.674389839172363
entropies:22.524810791015625
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2231063842773438 seconds
policy loss:-275.2063903808594
value loss:14.086366653442383
entropies:35.520233154296875
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1516106128692627 seconds
policy loss:192.80001831054688
value loss:10.608368873596191
entropies:22.553892135620117
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1112.5060)
ToM Target loss= tensor(2180.2139)
optimized based on ToM loss
---------------------
gamma: 0.1
training start after waiting for 1.2089037895202637 seconds
policy loss:-168.10400390625
value loss:9.183134078979492
entropies:24.6341552734375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.2229533195495605 seconds
policy loss:-348.6049499511719
value loss:16.898265838623047
entropies:38.05511474609375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1613309383392334 seconds
policy loss:-283.14825439453125
value loss:18.880126953125
entropies:38.71771240234375
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.1547393798828125 seconds
policy loss:-98.98050689697266
value loss:13.746811866760254
entropies:31.40761947631836
Policy training finished
---------------------
gamma: 0.1
training start after waiting for 1.168506383895874 seconds
policy loss:-359.72052001953125
value loss:12.680789947509766
entropies:19.189029693603516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1079.5334)
ToM Target loss= tensor(2249.6106)
optimized based on ToM loss
---------------------
gamma: 0.10020000000000001
training start after waiting for 1.203916072845459 seconds
policy loss:-52.84983825683594
value loss:5.631466865539551
entropies:15.923550605773926
Policy training finished
---------------------
gamma: 0.10020000000000001
training start after waiting for 1.1991164684295654 seconds
policy loss:-723.4752197265625
value loss:31.432538986206055
entropies:34.83568572998047
Policy training finished
---------------------
gamma: 0.10020000000000001
training start after waiting for 1.1680498123168945 seconds
policy loss:-693.2691040039062
value loss:50.6556510925293
entropies:36.0238151550293
Policy training finished
---------------------
gamma: 0.10020000000000001
training start after waiting for 1.1696667671203613 seconds
policy loss:-1167.4005126953125
value loss:11.649776458740234
entropies:29.062252044677734
Policy training finished
---------------------
gamma: 0.10020000000000001
training start after waiting for 1.2346735000610352 seconds
policy loss:-1303.2203369140625
value loss:63.566131591796875
entropies:33.91402816772461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1122.0812)
ToM Target loss= tensor(2266.8882)
optimized based on ToM loss
---------------------
gamma: 0.10040040000000001
training start after waiting for 1.1647586822509766 seconds
policy loss:-102.64861297607422
value loss:8.53718090057373
entropies:22.53699493408203
Policy training finished
---------------------
gamma: 0.10040040000000001
training start after waiting for 1.170337438583374 seconds
policy loss:-470.7591857910156
value loss:12.015087127685547
entropies:27.556400299072266
Policy training finished
---------------------
gamma: 0.10040040000000001
training start after waiting for 1.2177822589874268 seconds
policy loss:-516.7322998046875
value loss:16.16562843322754
entropies:26.348159790039062
Policy training finished
---------------------
gamma: 0.10040040000000001
training start after waiting for 1.2044975757598877 seconds
policy loss:-352.37030029296875
value loss:17.900447845458984
entropies:26.182422637939453
Policy training finished
---------------------
gamma: 0.10040040000000001
training start after waiting for 1.1862547397613525 seconds
policy loss:-164.75634765625
value loss:19.794336318969727
entropies:37.06302261352539
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1108.1172)
ToM Target loss= tensor(2262.5308)
optimized based on ToM loss
---------------------
gamma: 0.10060120080000001
training start after waiting for 1.2293140888214111 seconds
policy loss:-944.9407348632812
value loss:22.21026039123535
entropies:33.38092803955078
Policy training finished
---------------------
gamma: 0.10060120080000001
training start after waiting for 1.1937403678894043 seconds
policy loss:-913.7077026367188
value loss:20.652080535888672
entropies:37.18388748168945
Policy training finished
---------------------
gamma: 0.10060120080000001
training start after waiting for 1.16994047164917 seconds
policy loss:-792.430419921875
value loss:14.065899848937988
entropies:30.57018280029297
Policy training finished
---------------------
gamma: 0.10060120080000001
training start after waiting for 1.1817829608917236 seconds
policy loss:-542.09228515625
value loss:24.209457397460938
entropies:26.775230407714844
Policy training finished
---------------------
gamma: 0.10060120080000001
training start after waiting for 1.2310729026794434 seconds
policy loss:26.269702911376953
value loss:22.20326042175293
entropies:34.519012451171875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1170.4822)
ToM Target loss= tensor(2178.5034)
optimized based on ToM loss
---------------------
gamma: 0.10080240320160001
training start after waiting for 1.1835954189300537 seconds
policy loss:-93.94599151611328
value loss:16.755889892578125
entropies:39.42852020263672
Policy training finished
---------------------
gamma: 0.10080240320160001
training start after waiting for 1.2142109870910645 seconds
policy loss:433.25677490234375
value loss:17.620651245117188
entropies:31.58769416809082
Policy training finished
---------------------
gamma: 0.10080240320160001
training start after waiting for 1.152151107788086 seconds
policy loss:-162.4092254638672
value loss:9.099851608276367
entropies:33.91189193725586
Policy training finished
---------------------
gamma: 0.10080240320160001
training start after waiting for 1.2167901992797852 seconds
policy loss:-9.887833595275879
value loss:19.40703773498535
entropies:32.15449905395508
Policy training finished
---------------------
gamma: 0.10080240320160001
training start after waiting for 1.2299742698669434 seconds
policy loss:-421.03369140625
value loss:16.41556739807129
entropies:29.117115020751953
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1250.8713)
ToM Target loss= tensor(2288.1956)
optimized based on ToM loss
---------------------
gamma: 0.10100400800800322
training start after waiting for 1.157222032546997 seconds
policy loss:172.8408966064453
value loss:13.459639549255371
entropies:34.5057373046875
Policy training finished
---------------------
gamma: 0.10100400800800322
training start after waiting for 1.2184491157531738 seconds
policy loss:-619.8023071289062
value loss:14.281702041625977
entropies:35.5019416809082
Policy training finished
---------------------
gamma: 0.10100400800800322
training start after waiting for 1.1452429294586182 seconds
policy loss:-337.3912048339844
value loss:9.92728042602539
entropies:25.122772216796875
Policy training finished
---------------------
gamma: 0.10100400800800322
training start after waiting for 1.2070808410644531 seconds
policy loss:60.52042007446289
value loss:20.493200302124023
entropies:39.64015197753906
Policy training finished
---------------------
gamma: 0.10100400800800322
training start after waiting for 1.1461491584777832 seconds
policy loss:314.94805908203125
value loss:26.129974365234375
entropies:41.97669982910156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1230.8859)
ToM Target loss= tensor(2266.9839)
optimized based on ToM loss
---------------------
gamma: 0.10120601602401923
training start after waiting for 1.201805830001831 seconds
policy loss:-7.950188636779785
value loss:20.403980255126953
entropies:32.53925323486328
Policy training finished
---------------------
gamma: 0.10120601602401923
training start after waiting for 1.1944375038146973 seconds
policy loss:397.95916748046875
value loss:17.20895767211914
entropies:28.381732940673828
Policy training finished
---------------------
gamma: 0.10120601602401923
training start after waiting for 1.1718182563781738 seconds
policy loss:-610.2785034179688
value loss:10.867438316345215
entropies:40.903358459472656
Policy training finished
---------------------
gamma: 0.10120601602401923
training start after waiting for 1.2396321296691895 seconds
policy loss:-1126.39599609375
value loss:11.802020072937012
entropies:25.868724822998047
Policy training finished
---------------------
gamma: 0.10120601602401923
training start after waiting for 1.2260377407073975 seconds
policy loss:-236.68960571289062
value loss:12.557884216308594
entropies:36.78800964355469
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1186.8555)
ToM Target loss= tensor(2264.2905)
optimized based on ToM loss
---------------------
gamma: 0.10140842805606727
training start after waiting for 1.1882152557373047 seconds
policy loss:-571.2288818359375
value loss:16.559158325195312
entropies:24.793399810791016
Policy training finished
---------------------
gamma: 0.10140842805606727
training start after waiting for 1.162675380706787 seconds
policy loss:41.860042572021484
value loss:6.866443634033203
entropies:31.366243362426758
Policy training finished
---------------------
gamma: 0.10140842805606727
training start after waiting for 1.1962883472442627 seconds
policy loss:-394.0803527832031
value loss:8.389259338378906
entropies:21.363739013671875
Policy training finished
---------------------
gamma: 0.10140842805606727
training start after waiting for 1.1959867477416992 seconds
policy loss:-359.99249267578125
value loss:4.169435977935791
entropies:32.89876174926758
Policy training finished
---------------------
gamma: 0.10140842805606727
training start after waiting for 1.2064697742462158 seconds
policy loss:-1338.340087890625
value loss:23.130237579345703
entropies:52.261932373046875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1190.9758)
ToM Target loss= tensor(2336.1687)
optimized based on ToM loss
---------------------
gamma: 0.1016112449121794
training start after waiting for 1.1476426124572754 seconds
policy loss:-62.162357330322266
value loss:6.363889217376709
entropies:36.23946762084961
Policy training finished
---------------------
gamma: 0.1016112449121794
training start after waiting for 1.193267583847046 seconds
policy loss:-569.6177978515625
value loss:13.542450904846191
entropies:26.224977493286133
Policy training finished
---------------------
gamma: 0.1016112449121794
training start after waiting for 1.1894774436950684 seconds
policy loss:-365.9743347167969
value loss:12.999316215515137
entropies:43.8163948059082
Policy training finished
---------------------
gamma: 0.1016112449121794
training start after waiting for 1.2164618968963623 seconds
policy loss:-414.6258850097656
value loss:15.436864852905273
entropies:37.44660568237305
Policy training finished
---------------------
gamma: 0.1016112449121794
training start after waiting for 1.1884582042694092 seconds
policy loss:-626.9067993164062
value loss:20.431474685668945
entropies:44.00646209716797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1213.1208)
ToM Target loss= tensor(2221.5544)
optimized based on ToM loss
---------------------
gamma: 0.10181446740200377
training start after waiting for 1.1741724014282227 seconds
policy loss:-276.2877502441406
value loss:10.98094367980957
entropies:35.43089294433594
Policy training finished
---------------------
gamma: 0.10181446740200377
training start after waiting for 1.1989407539367676 seconds
policy loss:-1411.392822265625
value loss:20.16488265991211
entropies:46.540733337402344
Policy training finished
---------------------
gamma: 0.10181446740200377
training start after waiting for 1.1911075115203857 seconds
policy loss:183.65179443359375
value loss:10.86349868774414
entropies:36.27197265625
Policy training finished
---------------------
gamma: 0.10181446740200377
training start after waiting for 1.1586227416992188 seconds
policy loss:-726.7236328125
value loss:27.921918869018555
entropies:38.59864044189453
Policy training finished
---------------------
gamma: 0.10181446740200377
training start after waiting for 1.1665985584259033 seconds
policy loss:-524.1934814453125
value loss:12.259785652160645
entropies:50.25129318237305
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1270.1974)
ToM Target loss= tensor(2218.9797)
optimized based on ToM loss
---------------------
gamma: 0.10201809633680778
training start after waiting for 1.2237095832824707 seconds
policy loss:12.199536323547363
value loss:9.541298866271973
entropies:27.7791748046875
Policy training finished
---------------------
gamma: 0.10201809633680778
training start after waiting for 1.232187271118164 seconds
policy loss:-995.248291015625
value loss:27.468233108520508
entropies:48.3839111328125
Policy training finished
---------------------
gamma: 0.10201809633680778
training start after waiting for 1.2207770347595215 seconds
policy loss:-1318.826416015625
value loss:24.649255752563477
entropies:54.42070007324219
Policy training finished
---------------------
gamma: 0.10201809633680778
training start after waiting for 1.2343015670776367 seconds
policy loss:-439.3716125488281
value loss:13.411347389221191
entropies:35.2595100402832
Policy training finished
---------------------
gamma: 0.10201809633680778
training start after waiting for 1.211522102355957 seconds
policy loss:-853.091552734375
value loss:17.55209732055664
entropies:36.59849548339844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1239.7808)
ToM Target loss= tensor(2266.3821)
optimized based on ToM loss
---------------------
gamma: 0.1022221325294814
training start after waiting for 1.2145264148712158 seconds
policy loss:-85.0833511352539
value loss:13.272764205932617
entropies:26.883474349975586
Policy training finished
---------------------
gamma: 0.1022221325294814
training start after waiting for 1.1573054790496826 seconds
policy loss:-1086.4111328125
value loss:27.65964126586914
entropies:51.465660095214844
Policy training finished
---------------------
gamma: 0.1022221325294814
training start after waiting for 1.2369744777679443 seconds
policy loss:-648.9122314453125
value loss:33.985069274902344
entropies:48.6923713684082
Policy training finished
---------------------
gamma: 0.1022221325294814
training start after waiting for 1.2041373252868652 seconds
policy loss:-98.06285858154297
value loss:19.936307907104492
entropies:33.399166107177734
Policy training finished
---------------------
gamma: 0.1022221325294814
training start after waiting for 1.2256982326507568 seconds
policy loss:-421.8407287597656
value loss:32.351585388183594
entropies:33.16233825683594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1283.5205)
ToM Target loss= tensor(2256.9521)
optimized based on ToM loss
---------------------
gamma: 0.10242657679454036
training start after waiting for 1.215301752090454 seconds
policy loss:335.67486572265625
value loss:27.50116539001465
entropies:29.495786666870117
Policy training finished
---------------------
gamma: 0.10242657679454036
training start after waiting for 1.1937789916992188 seconds
policy loss:171.16000366210938
value loss:14.316859245300293
entropies:39.30455780029297
Policy training finished
---------------------
gamma: 0.10242657679454036
training start after waiting for 1.1511163711547852 seconds
policy loss:-495.65802001953125
value loss:8.896346092224121
entropies:25.520238876342773
Policy training finished
---------------------
gamma: 0.10242657679454036
training start after waiting for 1.210200548171997 seconds
policy loss:-1295.86669921875
value loss:18.2832088470459
entropies:38.18968200683594
Policy training finished
---------------------
gamma: 0.10242657679454036
training start after waiting for 1.167053461074829 seconds
policy loss:-1010.50439453125
value loss:27.39392852783203
entropies:45.71764373779297
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1240.0867)
ToM Target loss= tensor(2254.7424)
optimized based on ToM loss
---------------------
gamma: 0.10263142994812943
training start after waiting for 1.1768748760223389 seconds
policy loss:-211.1558380126953
value loss:12.570413589477539
entropies:35.81952667236328
Policy training finished
---------------------
gamma: 0.10263142994812943
training start after waiting for 1.217714786529541 seconds
policy loss:-1000.6390380859375
value loss:10.704432487487793
entropies:28.303802490234375
Policy training finished
---------------------
gamma: 0.10263142994812943
training start after waiting for 1.1987545490264893 seconds
policy loss:-660.7359619140625
value loss:21.940601348876953
entropies:40.428627014160156
Policy training finished
---------------------
gamma: 0.10263142994812943
training start after waiting for 1.1736290454864502 seconds
policy loss:-993.6688232421875
value loss:22.01008415222168
entropies:42.67210388183594
Policy training finished
---------------------
gamma: 0.10263142994812943
training start after waiting for 1.1664834022521973 seconds
policy loss:-80.41230010986328
value loss:19.3690185546875
entropies:54.29249572753906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1264.5945)
ToM Target loss= tensor(2302.4407)
optimized based on ToM loss
---------------------
gamma: 0.1028366928080257
training start after waiting for 1.158250093460083 seconds
policy loss:-318.30657958984375
value loss:14.952208518981934
entropies:38.05565643310547
Policy training finished
---------------------
gamma: 0.1028366928080257
training start after waiting for 1.1521191596984863 seconds
policy loss:162.00283813476562
value loss:8.285245895385742
entropies:26.76997947692871
Policy training finished
---------------------
gamma: 0.1028366928080257
training start after waiting for 1.2057583332061768 seconds
policy loss:114.9432144165039
value loss:6.365357398986816
entropies:26.78740882873535
Policy training finished
---------------------
gamma: 0.1028366928080257
training start after waiting for 1.1674408912658691 seconds
policy loss:-1188.7930908203125
value loss:21.361509323120117
entropies:48.762874603271484
Policy training finished
---------------------
gamma: 0.1028366928080257
training start after waiting for 1.1879267692565918 seconds
policy loss:-312.09710693359375
value loss:16.835010528564453
entropies:34.20435333251953
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1221.4165)
ToM Target loss= tensor(2264.6304)
optimized based on ToM loss
---------------------
gamma: 0.10304236619364175
training start after waiting for 1.2157800197601318 seconds
policy loss:-651.454833984375
value loss:12.544294357299805
entropies:34.045841217041016
Policy training finished
---------------------
gamma: 0.10304236619364175
training start after waiting for 1.1579968929290771 seconds
policy loss:-1720.5557861328125
value loss:32.68399429321289
entropies:38.853782653808594
Policy training finished
---------------------
gamma: 0.10304236619364175
training start after waiting for 1.1680872440338135 seconds
policy loss:-1746.6524658203125
value loss:37.26587677001953
entropies:55.34956741333008
Policy training finished
---------------------
gamma: 0.10304236619364175
training start after waiting for 1.2019915580749512 seconds
policy loss:-254.45848083496094
value loss:7.951044082641602
entropies:25.152755737304688
Policy training finished
---------------------
gamma: 0.10304236619364175
training start after waiting for 1.2152044773101807 seconds
policy loss:148.17535400390625
value loss:13.680280685424805
entropies:37.955108642578125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1282.4675)
ToM Target loss= tensor(2315.3750)
optimized based on ToM loss
---------------------
gamma: 0.10324845092602904
training start after waiting for 1.1806414127349854 seconds
policy loss:-915.9730224609375
value loss:14.19194507598877
entropies:36.42665100097656
Policy training finished
---------------------
gamma: 0.10324845092602904
training start after waiting for 1.1991193294525146 seconds
policy loss:-230.27784729003906
value loss:15.084678649902344
entropies:35.06223678588867
Policy training finished
---------------------
gamma: 0.10324845092602904
training start after waiting for 1.1607024669647217 seconds
policy loss:-172.51766967773438
value loss:15.225772857666016
entropies:35.16041564941406
Policy training finished
---------------------
gamma: 0.10324845092602904
training start after waiting for 1.1547291278839111 seconds
policy loss:-167.34161376953125
value loss:7.97646951675415
entropies:21.908849716186523
Policy training finished
---------------------
gamma: 0.10324845092602904
training start after waiting for 1.2247085571289062 seconds
policy loss:-1737.42724609375
value loss:31.107025146484375
entropies:31.03499412536621
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1231.4999)
ToM Target loss= tensor(2359.2109)
optimized based on ToM loss
---------------------
gamma: 0.10345494782788109
training start after waiting for 1.200448751449585 seconds
policy loss:-752.1272583007812
value loss:10.799930572509766
entropies:20.204708099365234
Policy training finished
---------------------
gamma: 0.10345494782788109
training start after waiting for 1.2128887176513672 seconds
policy loss:-241.06312561035156
value loss:8.816033363342285
entropies:28.960813522338867
Policy training finished
---------------------
gamma: 0.10345494782788109
training start after waiting for 1.226891279220581 seconds
policy loss:363.0726318359375
value loss:8.47193717956543
entropies:28.046615600585938
Policy training finished
---------------------
gamma: 0.10345494782788109
training start after waiting for 1.2213177680969238 seconds
policy loss:96.47844696044922
value loss:14.787004470825195
entropies:18.28423309326172
Policy training finished
---------------------
gamma: 0.10345494782788109
training start after waiting for 1.2300095558166504 seconds
policy loss:-501.12353515625
value loss:9.490030288696289
entropies:26.75701904296875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1174.3270)
ToM Target loss= tensor(2393.1475)
optimized based on ToM loss
---------------------
gamma: 0.10366185772353685
training start after waiting for 1.169485330581665 seconds
policy loss:-522.2711791992188
value loss:12.331271171569824
entropies:34.42203903198242
Policy training finished
---------------------
gamma: 0.10366185772353685
training start after waiting for 1.22456955909729 seconds
policy loss:-757.99365234375
value loss:19.19062042236328
entropies:45.411949157714844
Policy training finished
---------------------
gamma: 0.10366185772353685
training start after waiting for 1.2250065803527832 seconds
policy loss:-1433.8302001953125
value loss:23.39826202392578
entropies:34.82789611816406
Policy training finished
---------------------
gamma: 0.10366185772353685
training start after waiting for 1.223973274230957 seconds
policy loss:-719.010498046875
value loss:18.001832962036133
entropies:28.99651527404785
Policy training finished
---------------------
gamma: 0.10366185772353685
training start after waiting for 1.1633117198944092 seconds
policy loss:-764.556884765625
value loss:21.31963539123535
entropies:33.26189041137695
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1319.9478)
ToM Target loss= tensor(2294.4626)
optimized based on ToM loss
---------------------
gamma: 0.10386918143898392
training start after waiting for 1.153289556503296 seconds
policy loss:-849.109375
value loss:16.829086303710938
entropies:26.223270416259766
Policy training finished
---------------------
gamma: 0.10386918143898392
training start after waiting for 1.1671311855316162 seconds
policy loss:-700.4269409179688
value loss:33.4627571105957
entropies:40.209747314453125
Policy training finished
---------------------
gamma: 0.10386918143898392
training start after waiting for 1.2130794525146484 seconds
policy loss:-154.25547790527344
value loss:11.27562141418457
entropies:19.770774841308594
Policy training finished
---------------------
gamma: 0.10386918143898392
training start after waiting for 1.157111644744873 seconds
policy loss:162.13075256347656
value loss:15.823716163635254
entropies:35.26350402832031
Policy training finished
---------------------
gamma: 0.10386918143898392
training start after waiting for 1.2342455387115479 seconds
policy loss:-81.15380096435547
value loss:8.536118507385254
entropies:20.36031723022461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1152.7529)
ToM Target loss= tensor(2256.0537)
optimized based on ToM loss
---------------------
gamma: 0.10407691980186189
training start after waiting for 1.1657700538635254 seconds
policy loss:-460.4925537109375
value loss:11.818880081176758
entropies:30.705928802490234
Policy training finished
---------------------
gamma: 0.10407691980186189
training start after waiting for 1.171616792678833 seconds
policy loss:-690.7927856445312
value loss:15.797628402709961
entropies:20.901533126831055
Policy training finished
---------------------
gamma: 0.10407691980186189
training start after waiting for 1.1979389190673828 seconds
policy loss:-677.28564453125
value loss:15.83221435546875
entropies:33.83615493774414
Policy training finished
---------------------
gamma: 0.10407691980186189
training start after waiting for 1.1710655689239502 seconds
policy loss:-1228.3858642578125
value loss:30.0537052154541
entropies:32.12849044799805
Policy training finished
---------------------
gamma: 0.10407691980186189
training start after waiting for 1.1914455890655518 seconds
policy loss:570.7433471679688
value loss:11.206551551818848
entropies:28.678054809570312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1102.4041)
ToM Target loss= tensor(2255.0493)
optimized based on ToM loss
---------------------
gamma: 0.10428507364146561
training start after waiting for 1.194223165512085 seconds
policy loss:-212.9280548095703
value loss:23.080276489257812
entropies:27.698564529418945
Policy training finished
---------------------
gamma: 0.10428507364146561
training start after waiting for 1.1885745525360107 seconds
policy loss:-907.8265991210938
value loss:15.682344436645508
entropies:26.9329891204834
Policy training finished
---------------------
gamma: 0.10428507364146561
training start after waiting for 1.1823618412017822 seconds
policy loss:-235.1884307861328
value loss:14.537206649780273
entropies:27.537660598754883
Policy training finished
---------------------
gamma: 0.10428507364146561
training start after waiting for 1.1867125034332275 seconds
policy loss:3.993051767349243
value loss:22.27808380126953
entropies:46.612144470214844
Policy training finished
---------------------
gamma: 0.10428507364146561
training start after waiting for 1.2341508865356445 seconds
policy loss:714.1795043945312
value loss:12.193143844604492
entropies:35.736114501953125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1337.9404)
ToM Target loss= tensor(2341.8521)
optimized based on ToM loss
---------------------
gamma: 0.10449364378874854
training start after waiting for 1.219285249710083 seconds
policy loss:-346.54571533203125
value loss:4.534066677093506
entropies:21.455177307128906
Policy training finished
---------------------
gamma: 0.10449364378874854
training start after waiting for 1.1855590343475342 seconds
policy loss:-135.43301391601562
value loss:7.925279140472412
entropies:17.46253204345703
Policy training finished
---------------------
gamma: 0.10449364378874854
training start after waiting for 1.1653623580932617 seconds
policy loss:-1508.744873046875
value loss:20.526968002319336
entropies:41.68744659423828
Policy training finished
---------------------
gamma: 0.10449364378874854
training start after waiting for 1.2125113010406494 seconds
policy loss:-1528.4312744140625
value loss:24.7076358795166
entropies:36.6212043762207
Policy training finished
---------------------
gamma: 0.10449364378874854
training start after waiting for 1.1632826328277588 seconds
policy loss:-1015.2981567382812
value loss:11.894500732421875
entropies:34.054344177246094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1213.8203)
ToM Target loss= tensor(2339.7234)
optimized based on ToM loss
---------------------
gamma: 0.10470263107632603
training start after waiting for 1.153576374053955 seconds
policy loss:-868.4605712890625
value loss:23.37327003479004
entropies:28.092262268066406
Policy training finished
---------------------
gamma: 0.10470263107632603
training start after waiting for 1.16642427444458 seconds
policy loss:-35.04580307006836
value loss:12.790703773498535
entropies:37.3856086730957
Policy training finished
---------------------
gamma: 0.10470263107632603
training start after waiting for 1.2061283588409424 seconds
policy loss:-58.959808349609375
value loss:14.797605514526367
entropies:32.00288391113281
Policy training finished
---------------------
gamma: 0.10470263107632603
training start after waiting for 1.1937134265899658 seconds
policy loss:-503.3782043457031
value loss:42.49636459350586
entropies:48.84231948852539
Policy training finished
---------------------
gamma: 0.10470263107632603
training start after waiting for 1.1699018478393555 seconds
policy loss:126.80874633789062
value loss:18.421142578125
entropies:28.30704689025879
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1290.0208)
ToM Target loss= tensor(2375.8438)
optimized based on ToM loss
---------------------
gamma: 0.10491203633847869
training start after waiting for 1.226017951965332 seconds
policy loss:-343.47686767578125
value loss:21.607961654663086
entropies:36.79305648803711
Policy training finished
---------------------
gamma: 0.10491203633847869
training start after waiting for 1.220435619354248 seconds
policy loss:-1988.1278076171875
value loss:33.944175720214844
entropies:39.01431655883789
Policy training finished
---------------------
gamma: 0.10491203633847869
training start after waiting for 1.199946403503418 seconds
policy loss:-663.327392578125
value loss:13.18294620513916
entropies:26.758682250976562
Policy training finished
---------------------
gamma: 0.10491203633847869
training start after waiting for 1.2180657386779785 seconds
policy loss:-597.9654541015625
value loss:17.64066505432129
entropies:22.45606231689453
Policy training finished
---------------------
gamma: 0.10491203633847869
training start after waiting for 1.1721410751342773 seconds
policy loss:-500.0558776855469
value loss:9.689495086669922
entropies:29.55936050415039
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1237.5460)
ToM Target loss= tensor(2333.8577)
optimized based on ToM loss
---------------------
gamma: 0.10512186041115565
training start after waiting for 1.1665430068969727 seconds
policy loss:-803.314208984375
value loss:11.820775985717773
entropies:24.78244972229004
Policy training finished
---------------------
gamma: 0.10512186041115565
training start after waiting for 1.161961317062378 seconds
policy loss:-1362.6492919921875
value loss:23.853343963623047
entropies:40.54152297973633
Policy training finished
---------------------
gamma: 0.10512186041115565
training start after waiting for 1.162994146347046 seconds
policy loss:-1381.2452392578125
value loss:27.77677345275879
entropies:39.55112838745117
Policy training finished
---------------------
gamma: 0.10512186041115565
training start after waiting for 1.157522201538086 seconds
policy loss:-183.62327575683594
value loss:17.788867950439453
entropies:37.95570373535156
Policy training finished
---------------------
gamma: 0.10512186041115565
training start after waiting for 1.2229061126708984 seconds
policy loss:208.7347412109375
value loss:5.022944927215576
entropies:31.22852897644043
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1230.2577)
ToM Target loss= tensor(2321.4202)
optimized based on ToM loss
---------------------
gamma: 0.10533210413197797
training start after waiting for 1.1841638088226318 seconds
policy loss:689.9014892578125
value loss:17.033029556274414
entropies:24.017921447753906
Policy training finished
---------------------
gamma: 0.10533210413197797
training start after waiting for 1.1574962139129639 seconds
policy loss:-201.5840301513672
value loss:5.7247161865234375
entropies:24.397647857666016
Policy training finished
---------------------
gamma: 0.10533210413197797
training start after waiting for 1.204136610031128 seconds
policy loss:-284.9031066894531
value loss:8.681532859802246
entropies:21.661376953125
Policy training finished
---------------------
gamma: 0.10533210413197797
training start after waiting for 1.1967942714691162 seconds
policy loss:-489.239013671875
value loss:10.67911148071289
entropies:31.22978973388672
Policy training finished
---------------------
gamma: 0.10533210413197797
training start after waiting for 1.1544535160064697 seconds
policy loss:55.709571838378906
value loss:13.591275215148926
entropies:33.28496551513672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1186.2705)
ToM Target loss= tensor(2395.4382)
optimized based on ToM loss
---------------------
gamma: 0.10554276834024193
training start after waiting for 1.142425775527954 seconds
policy loss:-932.1493530273438
value loss:14.8665771484375
entropies:32.4797477722168
Policy training finished
---------------------
gamma: 0.10554276834024193
training start after waiting for 1.2056853771209717 seconds
policy loss:-373.49310302734375
value loss:18.11166000366211
entropies:29.791576385498047
Policy training finished
---------------------
gamma: 0.10554276834024193
training start after waiting for 1.2014286518096924 seconds
policy loss:-611.4572143554688
value loss:11.5371732711792
entropies:34.71310043334961
Policy training finished
---------------------
gamma: 0.10554276834024193
training start after waiting for 1.1699633598327637 seconds
policy loss:-168.026123046875
value loss:14.35633659362793
entropies:40.105247497558594
Policy training finished
---------------------
gamma: 0.10554276834024193
training start after waiting for 1.2176151275634766 seconds
policy loss:383.1053161621094
value loss:9.312700271606445
entropies:31.9591064453125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1228.1763)
ToM Target loss= tensor(2329.8232)
optimized based on ToM loss
---------------------
gamma: 0.1057538538769224
training start after waiting for 1.2284584045410156 seconds
policy loss:-1978.23974609375
value loss:31.419116973876953
entropies:26.412372589111328
Policy training finished
---------------------
gamma: 0.1057538538769224
training start after waiting for 1.1574528217315674 seconds
policy loss:-905.8057861328125
value loss:19.80583381652832
entropies:35.5986213684082
Policy training finished
---------------------
gamma: 0.1057538538769224
training start after waiting for 1.2116525173187256 seconds
policy loss:-1098.7706298828125
value loss:22.80190658569336
entropies:45.139225006103516
Policy training finished
---------------------
gamma: 0.1057538538769224
training start after waiting for 1.1623990535736084 seconds
policy loss:-646.7132568359375
value loss:35.7030029296875
entropies:37.03457260131836
Policy training finished
---------------------
gamma: 0.1057538538769224
training start after waiting for 1.1740145683288574 seconds
policy loss:224.7379608154297
value loss:9.303874015808105
entropies:24.118045806884766
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1231.5156)
ToM Target loss= tensor(2306.2627)
optimized based on ToM loss
---------------------
gamma: 0.10596536158467625
training start after waiting for 1.1920490264892578 seconds
policy loss:5.836006164550781
value loss:14.378561019897461
entropies:27.462360382080078
Policy training finished
---------------------
gamma: 0.10596536158467625
training start after waiting for 1.1583397388458252 seconds
policy loss:93.08174133300781
value loss:21.946252822875977
entropies:38.91590881347656
Policy training finished
---------------------
gamma: 0.10596536158467625
training start after waiting for 1.2233235836029053 seconds
policy loss:209.0982208251953
value loss:13.088052749633789
entropies:23.44508934020996
Policy training finished
---------------------
gamma: 0.10596536158467625
training start after waiting for 1.1611101627349854 seconds
policy loss:158.00668334960938
value loss:14.335159301757812
entropies:26.514877319335938
Policy training finished
---------------------
gamma: 0.10596536158467625
training start after waiting for 1.2239532470703125 seconds
policy loss:-907.7531127929688
value loss:33.56937026977539
entropies:35.50397872924805
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1135.1846)
ToM Target loss= tensor(2252.3530)
optimized based on ToM loss
---------------------
gamma: 0.1061772923078456
training start after waiting for 1.208160638809204 seconds
policy loss:38.4162483215332
value loss:10.969264030456543
entropies:27.511295318603516
Policy training finished
---------------------
gamma: 0.1061772923078456
training start after waiting for 1.1974828243255615 seconds
policy loss:-1298.154296875
value loss:34.88523483276367
entropies:41.563880920410156
Policy training finished
---------------------
gamma: 0.1061772923078456
training start after waiting for 1.2102832794189453 seconds
policy loss:-488.4581298828125
value loss:17.09813117980957
entropies:35.39685821533203
Policy training finished
---------------------
gamma: 0.1061772923078456
training start after waiting for 1.1524286270141602 seconds
policy loss:9.172124862670898
value loss:12.231197357177734
entropies:35.31684112548828
Policy training finished
---------------------
gamma: 0.1061772923078456
training start after waiting for 1.213778018951416 seconds
policy loss:-620.314453125
value loss:12.34591293334961
entropies:33.22642517089844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1153.7554)
ToM Target loss= tensor(2283.1292)
optimized based on ToM loss
---------------------
gamma: 0.10638964689246129
training start after waiting for 1.2188200950622559 seconds
policy loss:219.9544677734375
value loss:17.619802474975586
entropies:35.497467041015625
Policy training finished
---------------------
gamma: 0.10638964689246129
training start after waiting for 1.1863024234771729 seconds
policy loss:315.026123046875
value loss:12.305662155151367
entropies:28.504615783691406
Policy training finished
---------------------
gamma: 0.10638964689246129
training start after waiting for 1.222785472869873 seconds
policy loss:-316.37152099609375
value loss:35.925201416015625
entropies:29.281051635742188
Policy training finished
---------------------
gamma: 0.10638964689246129
training start after waiting for 1.2114176750183105 seconds
policy loss:19.753982543945312
value loss:13.86367416381836
entropies:31.96812629699707
Policy training finished
---------------------
gamma: 0.10638964689246129
training start after waiting for 1.1927969455718994 seconds
policy loss:373.61456298828125
value loss:18.897415161132812
entropies:33.878265380859375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1148.8195)
ToM Target loss= tensor(2322.9602)
optimized based on ToM loss
---------------------
gamma: 0.10660242618624621
training start after waiting for 1.1972503662109375 seconds
policy loss:-411.1468811035156
value loss:20.487455368041992
entropies:31.029216766357422
Policy training finished
---------------------
gamma: 0.10660242618624621
training start after waiting for 1.1417500972747803 seconds
policy loss:-1152.91357421875
value loss:31.633108139038086
entropies:45.11174011230469
Policy training finished
---------------------
gamma: 0.10660242618624621
training start after waiting for 1.1461801528930664 seconds
policy loss:57.25151062011719
value loss:8.04836654663086
entropies:27.8719482421875
Policy training finished
---------------------
gamma: 0.10660242618624621
training start after waiting for 1.1796448230743408 seconds
policy loss:183.7155303955078
value loss:10.70411491394043
entropies:22.059816360473633
Policy training finished
---------------------
gamma: 0.10660242618624621
training start after waiting for 1.1477437019348145 seconds
policy loss:-1201.880859375
value loss:17.88616943359375
entropies:40.90869140625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1177.1025)
ToM Target loss= tensor(2268.2043)
optimized based on ToM loss
---------------------
gamma: 0.1068156310386187
training start after waiting for 1.181018590927124 seconds
policy loss:-1729.2730712890625
value loss:38.182621002197266
entropies:47.613922119140625
Policy training finished
---------------------
gamma: 0.1068156310386187
training start after waiting for 1.2063093185424805 seconds
policy loss:-466.98895263671875
value loss:16.166080474853516
entropies:34.23878860473633
Policy training finished
---------------------
gamma: 0.1068156310386187
training start after waiting for 1.1848080158233643 seconds
policy loss:225.4957733154297
value loss:8.49068832397461
entropies:18.847206115722656
Policy training finished
---------------------
gamma: 0.1068156310386187
training start after waiting for 1.168877363204956 seconds
policy loss:-402.62396240234375
value loss:43.727561950683594
entropies:39.23005676269531
Policy training finished
---------------------
gamma: 0.1068156310386187
training start after waiting for 1.2117226123809814 seconds
policy loss:-414.3809814453125
value loss:19.358261108398438
entropies:40.54951477050781
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1190.9723)
ToM Target loss= tensor(2231.3916)
optimized based on ToM loss
---------------------
gamma: 0.10702926230069594
training start after waiting for 1.1919817924499512 seconds
policy loss:-78.74488830566406
value loss:24.48036766052246
entropies:33.118797302246094
Policy training finished
---------------------
gamma: 0.10702926230069594
training start after waiting for 1.1870882511138916 seconds
policy loss:-366.3957214355469
value loss:27.514467239379883
entropies:40.03731155395508
Policy training finished
---------------------
gamma: 0.10702926230069594
training start after waiting for 1.1793687343597412 seconds
policy loss:-698.6754760742188
value loss:30.577505111694336
entropies:37.185829162597656
Policy training finished
---------------------
gamma: 0.10702926230069594
training start after waiting for 1.133476734161377 seconds
policy loss:-456.6588439941406
value loss:22.83066177368164
entropies:37.642311096191406
Policy training finished
---------------------
gamma: 0.10702926230069594
training start after waiting for 1.155149221420288 seconds
policy loss:-52.88121795654297
value loss:13.725546836853027
entropies:25.168155670166016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1237.9104)
ToM Target loss= tensor(2273.7151)
optimized based on ToM loss
---------------------
gamma: 0.10724332082529733
training start after waiting for 1.2021019458770752 seconds
policy loss:178.94749450683594
value loss:6.866025924682617
entropies:22.78409194946289
Policy training finished
---------------------
gamma: 0.10724332082529733
training start after waiting for 1.1918344497680664 seconds
policy loss:-555.64208984375
value loss:12.263145446777344
entropies:37.77714920043945
Policy training finished
---------------------
gamma: 0.10724332082529733
training start after waiting for 1.2107081413269043 seconds
policy loss:-643.6416625976562
value loss:13.22716236114502
entropies:33.13045883178711
Policy training finished
---------------------
gamma: 0.10724332082529733
training start after waiting for 1.2111632823944092 seconds
policy loss:-789.36279296875
value loss:23.96926498413086
entropies:37.18231964111328
Policy training finished
---------------------
gamma: 0.10724332082529733
training start after waiting for 1.1759107112884521 seconds
policy loss:-374.3936462402344
value loss:4.508459568023682
entropies:20.354263305664062
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1114.3873)
ToM Target loss= tensor(2255.6638)
optimized based on ToM loss
---------------------
gamma: 0.10745780746694793
training start after waiting for 1.1120021343231201 seconds
policy loss:99.84593963623047
value loss:7.982306480407715
entropies:20.76483154296875
Policy training finished
---------------------
gamma: 0.10745780746694793
training start after waiting for 1.165687084197998 seconds
policy loss:-298.6527099609375
value loss:12.32947826385498
entropies:31.979772567749023
Policy training finished
---------------------
gamma: 0.10745780746694793
training start after waiting for 1.19331955909729 seconds
policy loss:-1289.16259765625
value loss:38.862979888916016
entropies:52.087806701660156
Policy training finished
---------------------
gamma: 0.10745780746694793
training start after waiting for 1.1860380172729492 seconds
policy loss:200.23336791992188
value loss:14.50432014465332
entropies:26.38031005859375
Policy training finished
---------------------
gamma: 0.10745780746694793
training start after waiting for 1.1995019912719727 seconds
policy loss:-923.192626953125
value loss:20.104816436767578
entropies:36.247493743896484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1158.8322)
ToM Target loss= tensor(2256.4492)
optimized based on ToM loss
---------------------
gamma: 0.10767272308188182
training start after waiting for 1.1476714611053467 seconds
policy loss:-445.0504150390625
value loss:14.72870922088623
entropies:39.26744079589844
Policy training finished
---------------------
gamma: 0.10767272308188182
training start after waiting for 1.1705036163330078 seconds
policy loss:297.50244140625
value loss:20.329124450683594
entropies:38.460044860839844
Policy training finished
---------------------
gamma: 0.10767272308188182
training start after waiting for 1.2143359184265137 seconds
policy loss:-321.0815734863281
value loss:20.147138595581055
entropies:48.68956756591797
Policy training finished
---------------------
gamma: 0.10767272308188182
training start after waiting for 1.206885576248169 seconds
policy loss:-1096.25048828125
value loss:24.976715087890625
entropies:38.0318489074707
Policy training finished
---------------------
gamma: 0.10767272308188182
training start after waiting for 1.1978201866149902 seconds
policy loss:55.267093658447266
value loss:6.934369087219238
entropies:21.61235809326172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1387.7301)
ToM Target loss= tensor(2341.1184)
optimized based on ToM loss
---------------------
gamma: 0.10788806852804558
training start after waiting for 1.1661977767944336 seconds
policy loss:-307.278076171875
value loss:10.0068941116333
entropies:19.03542709350586
Policy training finished
---------------------
gamma: 0.10788806852804558
training start after waiting for 1.2088539600372314 seconds
policy loss:-455.77392578125
value loss:8.881795883178711
entropies:34.337120056152344
Policy training finished
---------------------
gamma: 0.10788806852804558
training start after waiting for 1.1557347774505615 seconds
policy loss:-329.666015625
value loss:18.061996459960938
entropies:38.671974182128906
Policy training finished
---------------------
gamma: 0.10788806852804558
training start after waiting for 1.231877326965332 seconds
policy loss:-382.01690673828125
value loss:14.74712085723877
entropies:30.646615982055664
Policy training finished
---------------------
gamma: 0.10788806852804558
training start after waiting for 1.1444716453552246 seconds
policy loss:-456.6296691894531
value loss:11.322888374328613
entropies:33.52006912231445
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1175.2490)
ToM Target loss= tensor(2370.4653)
optimized based on ToM loss
---------------------
gamma: 0.10810384466510167
training start after waiting for 1.1461091041564941 seconds
policy loss:-257.3047790527344
value loss:11.612190246582031
entropies:32.3509635925293
Policy training finished
---------------------
gamma: 0.10810384466510167
training start after waiting for 1.1628620624542236 seconds
policy loss:-571.7593383789062
value loss:17.342044830322266
entropies:36.29186248779297
Policy training finished
---------------------
gamma: 0.10810384466510167
training start after waiting for 1.1991047859191895 seconds
policy loss:-91.11751556396484
value loss:12.837400436401367
entropies:24.528697967529297
Policy training finished
---------------------
gamma: 0.10810384466510167
training start after waiting for 1.177485704421997 seconds
policy loss:-205.58868408203125
value loss:9.60213851928711
entropies:28.673616409301758
Policy training finished
---------------------
gamma: 0.10810384466510167
training start after waiting for 1.1402819156646729 seconds
policy loss:-122.44818115234375
value loss:8.104159355163574
entropies:29.754268646240234
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1202.7815)
ToM Target loss= tensor(2299.2083)
optimized based on ToM loss
---------------------
gamma: 0.10832005235443187
training start after waiting for 1.1977863311767578 seconds
policy loss:101.94707489013672
value loss:14.690860748291016
entropies:31.812667846679688
Policy training finished
---------------------
gamma: 0.10832005235443187
training start after waiting for 1.1820766925811768 seconds
policy loss:-1334.7469482421875
value loss:45.88475036621094
entropies:42.76719284057617
Policy training finished
---------------------
gamma: 0.10832005235443187
training start after waiting for 1.1395161151885986 seconds
policy loss:-1763.4154052734375
value loss:22.187068939208984
entropies:36.6385498046875
Policy training finished
---------------------
gamma: 0.10832005235443187
training start after waiting for 1.1904096603393555 seconds
policy loss:-1167.2457275390625
value loss:15.829301834106445
entropies:43.9483528137207
Policy training finished
---------------------
gamma: 0.10832005235443187
training start after waiting for 1.2034947872161865 seconds
policy loss:43.46532440185547
value loss:5.900914192199707
entropies:22.952880859375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1237.9666)
ToM Target loss= tensor(2270.0417)
optimized based on ToM loss
---------------------
gamma: 0.10853669245914074
training start after waiting for 1.2022194862365723 seconds
policy loss:109.98412322998047
value loss:6.078396797180176
entropies:19.83470916748047
Policy training finished
---------------------
gamma: 0.10853669245914074
training start after waiting for 1.1771011352539062 seconds
policy loss:-528.48876953125
value loss:39.549842834472656
entropies:31.92392349243164
Policy training finished
---------------------
gamma: 0.10853669245914074
training start after waiting for 1.1843295097351074 seconds
policy loss:-174.95626831054688
value loss:13.63066577911377
entropies:28.378538131713867
Policy training finished
---------------------
gamma: 0.10853669245914074
training start after waiting for 1.2075090408325195 seconds
policy loss:31.063621520996094
value loss:25.18790054321289
entropies:31.31340217590332
Policy training finished
---------------------
gamma: 0.10853669245914074
training start after waiting for 1.2156312465667725 seconds
policy loss:-97.21471405029297
value loss:62.029083251953125
entropies:29.149517059326172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1103.0603)
ToM Target loss= tensor(2197.3728)
optimized based on ToM loss
---------------------
gamma: 0.10875376584405902
training start after waiting for 1.1549038887023926 seconds
policy loss:-632.9883422851562
value loss:20.0394344329834
entropies:38.7982292175293
Policy training finished
---------------------
gamma: 0.10875376584405902
training start after waiting for 1.2053921222686768 seconds
policy loss:-80.72715759277344
value loss:19.564138412475586
entropies:27.281787872314453
Policy training finished
---------------------
gamma: 0.10875376584405902
training start after waiting for 1.1470105648040771 seconds
policy loss:-23.43071174621582
value loss:12.500898361206055
entropies:16.321762084960938
Policy training finished
---------------------
gamma: 0.10875376584405902
training start after waiting for 1.1730151176452637 seconds
policy loss:-164.19085693359375
value loss:6.375774383544922
entropies:37.514156341552734
Policy training finished
---------------------
gamma: 0.10875376584405902
training start after waiting for 1.2354445457458496 seconds
policy loss:71.6562271118164
value loss:10.496525764465332
entropies:27.03883934020996
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1205.5403)
ToM Target loss= tensor(2434.6089)
optimized based on ToM loss
---------------------
gamma: 0.10897127337574714
training start after waiting for 1.1922111511230469 seconds
policy loss:-60.58558654785156
value loss:8.105746269226074
entropies:22.451183319091797
Policy training finished
---------------------
gamma: 0.10897127337574714
training start after waiting for 1.148430585861206 seconds
policy loss:-1075.3507080078125
value loss:16.507400512695312
entropies:30.477882385253906
Policy training finished
---------------------
gamma: 0.10897127337574714
training start after waiting for 1.1506102085113525 seconds
policy loss:-1366.788330078125
value loss:27.21765899658203
entropies:25.933998107910156
Policy training finished
---------------------
gamma: 0.10897127337574714
training start after waiting for 1.204488754272461 seconds
policy loss:60.32843780517578
value loss:3.0106165409088135
entropies:15.508621215820312
Policy training finished
---------------------
gamma: 0.10897127337574714
training start after waiting for 1.2082960605621338 seconds
policy loss:-681.990966796875
value loss:11.86587142944336
entropies:36.13495635986328
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1146.7507)
ToM Target loss= tensor(2371.0344)
optimized based on ToM loss
---------------------
gamma: 0.10918921592249863
training start after waiting for 1.2054147720336914 seconds
policy loss:20.751697540283203
value loss:14.09568977355957
entropies:21.793865203857422
Policy training finished
---------------------
gamma: 0.10918921592249863
training start after waiting for 1.1493117809295654 seconds
policy loss:-312.5281677246094
value loss:32.88251495361328
entropies:34.82190704345703
Policy training finished
---------------------
gamma: 0.10918921592249863
training start after waiting for 1.1878926753997803 seconds
policy loss:-657.65087890625
value loss:19.308629989624023
entropies:34.208518981933594
Policy training finished
---------------------
gamma: 0.10918921592249863
training start after waiting for 1.1456573009490967 seconds
policy loss:-446.2275390625
value loss:15.257497787475586
entropies:38.87364959716797
Policy training finished
---------------------
gamma: 0.10918921592249863
training start after waiting for 1.1445317268371582 seconds
policy loss:-302.4574890136719
value loss:21.354238510131836
entropies:35.81541061401367
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1215.5088)
ToM Target loss= tensor(2350.2612)
optimized based on ToM loss
---------------------
gamma: 0.10940759435434362
training start after waiting for 1.2056870460510254 seconds
policy loss:63.71373748779297
value loss:6.76320743560791
entropies:23.08308219909668
Policy training finished
---------------------
gamma: 0.10940759435434362
training start after waiting for 1.1877193450927734 seconds
policy loss:-1472.1612548828125
value loss:29.853769302368164
entropies:39.33687973022461
Policy training finished
---------------------
gamma: 0.10940759435434362
training start after waiting for 1.1827776432037354 seconds
policy loss:-956.931640625
value loss:13.517696380615234
entropies:28.635208129882812
Policy training finished
---------------------
gamma: 0.10940759435434362
training start after waiting for 1.2048537731170654 seconds
policy loss:-1129.230712890625
value loss:17.907636642456055
entropies:28.889728546142578
Policy training finished
---------------------
gamma: 0.10940759435434362
training start after waiting for 1.1853678226470947 seconds
policy loss:-1305.4041748046875
value loss:27.198894500732422
entropies:38.363338470458984
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1189.5048)
ToM Target loss= tensor(2267.7505)
optimized based on ToM loss
---------------------
gamma: 0.1096264095430523
training start after waiting for 1.1840853691101074 seconds
policy loss:-563.141357421875
value loss:15.179914474487305
entropies:34.82683563232422
Policy training finished
---------------------
gamma: 0.1096264095430523
training start after waiting for 1.161374807357788 seconds
policy loss:-848.9598388671875
value loss:25.38654899597168
entropies:49.911903381347656
Policy training finished
---------------------
gamma: 0.1096264095430523
training start after waiting for 1.1684627532958984 seconds
policy loss:78.75651550292969
value loss:11.02902889251709
entropies:21.33626365661621
Policy training finished
---------------------
gamma: 0.1096264095430523
training start after waiting for 1.1427943706512451 seconds
policy loss:-464.5887145996094
value loss:29.175931930541992
entropies:26.32168197631836
Policy training finished
---------------------
gamma: 0.1096264095430523
training start after waiting for 1.1663391590118408 seconds
policy loss:524.64111328125
value loss:8.216883659362793
entropies:30.208255767822266
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1199.2822)
ToM Target loss= tensor(2330.1897)
optimized based on ToM loss
---------------------
gamma: 0.10984566236213841
training start after waiting for 1.1965603828430176 seconds
policy loss:546.4547729492188
value loss:9.73659896850586
entropies:26.09542465209961
Policy training finished
---------------------
gamma: 0.10984566236213841
training start after waiting for 1.1976211071014404 seconds
policy loss:-306.5376892089844
value loss:11.199810028076172
entropies:38.53839111328125
Policy training finished
---------------------
gamma: 0.10984566236213841
training start after waiting for 1.1982841491699219 seconds
policy loss:-294.9709167480469
value loss:15.684553146362305
entropies:33.376129150390625
Policy training finished
---------------------
gamma: 0.10984566236213841
training start after waiting for 1.179521083831787 seconds
policy loss:-538.8529663085938
value loss:11.999641418457031
entropies:33.70631408691406
Policy training finished
---------------------
gamma: 0.10984566236213841
training start after waiting for 1.1988344192504883 seconds
policy loss:-1078.48974609375
value loss:21.7224178314209
entropies:38.00967025756836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1121.5735)
ToM Target loss= tensor(2268.0032)
optimized based on ToM loss
---------------------
gamma: 0.1100653536868627
training start after waiting for 1.1707932949066162 seconds
policy loss:-605.6678466796875
value loss:37.21152877807617
entropies:35.57499313354492
Policy training finished
---------------------
gamma: 0.1100653536868627
training start after waiting for 1.2161767482757568 seconds
policy loss:-658.41455078125
value loss:18.705644607543945
entropies:37.363189697265625
Policy training finished
---------------------
gamma: 0.1100653536868627
training start after waiting for 1.1910088062286377 seconds
policy loss:412.8481140136719
value loss:14.474206924438477
entropies:36.47747802734375
Policy training finished
---------------------
gamma: 0.1100653536868627
training start after waiting for 1.2320294380187988 seconds
policy loss:282.7010498046875
value loss:10.807821273803711
entropies:20.50918197631836
Policy training finished
---------------------
gamma: 0.1100653536868627
training start after waiting for 1.2135539054870605 seconds
policy loss:359.5592041015625
value loss:8.92497730255127
entropies:22.146263122558594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1089.6095)
ToM Target loss= tensor(2194.7168)
optimized based on ToM loss
---------------------
gamma: 0.11028548439423642
training start after waiting for 1.1734931468963623 seconds
policy loss:64.55730438232422
value loss:9.16891860961914
entropies:28.8339786529541
Policy training finished
---------------------
gamma: 0.11028548439423642
training start after waiting for 1.195373296737671 seconds
policy loss:-1054.94140625
value loss:19.644882202148438
entropies:41.510292053222656
Policy training finished
---------------------
gamma: 0.11028548439423642
training start after waiting for 1.157895803451538 seconds
policy loss:19.925891876220703
value loss:4.780683517456055
entropies:18.05567169189453
Policy training finished
---------------------
gamma: 0.11028548439423642
training start after waiting for 1.2031588554382324 seconds
policy loss:-39.38477325439453
value loss:6.471826553344727
entropies:23.125553131103516
Policy training finished
---------------------
gamma: 0.11028548439423642
training start after waiting for 1.1442160606384277 seconds
policy loss:-514.212890625
value loss:17.626461029052734
entropies:49.282955169677734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1189.5393)
ToM Target loss= tensor(2336.2419)
optimized based on ToM loss
---------------------
gamma: 0.1105060553630249
training start after waiting for 1.205488681793213 seconds
policy loss:-1010.573486328125
value loss:21.096267700195312
entropies:46.65614318847656
Policy training finished
---------------------
gamma: 0.1105060553630249
training start after waiting for 1.1790506839752197 seconds
policy loss:-256.9954528808594
value loss:15.610591888427734
entropies:48.10353088378906
Policy training finished
---------------------
gamma: 0.1105060553630249
training start after waiting for 1.1989240646362305 seconds
policy loss:-1081.504150390625
value loss:31.234527587890625
entropies:43.702205657958984
Policy training finished
---------------------
gamma: 0.1105060553630249
training start after waiting for 1.1979961395263672 seconds
policy loss:-567.9214477539062
value loss:38.62150573730469
entropies:33.30101776123047
Policy training finished
---------------------
gamma: 0.1105060553630249
training start after waiting for 1.1823737621307373 seconds
policy loss:296.3732604980469
value loss:13.589498519897461
entropies:44.980491638183594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1287.0360)
ToM Target loss= tensor(2213.7349)
optimized based on ToM loss
---------------------
gamma: 0.11072706747375094
training start after waiting for 1.2051365375518799 seconds
policy loss:272.1956481933594
value loss:29.116121292114258
entropies:41.190101623535156
Policy training finished
---------------------
gamma: 0.11072706747375094
training start after waiting for 1.2146861553192139 seconds
policy loss:370.491943359375
value loss:16.430349349975586
entropies:26.936656951904297
Policy training finished
---------------------
gamma: 0.11072706747375094
training start after waiting for 1.1594140529632568 seconds
policy loss:486.79290771484375
value loss:20.235515594482422
entropies:36.384708404541016
Policy training finished
---------------------
gamma: 0.11072706747375094
training start after waiting for 1.1575696468353271 seconds
policy loss:76.05856323242188
value loss:7.559706211090088
entropies:13.895692825317383
Policy training finished
---------------------
gamma: 0.11072706747375094
training start after waiting for 1.1842427253723145 seconds
policy loss:-1671.1258544921875
value loss:30.563867568969727
entropies:51.61884307861328
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1175.2690)
ToM Target loss= tensor(2139.5454)
optimized based on ToM loss
---------------------
gamma: 0.11094852160869845
training start after waiting for 1.2099087238311768 seconds
policy loss:-1206.451416015625
value loss:31.122329711914062
entropies:45.61483383178711
Policy training finished
---------------------
gamma: 0.11094852160869845
training start after waiting for 1.1593153476715088 seconds
policy loss:-854.9724731445312
value loss:15.254090309143066
entropies:31.036428451538086
Policy training finished
---------------------
gamma: 0.11094852160869845
training start after waiting for 1.1554937362670898 seconds
policy loss:-306.86932373046875
value loss:25.154067993164062
entropies:37.57361602783203
Policy training finished
---------------------
gamma: 0.11094852160869845
training start after waiting for 1.1444365978240967 seconds
policy loss:-803.2813110351562
value loss:16.46266746520996
entropies:25.67068099975586
Policy training finished
---------------------
gamma: 0.11094852160869845
training start after waiting for 1.1930077075958252 seconds
policy loss:-1095.9610595703125
value loss:20.76700782775879
entropies:51.927459716796875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1233.7761)
ToM Target loss= tensor(2232.3525)
optimized based on ToM loss
---------------------
gamma: 0.11117041865191585
training start after waiting for 1.1747522354125977 seconds
policy loss:38.67493438720703
value loss:9.053475379943848
entropies:28.498050689697266
Policy training finished
---------------------
gamma: 0.11117041865191585
training start after waiting for 1.2121753692626953 seconds
policy loss:17.194316864013672
value loss:14.218103408813477
entropies:27.327329635620117
Policy training finished
---------------------
gamma: 0.11117041865191585
training start after waiting for 1.2061142921447754 seconds
policy loss:-1115.7977294921875
value loss:21.90456199645996
entropies:38.800987243652344
Policy training finished
---------------------
gamma: 0.11117041865191585
training start after waiting for 1.2130091190338135 seconds
policy loss:134.8355712890625
value loss:8.513189315795898
entropies:29.171443939208984
Policy training finished
---------------------
gamma: 0.11117041865191585
training start after waiting for 1.2201170921325684 seconds
policy loss:320.2294616699219
value loss:12.201698303222656
entropies:24.17179298400879
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1156.1265)
ToM Target loss= tensor(2269.9685)
optimized based on ToM loss
---------------------
gamma: 0.11139275948921969
training start after waiting for 1.2146637439727783 seconds
policy loss:-812.3822631835938
value loss:11.639754295349121
entropies:27.366233825683594
Policy training finished
---------------------
gamma: 0.11139275948921969
training start after waiting for 1.2342300415039062 seconds
policy loss:-749.9741821289062
value loss:8.2225923538208
entropies:22.79279327392578
Policy training finished
---------------------
gamma: 0.11139275948921969
training start after waiting for 1.155944585800171 seconds
policy loss:-46.77391815185547
value loss:9.0267915725708
entropies:41.78820037841797
Policy training finished
---------------------
gamma: 0.11139275948921969
training start after waiting for 1.2033729553222656 seconds
policy loss:-472.36700439453125
value loss:12.257386207580566
entropies:31.565317153930664
Policy training finished
---------------------
gamma: 0.11139275948921969
training start after waiting for 1.15755295753479 seconds
policy loss:-436.04290771484375
value loss:20.79050064086914
entropies:24.10079574584961
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1147.3043)
ToM Target loss= tensor(2153.7048)
optimized based on ToM loss
---------------------
gamma: 0.11161554500819812
training start after waiting for 1.1390886306762695 seconds
policy loss:-808.7998046875
value loss:10.890486717224121
entropies:29.118457794189453
Policy training finished
---------------------
gamma: 0.11161554500819812
training start after waiting for 1.203049898147583 seconds
policy loss:-702.9458618164062
value loss:8.952369689941406
entropies:25.942554473876953
Policy training finished
---------------------
gamma: 0.11161554500819812
training start after waiting for 1.195575475692749 seconds
policy loss:-833.7607421875
value loss:19.884737014770508
entropies:29.608768463134766
Policy training finished
---------------------
gamma: 0.11161554500819812
training start after waiting for 1.2097373008728027 seconds
policy loss:-42.71237564086914
value loss:7.823237419128418
entropies:29.807565689086914
Policy training finished
---------------------
gamma: 0.11161554500819812
training start after waiting for 1.1503255367279053 seconds
policy loss:-209.599609375
value loss:14.600187301635742
entropies:32.72547912597656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1113.5139)
ToM Target loss= tensor(2240.3008)
optimized based on ToM loss
---------------------
gamma: 0.11183877609821452
training start after waiting for 1.189971685409546 seconds
policy loss:-105.34156799316406
value loss:12.07448673248291
entropies:28.203182220458984
Policy training finished
---------------------
gamma: 0.11183877609821452
training start after waiting for 1.207406759262085 seconds
policy loss:-1075.9022216796875
value loss:21.17346954345703
entropies:47.11717224121094
Policy training finished
---------------------
gamma: 0.11183877609821452
training start after waiting for 1.1908221244812012 seconds
policy loss:-687.3090209960938
value loss:23.41102409362793
entropies:36.548885345458984
Policy training finished
---------------------
gamma: 0.11183877609821452
training start after waiting for 1.196479320526123 seconds
policy loss:-295.4017333984375
value loss:17.595001220703125
entropies:25.869510650634766
Policy training finished
---------------------
gamma: 0.11183877609821452
training start after waiting for 1.14607572555542 seconds
policy loss:-456.57977294921875
value loss:17.49997329711914
entropies:45.61386489868164
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1199.5995)
ToM Target loss= tensor(2176.1670)
optimized based on ToM loss
---------------------
gamma: 0.11206245365041095
training start after waiting for 1.2269446849822998 seconds
policy loss:-457.651123046875
value loss:20.3416805267334
entropies:37.73323059082031
Policy training finished
---------------------
gamma: 0.11206245365041095
training start after waiting for 1.1520702838897705 seconds
policy loss:-496.4708557128906
value loss:22.739761352539062
entropies:46.15633773803711
Policy training finished
---------------------
gamma: 0.11206245365041095
training start after waiting for 1.1427814960479736 seconds
policy loss:225.4268341064453
value loss:15.561975479125977
entropies:35.38397979736328
Policy training finished
---------------------
gamma: 0.11206245365041095
training start after waiting for 1.1811542510986328 seconds
policy loss:-562.8798217773438
value loss:22.796405792236328
entropies:44.093570709228516
Policy training finished
---------------------
gamma: 0.11206245365041095
training start after waiting for 1.0980029106140137 seconds
policy loss:-402.6688232421875
value loss:16.189430236816406
entropies:40.80836486816406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1310.8220)
ToM Target loss= tensor(2185.7332)
optimized based on ToM loss
---------------------
gamma: 0.11228657855771178
training start after waiting for 1.1697192192077637 seconds
policy loss:86.42076873779297
value loss:16.888704299926758
entropies:38.80772399902344
Policy training finished
---------------------
gamma: 0.11228657855771178
training start after waiting for 1.1691300868988037 seconds
policy loss:507.4211730957031
value loss:9.564873695373535
entropies:22.977861404418945
Policy training finished
---------------------
gamma: 0.11228657855771178
training start after waiting for 1.1546533107757568 seconds
policy loss:-430.3224182128906
value loss:20.579431533813477
entropies:26.091562271118164
Policy training finished
---------------------
gamma: 0.11228657855771178
training start after waiting for 1.1888065338134766 seconds
policy loss:-112.2603530883789
value loss:11.72723388671875
entropies:27.974288940429688
Policy training finished
---------------------
gamma: 0.11228657855771178
training start after waiting for 1.17787504196167 seconds
policy loss:-47.3936767578125
value loss:13.965206146240234
entropies:26.78830909729004
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1276.1664)
ToM Target loss= tensor(2259.9568)
optimized based on ToM loss
---------------------
gamma: 0.1125111517148272
training start after waiting for 1.1878101825714111 seconds
policy loss:-370.259765625
value loss:19.747543334960938
entropies:37.43351745605469
Policy training finished
---------------------
gamma: 0.1125111517148272
training start after waiting for 1.2079424858093262 seconds
policy loss:187.7503662109375
value loss:8.624011993408203
entropies:32.07207489013672
Policy training finished
---------------------
gamma: 0.1125111517148272
training start after waiting for 1.2102997303009033 seconds
policy loss:-196.7257537841797
value loss:3.951167106628418
entropies:31.520305633544922
Policy training finished
---------------------
gamma: 0.1125111517148272
training start after waiting for 1.2022569179534912 seconds
policy loss:-401.8408508300781
value loss:13.548866271972656
entropies:36.135986328125
Policy training finished
---------------------
gamma: 0.1125111517148272
training start after waiting for 1.1984026432037354 seconds
policy loss:-530.3531494140625
value loss:21.07439613342285
entropies:38.255313873291016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1207.5743)
ToM Target loss= tensor(2226.5688)
optimized based on ToM loss
---------------------
gamma: 0.11273617401825685
training start after waiting for 1.1470048427581787 seconds
policy loss:-994.1494140625
value loss:24.209012985229492
entropies:53.511817932128906
Policy training finished
---------------------
gamma: 0.11273617401825685
training start after waiting for 1.205880880355835 seconds
policy loss:-431.4720764160156
value loss:12.559340476989746
entropies:27.938220977783203
Policy training finished
---------------------
gamma: 0.11273617401825685
training start after waiting for 1.2626523971557617 seconds
policy loss:-141.89088439941406
value loss:14.77294921875
entropies:44.6976318359375
Policy training finished
---------------------
gamma: 0.11273617401825685
training start after waiting for 1.192129135131836 seconds
policy loss:-854.822021484375
value loss:17.55196762084961
entropies:28.685001373291016
Policy training finished
---------------------
gamma: 0.11273617401825685
training start after waiting for 1.195563554763794 seconds
policy loss:-120.70803833007812
value loss:8.770519256591797
entropies:35.08073425292969
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1195.1084)
ToM Target loss= tensor(2222.3223)
optimized based on ToM loss
---------------------
gamma: 0.11296164636629337
training start after waiting for 1.1955432891845703 seconds
policy loss:-63.811275482177734
value loss:11.607872009277344
entropies:27.95818519592285
Policy training finished
---------------------
gamma: 0.11296164636629337
training start after waiting for 1.1466212272644043 seconds
policy loss:-826.2991943359375
value loss:13.221315383911133
entropies:33.290313720703125
Policy training finished
---------------------
gamma: 0.11296164636629337
training start after waiting for 1.1841397285461426 seconds
policy loss:-999.2498168945312
value loss:17.47258758544922
entropies:42.73298645019531
Policy training finished
---------------------
gamma: 0.11296164636629337
training start after waiting for 1.203439474105835 seconds
policy loss:-1104.4337158203125
value loss:23.7791690826416
entropies:35.51823806762695
Policy training finished
---------------------
gamma: 0.11296164636629337
training start after waiting for 1.2198455333709717 seconds
policy loss:-1422.2706298828125
value loss:31.386301040649414
entropies:42.08666229248047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1233.1187)
ToM Target loss= tensor(2280.9285)
optimized based on ToM loss
---------------------
gamma: 0.11318756965902596
training start after waiting for 1.1352205276489258 seconds
policy loss:-391.45111083984375
value loss:7.548916816711426
entropies:28.822452545166016
Policy training finished
---------------------
gamma: 0.11318756965902596
training start after waiting for 1.1950430870056152 seconds
policy loss:4.954728126525879
value loss:8.320611000061035
entropies:25.49802017211914
Policy training finished
---------------------
gamma: 0.11318756965902596
training start after waiting for 1.1984586715698242 seconds
policy loss:-1321.654052734375
value loss:34.31349182128906
entropies:43.415557861328125
Policy training finished
---------------------
gamma: 0.11318756965902596
training start after waiting for 1.215324878692627 seconds
policy loss:-22.74025535583496
value loss:14.382118225097656
entropies:21.976295471191406
Policy training finished
---------------------
gamma: 0.11318756965902596
training start after waiting for 1.2117769718170166 seconds
policy loss:-481.6394958496094
value loss:16.204315185546875
entropies:32.27283477783203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1190.8818)
ToM Target loss= tensor(2269.3992)
optimized based on ToM loss
---------------------
gamma: 0.11341394479834402
training start after waiting for 1.1802232265472412 seconds
policy loss:52.413124084472656
value loss:20.491304397583008
entropies:53.85896682739258
Policy training finished
---------------------
gamma: 0.11341394479834402
training start after waiting for 1.1945173740386963 seconds
policy loss:-745.5185546875
value loss:29.687536239624023
entropies:43.84314727783203
Policy training finished
---------------------
gamma: 0.11341394479834402
training start after waiting for 1.1850833892822266 seconds
policy loss:-154.06365966796875
value loss:10.304862976074219
entropies:36.37528610229492
Policy training finished
---------------------
gamma: 0.11341394479834402
training start after waiting for 1.1935014724731445 seconds
policy loss:-457.62371826171875
value loss:9.515039443969727
entropies:35.29502487182617
Policy training finished
---------------------
gamma: 0.11341394479834402
training start after waiting for 1.2028284072875977 seconds
policy loss:-713.9110107421875
value loss:15.394256591796875
entropies:24.225543975830078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1286.2195)
ToM Target loss= tensor(2156.0718)
optimized based on ToM loss
---------------------
gamma: 0.1136407726879407
training start after waiting for 1.2024052143096924 seconds
policy loss:-193.072509765625
value loss:4.40868616104126
entropies:25.049636840820312
Policy training finished
---------------------
gamma: 0.1136407726879407
training start after waiting for 1.1932146549224854 seconds
policy loss:-735.2962036132812
value loss:18.924203872680664
entropies:33.59022903442383
Policy training finished
---------------------
gamma: 0.1136407726879407
training start after waiting for 1.2604758739471436 seconds
policy loss:-557.925537109375
value loss:10.538883209228516
entropies:30.286617279052734
Policy training finished
---------------------
gamma: 0.1136407726879407
training start after waiting for 1.2045979499816895 seconds
policy loss:-1266.697998046875
value loss:27.327423095703125
entropies:59.09989929199219
Policy training finished
---------------------
gamma: 0.1136407726879407
training start after waiting for 1.1581871509552002 seconds
policy loss:-1447.97705078125
value loss:36.76858901977539
entropies:46.447021484375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1281.3744)
ToM Target loss= tensor(2250.8394)
optimized based on ToM loss
---------------------
gamma: 0.11386805423331658
training start after waiting for 1.183159589767456 seconds
policy loss:-129.56898498535156
value loss:9.168025970458984
entropies:18.25725555419922
Policy training finished
---------------------
gamma: 0.11386805423331658
training start after waiting for 1.191225528717041 seconds
policy loss:-176.20143127441406
value loss:16.346515655517578
entropies:31.84209442138672
Policy training finished
---------------------
gamma: 0.11386805423331658
training start after waiting for 1.1473934650421143 seconds
policy loss:258.1298522949219
value loss:13.591422080993652
entropies:41.55620193481445
Policy training finished
---------------------
gamma: 0.11386805423331658
training start after waiting for 1.1837208271026611 seconds
policy loss:175.51124572753906
value loss:10.77334213256836
entropies:24.964149475097656
Policy training finished
---------------------
gamma: 0.11386805423331658
training start after waiting for 1.1980016231536865 seconds
policy loss:145.38943481445312
value loss:6.7679362297058105
entropies:23.499475479125977
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1117.3579)
ToM Target loss= tensor(2304.5627)
optimized based on ToM loss
---------------------
gamma: 0.11409579034178322
training start after waiting for 1.2096519470214844 seconds
policy loss:271.93829345703125
value loss:4.583829879760742
entropies:25.689735412597656
Policy training finished
---------------------
gamma: 0.11409579034178322
training start after waiting for 1.1946842670440674 seconds
policy loss:-870.3117065429688
value loss:19.0548152923584
entropies:27.621498107910156
Policy training finished
---------------------
gamma: 0.11409579034178322
training start after waiting for 1.182950735092163 seconds
policy loss:-1828.6788330078125
value loss:60.377925872802734
entropies:37.159767150878906
Policy training finished
---------------------
gamma: 0.11409579034178322
training start after waiting for 1.1433370113372803 seconds
policy loss:-118.8647232055664
value loss:9.050211906433105
entropies:32.68193435668945
Policy training finished
---------------------
gamma: 0.11409579034178322
training start after waiting for 1.188448190689087 seconds
policy loss:-777.3585205078125
value loss:24.414154052734375
entropies:44.42985534667969
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1141.6328)
ToM Target loss= tensor(2272.7512)
optimized based on ToM loss
---------------------
gamma: 0.11432398192246679
training start after waiting for 1.1752889156341553 seconds
policy loss:-759.8916015625
value loss:13.587761878967285
entropies:27.823118209838867
Policy training finished
---------------------
gamma: 0.11432398192246679
training start after waiting for 1.2013559341430664 seconds
policy loss:-236.5548553466797
value loss:14.69102668762207
entropies:30.133853912353516
Policy training finished
---------------------
gamma: 0.11432398192246679
training start after waiting for 1.1653661727905273 seconds
policy loss:-350.8808288574219
value loss:19.537216186523438
entropies:46.37149429321289
Policy training finished
---------------------
gamma: 0.11432398192246679
training start after waiting for 1.175325632095337 seconds
policy loss:161.11221313476562
value loss:10.174932479858398
entropies:25.933670043945312
Policy training finished
---------------------
gamma: 0.11432398192246679
training start after waiting for 1.2329668998718262 seconds
policy loss:-132.39556884765625
value loss:17.976276397705078
entropies:37.843467712402344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1186.3917)
ToM Target loss= tensor(2306.9624)
optimized based on ToM loss
---------------------
gamma: 0.11455262988631172
training start after waiting for 1.194136142730713 seconds
policy loss:209.60372924804688
value loss:13.609773635864258
entropies:33.50108337402344
Policy training finished
---------------------
gamma: 0.11455262988631172
training start after waiting for 1.1911988258361816 seconds
policy loss:-924.4088745117188
value loss:23.07453155517578
entropies:41.6825065612793
Policy training finished
---------------------
gamma: 0.11455262988631172
training start after waiting for 1.1960418224334717 seconds
policy loss:-517.5211181640625
value loss:17.811004638671875
entropies:34.90781021118164
Policy training finished
---------------------
gamma: 0.11455262988631172
training start after waiting for 1.1080615520477295 seconds
policy loss:-2321.71728515625
value loss:39.65427017211914
entropies:47.342857360839844
Policy training finished
---------------------
gamma: 0.11455262988631172
training start after waiting for 1.2062866687774658 seconds
policy loss:-268.1190185546875
value loss:11.06332015991211
entropies:29.246370315551758
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1154.3884)
ToM Target loss= tensor(2160.4915)
optimized based on ToM loss
---------------------
gamma: 0.11478173514608435
training start after waiting for 1.1928303241729736 seconds
policy loss:-739.0942993164062
value loss:16.65614128112793
entropies:20.590320587158203
Policy training finished
---------------------
gamma: 0.11478173514608435
training start after waiting for 1.1568975448608398 seconds
policy loss:63.68084716796875
value loss:16.70970344543457
entropies:53.500308990478516
Policy training finished
---------------------
gamma: 0.11478173514608435
training start after waiting for 1.1899492740631104 seconds
policy loss:-199.74957275390625
value loss:19.76374626159668
entropies:30.558353424072266
Policy training finished
---------------------
gamma: 0.11478173514608435
training start after waiting for 1.1453392505645752 seconds
policy loss:285.6282043457031
value loss:7.0346198081970215
entropies:26.140464782714844
Policy training finished
---------------------
gamma: 0.11478173514608435
training start after waiting for 1.194157600402832 seconds
policy loss:-530.5390625
value loss:12.417579650878906
entropies:39.320011138916016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1167.7999)
ToM Target loss= tensor(2195.9253)
optimized based on ToM loss
2024-04-07 06:40:29,122 : Time 05h 22m 11s, ave eps reward [-4.25 -4.25 -4.25], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 9.53, mean reward -4.251929648394353, std reward 2.689396849062942, AG 0.0
2024-04-07 06:40:55,561 : Time 05h 22m 37s, ave eps reward [-5.06 -5.06 -5.06], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.57, mean reward -5.0585276492684095, std reward 3.1354213440460095, AG 0.0
2024-04-07 06:41:22,261 : Time 05h 23m 04s, ave eps reward [-5.73 -5.73 -5.73], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 8.6, mean reward -5.7324796679356975, std reward 4.759198463304307, AG 0.0
2024-04-07 06:41:48,367 : Time 05h 23m 30s, ave eps reward [-3.79 -3.79 -3.79], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 9.08, mean reward -3.7886335352016856, std reward 2.3787914080320727, AG 0.0
2024-04-07 06:42:14,799 : Time 05h 23m 56s, ave eps reward [-5.86 -5.86 -5.86], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 6.84, mean reward -5.864310392579169, std reward 4.795837258897362, AG 0.0
2024-04-07 06:42:41,030 : Time 05h 24m 22s, ave eps reward [-5.16 -5.16 -5.16], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 7.33, mean reward -5.163159984079158, std reward 5.3444456735306645, AG 0.0
2024-04-07 06:43:07,538 : Time 05h 24m 49s, ave eps reward [-4.88 -4.88 -4.88], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.88, mean reward -4.881115635580153, std reward 3.821434706934943, AG 0.0
2024-04-07 06:43:33,990 : Time 05h 25m 15s, ave eps reward [-5.39 -5.39 -5.39], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 6.78, mean reward -5.391860264590888, std reward 5.023886942521466, AG 0.0
2024-04-07 06:43:59,990 : Time 05h 25m 41s, ave eps reward [-3.32 -3.32 -3.32], ave eps length 10.0, reward step [-0.33 -0.33 -0.33], FPS 9.73, mean reward -3.316187209643305, std reward 3.3900969472502767, AG 0.0
2024-04-07 06:44:26,258 : Time 05h 26m 08s, ave eps reward [-5.24 -5.24 -5.24], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 7.94, mean reward -5.240983946039974, std reward 4.61521429181312, AG 0.0
2024-04-07 06:44:52,681 : Time 05h 26m 34s, ave eps reward [-5.65 -5.65 -5.65], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.9, mean reward -5.648481463864211, std reward 4.228192920489154, AG 0.0
2024-04-07 06:45:19,090 : Time 05h 27m 01s, ave eps reward [-4.88 -4.88 -4.88], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 10.03, mean reward -4.878893063170442, std reward 4.13377793766016, AG 0.0
2024-04-07 06:45:45,503 : Time 05h 27m 27s, ave eps reward [-4. -4. -4.], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 6.72, mean reward -4.004002737048002, std reward 2.540515654839163, AG 0.0
2024-04-07 06:46:11,838 : Time 05h 27m 53s, ave eps reward [-4.31 -4.31 -4.31], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 6.87, mean reward -4.309178072787762, std reward 2.81260760090628, AG 0.0
2024-04-07 06:46:37,761 : Time 05h 28m 19s, ave eps reward [-4.21 -4.21 -4.21], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 9.83, mean reward -4.209417720512511, std reward 5.352614444901393, AG 0.0
2024-04-07 06:47:04,576 : Time 05h 28m 46s, ave eps reward [-6.37 -6.37 -6.37], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 6.96, mean reward -6.366462098691777, std reward 6.341100580350364, AG 0.0
2024-04-07 06:47:30,849 : Time 05h 29m 12s, ave eps reward [-3.65 -3.65 -3.65], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 7.73, mean reward -3.654647112864809, std reward 2.780525458136148, AG 0.0
2024-04-07 06:47:56,923 : Time 05h 29m 38s, ave eps reward [-5.69 -5.69 -5.69], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 9.73, mean reward -5.692979249804797, std reward 5.323076704759068, AG 0.0
2024-04-07 06:48:23,217 : Time 05h 30m 05s, ave eps reward [-6.01 -6.01 -6.01], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 7.78, mean reward -6.006521302521473, std reward 4.274069037147099, AG 0.0
2024-04-07 06:48:49,605 : Time 05h 30m 31s, ave eps reward [-3.83 -3.83 -3.83], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 6.87, mean reward -3.8283881915396756, std reward 2.4783404106992055, AG 0.0
2024-04-07 06:49:15,983 : Time 05h 30m 57s, ave eps reward [-5.21 -5.21 -5.21], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 9.51, mean reward -5.213209081016794, std reward 4.193274994645993, AG 0.0
2024-04-07 06:49:42,404 : Time 05h 31m 24s, ave eps reward [-4.4 -4.4 -4.4], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 6.82, mean reward -4.398328483535995, std reward 3.0437535610999746, AG 0.0
2024-04-07 06:50:08,772 : Time 05h 31m 50s, ave eps reward [-4.21 -4.21 -4.21], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 7.27, mean reward -4.214996973350861, std reward 2.5405652363178874, AG 0.0
2024-04-07 06:50:34,684 : Time 05h 32m 16s, ave eps reward [-3.76 -3.76 -3.76], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 10.44, mean reward -3.7644556039228947, std reward 2.9789187686941583, AG 0.0
2024-04-07 06:51:01,684 : Time 05h 32m 43s, ave eps reward [-5.17 -5.17 -5.17], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 6.83, mean reward -5.167097989001219, std reward 5.137402853704355, AG 0.0
2024-04-07 06:51:27,736 : Time 05h 33m 09s, ave eps reward [-4.22 -4.22 -4.22], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 8.86, mean reward -4.218850477577577, std reward 3.2967639807583833, AG 0.0
2024-04-07 06:51:53,920 : Time 05h 33m 35s, ave eps reward [-4.58 -4.58 -4.58], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 8.42, mean reward -4.584984349348917, std reward 4.037419575991681, AG 0.0
2024-04-07 06:52:20,399 : Time 05h 34m 02s, ave eps reward [-6.73 -6.73 -6.73], ave eps length 10.0, reward step [-0.67 -0.67 -0.67], FPS 6.61, mean reward -6.729712346973232, std reward 6.773831867311872, AG 0.0
2024-04-07 06:52:46,735 : Time 05h 34m 28s, ave eps reward [-5.13 -5.13 -5.13], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.63, mean reward -5.129456041197378, std reward 4.462904005646995, AG 0.0
2024-04-07 06:53:13,048 : Time 05h 34m 54s, ave eps reward [-3.51 -3.51 -3.51], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 6.87, mean reward -3.5092874117181205, std reward 2.10926911425284, AG 0.0
2024-04-07 06:53:39,387 : Time 05h 35m 21s, ave eps reward [-4.16 -4.16 -4.16], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.95, mean reward -4.163137471883611, std reward 2.695516138328366, AG 0.0
2024-04-07 06:54:05,641 : Time 05h 35m 47s, ave eps reward [-6.12 -6.12 -6.12], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 8.32, mean reward -6.123255583357527, std reward 5.558250650638435, AG 0.0
2024-04-07 06:54:31,819 : Time 05h 36m 13s, ave eps reward [-5.55 -5.55 -5.55], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 9.17, mean reward -5.545157274440258, std reward 4.533912906961695, AG 0.0
2024-04-07 06:54:58,627 : Time 05h 36m 40s, ave eps reward [-7.06 -7.06 -7.06], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 7.54, mean reward -7.064268539666432, std reward 5.842538651767047, AG 0.0
2024-04-07 06:55:24,655 : Time 05h 37m 06s, ave eps reward [-5.96 -5.96 -5.96], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 10.28, mean reward -5.955827713280153, std reward 4.540813726808187, AG 0.0
2024-04-07 06:55:51,137 : Time 05h 37m 33s, ave eps reward [-7.14 -7.14 -7.14], ave eps length 10.0, reward step [-0.71 -0.71 -0.71], FPS 6.58, mean reward -7.136446019372064, std reward 6.413288575911785, AG 0.0
2024-04-07 06:56:17,521 : Time 05h 37m 59s, ave eps reward [-6.45 -6.45 -6.45], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 7.15, mean reward -6.450232602161179, std reward 6.003634722053204, AG 0.0
2024-04-07 06:56:44,025 : Time 05h 38m 25s, ave eps reward [-3.76 -3.76 -3.76], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 6.62, mean reward -3.7574725581345754, std reward 3.253672478159568, AG 0.0
2024-04-07 06:57:10,477 : Time 05h 38m 52s, ave eps reward [-4.92 -4.92 -4.92], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.87, mean reward -4.916959434093535, std reward 4.295809173622927, AG 0.0
2024-04-07 06:57:36,875 : Time 05h 39m 18s, ave eps reward [-3.92 -3.92 -3.92], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 6.86, mean reward -3.9218192554073115, std reward 2.16489501049941, AG 0.0
2024-04-07 06:58:02,753 : Time 05h 39m 44s, ave eps reward [-6.53 -6.53 -6.53], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 9.4, mean reward -6.530563347870862, std reward 6.517606360567759, AG 0.0
2024-04-07 06:58:28,941 : Time 05h 40m 10s, ave eps reward [-3. -3. -3.], ave eps length 10.0, reward step [-0.3 -0.3 -0.3], FPS 8.06, mean reward -2.9985008418124974, std reward 1.0791431064360018, AG 0.0
2024-04-07 06:58:55,580 : Time 05h 40m 37s, ave eps reward [-3.36 -3.36 -3.36], ave eps length 10.0, reward step [-0.34 -0.34 -0.34], FPS 8.23, mean reward -3.3594774628885715, std reward 1.9502141141667648, AG 0.0
2024-04-07 06:59:21,661 : Time 05h 41m 03s, ave eps reward [-5.41 -5.41 -5.41], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 9.68, mean reward -5.410284518399361, std reward 3.4201998090530963, AG 0.0
2024-04-07 06:59:48,073 : Time 05h 41m 30s, ave eps reward [-5.49 -5.49 -5.49], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.71, mean reward -5.493496049068092, std reward 6.17940274755716, AG 0.0
2024-04-07 07:00:14,515 : Time 05h 41m 56s, ave eps reward [-4.69 -4.69 -4.69], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.71, mean reward -4.686840814843858, std reward 4.226381452100679, AG 0.0
2024-04-07 07:00:40,983 : Time 05h 42m 22s, ave eps reward [-3.44 -3.44 -3.44], ave eps length 10.0, reward step [-0.34 -0.34 -0.34], FPS 6.82, mean reward -3.4446105347694265, std reward 3.0501556841825725, AG 0.0
2024-04-07 07:01:07,440 : Time 05h 42m 49s, ave eps reward [-4.67 -4.67 -4.67], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.67, mean reward -4.673911132660778, std reward 4.297779898132143, AG 0.0
2024-04-07 07:01:33,801 : Time 05h 43m 15s, ave eps reward [-4.91 -4.91 -4.91], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 7.02, mean reward -4.908548185850851, std reward 4.352377968473056, AG 0.0
2024-04-07 07:01:59,809 : Time 05h 43m 41s, ave eps reward [-4.57 -4.57 -4.57], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 10.2, mean reward -4.574042334399216, std reward 3.9128366167200643, AG 0.0
2024-04-07 07:02:26,133 : Time 05h 44m 08s, ave eps reward [-4.99 -4.99 -4.99], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 7.32, mean reward -4.993261054389551, std reward 3.267430609983583, AG 0.0
2024-04-07 07:02:52,695 : Time 05h 44m 34s, ave eps reward [-4.2 -4.2 -4.2], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 8.6, mean reward -4.199381732124955, std reward 4.3690232468754155, AG 0.0
2024-04-07 07:03:18,842 : Time 05h 45m 00s, ave eps reward [-5.65 -5.65 -5.65], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 8.2, mean reward -5.645711367136122, std reward 5.375076173107064, AG 0.0
2024-04-07 07:03:45,250 : Time 05h 45m 27s, ave eps reward [-4.21 -4.21 -4.21], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.8, mean reward -4.206331196938558, std reward 2.0688364525029592, AG 0.0
2024-04-07 07:04:11,377 : Time 05h 45m 53s, ave eps reward [-4.98 -4.98 -4.98], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 7.97, mean reward -4.98229845706693, std reward 3.818377034434198, AG 0.0
2024-04-07 07:04:37,907 : Time 05h 46m 19s, ave eps reward [-5.71 -5.71 -5.71], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 6.87, mean reward -5.705777093248526, std reward 4.072580996147699, AG 0.0
2024-04-07 07:05:04,357 : Time 05h 46m 46s, ave eps reward [-4.53 -4.53 -4.53], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.73, mean reward -4.528033889248175, std reward 2.271592771930875, AG 0.0
2024-04-07 07:05:30,292 : Time 05h 47m 12s, ave eps reward [-4.16 -4.16 -4.16], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 9.33, mean reward -4.156323208338066, std reward 2.460358033091712, AG 0.0
2024-04-07 07:05:56,484 : Time 05h 47m 38s, ave eps reward [-4.35 -4.35 -4.35], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 8.28, mean reward -4.352090216771308, std reward 3.5837495438227833, AG 0.0
2024-04-07 07:06:22,936 : Time 05h 48m 04s, ave eps reward [-4.51 -4.51 -4.51], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.76, mean reward -4.512263383415577, std reward 2.9059030752869246, AG 0.0
2024-04-07 07:06:49,451 : Time 05h 48m 31s, ave eps reward [-6.26 -6.26 -6.26], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 9.0, mean reward -6.262369275627902, std reward 4.832107558897307, AG 0.0
2024-04-07 07:07:15,628 : Time 05h 48m 57s, ave eps reward [-6.12 -6.12 -6.12], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 8.34, mean reward -6.116921693874589, std reward 6.465320167755784, AG 0.0
2024-04-07 07:07:42,061 : Time 05h 49m 23s, ave eps reward [-5. -5. -5.], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 6.98, mean reward -5.000317831292257, std reward 4.722825914288171, AG 0.0
2024-04-07 07:08:08,351 : Time 05h 49m 50s, ave eps reward [-5.34 -5.34 -5.34], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 8.01, mean reward -5.338979454606622, std reward 4.982425870597332, AG 0.0
2024-04-07 07:08:34,994 : Time 05h 50m 16s, ave eps reward [-5.15 -5.15 -5.15], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.93, mean reward -5.145033811739389, std reward 4.092795636685798, AG 0.0
2024-04-07 07:09:01,341 : Time 05h 50m 43s, ave eps reward [-4.78 -4.78 -4.78], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.75, mean reward -4.775023468682958, std reward 4.411852152044609, AG 0.0
2024-04-07 07:09:27,278 : Time 05h 51m 09s, ave eps reward [-4.74 -4.74 -4.74], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 9.03, mean reward -4.739985862374607, std reward 2.6780955454430115, AG 0.0
2024-04-07 07:09:53,520 : Time 05h 51m 35s, ave eps reward [-5.16 -5.16 -5.16], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 8.23, mean reward -5.158958307698377, std reward 4.697209186214167, AG 0.0
2024-04-07 07:10:19,941 : Time 05h 52m 01s, ave eps reward [-5.19 -5.19 -5.19], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 6.76, mean reward -5.189561234245386, std reward 4.185586221646133, AG 0.0
2024-04-07 07:10:46,267 : Time 05h 52m 28s, ave eps reward [-5.45 -5.45 -5.45], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 9.28, mean reward -5.451028632465768, std reward 4.371150499170177, AG 0.0
2024-04-07 07:11:12,689 : Time 05h 52m 54s, ave eps reward [-3.97 -3.97 -3.97], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 6.75, mean reward -3.9735172125111324, std reward 2.9456773078803984, AG 0.0
2024-04-07 07:11:38,961 : Time 05h 53m 20s, ave eps reward [-6.41 -6.41 -6.41], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 7.27, mean reward -6.408493731743204, std reward 5.8797351847291335, AG 0.0
2024-04-07 07:12:04,958 : Time 05h 53m 46s, ave eps reward [-5.12 -5.12 -5.12], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 10.54, mean reward -5.120224477815397, std reward 4.621747786628238, AG 0.0
2024-04-07 07:12:31,917 : Time 05h 54m 13s, ave eps reward [-3.71 -3.71 -3.71], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.77, mean reward -3.7072680286040374, std reward 2.3243165944745128, AG 0.0
2024-04-07 07:12:58,238 : Time 05h 54m 40s, ave eps reward [-3.8 -3.8 -3.8], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 6.84, mean reward -3.804694956162362, std reward 3.5000924882754254, AG 0.0
2024-04-07 07:13:24,206 : Time 05h 55m 06s, ave eps reward [-3.44 -3.44 -3.44], ave eps length 10.0, reward step [-0.34 -0.34 -0.34], FPS 10.57, mean reward -3.4449196879294015, std reward 1.5813469553673904, AG 0.0
2024-04-07 07:13:50,544 : Time 05h 55m 32s, ave eps reward [-3.87 -3.87 -3.87], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 7.32, mean reward -3.873680930060776, std reward 2.0451244823808756, AG 0.0
2024-04-07 07:14:16,838 : Time 05h 55m 58s, ave eps reward [-4.23 -4.23 -4.23], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.8, mean reward -4.232848666483961, std reward 4.1129790991408655, AG 0.0
2024-04-07 07:14:43,213 : Time 05h 56m 25s, ave eps reward [-6.93 -6.93 -6.93], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 8.6, mean reward -6.933368798574653, std reward 5.940530265185905, AG 0.0
2024-04-07 07:15:09,677 : Time 05h 56m 51s, ave eps reward [-4.12 -4.12 -4.12], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 6.94, mean reward -4.119803597363898, std reward 2.579823242953996, AG 0.0
2024-04-07 07:15:35,861 : Time 05h 57m 17s, ave eps reward [-3.95 -3.95 -3.95], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 8.77, mean reward -3.9503037242036605, std reward 2.4927790710982687, AG 0.0
2024-04-07 07:16:02,011 : Time 05h 57m 43s, ave eps reward [-3.56 -3.56 -3.56], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 8.73, mean reward -3.562812470520341, std reward 2.5877467002542796, AG 0.0
2024-04-07 07:16:28,820 : Time 05h 58m 10s, ave eps reward [-5.06 -5.06 -5.06], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 7.46, mean reward -5.063505438562615, std reward 3.5953832880649035, AG 0.0
2024-04-07 07:16:54,769 : Time 05h 58m 36s, ave eps reward [-4.63 -4.63 -4.63], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 10.56, mean reward -4.630889098265604, std reward 3.087294225626076, AG 0.0
2024-04-07 07:17:21,247 : Time 05h 59m 03s, ave eps reward [-4.47 -4.47 -4.47], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 8.88, mean reward -4.466155397958858, std reward 3.4856067195414533, AG 0.0
2024-04-07 07:17:47,658 : Time 05h 59m 29s, ave eps reward [-4.89 -4.89 -4.89], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.77, mean reward -4.888522274808901, std reward 3.755129531329757, AG 0.0
2024-04-07 07:18:14,039 : Time 05h 59m 55s, ave eps reward [-5.53 -5.53 -5.53], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.78, mean reward -5.534249439305117, std reward 5.550647707806631, AG 0.0
2024-04-07 07:18:40,415 : Time 06h 00m 22s, ave eps reward [-5.77 -5.77 -5.77], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 7.12, mean reward -5.765650255977005, std reward 4.991633074623684, AG 0.0
2024-04-07 07:19:06,656 : Time 06h 00m 48s, ave eps reward [-4.89 -4.89 -4.89], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.7, mean reward -4.891160974269573, std reward 4.712017180259564, AG 0.0
2024-04-07 07:19:32,920 : Time 06h 01m 14s, ave eps reward [-4.47 -4.47 -4.47], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 7.98, mean reward -4.466760643159352, std reward 3.447880933950528, AG 0.0
2024-04-07 07:19:58,924 : Time 06h 01m 40s, ave eps reward [-4.77 -4.77 -4.77], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 10.25, mean reward -4.768019958721714, std reward 3.7406129332969313, AG 0.0
2024-04-07 07:20:25,891 : Time 06h 02m 07s, ave eps reward [-5.71 -5.71 -5.71], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 6.54, mean reward -5.709783352709485, std reward 3.524420743690881, AG 0.0
2024-04-07 07:20:51,877 : Time 06h 02m 33s, ave eps reward [-3.96 -3.96 -3.96], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 9.98, mean reward -3.958995867475795, std reward 1.70877202417018, AG 0.0
2024-04-07 07:21:18,194 : Time 06h 03m 00s, ave eps reward [-5.35 -5.35 -5.35], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 7.55, mean reward -5.345280491447278, std reward 4.942942272105896, AG 0.0
2024-04-07 07:21:44,639 : Time 06h 03m 26s, ave eps reward [-4.31 -4.31 -4.31], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 6.92, mean reward -4.306452389013453, std reward 3.7105095755930715, AG 0.0
2024-04-07 07:22:10,994 : Time 06h 03m 52s, ave eps reward [-3.75 -3.75 -3.75], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.86, mean reward -3.7454683764884655, std reward 1.5901052653419778, AG 0.0
2024-04-07 07:22:37,434 : Time 06h 04m 19s, ave eps reward [-3.39 -3.39 -3.39], ave eps length 10.0, reward step [-0.34 -0.34 -0.34], FPS 6.72, mean reward -3.3914350197545198, std reward 1.7914495991564334, AG 0.0
2024-04-07 07:23:03,621 : Time 06h 04m 45s, ave eps reward [-3.82 -3.82 -3.82], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 7.6, mean reward -3.8230979313498836, std reward 1.8592061503554749, AG 0.0
2024-04-07 07:23:29,596 : Time 06h 05m 11s, ave eps reward [-6.26 -6.26 -6.26], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 9.47, mean reward -6.259355339701701, std reward 5.63779838038815, AG 0.0
2024-04-07 07:23:56,009 : Time 06h 05m 37s, ave eps reward [-5.47 -5.47 -5.47], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 7.0, mean reward -5.4679165379819334, std reward 4.799176863922978, AG 0.0
2024-04-07 07:24:22,539 : Time 06h 06m 04s, ave eps reward [-4.54 -4.54 -4.54], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 10.26, mean reward -4.53908443360495, std reward 3.926365609069539, AG 0.0
2024-04-07 07:24:48,927 : Time 06h 06m 30s, ave eps reward [-4.18 -4.18 -4.18], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 7.28, mean reward -4.18267675371633, std reward 2.248384886017656, AG 0.0
2024-04-07 07:25:15,238 : Time 06h 06m 57s, ave eps reward [-5.6 -5.6 -5.6], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.73, mean reward -5.601948978170545, std reward 3.882998328930626, AG 0.0
2024-04-07 07:25:41,489 : Time 06h 07m 23s, ave eps reward [-4.16 -4.16 -4.16], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 7.91, mean reward -4.1601353785482225, std reward 2.2319363002655486, AG 0.0
2024-04-07 07:26:08,187 : Time 06h 07m 50s, ave eps reward [-3.96 -3.96 -3.96], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 6.74, mean reward -3.961064302558757, std reward 2.132654006429536, AG 0.0
2024-04-07 07:26:34,581 : Time 06h 08m 16s, ave eps reward [-4.28 -4.28 -4.28], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 6.85, mean reward -4.275064032154365, std reward 2.4102774565019436, AG 0.0
2024-04-07 07:27:00,616 : Time 06h 08m 42s, ave eps reward [-5.94 -5.94 -5.94], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 9.25, mean reward -5.935315360344344, std reward 5.0869194222703, AG 0.0
2024-04-07 07:27:26,860 : Time 06h 09m 08s, ave eps reward [-6.88 -6.88 -6.88], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 8.29, mean reward -6.882694614626516, std reward 5.038946609171142, AG 0.0
2024-04-07 07:27:53,227 : Time 06h 09m 35s, ave eps reward [-7.32 -7.32 -7.32], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 6.82, mean reward -7.316685959137475, std reward 5.942468412757541, AG 0.0
2024-04-07 07:28:19,552 : Time 06h 10m 01s, ave eps reward [-4.94 -4.94 -4.94], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 9.96, mean reward -4.942211203909252, std reward 4.396388876087183, AG 0.0
2024-04-07 07:28:45,993 : Time 06h 10m 27s, ave eps reward [-3.81 -3.81 -3.81], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 6.76, mean reward -3.809170843376138, std reward 2.1504282347818306, AG 0.0
2024-04-07 07:29:12,306 : Time 06h 10m 54s, ave eps reward [-4.73 -4.73 -4.73], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 7.29, mean reward -4.731801382044682, std reward 5.204549453859241, AG 0.0
2024-04-07 07:29:38,249 : Time 06h 11m 20s, ave eps reward [-3.58 -3.58 -3.58], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 10.35, mean reward -3.5827045151256245, std reward 1.9809239254475157, AG 0.0
2024-04-07 07:30:05,177 : Time 06h 11m 47s, ave eps reward [-4.03 -4.03 -4.03], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 6.74, mean reward -4.031515893076812, std reward 3.036561833182625, AG 0.0
2024-04-07 07:30:31,139 : Time 06h 12m 13s, ave eps reward [-5.55 -5.55 -5.55], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 9.87, mean reward -5.547747329469075, std reward 3.3367398010745055, AG 0.0
2024-04-07 07:30:57,474 : Time 06h 12m 39s, ave eps reward [-8.37 -8.37 -8.37], ave eps length 10.0, reward step [-0.84 -0.84 -0.84], FPS 7.43, mean reward -8.365079451717587, std reward 5.053834607308762, AG 0.0
2024-04-07 07:31:23,812 : Time 06h 13m 05s, ave eps reward [-3.71 -3.71 -3.71], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.77, mean reward -3.7079100429598766, std reward 2.6938623990703325, AG 0.0
2024-04-07 07:31:50,220 : Time 06h 13m 32s, ave eps reward [-6.18 -6.18 -6.18], ave eps length 10.0, reward step [-0.62 -0.62 -0.62], FPS 6.85, mean reward -6.178422578423398, std reward 6.291649230602963, AG 0.0
2024-04-07 07:32:16,621 : Time 06h 13m 58s, ave eps reward [-4.66 -4.66 -4.66], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 7.58, mean reward -4.655877530465327, std reward 3.8939912438498827, AG 0.0
2024-04-07 07:32:43,046 : Time 06h 14m 24s, ave eps reward [-3.78 -3.78 -3.78], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 6.82, mean reward -3.7818093329279163, std reward 3.2557376275113348, AG 0.0
2024-04-07 07:33:09,042 : Time 06h 14m 50s, ave eps reward [-3.96 -3.96 -3.96], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 9.26, mean reward -3.9563445539723796, std reward 3.0003757004658342, AG 0.0
2024-04-07 07:33:35,261 : Time 06h 15m 17s, ave eps reward [-5.06 -5.06 -5.06], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 8.82, mean reward -5.061955699118973, std reward 3.9887320733757985, AG 0.0
2024-04-07 07:34:02,176 : Time 06h 15m 44s, ave eps reward [-3.42 -3.42 -3.42], ave eps length 10.0, reward step [-0.34 -0.34 -0.34], FPS 7.49, mean reward -3.42263645800803, std reward 2.1875487768496122, AG 0.0
2024-04-07 07:34:28,179 : Time 06h 16m 10s, ave eps reward [-6.26 -6.26 -6.26], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 10.26, mean reward -6.255902055188051, std reward 6.871899378624909, AG 0.0
2024-04-07 07:34:54,539 : Time 06h 16m 36s, ave eps reward [-4.6 -4.6 -4.6], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 6.76, mean reward -4.597575833104356, std reward 4.279858815000083, AG 0.0
2024-04-07 07:35:20,939 : Time 06h 17m 02s, ave eps reward [-5.95 -5.95 -5.95], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 7.17, mean reward -5.947394355446207, std reward 4.583672311152436, AG 0.0
2024-04-07 07:35:47,409 : Time 06h 17m 29s, ave eps reward [-4.8 -4.8 -4.8], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.94, mean reward -4.796618729252982, std reward 3.036225958122893, AG 0.0
2024-04-07 07:36:13,837 : Time 06h 17m 55s, ave eps reward [-4.12 -4.12 -4.12], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 6.91, mean reward -4.120088524532889, std reward 3.1400060557520044, AG 0.0
2024-04-07 07:36:40,003 : Time 06h 18m 21s, ave eps reward [-4.24 -4.24 -4.24], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 8.44, mean reward -4.237701811761933, std reward 2.6276721165884447, AG 0.0
2024-04-07 07:37:06,072 : Time 06h 18m 48s, ave eps reward [-4.17 -4.17 -4.17], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 9.2, mean reward -4.171249323504689, std reward 3.1042014637512083, AG 0.0
2024-04-07 07:37:32,572 : Time 06h 19m 14s, ave eps reward [-4.66 -4.66 -4.66], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.95, mean reward -4.65610037367779, std reward 3.235065160572993, AG 0.0
2024-04-07 07:37:59,267 : Time 06h 19m 41s, ave eps reward [-3.58 -3.58 -3.58], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 7.53, mean reward -3.576543602843226, std reward 2.124094887331587, AG 0.0
2024-04-07 07:38:25,196 : Time 06h 20m 07s, ave eps reward [-6.13 -6.13 -6.13], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 10.77, mean reward -6.134839190600562, std reward 5.356407169333664, AG 0.0
2024-04-07 07:38:51,572 : Time 06h 20m 33s, ave eps reward [-5.42 -5.42 -5.42], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 7.29, mean reward -5.423233820405509, std reward 4.803017106446881, AG 0.0
2024-04-07 07:39:17,998 : Time 06h 20m 59s, ave eps reward [-6.25 -6.25 -6.25], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 6.98, mean reward -6.254233007092306, std reward 5.877983831017567, AG 0.0
2024-04-07 07:39:44,450 : Time 06h 21m 26s, ave eps reward [-5.67 -5.67 -5.67], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 6.73, mean reward -5.671276680228201, std reward 4.5416574519471835, AG 0.0
2024-04-07 07:40:10,855 : Time 06h 21m 52s, ave eps reward [-4.15 -4.15 -4.15], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.89, mean reward -4.152776463405316, std reward 4.2445679663992335, AG 0.0
2024-04-07 07:40:36,968 : Time 06h 22m 18s, ave eps reward [-4.54 -4.54 -4.54], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 9.23, mean reward -4.540101923771334, std reward 3.3955588995766792, AG 0.0
2024-04-07 07:41:03,121 : Time 06h 22m 45s, ave eps reward [-5.32 -5.32 -5.32], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 8.42, mean reward -5.324322976077144, std reward 3.545473774091619, AG 0.0
2024-04-07 07:41:29,504 : Time 06h 23m 11s, ave eps reward [-4.73 -4.73 -4.73], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.75, mean reward -4.731825463756895, std reward 3.898746294320507, AG 0.0
2024-04-07 07:41:55,969 : Time 06h 23m 37s, ave eps reward [-4.46 -4.46 -4.46], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 10.56, mean reward -4.457339361035185, std reward 4.307752373526643, AG 0.0
2024-04-07 07:42:22,264 : Time 06h 24m 04s, ave eps reward [-5.77 -5.77 -5.77], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 7.66, mean reward -5.772734392729108, std reward 4.8642227596050995, AG 0.0
2024-04-07 07:42:48,667 : Time 06h 24m 30s, ave eps reward [-3.92 -3.92 -3.92], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 6.69, mean reward -3.91815445077249, std reward 1.7408823783480827, AG 0.0
2024-04-07 07:43:14,638 : Time 06h 24m 56s, ave eps reward [-6.12 -6.12 -6.12], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 9.1, mean reward -6.117778228478296, std reward 5.551293876330923, AG 0.0
2024-04-07 07:43:41,414 : Time 06h 25m 23s, ave eps reward [-3.61 -3.61 -3.61], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 6.84, mean reward -3.608895522413463, std reward 1.8916440745006244, AG 0.0
2024-04-07 07:44:07,610 : Time 06h 25m 49s, ave eps reward [-3.91 -3.91 -3.91], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 7.79, mean reward -3.9113750982845032, std reward 3.3240368951449746, AG 0.0
2024-04-07 07:44:33,512 : Time 06h 26m 15s, ave eps reward [-2.94 -2.94 -2.94], ave eps length 10.0, reward step [-0.29 -0.29 -0.29], FPS 10.07, mean reward -2.939497019794348, std reward 1.0289319972741293, AG 0.0
2024-04-07 07:44:59,965 : Time 06h 26m 41s, ave eps reward [-4.95 -4.95 -4.95], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.64, mean reward -4.945234675824002, std reward 5.294115271977651, AG 0.0
2024-04-07 07:45:26,417 : Time 06h 27m 08s, ave eps reward [-4.32 -4.32 -4.32], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 6.54, mean reward -4.320660164593005, std reward 3.8875877033375104, AG 0.0
2024-04-07 07:45:52,747 : Time 06h 27m 34s, ave eps reward [-4.27 -4.27 -4.27], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 9.35, mean reward -4.271172847483645, std reward 3.6094484840974856, AG 0.0
2024-04-07 07:46:19,157 : Time 06h 28m 01s, ave eps reward [-5.16 -5.16 -5.16], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 6.77, mean reward -5.164663090156767, std reward 6.110714903399208, AG 0.0
2024-04-07 07:46:45,534 : Time 06h 28m 27s, ave eps reward [-4.83 -4.83 -4.83], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.95, mean reward -4.832455152244909, std reward 4.542946640560854, AG 0.0
2024-04-07 07:47:11,473 : Time 06h 28m 53s, ave eps reward [-3.74 -3.74 -3.74], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 10.13, mean reward -3.743617423336853, std reward 3.1757232283376826, AG 0.0
2024-04-07 07:47:38,418 : Time 06h 29m 20s, ave eps reward [-4.74 -4.74 -4.74], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.81, mean reward -4.741071123088451, std reward 2.930144023128303, AG 0.0
2024-04-07 07:48:04,418 : Time 06h 29m 46s, ave eps reward [-4.9 -4.9 -4.9], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 8.66, mean reward -4.899916791099779, std reward 4.683825928972557, AG 0.0
2024-04-07 07:48:30,634 : Time 06h 30m 12s, ave eps reward [-4.15 -4.15 -4.15], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 8.31, mean reward -4.14676681421522, std reward 2.7050719505635916, AG 0.0
2024-04-07 07:48:57,010 : Time 06h 30m 38s, ave eps reward [-4.24 -4.24 -4.24], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.71, mean reward -4.243993193633623, std reward 3.379938478483109, AG 0.0
2024-04-07 07:49:23,446 : Time 06h 31m 05s, ave eps reward [-6.11 -6.11 -6.11], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 6.86, mean reward -6.109062096372079, std reward 5.650748363290872, AG 0.0
2024-04-07 07:49:49,888 : Time 06h 31m 31s, ave eps reward [-6.85 -6.85 -6.85], ave eps length 10.0, reward step [-0.69 -0.69 -0.69], FPS 6.75, mean reward -6.852295442493627, std reward 4.580750283125118, AG 0.0
2024-04-07 07:50:16,084 : Time 06h 31m 58s, ave eps reward [-3.82 -3.82 -3.82], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 7.57, mean reward -3.8236160546420415, std reward 2.0695665210918364, AG 0.0
2024-04-07 07:50:42,062 : Time 06h 32m 23s, ave eps reward [-5. -5. -5.], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 10.41, mean reward -4.996596309294194, std reward 3.7954370073368753, AG 0.0
2024-04-07 07:51:08,479 : Time 06h 32m 50s, ave eps reward [-4.55 -4.55 -4.55], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.82, mean reward -4.545982034400891, std reward 3.89873627545977, AG 0.0
2024-04-07 07:51:35,190 : Time 06h 33m 17s, ave eps reward [-3.95 -3.95 -3.95], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 7.8, mean reward -3.947005944643732, std reward 2.8503121220984804, AG 0.0
2024-04-07 07:52:01,278 : Time 06h 33m 43s, ave eps reward [-5.54 -5.54 -5.54], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 9.35, mean reward -5.540637079186569, std reward 4.533050285142386, AG 0.0
2024-04-07 07:52:27,658 : Time 06h 34m 09s, ave eps reward [-6.4 -6.4 -6.4], ave eps length 10.0, reward step [-0.64 -0.64 -0.64], FPS 6.8, mean reward -6.3952280316035415, std reward 5.007578243260098, AG 0.0
2024-04-07 07:52:53,858 : Time 06h 34m 35s, ave eps reward [-4. -4. -4.], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 7.27, mean reward -3.9984343701733094, std reward 3.6073029601823317, AG 0.0
2024-04-07 07:53:20,315 : Time 06h 35m 02s, ave eps reward [-4.41 -4.41 -4.41], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 6.98, mean reward -4.409859348653716, std reward 4.518633260180689, AG 0.0
2024-04-07 07:53:46,672 : Time 06h 35m 28s, ave eps reward [-3.19 -3.19 -3.19], ave eps length 10.0, reward step [-0.32 -0.32 -0.32], FPS 6.63, mean reward -3.1854632921499384, std reward 1.2858674624331206, AG 0.0
2024-04-07 07:54:13,087 : Time 06h 35m 55s, ave eps reward [-5.13 -5.13 -5.13], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 10.33, mean reward -5.131850968071713, std reward 3.649935097733224, AG 0.0
2024-04-07 07:54:39,443 : Time 06h 36m 21s, ave eps reward [-4.54 -4.54 -4.54], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 7.09, mean reward -4.537552702991459, std reward 3.5951960728483154, AG 0.0
2024-04-07 07:55:05,853 : Time 06h 36m 47s, ave eps reward [-4.72 -4.72 -4.72], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.92, mean reward -4.715834213679576, std reward 3.6733865705511506, AG 0.0
2024-04-07 07:55:32,263 : Time 06h 37m 14s, ave eps reward [-3.46 -3.46 -3.46], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 7.66, mean reward -3.4582601042727426, std reward 3.040761475645172, AG 0.0
2024-04-07 07:55:58,698 : Time 06h 37m 40s, ave eps reward [-3.96 -3.96 -3.96], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 6.69, mean reward -3.96391234171742, std reward 2.561307569191064, AG 0.0
2024-04-07 07:56:24,722 : Time 06h 38m 06s, ave eps reward [-3.18 -3.18 -3.18], ave eps length 10.0, reward step [-0.32 -0.32 -0.32], FPS 9.28, mean reward -3.180226510538037, std reward 1.2205852103638049, AG 0.0
2024-04-07 07:56:50,898 : Time 06h 38m 32s, ave eps reward [-3.1 -3.1 -3.1], ave eps length 10.0, reward step [-0.31 -0.31 -0.31], FPS 8.24, mean reward -3.1002631109642573, std reward 2.2168498541167807, AG 0.0
2024-04-07 07:57:17,619 : Time 06h 38m 59s, ave eps reward [-4.91 -4.91 -4.91], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 8.12, mean reward -4.912294249479019, std reward 4.259273794354266, AG 0.0
2024-04-07 07:57:43,806 : Time 06h 39m 25s, ave eps reward [-5.13 -5.13 -5.13], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 10.27, mean reward -5.125974765712271, std reward 3.9707164119919565, AG 0.0
2024-04-07 07:58:10,142 : Time 06h 39m 52s, ave eps reward [-5.17 -5.17 -5.17], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 8.43, mean reward -5.173089330798219, std reward 5.565517428243135, AG 0.0
2024-04-07 07:58:36,599 : Time 06h 40m 18s, ave eps reward [-3.73 -3.73 -3.73], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.81, mean reward -3.7257886462875307, std reward 2.2449891079937254, AG 0.0
2024-04-07 07:59:03,057 : Time 06h 40m 44s, ave eps reward [-3.32 -3.32 -3.32], ave eps length 10.0, reward step [-0.33 -0.33 -0.33], FPS 6.9, mean reward -3.3187814538313147, std reward 2.212298608525192, AG 0.0
2024-04-07 07:59:29,466 : Time 06h 41m 11s, ave eps reward [-3.58 -3.58 -3.58], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 7.06, mean reward -3.5803053496169737, std reward 1.9218055227351096, AG 0.0
2024-04-07 07:59:55,816 : Time 06h 41m 37s, ave eps reward [-5.02 -5.02 -5.02], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 6.88, mean reward -5.020042468299071, std reward 4.493267858664464, AG 0.0
2024-04-07 08:00:21,966 : Time 06h 42m 03s, ave eps reward [-5.58 -5.58 -5.58], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 8.64, mean reward -5.577780219962843, std reward 4.320415717713806, AG 0.0
2024-04-07 08:00:48,154 : Time 06h 42m 30s, ave eps reward [-3.54 -3.54 -3.54], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 8.82, mean reward -3.539092554895844, std reward 1.9843628318021596, AG 0.0
2024-04-07 08:01:14,957 : Time 06h 42m 56s, ave eps reward [-3.36 -3.36 -3.36], ave eps length 10.0, reward step [-0.34 -0.34 -0.34], FPS 7.25, mean reward -3.3623913642345733, std reward 1.3222753690908928, AG 0.0
2024-04-07 08:01:40,836 : Time 06h 43m 22s, ave eps reward [-4.42 -4.42 -4.42], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 10.74, mean reward -4.421220538612047, std reward 2.341739116948716, AG 0.0
2024-04-07 08:02:07,261 : Time 06h 43m 49s, ave eps reward [-4.01 -4.01 -4.01], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 7.24, mean reward -4.006420723312997, std reward 2.637479872214809, AG 0.0
2024-04-07 08:02:33,644 : Time 06h 44m 15s, ave eps reward [-4.49 -4.49 -4.49], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.85, mean reward -4.494935305845444, std reward 5.243781996577775, AG 0.0
2024-04-07 08:02:59,955 : Time 06h 44m 41s, ave eps reward [-4.43 -4.43 -4.43], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 6.82, mean reward -4.434103614602556, std reward 3.2552913535633516, AG 0.0
2024-04-07 08:03:26,321 : Time 06h 45m 08s, ave eps reward [-4.25 -4.25 -4.25], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 6.86, mean reward -4.252528235700168, std reward 4.178942762516675, AG 0.0
2024-04-07 08:03:52,657 : Time 06h 45m 34s, ave eps reward [-4.67 -4.67 -4.67], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 7.2, mean reward -4.670649140639484, std reward 3.4752046318069443, AG 0.0
2024-04-07 08:04:18,714 : Time 06h 46m 00s, ave eps reward [-5.21 -5.21 -5.21], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 9.44, mean reward -5.205903800272881, std reward 4.325848011339312, AG 0.0
2024-04-07 08:04:44,980 : Time 06h 46m 26s, ave eps reward [-3.69 -3.69 -3.69], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 7.96, mean reward -3.686476283185648, std reward 4.069796943571935, AG 0.0
2024-04-07 08:05:11,726 : Time 06h 46m 53s, ave eps reward [-3.8 -3.8 -3.8], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 7.77, mean reward -3.8049813844761013, std reward 2.506829464174017, AG 0.0
2024-04-07 08:05:37,712 : Time 06h 47m 19s, ave eps reward [-3.96 -3.96 -3.96], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 9.76, mean reward -3.960629282889483, std reward 1.7962789992396748, AG 0.0
2024-04-07 08:06:04,202 : Time 06h 47m 46s, ave eps reward [-4.23 -4.23 -4.23], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.51, mean reward -4.229872148400612, std reward 2.7233181360695204, AG 0.0
2024-04-07 08:06:30,440 : Time 06h 48m 12s, ave eps reward [-4.55 -4.55 -4.55], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 7.79, mean reward -4.551434576675855, std reward 2.5049182223893878, AG 0.0
2024-04-07 08:06:56,971 : Time 06h 48m 38s, ave eps reward [-3.28 -3.28 -3.28], ave eps length 10.0, reward step [-0.33 -0.33 -0.33], FPS 6.75, mean reward -3.2797126744691, std reward 2.339810744953185, AG 0.0
2024-04-07 08:07:23,284 : Time 06h 49m 05s, ave eps reward [-4.25 -4.25 -4.25], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.92, mean reward -4.249750993494946, std reward 4.957543557546374, AG 0.0
2024-04-07 08:07:49,346 : Time 06h 49m 31s, ave eps reward [-7.54 -7.54 -7.54], ave eps length 10.0, reward step [-0.75 -0.75 -0.75], FPS 9.22, mean reward -7.542381085961951, std reward 5.321972636425474, AG 0.0
2024-04-07 08:08:15,701 : Time 06h 49m 57s, ave eps reward [-5.21 -5.21 -5.21], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 7.23, mean reward -5.2111706427281685, std reward 4.781511280268195, AG 0.0
2024-04-07 08:08:42,133 : Time 06h 50m 24s, ave eps reward [-4.4 -4.4 -4.4], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 6.62, mean reward -4.396804626429878, std reward 3.645998855024913, AG 0.0
2024-04-07 08:09:08,459 : Time 06h 50m 50s, ave eps reward [-4.32 -4.32 -4.32], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 8.27, mean reward -4.32176355451128, std reward 2.126946361073968, AG 0.0
2024-04-07 08:09:34,922 : Time 06h 51m 16s, ave eps reward [-3.47 -3.47 -3.47], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 6.94, mean reward -3.467544690240744, std reward 2.294724015482337, AG 0.0
2024-04-07 08:10:00,960 : Time 06h 51m 42s, ave eps reward [-5.87 -5.87 -5.87], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 8.57, mean reward -5.866885626425521, std reward 5.940968994175106, AG 0.0
2024-04-07 08:10:27,111 : Time 06h 52m 09s, ave eps reward [-4.97 -4.97 -4.97], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 10.49, mean reward -4.968464010964473, std reward 3.893688323899335, AG 0.0
2024-04-07 08:10:54,117 : Time 06h 52m 36s, ave eps reward [-5.08 -5.08 -5.08], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.76, mean reward -5.080412557364878, std reward 4.286225603311512, AG 0.0
2024-04-07 08:11:20,133 : Time 06h 53m 02s, ave eps reward [-4.29 -4.29 -4.29], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 9.0, mean reward -4.289137650581872, std reward 3.9702535584617618, AG 0.0
2024-04-07 08:11:46,324 : Time 06h 53m 28s, ave eps reward [-4.69 -4.69 -4.69], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 8.39, mean reward -4.686345018032309, std reward 4.412879311115789, AG 0.0
2024-04-07 08:12:12,782 : Time 06h 53m 54s, ave eps reward [-4.54 -4.54 -4.54], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.72, mean reward -4.538659870287029, std reward 4.044933597160934, AG 0.0
2024-04-07 08:12:39,160 : Time 06h 54m 21s, ave eps reward [-3.67 -3.67 -3.67], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.89, mean reward -3.673259592505455, std reward 2.4258058362855386, AG 0.0
2024-04-07 08:13:05,560 : Time 06h 54m 47s, ave eps reward [-5.13 -5.13 -5.13], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.79, mean reward -5.134254656361131, std reward 2.6532833365681556, AG 0.0
2024-04-07 08:13:31,905 : Time 06h 55m 13s, ave eps reward [-3.27 -3.27 -3.27], ave eps length 10.0, reward step [-0.33 -0.33 -0.33], FPS 6.87, mean reward -3.2660045893771374, std reward 1.2947411252523588, AG 0.0
2024-04-07 08:13:57,846 : Time 06h 55m 39s, ave eps reward [-4.66 -4.66 -4.66], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 10.65, mean reward -4.662396880249611, std reward 2.6746067932661357, AG 0.0
2024-04-07 08:14:24,205 : Time 06h 56m 06s, ave eps reward [-4.79 -4.79 -4.79], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.88, mean reward -4.7863986420593125, std reward 3.996795273322851, AG 0.0
2024-04-07 08:14:50,716 : Time 06h 56m 32s, ave eps reward [-6.47 -6.47 -6.47], ave eps length 10.0, reward step [-0.65 -0.65 -0.65], FPS 8.79, mean reward -6.473140882529347, std reward 5.874543979609451, AG 0.0
2024-04-07 08:15:16,858 : Time 06h 56m 58s, ave eps reward [-4.3 -4.3 -4.3], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 8.76, mean reward -4.299773855194516, std reward 2.799218984088888, AG 0.0
2024-04-07 08:15:43,258 : Time 06h 57m 25s, ave eps reward [-4.27 -4.27 -4.27], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 6.75, mean reward -4.267584896434217, std reward 3.2514901224062935, AG 0.0
2024-04-07 08:16:09,429 : Time 06h 57m 51s, ave eps reward [-4.79 -4.79 -4.79], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 8.21, mean reward -4.787526560262221, std reward 2.271271199053429, AG 0.0
2024-04-07 08:16:36,077 : Time 06h 58m 18s, ave eps reward [-4.36 -4.36 -4.36], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 6.95, mean reward -4.36073913357764, std reward 3.226907773448139, AG 0.0
2024-04-07 08:17:02,435 : Time 06h 58m 44s, ave eps reward [-4.94 -4.94 -4.94], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.83, mean reward -4.941032170842696, std reward 3.5733815791251913, AG 0.0
2024-04-07 08:17:28,640 : Time 06h 59m 10s, ave eps reward [-5.15 -5.15 -5.15], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 7.87, mean reward -5.152777649123946, std reward 4.4268225779841055, AG 0.0
2024-04-07 08:17:54,617 : Time 06h 59m 36s, ave eps reward [-5.45 -5.45 -5.45], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 9.66, mean reward -5.451846757503034, std reward 5.660822565131965, AG 0.0
2024-04-07 08:18:21,039 : Time 07h 00m 02s, ave eps reward [-4.66 -4.66 -4.66], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.7, mean reward -4.663489763643406, std reward 3.555991333878945, AG 0.0
2024-04-07 08:18:47,852 : Time 07h 00m 29s, ave eps reward [-4.07 -4.07 -4.07], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 7.89, mean reward -4.067637319803412, std reward 1.853552281094174, AG 0.0
2024-04-07 08:19:13,898 : Time 07h 00m 55s, ave eps reward [-3.48 -3.48 -3.48], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 9.72, mean reward -3.475307503426709, std reward 1.6554778149211236, AG 0.0
2024-04-07 08:19:40,309 : Time 07h 01m 22s, ave eps reward [-5.52 -5.52 -5.52], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 6.6, mean reward -5.5153856415464, std reward 3.5456360545261534, AG 0.0
2024-04-07 08:20:06,685 : Time 07h 01m 48s, ave eps reward [-5.69 -5.69 -5.69], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 6.96, mean reward -5.6928644679720675, std reward 4.687146950041187, AG 0.0
2024-04-07 08:20:33,140 : Time 07h 02m 15s, ave eps reward [-5.59 -5.59 -5.59], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.93, mean reward -5.592069061502742, std reward 4.71886761890673, AG 0.0
2024-04-07 08:20:59,565 : Time 07h 02m 41s, ave eps reward [-3.65 -3.65 -3.65], ave eps length 10.0, reward step [-0.36 -0.36 -0.36], FPS 6.69, mean reward -3.646504018993634, std reward 2.1761374291776954, AG 0.0
2024-04-07 08:21:25,558 : Time 07h 03m 07s, ave eps reward [-5.37 -5.37 -5.37], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 9.44, mean reward -5.368254307677271, std reward 4.021200926651843, AG 0.0
2024-04-07 08:21:51,736 : Time 07h 03m 33s, ave eps reward [-3.72 -3.72 -3.72], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 8.29, mean reward -3.7245004350514948, std reward 1.8319652181939383, AG 0.0
2024-04-07 08:22:18,141 : Time 07h 04m 00s, ave eps reward [-4.24 -4.24 -4.24], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.77, mean reward -4.235217678153489, std reward 2.8262943784497003, AG 0.0
2024-04-07 08:22:44,613 : Time 07h 04m 26s, ave eps reward [-4.02 -4.02 -4.02], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 10.29, mean reward -4.022870839926048, std reward 2.6033380761931255, AG 0.0
2024-04-07 08:23:11,021 : Time 07h 04m 52s, ave eps reward [-3.48 -3.48 -3.48], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 7.51, mean reward -3.476300883902451, std reward 1.4486040513487812, AG 0.0
2024-04-07 08:23:37,486 : Time 07h 05m 19s, ave eps reward [-4.8 -4.8 -4.8], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.95, mean reward -4.801317116635834, std reward 5.9602302434649035, AG 0.0
2024-04-07 08:24:03,539 : Time 07h 05m 45s, ave eps reward [-5.53 -5.53 -5.53], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 9.4, mean reward -5.527121583446796, std reward 5.298409736126622, AG 0.0
2024-04-07 08:24:30,327 : Time 07h 06m 12s, ave eps reward [-3.72 -3.72 -3.72], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.84, mean reward -3.716841713413281, std reward 1.853640174145177, AG 0.0
2024-04-07 08:24:56,555 : Time 07h 06m 38s, ave eps reward [-5.53 -5.53 -5.53], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 7.93, mean reward -5.526823701096119, std reward 2.9771345792685127, AG 0.0
2024-04-07 08:25:22,542 : Time 07h 07m 04s, ave eps reward [-5.07 -5.07 -5.07], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 9.6, mean reward -5.072318887694322, std reward 3.348918860456953, AG 0.0
2024-04-07 08:25:49,000 : Time 07h 07m 30s, ave eps reward [-4.16 -4.16 -4.16], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.65, mean reward -4.160219671218931, std reward 2.6435234417568725, AG 0.0
2024-04-07 08:26:15,419 : Time 07h 07m 57s, ave eps reward [-4.49 -4.49 -4.49], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.75, mean reward -4.4887134897017305, std reward 3.161454554187215, AG 0.0
2024-04-07 08:26:41,804 : Time 07h 08m 23s, ave eps reward [-5.73 -5.73 -5.73], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 7.44, mean reward -5.727644962987585, std reward 4.660369834574892, AG 0.0
2024-04-07 08:27:08,373 : Time 07h 08m 50s, ave eps reward [-5.73 -5.73 -5.73], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 6.85, mean reward -5.726117408967783, std reward 4.640941936729554, AG 0.0
2024-04-07 08:27:34,422 : Time 07h 09m 16s, ave eps reward [-7.41 -7.41 -7.41], ave eps length 10.0, reward step [-0.74 -0.74 -0.74], FPS 8.61, mean reward -7.414675592902117, std reward 6.082241393517932, AG 0.0
2024-04-07 08:28:00,612 : Time 07h 09m 42s, ave eps reward [-4.56 -4.56 -4.56], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 8.1, mean reward -4.559782313869553, std reward 4.03666424744735, AG 0.0
2024-04-07 08:28:27,389 : Time 07h 10m 09s, ave eps reward [-4.3 -4.3 -4.3], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 7.61, mean reward -4.301966467627565, std reward 2.5067327394580468, AG 0.0
2024-04-07 08:28:53,546 : Time 07h 10m 35s, ave eps reward [-4.97 -4.97 -4.97], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 10.49, mean reward -4.968427608783076, std reward 2.423330784504209, AG 0.0
2024-04-07 08:29:19,899 : Time 07h 11m 01s, ave eps reward [-4.81 -4.81 -4.81], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 7.97, mean reward -4.8097311431557985, std reward 4.287769873319975, AG 0.0
2024-04-07 08:29:46,274 : Time 07h 11m 28s, ave eps reward [-4.75 -4.75 -4.75], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.68, mean reward -4.749068829330379, std reward 3.8146019574861487, AG 0.0
2024-04-07 08:30:12,719 : Time 07h 11m 54s, ave eps reward [-7.29 -7.29 -7.29], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 6.75, mean reward -7.290712886728967, std reward 5.3677719675691185, AG 0.0
2024-04-07 08:30:39,172 : Time 07h 12m 21s, ave eps reward [-5.4 -5.4 -5.4], ave eps length 10.0, reward step [-0.54 -0.54 -0.54], FPS 6.64, mean reward -5.399715559802928, std reward 4.117526147095026, AG 0.0
2024-04-07 08:31:05,509 : Time 07h 12m 47s, ave eps reward [-5.51 -5.51 -5.51], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 7.15, mean reward -5.509254005430865, std reward 3.8498012483338013, AG 0.0
2024-04-07 08:31:31,372 : Time 07h 13m 13s, ave eps reward [-5.15 -5.15 -5.15], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 10.79, mean reward -5.150482728359936, std reward 2.7347199228041803, AG 0.0
2024-04-07 08:31:57,814 : Time 07h 13m 39s, ave eps reward [-4.95 -4.95 -4.95], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 6.89, mean reward -4.953018175469781, std reward 2.92333901607429, AG 0.0
2024-04-07 08:32:24,326 : Time 07h 14m 06s, ave eps reward [-3.93 -3.93 -3.93], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 9.06, mean reward -3.928210419804268, std reward 2.1397765815889396, AG 0.0
2024-04-07 08:32:50,547 : Time 07h 14m 32s, ave eps reward [-5.6 -5.6 -5.6], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 7.81, mean reward -5.59915814085868, std reward 4.993580852312485, AG 0.0
2024-04-07 08:33:16,899 : Time 07h 14m 58s, ave eps reward [-7.62 -7.62 -7.62], ave eps length 10.0, reward step [-0.76 -0.76 -0.76], FPS 6.83, mean reward -7.617789123200969, std reward 6.128203227620051, AG 0.0
2024-04-07 08:33:43,023 : Time 07h 15m 24s, ave eps reward [-4.61 -4.61 -4.61], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 8.25, mean reward -4.60560494298345, std reward 3.128807851420481, AG 0.0
2024-04-07 08:34:09,772 : Time 07h 15m 51s, ave eps reward [-4.93 -4.93 -4.93], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.66, mean reward -4.929985521712673, std reward 3.262913273376563, AG 0.0
2024-04-07 08:34:36,130 : Time 07h 16m 18s, ave eps reward [-5.28 -5.28 -5.28], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 6.89, mean reward -5.276182372248934, std reward 2.9668821261250424, AG 0.0
2024-04-07 08:35:02,032 : Time 07h 16m 43s, ave eps reward [-4.8 -4.8 -4.8], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 9.87, mean reward -4.795489808834288, std reward 3.098167961724199, AG 0.0
2024-04-07 08:35:28,371 : Time 07h 17m 10s, ave eps reward [-6.65 -6.65 -6.65], ave eps length 10.0, reward step [-0.66 -0.66 -0.66], FPS 9.26, mean reward -6.647483143093351, std reward 6.18938424469312, AG 0.0
2024-04-07 08:35:54,780 : Time 07h 17m 36s, ave eps reward [-7.26 -7.26 -7.26], ave eps length 10.0, reward step [-0.73 -0.73 -0.73], FPS 6.71, mean reward -7.255147837195163, std reward 4.995151044547198, AG 0.0
2024-04-07 08:36:21,239 : Time 07h 18m 03s, ave eps reward [-4.82 -4.82 -4.82], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 9.96, mean reward -4.81970986348513, std reward 4.757717847532416, AG 0.0
2024-04-07 08:36:47,510 : Time 07h 18m 29s, ave eps reward [-3.92 -3.92 -3.92], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 8.22, mean reward -3.9227594107769628, std reward 2.034504364563454, AG 0.0
2024-04-07 08:37:13,957 : Time 07h 18m 55s, ave eps reward [-3.65 -3.65 -3.65], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.94, mean reward -3.6522916629538456, std reward 1.5327716561967655, AG 0.0
2024-04-07 08:37:39,996 : Time 07h 19m 21s, ave eps reward [-3.49 -3.49 -3.49], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 8.51, mean reward -3.4850401566758142, std reward 2.2639930520705898, AG 0.0
2024-04-07 08:38:06,622 : Time 07h 19m 48s, ave eps reward [-4.12 -4.12 -4.12], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 6.85, mean reward -4.120794622744642, std reward 1.86824114218499, AG 0.0
2024-04-07 08:38:32,945 : Time 07h 20m 14s, ave eps reward [-4.91 -4.91 -4.91], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 7.32, mean reward -4.907687188019191, std reward 3.2250436746251836, AG 0.0
2024-04-07 08:38:58,893 : Time 07h 20m 40s, ave eps reward [-6.3 -6.3 -6.3], ave eps length 10.0, reward step [-0.63 -0.63 -0.63], FPS 10.21, mean reward -6.29531439841084, std reward 6.086982320724241, AG 0.0
2024-04-07 08:39:25,281 : Time 07h 21m 07s, ave eps reward [-5.73 -5.73 -5.73], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 7.95, mean reward -5.73247490529325, std reward 4.426008899784047, AG 0.0
2024-04-07 08:39:51,712 : Time 07h 21m 33s, ave eps reward [-4.8 -4.8 -4.8], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.77, mean reward -4.802266245451184, std reward 5.021278197007537, AG 0.0
2024-04-07 08:40:18,014 : Time 07h 21m 59s, ave eps reward [-4.57 -4.57 -4.57], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 10.84, mean reward -4.570401463811852, std reward 3.2085478686784787, AG 0.0
2024-04-07 08:40:44,480 : Time 07h 22m 26s, ave eps reward [-3.87 -3.87 -3.87], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 7.4, mean reward -3.868662031586254, std reward 1.8548051844214892, AG 0.0
2024-04-07 08:41:10,846 : Time 07h 22m 52s, ave eps reward [-4.45 -4.45 -4.45], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 6.72, mean reward -4.446223603854093, std reward 3.11839837728276, AG 0.0
2024-04-07 08:41:37,031 : Time 07h 23m 18s, ave eps reward [-4.01 -4.01 -4.01], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 8.15, mean reward -4.012929903266707, std reward 2.232515063396539, AG 0.0
2024-04-07 08:42:03,686 : Time 07h 23m 45s, ave eps reward [-4.5 -4.5 -4.5], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.75, mean reward -4.500853436357891, std reward 3.6223515275797626, AG 0.0
2024-04-07 08:42:29,995 : Time 07h 24m 11s, ave eps reward [-5.63 -5.63 -5.63], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.84, mean reward -5.62588939117403, std reward 5.112338394584069, AG 0.0
2024-04-07 08:42:55,925 : Time 07h 24m 37s, ave eps reward [-4.62 -4.62 -4.62], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 9.27, mean reward -4.619506372365676, std reward 3.636074962044022, AG 0.0
2024-04-07 08:43:22,057 : Time 07h 25m 03s, ave eps reward [-3.34 -3.34 -3.34], ave eps length 10.0, reward step [-0.33 -0.33 -0.33], FPS 8.03, mean reward -3.341780845367235, std reward 1.2526828204074127, AG 0.0
2024-04-07 08:43:48,470 : Time 07h 25m 30s, ave eps reward [-4.19 -4.19 -4.19], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.9, mean reward -4.193940625390844, std reward 3.3007995094390306, AG 0.0
2024-04-07 08:44:14,855 : Time 07h 25m 56s, ave eps reward [-3.11 -3.11 -3.11], ave eps length 10.0, reward step [-0.31 -0.31 -0.31], FPS 9.3, mean reward -3.114966019677097, std reward 1.0225497428826504, AG 0.0
2024-04-07 08:44:41,218 : Time 07h 26m 23s, ave eps reward [-8.77 -8.77 -8.77], ave eps length 10.0, reward step [-0.88 -0.88 -0.88], FPS 6.68, mean reward -8.771223914850324, std reward 6.334316916355457, AG 0.0
2024-04-07 08:45:07,632 : Time 07h 26m 49s, ave eps reward [-5.09 -5.09 -5.09], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 7.05, mean reward -5.093292276130071, std reward 5.010704793997683, AG 0.0
2024-04-07 08:45:33,565 : Time 07h 27m 15s, ave eps reward [-3.89 -3.89 -3.89], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 10.8, mean reward -3.894338164103395, std reward 2.598195065254961, AG 0.0
2024-04-07 08:46:00,471 : Time 07h 27m 42s, ave eps reward [-3.31 -3.31 -3.31], ave eps length 10.0, reward step [-0.33 -0.33 -0.33], FPS 6.95, mean reward -3.310951130246643, std reward 1.9705855456874004, AG 0.0
2024-04-07 08:46:26,519 : Time 07h 28m 08s, ave eps reward [-4.28 -4.28 -4.28], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 8.43, mean reward -4.278814314356137, std reward 2.826228368032222, AG 0.0
2024-04-07 08:46:52,662 : Time 07h 28m 34s, ave eps reward [-4.61 -4.61 -4.61], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 8.83, mean reward -4.61393987738131, std reward 4.354503214052497, AG 0.0
2024-04-07 08:47:19,123 : Time 07h 29m 01s, ave eps reward [-4.2 -4.2 -4.2], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.76, mean reward -4.2000693144107455, std reward 2.5995585344403236, AG 0.0
2024-04-07 08:47:45,517 : Time 07h 29m 27s, ave eps reward [-4.06 -4.06 -4.06], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 6.9, mean reward -4.060249514963834, std reward 2.205875523013982, AG 0.0
2024-04-07 08:48:11,978 : Time 07h 29m 53s, ave eps reward [-3.78 -3.78 -3.78], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 7.44, mean reward -3.7831106262868373, std reward 2.7696475028562393, AG 0.0
2024-04-07 08:48:38,360 : Time 07h 30m 20s, ave eps reward [-4.85 -4.85 -4.85], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.94, mean reward -4.851392419735525, std reward 4.006304207058729, AG 0.0
2024-04-07 08:49:04,480 : Time 07h 30m 46s, ave eps reward [-4.5 -4.5 -4.5], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 9.0, mean reward -4.4989306420612545, std reward 3.208437996407246, AG 0.0
2024-04-07 08:49:30,664 : Time 07h 31m 12s, ave eps reward [-5.12 -5.12 -5.12], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 8.3, mean reward -5.115743035811508, std reward 4.079640984049155, AG 0.0
2024-04-07 08:49:57,453 : Time 07h 31m 39s, ave eps reward [-4.59 -4.59 -4.59], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 7.74, mean reward -4.592352623709293, std reward 3.4554177853688026, AG 0.0
2024-04-07 08:50:23,438 : Time 07h 32m 05s, ave eps reward [-3.22 -3.22 -3.22], ave eps length 10.0, reward step [-0.32 -0.32 -0.32], FPS 9.97, mean reward -3.215518535539233, std reward 1.5382642263063326, AG 0.0
2024-04-07 08:50:49,906 : Time 07h 32m 31s, ave eps reward [-4.9 -4.9 -4.9], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.62, mean reward -4.9038189242819366, std reward 4.222403284996246, AG 0.0
2024-04-07 08:51:16,201 : Time 07h 32m 58s, ave eps reward [-3.83 -3.83 -3.83], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 7.19, mean reward -3.828403078427801, std reward 2.5256993935667045, AG 0.0
2024-04-07 08:51:42,599 : Time 07h 33m 24s, ave eps reward [-3.42 -3.42 -3.42], ave eps length 10.0, reward step [-0.34 -0.34 -0.34], FPS 6.94, mean reward -3.4237493939002315, std reward 1.4327690792462475, AG 0.0
2024-04-07 08:52:08,999 : Time 07h 33m 50s, ave eps reward [-5.05 -5.05 -5.05], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 6.84, mean reward -5.054829072324654, std reward 2.891617033454543, AG 0.0
2024-04-07 08:52:35,023 : Time 07h 34m 16s, ave eps reward [-4.49 -4.49 -4.49], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 9.09, mean reward -4.490465397260745, std reward 2.8873110646378857, AG 0.0
2024-04-07 08:53:01,142 : Time 07h 34m 43s, ave eps reward [-3.89 -3.89 -3.89], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 8.29, mean reward -3.8927879335518982, std reward 2.0883185445532484, AG 0.0
2024-04-07 08:53:27,535 : Time 07h 35m 09s, ave eps reward [-3.71 -3.71 -3.71], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.85, mean reward -3.7086075919411825, std reward 1.6508908808242828, AG 0.0
2024-04-07 08:53:54,016 : Time 07h 35m 35s, ave eps reward [-4.56 -4.56 -4.56], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 10.22, mean reward -4.564500745412635, std reward 3.2064857454332283, AG 0.0
2024-04-07 08:54:20,293 : Time 07h 36m 02s, ave eps reward [-5.82 -5.82 -5.82], ave eps length 10.0, reward step [-0.58 -0.58 -0.58], FPS 8.18, mean reward -5.817644399742171, std reward 5.02661056048687, AG 0.0
2024-04-07 08:54:46,634 : Time 07h 36m 28s, ave eps reward [-5.61 -5.61 -5.61], ave eps length 10.0, reward step [-0.56 -0.56 -0.56], FPS 6.77, mean reward -5.607138786489889, std reward 4.624879973814715, AG 0.0
2024-04-07 08:55:12,764 : Time 07h 36m 54s, ave eps reward [-3.9 -3.9 -3.9], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 8.42, mean reward -3.895698473989401, std reward 2.4217276988816336, AG 0.0
2024-04-07 08:55:39,430 : Time 07h 37m 21s, ave eps reward [-4.14 -4.14 -4.14], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 6.88, mean reward -4.140823685137332, std reward 3.1773173458553208, AG 0.0
2024-04-07 08:56:05,734 : Time 07h 37m 47s, ave eps reward [-4.36 -4.36 -4.36], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 7.06, mean reward -4.3598465845287375, std reward 2.6595958209672528, AG 0.0
2024-04-07 08:56:31,691 : Time 07h 38m 13s, ave eps reward [-6.13 -6.13 -6.13], ave eps length 10.0, reward step [-0.61 -0.61 -0.61], FPS 9.7, mean reward -6.133755582587419, std reward 4.287651750098919, AG 0.0
2024-04-07 08:56:57,975 : Time 07h 38m 39s, ave eps reward [-5.07 -5.07 -5.07], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 8.06, mean reward -5.065733550382644, std reward 3.913699685534407, AG 0.0
2024-04-07 08:57:24,353 : Time 07h 39m 06s, ave eps reward [-7.17 -7.17 -7.17], ave eps length 10.0, reward step [-0.72 -0.72 -0.72], FPS 6.73, mean reward -7.166224238228116, std reward 7.400364459594783, AG 0.0
2024-04-07 08:57:50,750 : Time 07h 39m 32s, ave eps reward [-4.58 -4.58 -4.58], ave eps length 10.0, reward step [-0.46 -0.46 -0.46], FPS 9.01, mean reward -4.581473764331295, std reward 2.287025756451834, AG 0.0
2024-04-07 08:58:17,206 : Time 07h 39m 59s, ave eps reward [-4.77 -4.77 -4.77], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.57, mean reward -4.771199491609057, std reward 3.2618558828906115, AG 0.0
2024-04-07 08:58:43,460 : Time 07h 40m 25s, ave eps reward [-4.43 -4.43 -4.43], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 7.74, mean reward -4.425634912670232, std reward 3.125062062309222, AG 0.0
2024-04-07 08:59:09,506 : Time 07h 40m 51s, ave eps reward [-5.2 -5.2 -5.2], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 10.15, mean reward -5.201766298702387, std reward 3.6218822124032055, AG 0.0
2024-04-07 08:59:36,448 : Time 07h 41m 18s, ave eps reward [-4. -4. -4.], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 6.94, mean reward -3.996336975978174, std reward 2.684080881627701, AG 0.0
2024-04-07 09:00:02,755 : Time 07h 41m 44s, ave eps reward [-5.72 -5.72 -5.72], ave eps length 10.0, reward step [-0.57 -0.57 -0.57], FPS 7.42, mean reward -5.723281590906992, std reward 5.737462308552383, AG 0.0
2024-04-07 09:00:29,065 : Time 07h 42m 10s, ave eps reward [-4.39 -4.39 -4.39], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 9.07, mean reward -4.386127722462511, std reward 1.8979730518269218, AG 0.0
2024-04-07 09:00:55,285 : Time 07h 42m 37s, ave eps reward [-4.11 -4.11 -4.11], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 7.9, mean reward -4.110932581014798, std reward 3.235455135835372, AG 0.0
2024-04-07 09:01:21,745 : Time 07h 43m 03s, ave eps reward [-6. -6. -6.], ave eps length 10.0, reward step [-0.6 -0.6 -0.6], FPS 6.73, mean reward -6.0030872791588274, std reward 6.405284444141036, AG 0.0
2024-04-07 09:01:48,155 : Time 07h 43m 30s, ave eps reward [-4.81 -4.81 -4.81], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 9.59, mean reward -4.813250363695781, std reward 3.5590556665803175, AG 0.0
2024-04-07 09:02:14,505 : Time 07h 43m 56s, ave eps reward [-4.29 -4.29 -4.29], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 6.61, mean reward -4.291903572866928, std reward 3.6138251246070445, AG 0.0
2024-04-07 09:02:40,807 : Time 07h 44m 22s, ave eps reward [-3.78 -3.78 -3.78], ave eps length 10.0, reward step [-0.38 -0.38 -0.38], FPS 7.48, mean reward -3.779453288771866, std reward 2.9335969153802264, AG 0.0
2024-04-07 09:03:06,784 : Time 07h 44m 48s, ave eps reward [-5.52 -5.52 -5.52], ave eps length 10.0, reward step [-0.55 -0.55 -0.55], FPS 10.36, mean reward -5.524052304987466, std reward 3.852732197559472, AG 0.0
2024-04-07 09:03:33,868 : Time 07h 45m 15s, ave eps reward [-4.68 -4.68 -4.68], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 6.84, mean reward -4.682743017797715, std reward 2.7122329437963413, AG 0.0
2024-04-07 09:03:59,777 : Time 07h 45m 41s, ave eps reward [-4.01 -4.01 -4.01], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 9.86, mean reward -4.01211099918566, std reward 2.1417147161071486, AG 0.0
2024-04-07 09:04:26,054 : Time 07h 46m 07s, ave eps reward [-5.16 -5.16 -5.16], ave eps length 10.0, reward step [-0.52 -0.52 -0.52], FPS 7.8, mean reward -5.163768272075666, std reward 4.008472495943246, AG 0.0
2024-04-07 09:04:52,285 : Time 07h 46m 34s, ave eps reward [-2.92 -2.92 -2.92], ave eps length 10.0, reward step [-0.29 -0.29 -0.29], FPS 6.76, mean reward -2.916966211054085, std reward 1.2706910554702207, AG 0.0
2024-04-07 09:05:18,736 : Time 07h 47m 00s, ave eps reward [-4.04 -4.04 -4.04], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 6.61, mean reward -4.039839450039824, std reward 2.830077665967795, AG 0.0
2024-04-07 09:05:45,116 : Time 07h 47m 27s, ave eps reward [-3.2 -3.2 -3.2], ave eps length 10.0, reward step [-0.32 -0.32 -0.32], FPS 6.61, mean reward -3.201028887111484, std reward 1.8939685330520528, AG 0.0
2024-04-07 09:06:11,434 : Time 07h 47m 53s, ave eps reward [-3.86 -3.86 -3.86], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 7.03, mean reward -3.862018721682829, std reward 3.384142718467053, AG 0.0
2024-04-07 09:06:37,619 : Time 07h 48m 19s, ave eps reward [-4.26 -4.26 -4.26], ave eps length 10.0, reward step [-0.43 -0.43 -0.43], FPS 8.38, mean reward -4.258963732017849, std reward 3.704214378526543, AG 0.0
---------------------
gamma: 0.11501129861637652
training start after waiting for 1.1595022678375244 seconds
policy loss:-1450.058349609375
value loss:20.89120101928711
entropies:47.329105377197266
Policy training finished
---------------------
gamma: 0.11501129861637652
training start after waiting for 1.146362543106079 seconds
policy loss:-728.906005859375
value loss:16.66817283630371
entropies:39.753257751464844
Policy training finished
---------------------
gamma: 0.11501129861637652
training start after waiting for 1.1368789672851562 seconds
policy loss:-205.77418518066406
value loss:13.361159324645996
entropies:38.196067810058594
Policy training finished
---------------------
gamma: 0.11501129861637652
training start after waiting for 1.2046208381652832 seconds
policy loss:-234.10342407226562
value loss:10.150801658630371
entropies:34.282596588134766
Policy training finished
---------------------
gamma: 0.11501129861637652
training start after waiting for 1.1486413478851318 seconds
policy loss:-377.73431396484375
value loss:17.080791473388672
entropies:41.355621337890625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1259.5323)
ToM Target loss= tensor(2221.1392)
optimized based on ToM loss
---------------------
gamma: 0.11524132121360928
training start after waiting for 1.1986351013183594 seconds
policy loss:-661.598876953125
value loss:12.528477668762207
entropies:42.18391418457031
Policy training finished
---------------------
gamma: 0.11524132121360928
training start after waiting for 1.2062463760375977 seconds
policy loss:-402.791748046875
value loss:15.051278114318848
entropies:36.297569274902344
Policy training finished
---------------------
gamma: 0.11524132121360928
training start after waiting for 1.200331687927246 seconds
policy loss:-627.3470458984375
value loss:22.95509147644043
entropies:39.562896728515625
Policy training finished
---------------------
gamma: 0.11524132121360928
training start after waiting for 1.1730482578277588 seconds
policy loss:-377.4443359375
value loss:12.261276245117188
entropies:38.0476188659668
Policy training finished
---------------------
gamma: 0.11524132121360928
training start after waiting for 1.1778268814086914 seconds
policy loss:-995.0855102539062
value loss:25.458826065063477
entropies:41.96986389160156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1216.4952)
ToM Target loss= tensor(2263.6482)
optimized based on ToM loss
---------------------
gamma: 0.11547180385603649
training start after waiting for 1.1587140560150146 seconds
policy loss:-53.1423454284668
value loss:8.03797721862793
entropies:32.236663818359375
Policy training finished
---------------------
gamma: 0.11547180385603649
training start after waiting for 1.193333625793457 seconds
policy loss:-140.3190155029297
value loss:9.532489776611328
entropies:47.337677001953125
Policy training finished
---------------------
gamma: 0.11547180385603649
training start after waiting for 1.176530361175537 seconds
policy loss:-1364.91162109375
value loss:18.784034729003906
entropies:39.40982437133789
Policy training finished
---------------------
gamma: 0.11547180385603649
training start after waiting for 1.1869378089904785 seconds
policy loss:128.33554077148438
value loss:6.254181861877441
entropies:31.90376091003418
Policy training finished
---------------------
gamma: 0.11547180385603649
training start after waiting for 1.1447393894195557 seconds
policy loss:-260.6503601074219
value loss:9.298542022705078
entropies:26.89952850341797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1130.4053)
ToM Target loss= tensor(2188.5061)
optimized based on ToM loss
---------------------
gamma: 0.11570274746374856
training start after waiting for 1.2098991870880127 seconds
policy loss:-1232.4063720703125
value loss:13.052763938903809
entropies:34.038150787353516
Policy training finished
---------------------
gamma: 0.11570274746374856
training start after waiting for 1.1924569606781006 seconds
policy loss:-240.0006103515625
value loss:11.596725463867188
entropies:28.73678207397461
Policy training finished
---------------------
gamma: 0.11570274746374856
training start after waiting for 1.1876518726348877 seconds
policy loss:-479.880859375
value loss:7.917913436889648
entropies:19.69518280029297
Policy training finished
---------------------
gamma: 0.11570274746374856
training start after waiting for 1.2034761905670166 seconds
policy loss:201.7732696533203
value loss:3.7918930053710938
entropies:19.807933807373047
Policy training finished
---------------------
gamma: 0.11570274746374856
training start after waiting for 1.1887774467468262 seconds
policy loss:-1184.318603515625
value loss:27.722871780395508
entropies:42.679115295410156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1157.1631)
ToM Target loss= tensor(2285.8203)
optimized based on ToM loss
---------------------
gamma: 0.11593415295867605
training start after waiting for 1.144942283630371 seconds
policy loss:-624.4552612304688
value loss:11.564521789550781
entropies:24.112972259521484
Policy training finished
---------------------
gamma: 0.11593415295867605
training start after waiting for 1.1899611949920654 seconds
policy loss:-1002.2740478515625
value loss:27.636940002441406
entropies:46.342124938964844
Policy training finished
---------------------
gamma: 0.11593415295867605
training start after waiting for 1.144561767578125 seconds
policy loss:-56.49855041503906
value loss:20.461599349975586
entropies:34.709815979003906
Policy training finished
---------------------
gamma: 0.11593415295867605
training start after waiting for 1.1881616115570068 seconds
policy loss:-245.39393615722656
value loss:13.11188793182373
entropies:27.056610107421875
Policy training finished
---------------------
gamma: 0.11593415295867605
training start after waiting for 1.1526784896850586 seconds
policy loss:-801.1976318359375
value loss:23.17729377746582
entropies:30.854175567626953
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1173.8885)
ToM Target loss= tensor(2276.3792)
optimized based on ToM loss
---------------------
gamma: 0.11616602126459341
training start after waiting for 1.2052581310272217 seconds
policy loss:-262.4305725097656
value loss:11.350410461425781
entropies:34.57183074951172
Policy training finished
---------------------
gamma: 0.11616602126459341
training start after waiting for 1.1877331733703613 seconds
policy loss:234.65997314453125
value loss:8.83484172821045
entropies:20.043886184692383
Policy training finished
---------------------
gamma: 0.11616602126459341
training start after waiting for 1.2131423950195312 seconds
policy loss:-463.6208801269531
value loss:10.31957721710205
entropies:41.66743469238281
Policy training finished
---------------------
gamma: 0.11616602126459341
training start after waiting for 1.1893489360809326 seconds
policy loss:-641.1985473632812
value loss:36.356964111328125
entropies:29.71234893798828
Policy training finished
---------------------
gamma: 0.11616602126459341
training start after waiting for 1.141547679901123 seconds
policy loss:-1720.31591796875
value loss:33.01971435546875
entropies:42.103675842285156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1177.3903)
ToM Target loss= tensor(2144.4866)
optimized based on ToM loss
---------------------
gamma: 0.1163983533071226
training start after waiting for 1.1806745529174805 seconds
policy loss:-796.2725219726562
value loss:33.19984817504883
entropies:43.6507682800293
Policy training finished
---------------------
gamma: 0.1163983533071226
training start after waiting for 1.2092156410217285 seconds
policy loss:-1208.4306640625
value loss:17.798526763916016
entropies:50.65614318847656
Policy training finished
---------------------
gamma: 0.1163983533071226
training start after waiting for 1.1771132946014404 seconds
policy loss:-323.6632385253906
value loss:20.05682373046875
entropies:57.82175064086914
Policy training finished
---------------------
gamma: 0.1163983533071226
training start after waiting for 1.1737191677093506 seconds
policy loss:-1.7848320007324219
value loss:14.919803619384766
entropies:40.72494888305664
Policy training finished
---------------------
gamma: 0.1163983533071226
training start after waiting for 1.1623740196228027 seconds
policy loss:-698.7606811523438
value loss:20.908061981201172
entropies:45.24824142456055
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1258.9236)
ToM Target loss= tensor(2173.0591)
optimized based on ToM loss
---------------------
gamma: 0.11663115001373685
training start after waiting for 1.1822731494903564 seconds
policy loss:-897.2123413085938
value loss:37.18812561035156
entropies:30.22403335571289
Policy training finished
---------------------
gamma: 0.11663115001373685
training start after waiting for 1.179265022277832 seconds
policy loss:-879.3823852539062
value loss:16.46752166748047
entropies:45.03620910644531
Policy training finished
---------------------
gamma: 0.11663115001373685
training start after waiting for 1.1432695388793945 seconds
policy loss:-1927.3072509765625
value loss:35.12309646606445
entropies:55.6465950012207
Policy training finished
---------------------
gamma: 0.11663115001373685
training start after waiting for 1.1921966075897217 seconds
policy loss:96.40235137939453
value loss:9.3477783203125
entropies:28.256378173828125
Policy training finished
---------------------
gamma: 0.11663115001373685
training start after waiting for 1.1983110904693604 seconds
policy loss:89.28805541992188
value loss:7.397747039794922
entropies:21.91640281677246
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1179.8864)
ToM Target loss= tensor(2050.8757)
optimized based on ToM loss
---------------------
gamma: 0.11686441231376432
training start after waiting for 1.1349594593048096 seconds
policy loss:-248.26461791992188
value loss:7.152536392211914
entropies:41.43373489379883
Policy training finished
---------------------
gamma: 0.11686441231376432
training start after waiting for 1.1809766292572021 seconds
policy loss:-560.2848510742188
value loss:16.09808349609375
entropies:23.93354034423828
Policy training finished
---------------------
gamma: 0.11686441231376432
training start after waiting for 1.2080860137939453 seconds
policy loss:-835.373046875
value loss:13.894540786743164
entropies:40.10268783569336
Policy training finished
---------------------
gamma: 0.11686441231376432
training start after waiting for 1.1835949420928955 seconds
policy loss:-7.863705158233643
value loss:11.65027141571045
entropies:27.22333526611328
Policy training finished
---------------------
gamma: 0.11686441231376432
training start after waiting for 1.246335506439209 seconds
policy loss:-747.43896484375
value loss:20.11148452758789
entropies:50.64763641357422
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1142.5692)
ToM Target loss= tensor(2109.2866)
optimized based on ToM loss
---------------------
gamma: 0.11709814113839186
training start after waiting for 1.1437315940856934 seconds
policy loss:582.8511352539062
value loss:11.548166275024414
entropies:24.863452911376953
Policy training finished
---------------------
gamma: 0.11709814113839186
training start after waiting for 1.1576597690582275 seconds
policy loss:-218.16746520996094
value loss:17.247976303100586
entropies:32.0698356628418
Policy training finished
---------------------
gamma: 0.11709814113839186
training start after waiting for 1.1685676574707031 seconds
policy loss:382.11187744140625
value loss:10.617079734802246
entropies:27.622509002685547
Policy training finished
---------------------
gamma: 0.11709814113839186
training start after waiting for 1.142967939376831 seconds
policy loss:-368.5874938964844
value loss:19.62395477294922
entropies:37.20121765136719
Policy training finished
---------------------
gamma: 0.11709814113839186
training start after waiting for 1.1996521949768066 seconds
policy loss:-532.6455688476562
value loss:16.631502151489258
entropies:44.209716796875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1125.0511)
ToM Target loss= tensor(2161.5327)
optimized based on ToM loss
---------------------
gamma: 0.11733233742066863
training start after waiting for 1.2136361598968506 seconds
policy loss:-356.8011474609375
value loss:17.256427764892578
entropies:22.1505184173584
Policy training finished
---------------------
gamma: 0.11733233742066863
training start after waiting for 1.1851441860198975 seconds
policy loss:-1126.3238525390625
value loss:26.81068229675293
entropies:36.40791702270508
Policy training finished
---------------------
gamma: 0.11733233742066863
training start after waiting for 1.1423816680908203 seconds
policy loss:7.700441837310791
value loss:5.279061794281006
entropies:18.814746856689453
Policy training finished
---------------------
gamma: 0.11733233742066863
training start after waiting for 1.149097204208374 seconds
policy loss:-142.2397918701172
value loss:13.331076622009277
entropies:20.81458282470703
Policy training finished
---------------------
gamma: 0.11733233742066863
training start after waiting for 1.1606621742248535 seconds
policy loss:-935.9055786132812
value loss:23.142017364501953
entropies:42.11700439453125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1184.3369)
ToM Target loss= tensor(2224.6140)
optimized based on ToM loss
---------------------
gamma: 0.11756700209550996
training start after waiting for 1.2019438743591309 seconds
policy loss:-466.6668395996094
value loss:14.84054946899414
entropies:29.887882232666016
Policy training finished
---------------------
gamma: 0.11756700209550996
training start after waiting for 1.214465856552124 seconds
policy loss:-2553.240234375
value loss:34.74100875854492
entropies:30.352344512939453
Policy training finished
---------------------
gamma: 0.11756700209550996
training start after waiting for 1.193580150604248 seconds
policy loss:-288.4942932128906
value loss:9.20053482055664
entropies:21.254030227661133
Policy training finished
---------------------
gamma: 0.11756700209550996
training start after waiting for 1.2062742710113525 seconds
policy loss:215.48146057128906
value loss:11.820099830627441
entropies:33.02063751220703
Policy training finished
---------------------
gamma: 0.11756700209550996
training start after waiting for 1.196761131286621 seconds
policy loss:-187.9016571044922
value loss:32.95768356323242
entropies:27.361495971679688
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1139.3979)
ToM Target loss= tensor(2197.3560)
optimized based on ToM loss
---------------------
gamma: 0.11780213609970099
training start after waiting for 1.2134270668029785 seconds
policy loss:-1007.497802734375
value loss:29.83546257019043
entropies:59.439762115478516
Policy training finished
---------------------
gamma: 0.11780213609970099
training start after waiting for 1.2004244327545166 seconds
policy loss:369.6399230957031
value loss:17.258628845214844
entropies:28.826210021972656
Policy training finished
---------------------
gamma: 0.11780213609970099
training start after waiting for 1.1880698204040527 seconds
policy loss:-297.02978515625
value loss:16.167997360229492
entropies:44.385398864746094
Policy training finished
---------------------
gamma: 0.11780213609970099
training start after waiting for 1.144883632659912 seconds
policy loss:-67.6414566040039
value loss:17.2479248046875
entropies:27.83409309387207
Policy training finished
---------------------
gamma: 0.11780213609970099
training start after waiting for 1.1750233173370361 seconds
policy loss:-254.61802673339844
value loss:17.250938415527344
entropies:40.057159423828125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1314.4866)
ToM Target loss= tensor(2169.9485)
optimized based on ToM loss
---------------------
gamma: 0.11803774037190039
training start after waiting for 1.1788392066955566 seconds
policy loss:-194.85906982421875
value loss:15.31528091430664
entropies:24.948999404907227
Policy training finished
---------------------
gamma: 0.11803774037190039
training start after waiting for 1.170212984085083 seconds
policy loss:-228.05624389648438
value loss:17.893678665161133
entropies:34.82130432128906
Policy training finished
---------------------
gamma: 0.11803774037190039
training start after waiting for 1.1715946197509766 seconds
policy loss:103.66378021240234
value loss:10.146422386169434
entropies:32.84545135498047
Policy training finished
---------------------
gamma: 0.11803774037190039
training start after waiting for 1.1830682754516602 seconds
policy loss:66.8487548828125
value loss:10.22263240814209
entropies:29.756385803222656
Policy training finished
---------------------
gamma: 0.11803774037190039
training start after waiting for 1.1919960975646973 seconds
policy loss:88.33995819091797
value loss:4.629539966583252
entropies:24.884578704833984
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1261.9878)
ToM Target loss= tensor(2368.3755)
optimized based on ToM loss
---------------------
gamma: 0.11827381585264418
training start after waiting for 1.1689560413360596 seconds
policy loss:-1193.1142578125
value loss:21.85605812072754
entropies:42.9607048034668
Policy training finished
---------------------
gamma: 0.11827381585264418
training start after waiting for 1.1813058853149414 seconds
policy loss:-213.47093200683594
value loss:14.504940032958984
entropies:32.10908508300781
Policy training finished
---------------------
gamma: 0.11827381585264418
training start after waiting for 1.14235258102417 seconds
policy loss:251.2362060546875
value loss:9.049071311950684
entropies:17.008167266845703
Policy training finished
---------------------
gamma: 0.11827381585264418
training start after waiting for 1.2096896171569824 seconds
policy loss:-261.768798828125
value loss:12.968222618103027
entropies:33.63712692260742
Policy training finished
---------------------
gamma: 0.11827381585264418
training start after waiting for 1.2101800441741943 seconds
policy loss:54.64543151855469
value loss:5.367866516113281
entropies:31.4989013671875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1092.5157)
ToM Target loss= tensor(2161.4199)
optimized based on ToM loss
---------------------
gamma: 0.11851036348434947
training start after waiting for 1.1761610507965088 seconds
policy loss:-1032.0875244140625
value loss:38.619407653808594
entropies:41.74205780029297
Policy training finished
---------------------
gamma: 0.11851036348434947
training start after waiting for 1.1887657642364502 seconds
policy loss:-617.35205078125
value loss:18.22733497619629
entropies:40.48637771606445
Policy training finished
---------------------
gamma: 0.11851036348434947
training start after waiting for 1.1971673965454102 seconds
policy loss:-1690.58447265625
value loss:32.72075653076172
entropies:49.999778747558594
Policy training finished
---------------------
gamma: 0.11851036348434947
training start after waiting for 1.198333740234375 seconds
policy loss:-1444.9259033203125
value loss:13.175899505615234
entropies:22.69272232055664
Policy training finished
---------------------
gamma: 0.11851036348434947
training start after waiting for 1.2030460834503174 seconds
policy loss:-9.490423202514648
value loss:3.631195306777954
entropies:17.124176025390625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1230.7998)
ToM Target loss= tensor(2233.3193)
optimized based on ToM loss
---------------------
gamma: 0.11874738421131817
training start after waiting for 1.1805620193481445 seconds
policy loss:-303.2877197265625
value loss:14.789344787597656
entropies:23.51531982421875
Policy training finished
---------------------
gamma: 0.11874738421131817
training start after waiting for 1.148397445678711 seconds
policy loss:-1288.0985107421875
value loss:23.55003547668457
entropies:28.173229217529297
Policy training finished
---------------------
gamma: 0.11874738421131817
training start after waiting for 1.1417911052703857 seconds
policy loss:107.4673843383789
value loss:17.570995330810547
entropies:40.88656997680664
Policy training finished
---------------------
gamma: 0.11874738421131817
training start after waiting for 1.1470332145690918 seconds
policy loss:-306.0404052734375
value loss:19.4024715423584
entropies:30.445758819580078
Policy training finished
---------------------
gamma: 0.11874738421131817
training start after waiting for 1.2094287872314453 seconds
policy loss:-131.75218200683594
value loss:19.250324249267578
entropies:25.17853546142578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1200.4115)
ToM Target loss= tensor(2210.4661)
optimized based on ToM loss
---------------------
gamma: 0.1189848789797408
training start after waiting for 1.1977193355560303 seconds
policy loss:-510.89166259765625
value loss:34.676570892333984
entropies:47.07330322265625
Policy training finished
---------------------
gamma: 0.1189848789797408
training start after waiting for 1.2234978675842285 seconds
policy loss:26.38091278076172
value loss:22.244709014892578
entropies:28.01834487915039
Policy training finished
---------------------
gamma: 0.1189848789797408
training start after waiting for 1.186129093170166 seconds
policy loss:-1177.599365234375
value loss:25.559185028076172
entropies:41.1028938293457
Policy training finished
---------------------
gamma: 0.1189848789797408
training start after waiting for 1.1784672737121582 seconds
policy loss:-280.4708251953125
value loss:20.72481346130371
entropies:37.311683654785156
Policy training finished
---------------------
gamma: 0.1189848789797408
training start after waiting for 1.177091121673584 seconds
policy loss:-349.1316833496094
value loss:4.767876148223877
entropies:19.613691329956055
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1203.8737)
ToM Target loss= tensor(2200.9915)
optimized based on ToM loss
---------------------
gamma: 0.11922284873770028
training start after waiting for 1.1486918926239014 seconds
policy loss:-922.1930541992188
value loss:12.253885269165039
entropies:27.33517837524414
Policy training finished
---------------------
gamma: 0.11922284873770028
training start after waiting for 1.180910587310791 seconds
policy loss:-665.761962890625
value loss:20.042797088623047
entropies:31.357391357421875
Policy training finished
---------------------
gamma: 0.11922284873770028
training start after waiting for 1.1393861770629883 seconds
policy loss:-649.1337280273438
value loss:21.517412185668945
entropies:37.51199722290039
Policy training finished
---------------------
gamma: 0.11922284873770028
training start after waiting for 1.1732699871063232 seconds
policy loss:-1471.7470703125
value loss:23.8478946685791
entropies:37.00368881225586
Policy training finished
---------------------
gamma: 0.11922284873770028
training start after waiting for 1.204606294631958 seconds
policy loss:-798.3659057617188
value loss:17.894657135009766
entropies:38.786285400390625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1140.1393)
ToM Target loss= tensor(2260.2659)
optimized based on ToM loss
---------------------
gamma: 0.11946129443517568
training start after waiting for 1.184471607208252 seconds
policy loss:-1279.2742919921875
value loss:18.054824829101562
entropies:40.527099609375
Policy training finished
---------------------
gamma: 0.11946129443517568
training start after waiting for 1.1612060070037842 seconds
policy loss:-383.56988525390625
value loss:24.057281494140625
entropies:52.0306396484375
Policy training finished
---------------------
gamma: 0.11946129443517568
training start after waiting for 1.2093348503112793 seconds
policy loss:436.0418395996094
value loss:12.124995231628418
entropies:23.51189613342285
Policy training finished
---------------------
gamma: 0.11946129443517568
training start after waiting for 1.2105326652526855 seconds
policy loss:484.3630065917969
value loss:13.464487075805664
entropies:28.02365493774414
Policy training finished
---------------------
gamma: 0.11946129443517568
training start after waiting for 1.2027950286865234 seconds
policy loss:-69.22228240966797
value loss:20.940732955932617
entropies:41.25372314453125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1131.4110)
ToM Target loss= tensor(2155.8882)
optimized based on ToM loss
---------------------
gamma: 0.11970021702404604
training start after waiting for 1.2044012546539307 seconds
policy loss:-404.49822998046875
value loss:9.929618835449219
entropies:27.63241958618164
Policy training finished
---------------------
gamma: 0.11970021702404604
training start after waiting for 1.1570487022399902 seconds
policy loss:-40.68526840209961
value loss:15.705272674560547
entropies:37.97967529296875
Policy training finished
---------------------
gamma: 0.11970021702404604
training start after waiting for 1.158970832824707 seconds
policy loss:-423.5312194824219
value loss:16.71934700012207
entropies:33.614192962646484
Policy training finished
---------------------
gamma: 0.11970021702404604
training start after waiting for 1.1456427574157715 seconds
policy loss:-461.333251953125
value loss:15.643775939941406
entropies:37.26670837402344
Policy training finished
---------------------
gamma: 0.11970021702404604
training start after waiting for 1.1537272930145264 seconds
policy loss:-1014.2843017578125
value loss:19.447269439697266
entropies:25.888874053955078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1246.3129)
ToM Target loss= tensor(2286.1238)
optimized based on ToM loss
---------------------
gamma: 0.11993961745809413
training start after waiting for 1.1429903507232666 seconds
policy loss:137.1138458251953
value loss:5.549587249755859
entropies:15.821453094482422
Policy training finished
---------------------
gamma: 0.11993961745809413
training start after waiting for 1.1925318241119385 seconds
policy loss:-501.89910888671875
value loss:22.241140365600586
entropies:45.834388732910156
Policy training finished
---------------------
gamma: 0.11993961745809413
training start after waiting for 1.172712802886963 seconds
policy loss:-813.6826782226562
value loss:17.899202346801758
entropies:39.67420959472656
Policy training finished
---------------------
gamma: 0.11993961745809413
training start after waiting for 1.1760993003845215 seconds
policy loss:-975.0952758789062
value loss:26.211898803710938
entropies:47.26519775390625
Policy training finished
---------------------
gamma: 0.11993961745809413
training start after waiting for 1.209336757659912 seconds
policy loss:-789.6478881835938
value loss:31.10930633544922
entropies:21.28627586364746
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1228.9878)
ToM Target loss= tensor(2304.6958)
optimized based on ToM loss
---------------------
gamma: 0.12017949669301031
training start after waiting for 1.1572656631469727 seconds
policy loss:-212.29086303710938
value loss:21.19782829284668
entropies:53.2857666015625
Policy training finished
---------------------
gamma: 0.12017949669301031
training start after waiting for 1.1726298332214355 seconds
policy loss:58.12908172607422
value loss:13.855656623840332
entropies:25.72862434387207
Policy training finished
---------------------
gamma: 0.12017949669301031
training start after waiting for 1.2040534019470215 seconds
policy loss:93.53585815429688
value loss:15.247395515441895
entropies:20.197879791259766
Policy training finished
---------------------
gamma: 0.12017949669301031
training start after waiting for 1.2011044025421143 seconds
policy loss:-369.7223815917969
value loss:10.03764533996582
entropies:30.098661422729492
Policy training finished
---------------------
gamma: 0.12017949669301031
training start after waiting for 1.1706104278564453 seconds
policy loss:-688.7493286132812
value loss:10.812041282653809
entropies:38.27760314941406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1308.8424)
ToM Target loss= tensor(2345.2646)
optimized based on ToM loss
---------------------
gamma: 0.12041985568639633
training start after waiting for 1.149428129196167 seconds
policy loss:-1683.456787109375
value loss:30.258512496948242
entropies:26.073537826538086
Policy training finished
---------------------
gamma: 0.12041985568639633
training start after waiting for 1.1467411518096924 seconds
policy loss:60.161643981933594
value loss:5.763126373291016
entropies:18.61602783203125
Policy training finished
---------------------
gamma: 0.12041985568639633
training start after waiting for 1.1745424270629883 seconds
policy loss:-721.0950927734375
value loss:23.017784118652344
entropies:39.45813751220703
Policy training finished
---------------------
gamma: 0.12041985568639633
training start after waiting for 1.20466947555542 seconds
policy loss:-608.88916015625
value loss:20.74258804321289
entropies:24.115798950195312
Policy training finished
---------------------
gamma: 0.12041985568639633
training start after waiting for 1.1789956092834473 seconds
policy loss:-733.1041259765625
value loss:34.721893310546875
entropies:39.384403228759766
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1167.8069)
ToM Target loss= tensor(2316.4331)
optimized based on ToM loss
---------------------
gamma: 0.12066069539776912
training start after waiting for 1.1557855606079102 seconds
policy loss:-101.43621063232422
value loss:14.858695030212402
entropies:29.633113861083984
Policy training finished
---------------------
gamma: 0.12066069539776912
training start after waiting for 1.1858599185943604 seconds
policy loss:-334.8503723144531
value loss:11.585807800292969
entropies:12.433914184570312
Policy training finished
---------------------
gamma: 0.12066069539776912
training start after waiting for 1.1942858695983887 seconds
policy loss:-168.47665405273438
value loss:13.804763793945312
entropies:31.47087860107422
Policy training finished
---------------------
gamma: 0.12066069539776912
training start after waiting for 1.2120494842529297 seconds
policy loss:-306.4110107421875
value loss:13.043712615966797
entropies:34.173667907714844
Policy training finished
---------------------
gamma: 0.12066069539776912
training start after waiting for 1.1765625476837158 seconds
policy loss:2.593564987182617
value loss:9.281819343566895
entropies:38.85258483886719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1155.3702)
ToM Target loss= tensor(2196.4121)
optimized based on ToM loss
---------------------
gamma: 0.12090201678856466
training start after waiting for 1.159090280532837 seconds
policy loss:-600.8489379882812
value loss:20.302413940429688
entropies:24.3885555267334
Policy training finished
---------------------
gamma: 0.12090201678856466
training start after waiting for 1.1736886501312256 seconds
policy loss:-463.3909912109375
value loss:7.17102575302124
entropies:24.073875427246094
Policy training finished
---------------------
gamma: 0.12090201678856466
training start after waiting for 1.1690974235534668 seconds
policy loss:230.56263732910156
value loss:10.169955253601074
entropies:15.836322784423828
Policy training finished
---------------------
gamma: 0.12090201678856466
training start after waiting for 1.1774086952209473 seconds
policy loss:190.13037109375
value loss:25.70969581604004
entropies:49.259925842285156
Policy training finished
---------------------
gamma: 0.12090201678856466
training start after waiting for 1.2119801044464111 seconds
policy loss:156.9068603515625
value loss:13.790392875671387
entropies:21.207679748535156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1166.9885)
ToM Target loss= tensor(2246.6628)
optimized based on ToM loss
---------------------
gamma: 0.12114382082214178
training start after waiting for 1.1844677925109863 seconds
policy loss:85.73719024658203
value loss:11.697546005249023
entropies:15.020526885986328
Policy training finished
---------------------
gamma: 0.12114382082214178
training start after waiting for 1.1914794445037842 seconds
policy loss:-104.5009994506836
value loss:15.16069221496582
entropies:22.124372482299805
Policy training finished
---------------------
gamma: 0.12114382082214178
training start after waiting for 1.1897051334381104 seconds
policy loss:-221.6533966064453
value loss:5.661972999572754
entropies:24.177766799926758
Policy training finished
---------------------
gamma: 0.12114382082214178
training start after waiting for 1.1668376922607422 seconds
policy loss:-583.5806274414062
value loss:15.454129219055176
entropies:23.288915634155273
Policy training finished
---------------------
gamma: 0.12114382082214178
training start after waiting for 1.193023443222046 seconds
policy loss:83.12064361572266
value loss:7.256520748138428
entropies:23.19289779663086
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1170.8766)
ToM Target loss= tensor(2286.9492)
optimized based on ToM loss
---------------------
gamma: 0.12138610846378607
training start after waiting for 1.2519199848175049 seconds
policy loss:-229.3150177001953
value loss:10.824679374694824
entropies:30.874794006347656
Policy training finished
---------------------
gamma: 0.12138610846378607
training start after waiting for 1.2032928466796875 seconds
policy loss:-358.2213439941406
value loss:8.380895614624023
entropies:17.84053611755371
Policy training finished
---------------------
gamma: 0.12138610846378607
training start after waiting for 1.2049510478973389 seconds
policy loss:-519.1712036132812
value loss:21.05562400817871
entropies:16.847606658935547
Policy training finished
---------------------
gamma: 0.12138610846378607
training start after waiting for 1.190859317779541 seconds
policy loss:-400.0575256347656
value loss:32.16434860229492
entropies:32.76054382324219
Policy training finished
---------------------
gamma: 0.12138610846378607
training start after waiting for 1.2032787799835205 seconds
policy loss:-39.310333251953125
value loss:7.970620155334473
entropies:18.55071258544922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1207.6815)
ToM Target loss= tensor(2437.9209)
optimized based on ToM loss
---------------------
gamma: 0.12162888068071365
training start after waiting for 1.2015917301177979 seconds
policy loss:-1186.2952880859375
value loss:26.781801223754883
entropies:33.97105026245117
Policy training finished
---------------------
gamma: 0.12162888068071365
training start after waiting for 1.1404075622558594 seconds
policy loss:-319.66119384765625
value loss:13.299571990966797
entropies:24.404727935791016
Policy training finished
---------------------
gamma: 0.12162888068071365
training start after waiting for 1.1403560638427734 seconds
policy loss:-796.646728515625
value loss:25.3238525390625
entropies:31.90418243408203
Policy training finished
---------------------
gamma: 0.12162888068071365
training start after waiting for 1.1948840618133545 seconds
policy loss:124.7752914428711
value loss:11.646368026733398
entropies:20.905107498168945
Policy training finished
---------------------
gamma: 0.12162888068071365
training start after waiting for 1.156524896621704 seconds
policy loss:215.36895751953125
value loss:7.420977592468262
entropies:16.600650787353516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1082.0715)
ToM Target loss= tensor(2128.2876)
optimized based on ToM loss
---------------------
gamma: 0.12187213844207508
training start after waiting for 1.197519302368164 seconds
policy loss:-501.602294921875
value loss:9.064199447631836
entropies:29.20492935180664
Policy training finished
---------------------
gamma: 0.12187213844207508
training start after waiting for 1.152824878692627 seconds
policy loss:-903.44580078125
value loss:14.78158950805664
entropies:37.954498291015625
Policy training finished
---------------------
gamma: 0.12187213844207508
training start after waiting for 1.1900832653045654 seconds
policy loss:-238.63221740722656
value loss:9.694195747375488
entropies:31.611202239990234
Policy training finished
---------------------
gamma: 0.12187213844207508
training start after waiting for 1.1389024257659912 seconds
policy loss:-1200.740234375
value loss:30.340463638305664
entropies:39.32695770263672
Policy training finished
---------------------
gamma: 0.12187213844207508
training start after waiting for 1.2081093788146973 seconds
policy loss:-279.91754150390625
value loss:17.63813018798828
entropies:30.529884338378906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1124.4651)
ToM Target loss= tensor(2126.5139)
optimized based on ToM loss
---------------------
gamma: 0.12211588271895922
training start after waiting for 1.190976858139038 seconds
policy loss:-699.3600463867188
value loss:16.457298278808594
entropies:30.056415557861328
Policy training finished
---------------------
gamma: 0.12211588271895922
training start after waiting for 1.1616754531860352 seconds
policy loss:-344.7608947753906
value loss:6.979272842407227
entropies:24.08112335205078
Policy training finished
---------------------
gamma: 0.12211588271895922
training start after waiting for 1.209282636642456 seconds
policy loss:-189.38055419921875
value loss:12.556495666503906
entropies:33.681976318359375
Policy training finished
---------------------
gamma: 0.12211588271895922
training start after waiting for 1.1663296222686768 seconds
policy loss:194.9233856201172
value loss:17.58941078186035
entropies:37.03604507446289
Policy training finished
---------------------
gamma: 0.12211588271895922
training start after waiting for 1.1895670890808105 seconds
policy loss:-358.422119140625
value loss:14.373549461364746
entropies:42.05922317504883
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1200.7003)
ToM Target loss= tensor(2329.0105)
optimized based on ToM loss
---------------------
gamma: 0.12236011448439714
training start after waiting for 1.190140724182129 seconds
policy loss:-51.6150016784668
value loss:6.08536434173584
entropies:19.31420135498047
Policy training finished
---------------------
gamma: 0.12236011448439714
training start after waiting for 1.169813632965088 seconds
policy loss:-797.4698486328125
value loss:36.93574523925781
entropies:40.810890197753906
Policy training finished
---------------------
gamma: 0.12236011448439714
training start after waiting for 1.208833932876587 seconds
policy loss:-889.0444946289062
value loss:29.26026725769043
entropies:40.84690856933594
Policy training finished
---------------------
gamma: 0.12236011448439714
training start after waiting for 1.186643362045288 seconds
policy loss:18.315025329589844
value loss:7.040535926818848
entropies:29.35870361328125
Policy training finished
---------------------
gamma: 0.12236011448439714
training start after waiting for 1.1462984085083008 seconds
policy loss:-481.3962097167969
value loss:24.571449279785156
entropies:36.105167388916016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1283.3198)
ToM Target loss= tensor(2252.5457)
optimized based on ToM loss
---------------------
gamma: 0.12260483471336593
training start after waiting for 1.2085883617401123 seconds
policy loss:-369.5360107421875
value loss:8.388833999633789
entropies:33.0362434387207
Policy training finished
---------------------
gamma: 0.12260483471336593
training start after waiting for 1.188405990600586 seconds
policy loss:70.52178192138672
value loss:19.635929107666016
entropies:27.365184783935547
Policy training finished
---------------------
gamma: 0.12260483471336593
training start after waiting for 1.152897596359253 seconds
policy loss:-330.90399169921875
value loss:10.282252311706543
entropies:31.405881881713867
Policy training finished
---------------------
gamma: 0.12260483471336593
training start after waiting for 1.1564505100250244 seconds
policy loss:-53.89772415161133
value loss:29.776214599609375
entropies:36.184932708740234
Policy training finished
---------------------
gamma: 0.12260483471336593
training start after waiting for 1.1468226909637451 seconds
policy loss:-269.3271179199219
value loss:11.885934829711914
entropies:34.89205551147461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1230.1843)
ToM Target loss= tensor(2228.6392)
optimized based on ToM loss
---------------------
gamma: 0.12285004438279266
training start after waiting for 1.1465566158294678 seconds
policy loss:-697.5252685546875
value loss:13.272838592529297
entropies:22.24355697631836
Policy training finished
---------------------
gamma: 0.12285004438279266
training start after waiting for 1.146071434020996 seconds
policy loss:-413.0057067871094
value loss:8.693190574645996
entropies:32.79594421386719
Policy training finished
---------------------
gamma: 0.12285004438279266
training start after waiting for 1.1952166557312012 seconds
policy loss:-205.83120727539062
value loss:8.744216918945312
entropies:32.1944580078125
Policy training finished
---------------------
gamma: 0.12285004438279266
training start after waiting for 1.192082166671753 seconds
policy loss:-329.1631164550781
value loss:8.638102531433105
entropies:37.93254089355469
Policy training finished
---------------------
gamma: 0.12285004438279266
training start after waiting for 1.1419999599456787 seconds
policy loss:-374.18316650390625
value loss:11.869575500488281
entropies:33.18894958496094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1161.3680)
ToM Target loss= tensor(2293.5088)
optimized based on ToM loss
---------------------
gamma: 0.12309574447155824
training start after waiting for 1.1432225704193115 seconds
policy loss:-639.1798095703125
value loss:10.122028350830078
entropies:32.2981071472168
Policy training finished
---------------------
gamma: 0.12309574447155824
training start after waiting for 1.1476843357086182 seconds
policy loss:-2269.17138671875
value loss:47.28509521484375
entropies:52.072235107421875
Policy training finished
---------------------
gamma: 0.12309574447155824
training start after waiting for 1.2049801349639893 seconds
policy loss:-140.75479125976562
value loss:5.524641513824463
entropies:14.516139030456543
Policy training finished
---------------------
gamma: 0.12309574447155824
training start after waiting for 1.1983635425567627 seconds
policy loss:-116.62220001220703
value loss:10.66775894165039
entropies:39.23757553100586
Policy training finished
---------------------
gamma: 0.12309574447155824
training start after waiting for 1.2052764892578125 seconds
policy loss:-679.5503540039062
value loss:22.2156982421875
entropies:30.50020980834961
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1230.8910)
ToM Target loss= tensor(2283.9507)
optimized based on ToM loss
---------------------
gamma: 0.12334193596050136
training start after waiting for 1.2011198997497559 seconds
policy loss:98.85711669921875
value loss:7.43792724609375
entropies:21.598644256591797
Policy training finished
---------------------
gamma: 0.12334193596050136
training start after waiting for 1.1723151206970215 seconds
policy loss:-370.9788818359375
value loss:20.92375946044922
entropies:30.417495727539062
Policy training finished
---------------------
gamma: 0.12334193596050136
training start after waiting for 1.1473026275634766 seconds
policy loss:-699.0369873046875
value loss:20.876462936401367
entropies:47.29810333251953
Policy training finished
---------------------
gamma: 0.12334193596050136
training start after waiting for 1.2112624645233154 seconds
policy loss:-748.107177734375
value loss:15.167284965515137
entropies:27.74237823486328
Policy training finished
---------------------
gamma: 0.12334193596050136
training start after waiting for 1.2054975032806396 seconds
policy loss:-76.58792114257812
value loss:8.541017532348633
entropies:35.07661056518555
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1156.2463)
ToM Target loss= tensor(2281.2466)
optimized based on ToM loss
---------------------
gamma: 0.12358861983242236
training start after waiting for 1.1958050727844238 seconds
policy loss:-1225.4722900390625
value loss:20.710479736328125
entropies:52.96803665161133
Policy training finished
---------------------
gamma: 0.12358861983242236
training start after waiting for 1.1553452014923096 seconds
policy loss:-348.1278076171875
value loss:29.758071899414062
entropies:34.11743927001953
Policy training finished
---------------------
gamma: 0.12358861983242236
training start after waiting for 1.1871540546417236 seconds
policy loss:7.6279449462890625
value loss:15.097498893737793
entropies:29.143917083740234
Policy training finished
---------------------
gamma: 0.12358861983242236
training start after waiting for 1.1830501556396484 seconds
policy loss:7.306813716888428
value loss:8.298245429992676
entropies:26.98138427734375
Policy training finished
---------------------
gamma: 0.12358861983242236
training start after waiting for 1.1736912727355957 seconds
policy loss:-66.1788330078125
value loss:11.803050994873047
entropies:25.96000862121582
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1191.3707)
ToM Target loss= tensor(2214.2751)
optimized based on ToM loss
---------------------
gamma: 0.12383579707208721
training start after waiting for 1.1425912380218506 seconds
policy loss:-755.391845703125
value loss:12.16283130645752
entropies:28.876710891723633
Policy training finished
---------------------
gamma: 0.12383579707208721
training start after waiting for 1.1502068042755127 seconds
policy loss:-762.0516357421875
value loss:12.427845001220703
entropies:35.78160858154297
Policy training finished
---------------------
gamma: 0.12383579707208721
training start after waiting for 1.2037878036499023 seconds
policy loss:-82.43563079833984
value loss:11.554679870605469
entropies:29.60177230834961
Policy training finished
---------------------
gamma: 0.12383579707208721
training start after waiting for 1.1728589534759521 seconds
policy loss:-1185.810546875
value loss:21.567514419555664
entropies:32.34522247314453
Policy training finished
---------------------
gamma: 0.12383579707208721
training start after waiting for 1.1400530338287354 seconds
policy loss:-1335.5157470703125
value loss:16.314807891845703
entropies:28.059663772583008
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1152.2197)
ToM Target loss= tensor(2308.5393)
optimized based on ToM loss
---------------------
gamma: 0.12408346866623138
training start after waiting for 1.219602108001709 seconds
policy loss:9.806745529174805
value loss:4.247206687927246
entropies:18.218170166015625
Policy training finished
---------------------
gamma: 0.12408346866623138
training start after waiting for 1.1412370204925537 seconds
policy loss:-315.36883544921875
value loss:13.416656494140625
entropies:36.14166259765625
Policy training finished
---------------------
gamma: 0.12408346866623138
training start after waiting for 1.1463377475738525 seconds
policy loss:-46.20039749145508
value loss:11.341109275817871
entropies:33.219398498535156
Policy training finished
---------------------
gamma: 0.12408346866623138
training start after waiting for 1.142815351486206 seconds
policy loss:-16.38971519470215
value loss:7.312282085418701
entropies:17.355327606201172
Policy training finished
---------------------
gamma: 0.12408346866623138
training start after waiting for 1.196044683456421 seconds
policy loss:-491.8648376464844
value loss:13.865660667419434
entropies:37.4932746887207
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1225.4006)
ToM Target loss= tensor(2387.6304)
optimized based on ToM loss
---------------------
gamma: 0.12433163560356385
training start after waiting for 1.2236013412475586 seconds
policy loss:-1152.4986572265625
value loss:48.11042785644531
entropies:53.56695556640625
Policy training finished
---------------------
gamma: 0.12433163560356385
training start after waiting for 1.1684134006500244 seconds
policy loss:-530.51416015625
value loss:14.342853546142578
entropies:36.931636810302734
Policy training finished
---------------------
gamma: 0.12433163560356385
training start after waiting for 1.14976167678833 seconds
policy loss:-430.2179870605469
value loss:21.48349380493164
entropies:44.82258224487305
Policy training finished
---------------------
gamma: 0.12433163560356385
training start after waiting for 1.1796014308929443 seconds
policy loss:-2278.080810546875
value loss:71.38153839111328
entropies:73.34380340576172
Policy training finished
---------------------
gamma: 0.12433163560356385
training start after waiting for 1.1845529079437256 seconds
policy loss:-759.0743408203125
value loss:13.846967697143555
entropies:32.64179611206055
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1248.7434)
ToM Target loss= tensor(2315.9072)
optimized based on ToM loss
---------------------
gamma: 0.12458029887477097
training start after waiting for 1.1782052516937256 seconds
policy loss:510.79913330078125
value loss:14.340696334838867
entropies:29.97863006591797
Policy training finished
---------------------
gamma: 0.12458029887477097
training start after waiting for 1.181663990020752 seconds
policy loss:-67.07998657226562
value loss:16.63629913330078
entropies:27.529647827148438
Policy training finished
---------------------
gamma: 0.12458029887477097
training start after waiting for 1.1524386405944824 seconds
policy loss:-170.29754638671875
value loss:10.624161720275879
entropies:20.921113967895508
Policy training finished
---------------------
gamma: 0.12458029887477097
training start after waiting for 1.1922895908355713 seconds
policy loss:-193.9018096923828
value loss:9.77151870727539
entropies:18.347488403320312
Policy training finished
---------------------
gamma: 0.12458029887477097
training start after waiting for 1.1908226013183594 seconds
policy loss:-123.6546859741211
value loss:14.765838623046875
entropies:32.2545166015625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1162.7175)
ToM Target loss= tensor(2337.5942)
optimized based on ToM loss
---------------------
gamma: 0.12482945947252051
training start after waiting for 1.1801295280456543 seconds
policy loss:-2031.7015380859375
value loss:42.105628967285156
entropies:39.07204818725586
Policy training finished
---------------------
gamma: 0.12482945947252051
training start after waiting for 1.2029798030853271 seconds
policy loss:-752.5687255859375
value loss:13.309025764465332
entropies:36.76410675048828
Policy training finished
---------------------
gamma: 0.12482945947252051
training start after waiting for 1.2175664901733398 seconds
policy loss:-66.01998138427734
value loss:8.321069717407227
entropies:21.261253356933594
Policy training finished
---------------------
gamma: 0.12482945947252051
training start after waiting for 1.1413676738739014 seconds
policy loss:-260.3095397949219
value loss:9.610057830810547
entropies:15.237068176269531
Policy training finished
---------------------
gamma: 0.12482945947252051
training start after waiting for 1.1725459098815918 seconds
policy loss:-196.10086059570312
value loss:8.6748628616333
entropies:29.75310516357422
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1139.0470)
ToM Target loss= tensor(2322.7441)
optimized based on ToM loss
---------------------
gamma: 0.12507911839146554
training start after waiting for 1.182316780090332 seconds
policy loss:-303.95953369140625
value loss:7.884382247924805
entropies:22.935087203979492
Policy training finished
---------------------
gamma: 0.12507911839146554
training start after waiting for 1.1813127994537354 seconds
policy loss:-1532.6341552734375
value loss:39.437255859375
entropies:55.95798110961914
Policy training finished
---------------------
gamma: 0.12507911839146554
training start after waiting for 1.1445331573486328 seconds
policy loss:-251.72793579101562
value loss:8.812347412109375
entropies:24.432754516601562
Policy training finished
---------------------
gamma: 0.12507911839146554
training start after waiting for 1.1919517517089844 seconds
policy loss:-941.39990234375
value loss:18.990406036376953
entropies:40.536842346191406
Policy training finished
---------------------
gamma: 0.12507911839146554
training start after waiting for 1.1485610008239746 seconds
policy loss:-150.05108642578125
value loss:7.640198230743408
entropies:32.66699981689453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1240.8170)
ToM Target loss= tensor(2339.8589)
optimized based on ToM loss
---------------------
gamma: 0.12532927662824847
training start after waiting for 1.2091271877288818 seconds
policy loss:-71.27304077148438
value loss:6.54768180847168
entropies:26.839860916137695
Policy training finished
---------------------
gamma: 0.12532927662824847
training start after waiting for 1.134629487991333 seconds
policy loss:-702.0009155273438
value loss:17.9483699798584
entropies:48.42634963989258
Policy training finished
---------------------
gamma: 0.12532927662824847
training start after waiting for 1.1841440200805664 seconds
policy loss:-1129.924560546875
value loss:21.084430694580078
entropies:48.12049102783203
Policy training finished
---------------------
gamma: 0.12532927662824847
training start after waiting for 1.2099905014038086 seconds
policy loss:-1218.064208984375
value loss:24.10393524169922
entropies:44.00489807128906
Policy training finished
---------------------
gamma: 0.12532927662824847
training start after waiting for 1.133143424987793 seconds
policy loss:-1391.8670654296875
value loss:21.01142120361328
entropies:46.47148895263672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1286.6208)
ToM Target loss= tensor(2256.0063)
optimized based on ToM loss
---------------------
gamma: 0.12557993518150498
training start after waiting for 1.1526575088500977 seconds
policy loss:-189.20175170898438
value loss:18.437089920043945
entropies:37.332611083984375
Policy training finished
---------------------
gamma: 0.12557993518150498
training start after waiting for 1.2008068561553955 seconds
policy loss:182.33880615234375
value loss:10.52665901184082
entropies:33.17629623413086
Policy training finished
---------------------
gamma: 0.12557993518150498
training start after waiting for 1.1950325965881348 seconds
policy loss:107.8287582397461
value loss:17.9708251953125
entropies:35.022186279296875
Policy training finished
---------------------
gamma: 0.12557993518150498
training start after waiting for 1.2286369800567627 seconds
policy loss:-95.49236297607422
value loss:8.97996997833252
entropies:18.731239318847656
Policy training finished
---------------------
gamma: 0.12557993518150498
training start after waiting for 1.2079885005950928 seconds
policy loss:669.6622314453125
value loss:10.843828201293945
entropies:27.547809600830078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1240.0908)
ToM Target loss= tensor(2256.3391)
optimized based on ToM loss
---------------------
gamma: 0.12583109505186799
training start after waiting for 1.188209056854248 seconds
policy loss:-847.822021484375
value loss:38.647823333740234
entropies:30.400588989257812
Policy training finished
---------------------
gamma: 0.12583109505186799
training start after waiting for 1.1850688457489014 seconds
policy loss:-51.54472732543945
value loss:14.833182334899902
entropies:30.98484230041504
Policy training finished
---------------------
gamma: 0.12583109505186799
training start after waiting for 1.2228202819824219 seconds
policy loss:-550.2868041992188
value loss:15.295875549316406
entropies:33.53878402709961
Policy training finished
---------------------
gamma: 0.12583109505186799
training start after waiting for 1.1761643886566162 seconds
policy loss:-623.6995239257812
value loss:15.674104690551758
entropies:39.499107360839844
Policy training finished
---------------------
gamma: 0.12583109505186799
training start after waiting for 1.1686880588531494 seconds
policy loss:-197.03561401367188
value loss:11.119290351867676
entropies:46.99662780761719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1211.3197)
ToM Target loss= tensor(2300.5403)
optimized based on ToM loss
---------------------
gamma: 0.1260827572419717
training start after waiting for 1.1395845413208008 seconds
policy loss:-183.31448364257812
value loss:12.664963722229004
entropies:25.788043975830078
Policy training finished
---------------------
gamma: 0.1260827572419717
training start after waiting for 1.1867213249206543 seconds
policy loss:-29.33880043029785
value loss:24.83245086669922
entropies:33.01708984375
Policy training finished
---------------------
gamma: 0.1260827572419717
training start after waiting for 1.1731977462768555 seconds
policy loss:-310.67041015625
value loss:19.00592041015625
entropies:38.593231201171875
Policy training finished
---------------------
gamma: 0.1260827572419717
training start after waiting for 1.1505355834960938 seconds
policy loss:370.4556884765625
value loss:11.009809494018555
entropies:27.80239486694336
Policy training finished
---------------------
gamma: 0.1260827572419717
training start after waiting for 1.225109338760376 seconds
policy loss:-55.52162170410156
value loss:10.853714942932129
entropies:28.0513916015625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1088.9994)
ToM Target loss= tensor(2197.9395)
optimized based on ToM loss
---------------------
gamma: 0.12633492275645566
training start after waiting for 1.1868207454681396 seconds
policy loss:-391.2231140136719
value loss:5.9446120262146
entropies:21.386310577392578
Policy training finished
---------------------
gamma: 0.12633492275645566
training start after waiting for 1.1520593166351318 seconds
policy loss:-1001.2861938476562
value loss:24.402090072631836
entropies:40.200531005859375
Policy training finished
---------------------
gamma: 0.12633492275645566
training start after waiting for 1.1897940635681152 seconds
policy loss:-849.839111328125
value loss:19.786388397216797
entropies:31.069942474365234
Policy training finished
---------------------
gamma: 0.12633492275645566
training start after waiting for 1.1468853950500488 seconds
policy loss:-3611.719970703125
value loss:43.05942916870117
entropies:37.16949462890625
Policy training finished
---------------------
gamma: 0.12633492275645566
training start after waiting for 1.1418485641479492 seconds
policy loss:-1582.6689453125
value loss:30.145545959472656
entropies:42.06104278564453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1160.9410)
ToM Target loss= tensor(2136.8569)
optimized based on ToM loss
---------------------
gamma: 0.12658759260196858
training start after waiting for 1.1869289875030518 seconds
policy loss:-46.62339782714844
value loss:8.933439254760742
entropies:23.098560333251953
Policy training finished
---------------------
gamma: 0.12658759260196858
training start after waiting for 1.1539552211761475 seconds
policy loss:-533.5939331054688
value loss:22.712623596191406
entropies:26.270301818847656
Policy training finished
---------------------
gamma: 0.12658759260196858
training start after waiting for 1.1763148307800293 seconds
policy loss:-56.99046325683594
value loss:30.685928344726562
entropies:38.50170135498047
Policy training finished
---------------------
gamma: 0.12658759260196858
training start after waiting for 1.1478040218353271 seconds
policy loss:-268.10565185546875
value loss:28.227066040039062
entropies:33.05133819580078
Policy training finished
---------------------
gamma: 0.12658759260196858
training start after waiting for 1.1937437057495117 seconds
policy loss:-396.54119873046875
value loss:9.726707458496094
entropies:19.561182022094727
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1061.0388)
ToM Target loss= tensor(2235.1458)
optimized based on ToM loss
---------------------
gamma: 0.12684076778717251
training start after waiting for 1.169245958328247 seconds
policy loss:-1220.121337890625
value loss:21.854429244995117
entropies:38.28799057006836
Policy training finished
---------------------
gamma: 0.12684076778717251
training start after waiting for 1.173607349395752 seconds
policy loss:380.59637451171875
value loss:4.951088905334473
entropies:30.520706176757812
Policy training finished
---------------------
gamma: 0.12684076778717251
training start after waiting for 1.2045152187347412 seconds
policy loss:-988.164794921875
value loss:20.742740631103516
entropies:46.237205505371094
Policy training finished
---------------------
gamma: 0.12684076778717251
training start after waiting for 1.1995737552642822 seconds
policy loss:-88.50701904296875
value loss:8.494734764099121
entropies:16.417648315429688
Policy training finished
---------------------
gamma: 0.12684076778717251
training start after waiting for 1.1489272117614746 seconds
policy loss:-1029.3399658203125
value loss:48.02991485595703
entropies:19.354572296142578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1141.8606)
ToM Target loss= tensor(2245.6797)
optimized based on ToM loss
---------------------
gamma: 0.12709444932274686
training start after waiting for 1.2112984657287598 seconds
policy loss:-113.06587982177734
value loss:12.88015365600586
entropies:32.55184555053711
Policy training finished
---------------------
gamma: 0.12709444932274686
training start after waiting for 1.1415348052978516 seconds
policy loss:-402.78985595703125
value loss:11.831293106079102
entropies:20.639270782470703
Policy training finished
---------------------
gamma: 0.12709444932274686
training start after waiting for 1.2105343341827393 seconds
policy loss:261.5383605957031
value loss:10.46166706085205
entropies:35.585723876953125
Policy training finished
---------------------
gamma: 0.12709444932274686
training start after waiting for 1.2051751613616943 seconds
policy loss:120.27421569824219
value loss:7.621068000793457
entropies:20.592914581298828
Policy training finished
---------------------
gamma: 0.12709444932274686
training start after waiting for 1.1520841121673584 seconds
policy loss:-622.8930053710938
value loss:14.323421478271484
entropies:30.315902709960938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1116.1179)
ToM Target loss= tensor(2254.8335)
optimized based on ToM loss
---------------------
gamma: 0.12734863822139236
training start after waiting for 1.1822006702423096 seconds
policy loss:-579.3770141601562
value loss:14.670795440673828
entropies:29.157161712646484
Policy training finished
---------------------
gamma: 0.12734863822139236
training start after waiting for 1.143878698348999 seconds
policy loss:-956.0610961914062
value loss:27.85280990600586
entropies:33.85954284667969
Policy training finished
---------------------
gamma: 0.12734863822139236
training start after waiting for 1.193894624710083 seconds
policy loss:-311.82196044921875
value loss:15.90576171875
entropies:28.279314041137695
Policy training finished
---------------------
gamma: 0.12734863822139236
training start after waiting for 1.204967737197876 seconds
policy loss:-585.495849609375
value loss:13.036441802978516
entropies:39.72828674316406
Policy training finished
---------------------
gamma: 0.12734863822139236
training start after waiting for 1.191739797592163 seconds
policy loss:-1125.47998046875
value loss:22.088153839111328
entropies:46.853275299072266
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1147.3451)
ToM Target loss= tensor(2188.5554)
optimized based on ToM loss
---------------------
gamma: 0.12760333549783515
training start after waiting for 1.1875388622283936 seconds
policy loss:-60.886741638183594
value loss:22.461729049682617
entropies:18.970226287841797
Policy training finished
---------------------
gamma: 0.12760333549783515
training start after waiting for 1.2063403129577637 seconds
policy loss:-255.4392547607422
value loss:11.175046920776367
entropies:34.282535552978516
Policy training finished
---------------------
gamma: 0.12760333549783515
training start after waiting for 1.178001880645752 seconds
policy loss:266.7955322265625
value loss:21.591102600097656
entropies:38.210479736328125
Policy training finished
---------------------
gamma: 0.12760333549783515
training start after waiting for 1.2064263820648193 seconds
policy loss:-250.06573486328125
value loss:7.740451812744141
entropies:43.43212890625
Policy training finished
---------------------
gamma: 0.12760333549783515
training start after waiting for 1.1953608989715576 seconds
policy loss:-275.83349609375
value loss:5.890594482421875
entropies:26.38926887512207
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1160.6871)
ToM Target loss= tensor(2248.6589)
optimized based on ToM loss
---------------------
gamma: 0.12785854216883083
training start after waiting for 1.1483049392700195 seconds
policy loss:-472.17291259765625
value loss:17.371248245239258
entropies:35.39955139160156
Policy training finished
---------------------
gamma: 0.12785854216883083
training start after waiting for 1.1841411590576172 seconds
policy loss:120.66999053955078
value loss:5.162528038024902
entropies:20.43280029296875
Policy training finished
---------------------
gamma: 0.12785854216883083
training start after waiting for 1.197554588317871 seconds
policy loss:1.590818166732788
value loss:7.486584186553955
entropies:29.238393783569336
Policy training finished
---------------------
gamma: 0.12785854216883083
training start after waiting for 1.1520195007324219 seconds
policy loss:-127.73699188232422
value loss:11.48024845123291
entropies:25.186290740966797
Policy training finished
---------------------
gamma: 0.12785854216883083
training start after waiting for 1.1799921989440918 seconds
policy loss:-613.46484375
value loss:16.3094482421875
entropies:40.47059631347656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1134.7341)
ToM Target loss= tensor(2233.1980)
optimized based on ToM loss
---------------------
gamma: 0.1281142592531685
training start after waiting for 1.1838562488555908 seconds
policy loss:-56.82258605957031
value loss:9.812102317810059
entropies:32.61121368408203
Policy training finished
---------------------
gamma: 0.1281142592531685
training start after waiting for 1.1983921527862549 seconds
policy loss:-647.1428833007812
value loss:22.139760971069336
entropies:32.366676330566406
Policy training finished
---------------------
gamma: 0.1281142592531685
training start after waiting for 1.194530725479126 seconds
policy loss:-487.6446838378906
value loss:12.364477157592773
entropies:33.5400276184082
Policy training finished
---------------------
gamma: 0.1281142592531685
training start after waiting for 1.1990363597869873 seconds
policy loss:-2421.38671875
value loss:48.52235412597656
entropies:34.00706100463867
Policy training finished
---------------------
gamma: 0.1281142592531685
training start after waiting for 1.173001766204834 seconds
policy loss:430.94873046875
value loss:6.905613422393799
entropies:21.2011661529541
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1148.2355)
ToM Target loss= tensor(2239.6926)
optimized based on ToM loss
---------------------
gamma: 0.12837048777167484
training start after waiting for 1.1336908340454102 seconds
policy loss:-589.3843994140625
value loss:8.561491012573242
entropies:29.36345672607422
Policy training finished
---------------------
gamma: 0.12837048777167484
training start after waiting for 1.1173303127288818 seconds
policy loss:-368.32244873046875
value loss:21.808168411254883
entropies:42.945892333984375
Policy training finished
---------------------
gamma: 0.12837048777167484
training start after waiting for 1.2200381755828857 seconds
policy loss:-347.19903564453125
value loss:11.225241661071777
entropies:24.517114639282227
Policy training finished
---------------------
gamma: 0.12837048777167484
training start after waiting for 1.128983736038208 seconds
policy loss:-373.3911437988281
value loss:13.017515182495117
entropies:34.042091369628906
Policy training finished
---------------------
gamma: 0.12837048777167484
training start after waiting for 1.1741211414337158 seconds
policy loss:78.9892349243164
value loss:9.160063743591309
entropies:27.78244972229004
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1124.0808)
ToM Target loss= tensor(2110.8313)
optimized based on ToM loss
---------------------
gamma: 0.1286272287472182
training start after waiting for 1.1893198490142822 seconds
policy loss:-1540.750244140625
value loss:17.34090232849121
entropies:38.11817932128906
Policy training finished
---------------------
gamma: 0.1286272287472182
training start after waiting for 1.1769745349884033 seconds
policy loss:-425.2794494628906
value loss:16.593017578125
entropies:30.102811813354492
Policy training finished
---------------------
gamma: 0.1286272287472182
training start after waiting for 1.1926491260528564 seconds
policy loss:-397.4202880859375
value loss:14.170272827148438
entropies:25.927419662475586
Policy training finished
---------------------
gamma: 0.1286272287472182
training start after waiting for 1.1456801891326904 seconds
policy loss:-1302.4993896484375
value loss:27.349727630615234
entropies:56.245086669921875
Policy training finished
---------------------
gamma: 0.1286272287472182
training start after waiting for 1.1705811023712158 seconds
policy loss:-721.6976318359375
value loss:20.872005462646484
entropies:25.36883544921875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1221.5736)
ToM Target loss= tensor(2180.5032)
optimized based on ToM loss
---------------------
gamma: 0.12888448320471263
training start after waiting for 1.1971964836120605 seconds
policy loss:216.24034118652344
value loss:13.695030212402344
entropies:37.00018310546875
Policy training finished
---------------------
gamma: 0.12888448320471263
training start after waiting for 1.1456425189971924 seconds
policy loss:-6.248250961303711
value loss:18.643287658691406
entropies:29.018531799316406
Policy training finished
---------------------
gamma: 0.12888448320471263
training start after waiting for 1.2021815776824951 seconds
policy loss:227.6460723876953
value loss:12.966182708740234
entropies:26.97281837463379
Policy training finished
---------------------
gamma: 0.12888448320471263
training start after waiting for 1.1109232902526855 seconds
policy loss:-317.6644287109375
value loss:25.21662712097168
entropies:42.949527740478516
Policy training finished
---------------------
gamma: 0.12888448320471263
training start after waiting for 1.1497802734375 seconds
policy loss:-86.75679016113281
value loss:16.649852752685547
entropies:38.58769989013672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1207.7300)
ToM Target loss= tensor(2203.0254)
optimized based on ToM loss
---------------------
gamma: 0.12914225217112205
training start after waiting for 1.1531398296356201 seconds
policy loss:-675.8677368164062
value loss:36.808006286621094
entropies:44.137290954589844
Policy training finished
---------------------
gamma: 0.12914225217112205
training start after waiting for 1.1753458976745605 seconds
policy loss:-1540.078857421875
value loss:38.916439056396484
entropies:51.43492126464844
Policy training finished
---------------------
gamma: 0.12914225217112205
training start after waiting for 1.197530746459961 seconds
policy loss:-277.6236572265625
value loss:10.467023849487305
entropies:28.273845672607422
Policy training finished
---------------------
gamma: 0.12914225217112205
training start after waiting for 1.1879665851593018 seconds
policy loss:-1408.643798828125
value loss:16.459362030029297
entropies:46.06669616699219
Policy training finished
---------------------
gamma: 0.12914225217112205
training start after waiting for 1.2066714763641357 seconds
policy loss:-242.75486755371094
value loss:13.626638412475586
entropies:25.40433120727539
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1195.5734)
ToM Target loss= tensor(2253.2983)
optimized based on ToM loss
---------------------
gamma: 0.12940053667546428
training start after waiting for 1.1886966228485107 seconds
policy loss:106.67030334472656
value loss:7.020766735076904
entropies:27.604278564453125
Policy training finished
---------------------
gamma: 0.12940053667546428
training start after waiting for 1.1989784240722656 seconds
policy loss:-546.8175048828125
value loss:31.33831787109375
entropies:36.54741668701172
Policy training finished
---------------------
gamma: 0.12940053667546428
training start after waiting for 1.1632826328277588 seconds
policy loss:226.7397003173828
value loss:9.03402328491211
entropies:22.956741333007812
Policy training finished
---------------------
gamma: 0.12940053667546428
training start after waiting for 1.1975626945495605 seconds
policy loss:-751.995849609375
value loss:20.344615936279297
entropies:38.67792510986328
Policy training finished
---------------------
gamma: 0.12940053667546428
training start after waiting for 1.177361011505127 seconds
policy loss:-1218.86669921875
value loss:18.138233184814453
entropies:42.923744201660156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1263.5316)
ToM Target loss= tensor(2212.9189)
optimized based on ToM loss
---------------------
gamma: 0.12965933774881522
training start after waiting for 1.1795284748077393 seconds
policy loss:-236.06814575195312
value loss:14.665857315063477
entropies:29.935077667236328
Policy training finished
---------------------
gamma: 0.12965933774881522
training start after waiting for 1.2257132530212402 seconds
policy loss:-297.190185546875
value loss:12.065576553344727
entropies:29.05398178100586
Policy training finished
---------------------
gamma: 0.12965933774881522
training start after waiting for 1.2078702449798584 seconds
policy loss:144.82217407226562
value loss:18.112287521362305
entropies:28.12436294555664
Policy training finished
---------------------
gamma: 0.12965933774881522
training start after waiting for 1.2169842720031738 seconds
policy loss:355.58587646484375
value loss:10.126943588256836
entropies:35.54193878173828
Policy training finished
---------------------
gamma: 0.12965933774881522
training start after waiting for 1.1884760856628418 seconds
policy loss:-353.5345458984375
value loss:10.682960510253906
entropies:27.89188003540039
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1116.3760)
ToM Target loss= tensor(2204.4939)
optimized based on ToM loss
---------------------
gamma: 0.12991865642431286
training start after waiting for 1.1774370670318604 seconds
policy loss:-634.301513671875
value loss:19.905385971069336
entropies:35.2198486328125
Policy training finished
---------------------
gamma: 0.12991865642431286
training start after waiting for 1.196913480758667 seconds
policy loss:207.89073181152344
value loss:16.191633224487305
entropies:26.909603118896484
Policy training finished
---------------------
gamma: 0.12991865642431286
training start after waiting for 1.1668341159820557 seconds
policy loss:-312.3717041015625
value loss:12.315898895263672
entropies:26.554155349731445
Policy training finished
---------------------
gamma: 0.12991865642431286
training start after waiting for 1.1468286514282227 seconds
policy loss:-1074.2957763671875
value loss:27.02433204650879
entropies:38.31793975830078
Policy training finished
---------------------
gamma: 0.12991865642431286
training start after waiting for 1.2063713073730469 seconds
policy loss:-486.8247985839844
value loss:17.45683479309082
entropies:35.170494079589844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1199.1509)
ToM Target loss= tensor(2191.6084)
optimized based on ToM loss
---------------------
gamma: 0.13017849373716148
training start after waiting for 1.1867382526397705 seconds
policy loss:661.1715698242188
value loss:14.68254280090332
entropies:30.897125244140625
Policy training finished
---------------------
gamma: 0.13017849373716148
training start after waiting for 1.1475286483764648 seconds
policy loss:-107.72998046875
value loss:12.03273868560791
entropies:21.421144485473633
Policy training finished
---------------------
gamma: 0.13017849373716148
training start after waiting for 1.152491807937622 seconds
policy loss:262.2293395996094
value loss:11.000974655151367
entropies:21.607627868652344
Policy training finished
---------------------
gamma: 0.13017849373716148
training start after waiting for 1.168337345123291 seconds
policy loss:-530.47216796875
value loss:13.423900604248047
entropies:23.46584701538086
Policy training finished
---------------------
gamma: 0.13017849373716148
training start after waiting for 1.1503281593322754 seconds
policy loss:-866.12255859375
value loss:29.631114959716797
entropies:41.8863639831543
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1104.1251)
ToM Target loss= tensor(2236.3992)
optimized based on ToM loss
---------------------
gamma: 0.1304388507246358
training start after waiting for 1.1856446266174316 seconds
policy loss:-332.5254211425781
value loss:26.29517364501953
entropies:17.055614471435547
Policy training finished
---------------------
gamma: 0.1304388507246358
training start after waiting for 1.1496562957763672 seconds
policy loss:-361.65374755859375
value loss:8.343107223510742
entropies:20.08724021911621
Policy training finished
---------------------
gamma: 0.1304388507246358
training start after waiting for 1.148209810256958 seconds
policy loss:-2141.336181640625
value loss:40.556827545166016
entropies:32.13509750366211
Policy training finished
---------------------
gamma: 0.1304388507246358
training start after waiting for 1.1492042541503906 seconds
policy loss:11.909200668334961
value loss:10.20788860321045
entropies:30.718320846557617
Policy training finished
---------------------
gamma: 0.1304388507246358
training start after waiting for 1.1949570178985596 seconds
policy loss:-37.4884033203125
value loss:19.415414810180664
entropies:20.92298126220703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1116.2726)
ToM Target loss= tensor(2275.4072)
optimized based on ToM loss
---------------------
gamma: 0.13069972842608507
training start after waiting for 1.2173247337341309 seconds
policy loss:-49.589717864990234
value loss:9.104730606079102
entropies:23.859630584716797
Policy training finished
---------------------
gamma: 0.13069972842608507
training start after waiting for 1.2002437114715576 seconds
policy loss:-17.719846725463867
value loss:7.172743320465088
entropies:20.748111724853516
Policy training finished
---------------------
gamma: 0.13069972842608507
training start after waiting for 1.1948776245117188 seconds
policy loss:97.73657989501953
value loss:6.959264755249023
entropies:26.21550941467285
Policy training finished
---------------------
gamma: 0.13069972842608507
training start after waiting for 1.1766974925994873 seconds
policy loss:-349.7994384765625
value loss:18.803997039794922
entropies:17.505464553833008
Policy training finished
---------------------
gamma: 0.13069972842608507
training start after waiting for 1.1840064525604248 seconds
policy loss:-541.2408447265625
value loss:8.574075698852539
entropies:22.20293426513672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1152.8312)
ToM Target loss= tensor(2293.1877)
optimized based on ToM loss
---------------------
gamma: 0.13096112788293723
training start after waiting for 1.1556856632232666 seconds
policy loss:-391.7225036621094
value loss:13.47728443145752
entropies:26.08527374267578
Policy training finished
---------------------
gamma: 0.13096112788293723
training start after waiting for 1.1376521587371826 seconds
policy loss:-488.35406494140625
value loss:19.256072998046875
entropies:28.723501205444336
Policy training finished
---------------------
gamma: 0.13096112788293723
training start after waiting for 1.1823313236236572 seconds
policy loss:-129.6053924560547
value loss:7.566488742828369
entropies:14.148675918579102
Policy training finished
---------------------
gamma: 0.13096112788293723
training start after waiting for 1.1445341110229492 seconds
policy loss:67.78118896484375
value loss:13.997725486755371
entropies:34.303375244140625
Policy training finished
---------------------
gamma: 0.13096112788293723
training start after waiting for 1.1462085247039795 seconds
policy loss:-44.47056579589844
value loss:8.753761291503906
entropies:37.7476921081543
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1141.7295)
ToM Target loss= tensor(2231.8958)
optimized based on ToM loss
---------------------
gamma: 0.1312230501387031
training start after waiting for 1.196678638458252 seconds
policy loss:-1.0549932718276978
value loss:7.8412957191467285
entropies:24.535953521728516
Policy training finished
---------------------
gamma: 0.1312230501387031
training start after waiting for 1.2034988403320312 seconds
policy loss:-531.496337890625
value loss:12.082165718078613
entropies:30.099443435668945
Policy training finished
---------------------
gamma: 0.1312230501387031
training start after waiting for 1.1950039863586426 seconds
policy loss:-801.1466064453125
value loss:15.572723388671875
entropies:32.04487991333008
Policy training finished
---------------------
gamma: 0.1312230501387031
training start after waiting for 1.1482553482055664 seconds
policy loss:-801.7574462890625
value loss:52.017364501953125
entropies:40.290565490722656
Policy training finished
---------------------
gamma: 0.1312230501387031
training start after waiting for 1.1929185390472412 seconds
policy loss:-596.4137573242188
value loss:18.709270477294922
entropies:30.755821228027344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1234.6710)
ToM Target loss= tensor(2245.6633)
optimized based on ToM loss
---------------------
gamma: 0.1314854962389805
training start after waiting for 1.184666633605957 seconds
policy loss:-120.36325073242188
value loss:12.317048072814941
entropies:24.443572998046875
Policy training finished
---------------------
gamma: 0.1314854962389805
training start after waiting for 1.1758239269256592 seconds
policy loss:-20.363229751586914
value loss:6.510056018829346
entropies:26.975616455078125
Policy training finished
---------------------
gamma: 0.1314854962389805
training start after waiting for 1.1714460849761963 seconds
policy loss:-116.91740417480469
value loss:13.672477722167969
entropies:32.988059997558594
Policy training finished
---------------------
gamma: 0.1314854962389805
training start after waiting for 1.1603033542633057 seconds
policy loss:-463.93975830078125
value loss:25.34747886657715
entropies:23.0870361328125
Policy training finished
---------------------
gamma: 0.1314854962389805
training start after waiting for 1.1774673461914062 seconds
policy loss:295.4649658203125
value loss:7.088515758514404
entropies:15.443330764770508
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1172.0083)
ToM Target loss= tensor(2290.2339)
optimized based on ToM loss
---------------------
gamma: 0.13174846723145847
training start after waiting for 1.1824028491973877 seconds
policy loss:65.16273498535156
value loss:9.297833442687988
entropies:28.023670196533203
Policy training finished
---------------------
gamma: 0.13174846723145847
training start after waiting for 1.1535158157348633 seconds
policy loss:-393.1069641113281
value loss:14.338778495788574
entropies:33.656986236572266
Policy training finished
---------------------
gamma: 0.13174846723145847
training start after waiting for 1.1835463047027588 seconds
policy loss:-479.94622802734375
value loss:15.570439338684082
entropies:24.43317985534668
Policy training finished
---------------------
gamma: 0.13174846723145847
training start after waiting for 1.1929678916931152 seconds
policy loss:-200.17820739746094
value loss:7.522124290466309
entropies:30.42804718017578
Policy training finished
---------------------
gamma: 0.13174846723145847
training start after waiting for 1.1794545650482178 seconds
policy loss:152.9527130126953
value loss:7.343157768249512
entropies:18.782516479492188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1168.2776)
ToM Target loss= tensor(2237.5212)
optimized based on ToM loss
---------------------
gamma: 0.1320119641659214
training start after waiting for 1.1684041023254395 seconds
policy loss:-863.4769287109375
value loss:16.72953224182129
entropies:40.668636322021484
Policy training finished
---------------------
gamma: 0.1320119641659214
training start after waiting for 1.15521240234375 seconds
policy loss:21.28329086303711
value loss:7.426206588745117
entropies:18.868274688720703
Policy training finished
---------------------
gamma: 0.1320119641659214
training start after waiting for 1.1954936981201172 seconds
policy loss:-882.7328491210938
value loss:25.104652404785156
entropies:41.96473693847656
Policy training finished
---------------------
gamma: 0.1320119641659214
training start after waiting for 1.2020282745361328 seconds
policy loss:-13.082773208618164
value loss:13.068222045898438
entropies:20.812637329101562
Policy training finished
---------------------
gamma: 0.1320119641659214
training start after waiting for 1.1918790340423584 seconds
policy loss:-397.281494140625
value loss:14.294041633605957
entropies:40.634281158447266
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1119.6447)
ToM Target loss= tensor(2062.9673)
optimized based on ToM loss
---------------------
gamma: 0.13227598809425323
training start after waiting for 1.154897928237915 seconds
policy loss:-214.0293426513672
value loss:24.326618194580078
entropies:50.36444854736328
Policy training finished
---------------------
gamma: 0.13227598809425323
training start after waiting for 1.1501963138580322 seconds
policy loss:57.19575500488281
value loss:10.10961627960205
entropies:22.435922622680664
Policy training finished
---------------------
gamma: 0.13227598809425323
training start after waiting for 1.1450765132904053 seconds
policy loss:-900.090576171875
value loss:22.326501846313477
entropies:43.088863372802734
Policy training finished
---------------------
gamma: 0.13227598809425323
training start after waiting for 1.2061891555786133 seconds
policy loss:-364.6094055175781
value loss:21.02798843383789
entropies:34.224510192871094
Policy training finished
---------------------
gamma: 0.13227598809425323
training start after waiting for 1.1370248794555664 seconds
policy loss:-662.3413696289062
value loss:21.174163818359375
entropies:30.836875915527344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1266.3488)
ToM Target loss= tensor(2246.5098)
optimized based on ToM loss
---------------------
gamma: 0.13254054007044172
training start after waiting for 1.1442534923553467 seconds
policy loss:49.614871978759766
value loss:25.407649993896484
entropies:38.047325134277344
Policy training finished
---------------------
gamma: 0.13254054007044172
training start after waiting for 1.1844351291656494 seconds
policy loss:-383.50616455078125
value loss:13.533239364624023
entropies:33.909549713134766
Policy training finished
---------------------
gamma: 0.13254054007044172
training start after waiting for 1.1743104457855225 seconds
policy loss:180.7459259033203
value loss:7.3510942459106445
entropies:23.49547004699707
Policy training finished
---------------------
gamma: 0.13254054007044172
training start after waiting for 1.2213680744171143 seconds
policy loss:-1090.04052734375
value loss:22.598360061645508
entropies:33.08349609375
Policy training finished
---------------------
gamma: 0.13254054007044172
training start after waiting for 1.1715872287750244 seconds
policy loss:-863.777587890625
value loss:19.95052719116211
entropies:32.84291458129883
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1293.8453)
ToM Target loss= tensor(2327.1648)
optimized based on ToM loss
---------------------
gamma: 0.13280562115058261
training start after waiting for 1.206054925918579 seconds
policy loss:-1912.356201171875
value loss:45.01472473144531
entropies:60.5528678894043
Policy training finished
---------------------
gamma: 0.13280562115058261
training start after waiting for 1.1458702087402344 seconds
policy loss:-1552.496826171875
value loss:33.82319641113281
entropies:46.88024139404297
Policy training finished
---------------------
gamma: 0.13280562115058261
training start after waiting for 1.1899261474609375 seconds
policy loss:-955.5089111328125
value loss:13.723443984985352
entropies:49.2969856262207
Policy training finished
---------------------
gamma: 0.13280562115058261
training start after waiting for 1.2013471126556396 seconds
policy loss:-691.8875732421875
value loss:28.783214569091797
entropies:28.157867431640625
Policy training finished
---------------------
gamma: 0.13280562115058261
training start after waiting for 1.204223394393921 seconds
policy loss:-358.69482421875
value loss:33.2452392578125
entropies:41.282352447509766
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1405.1322)
ToM Target loss= tensor(2278.9185)
optimized based on ToM loss
---------------------
gamma: 0.1330712323928838
training start after waiting for 1.1603777408599854 seconds
policy loss:-149.64846801757812
value loss:17.6043701171875
entropies:40.70085144042969
Policy training finished
---------------------
gamma: 0.1330712323928838
training start after waiting for 1.143625259399414 seconds
policy loss:612.7348022460938
value loss:30.006990432739258
entropies:31.396821975708008
Policy training finished
---------------------
gamma: 0.1330712323928838
training start after waiting for 1.1793310642242432 seconds
policy loss:71.061279296875
value loss:19.567712783813477
entropies:25.480314254760742
Policy training finished
---------------------
gamma: 0.1330712323928838
training start after waiting for 1.1481401920318604 seconds
policy loss:-741.4365844726562
value loss:26.416879653930664
entropies:53.840576171875
Policy training finished
---------------------
gamma: 0.1330712323928838
training start after waiting for 1.2156829833984375 seconds
policy loss:-514.0531005859375
value loss:12.727335929870605
entropies:28.94810676574707
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1286.6904)
ToM Target loss= tensor(2375.0740)
optimized based on ToM loss
---------------------
gamma: 0.13333737485766955
training start after waiting for 1.1361644268035889 seconds
policy loss:-1559.5400390625
value loss:34.1735725402832
entropies:46.95245361328125
Policy training finished
---------------------
gamma: 0.13333737485766955
training start after waiting for 1.1812174320220947 seconds
policy loss:-714.9779663085938
value loss:10.920089721679688
entropies:31.02019500732422
Policy training finished
---------------------
gamma: 0.13333737485766955
training start after waiting for 1.161071538925171 seconds
policy loss:-1071.622314453125
value loss:18.479265213012695
entropies:34.99359893798828
Policy training finished
---------------------
gamma: 0.13333737485766955
training start after waiting for 1.1907448768615723 seconds
policy loss:-481.0459289550781
value loss:16.64066505432129
entropies:45.73738098144531
Policy training finished
---------------------
gamma: 0.13333737485766955
training start after waiting for 1.148367166519165 seconds
policy loss:-630.9705810546875
value loss:10.805778503417969
entropies:28.195526123046875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1309.9119)
ToM Target loss= tensor(2464.4438)
optimized based on ToM loss
---------------------
gamma: 0.1336040496073849
training start after waiting for 1.1717374324798584 seconds
policy loss:-747.2950439453125
value loss:21.947830200195312
entropies:37.16265106201172
Policy training finished
---------------------
gamma: 0.1336040496073849
training start after waiting for 1.200333595275879 seconds
policy loss:21.02333641052246
value loss:8.613907814025879
entropies:38.6286735534668
Policy training finished
---------------------
gamma: 0.1336040496073849
training start after waiting for 1.1438708305358887 seconds
policy loss:-238.90647888183594
value loss:20.21385955810547
entropies:40.9473876953125
Policy training finished
---------------------
gamma: 0.1336040496073849
training start after waiting for 1.1945812702178955 seconds
policy loss:-66.9083251953125
value loss:8.919586181640625
entropies:25.75697135925293
Policy training finished
---------------------
gamma: 0.1336040496073849
training start after waiting for 1.1803319454193115 seconds
policy loss:-114.79743957519531
value loss:4.800272464752197
entropies:18.2933349609375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1314.7422)
ToM Target loss= tensor(2546.6221)
optimized based on ToM loss
---------------------
gamma: 0.13387125770659966
training start after waiting for 1.1901161670684814 seconds
policy loss:-1270.4912109375
value loss:23.170549392700195
entropies:40.693721771240234
Policy training finished
---------------------
gamma: 0.13387125770659966
training start after waiting for 1.140061855316162 seconds
policy loss:-361.9792175292969
value loss:14.366777420043945
entropies:29.259931564331055
Policy training finished
---------------------
gamma: 0.13387125770659966
training start after waiting for 1.1935465335845947 seconds
policy loss:-66.68226623535156
value loss:6.185231685638428
entropies:27.13367462158203
Policy training finished
---------------------
gamma: 0.13387125770659966
training start after waiting for 1.2268397808074951 seconds
policy loss:-1461.3900146484375
value loss:18.21002197265625
entropies:41.019596099853516
Policy training finished
---------------------
gamma: 0.13387125770659966
training start after waiting for 1.1482067108154297 seconds
policy loss:-633.2730102539062
value loss:18.62965202331543
entropies:43.74020004272461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1252.4921)
ToM Target loss= tensor(2428.3860)
optimized based on ToM loss
---------------------
gamma: 0.13413900022201286
training start after waiting for 1.1359059810638428 seconds
policy loss:-212.81105041503906
value loss:7.956960678100586
entropies:24.608516693115234
Policy training finished
---------------------
gamma: 0.13413900022201286
training start after waiting for 1.194958209991455 seconds
policy loss:376.56201171875
value loss:20.75924301147461
entropies:34.598045349121094
Policy training finished
---------------------
gamma: 0.13413900022201286
training start after waiting for 1.138885498046875 seconds
policy loss:-82.5656967163086
value loss:6.83008337020874
entropies:32.690452575683594
Policy training finished
---------------------
gamma: 0.13413900022201286
training start after waiting for 1.2041940689086914 seconds
policy loss:-721.708984375
value loss:10.756569862365723
entropies:25.994125366210938
Policy training finished
---------------------
gamma: 0.13413900022201286
training start after waiting for 1.185971975326538 seconds
policy loss:-897.4325561523438
value loss:18.732633590698242
entropies:47.1300048828125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1243.5933)
ToM Target loss= tensor(2430.6431)
optimized based on ToM loss
---------------------
gamma: 0.13440727822245688
training start after waiting for 1.190992832183838 seconds
policy loss:-208.39279174804688
value loss:4.6040873527526855
entropies:24.248401641845703
Policy training finished
---------------------
gamma: 0.13440727822245688
training start after waiting for 1.209272861480713 seconds
policy loss:-456.0858459472656
value loss:10.252181053161621
entropies:25.061767578125
Policy training finished
---------------------
gamma: 0.13440727822245688
training start after waiting for 1.2016868591308594 seconds
policy loss:115.29678344726562
value loss:4.635354042053223
entropies:17.69344711303711
Policy training finished
---------------------
gamma: 0.13440727822245688
training start after waiting for 1.186286211013794 seconds
policy loss:-2071.076904296875
value loss:38.11650085449219
entropies:42.49247360229492
Policy training finished
---------------------
gamma: 0.13440727822245688
training start after waiting for 1.1811497211456299 seconds
policy loss:-1434.8187255859375
value loss:39.1787109375
entropies:33.91460418701172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1196.1731)
ToM Target loss= tensor(2350.9592)
optimized based on ToM loss
---------------------
gamma: 0.1346760927789018
training start after waiting for 1.2126083374023438 seconds
policy loss:-180.54644775390625
value loss:11.841160774230957
entropies:26.43074607849121
Policy training finished
---------------------
gamma: 0.1346760927789018
training start after waiting for 1.109499454498291 seconds
policy loss:-189.83140563964844
value loss:21.754335403442383
entropies:34.82898712158203
Policy training finished
---------------------
gamma: 0.1346760927789018
training start after waiting for 1.2117843627929688 seconds
policy loss:-248.85458374023438
value loss:14.287188529968262
entropies:24.307262420654297
Policy training finished
---------------------
gamma: 0.1346760927789018
training start after waiting for 1.1603679656982422 seconds
policy loss:-521.2696533203125
value loss:12.557058334350586
entropies:32.56691360473633
Policy training finished
---------------------
gamma: 0.1346760927789018
training start after waiting for 1.1364266872406006 seconds
policy loss:-295.3305969238281
value loss:9.788877487182617
entropies:22.267004013061523
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1192.3781)
ToM Target loss= tensor(2402.2742)
optimized based on ToM loss
---------------------
gamma: 0.1349454449644596
training start after waiting for 1.177095651626587 seconds
policy loss:-527.2034912109375
value loss:19.044109344482422
entropies:34.59585189819336
Policy training finished
---------------------
gamma: 0.1349454449644596
training start after waiting for 1.193424940109253 seconds
policy loss:-187.59304809570312
value loss:22.64476203918457
entropies:26.155696868896484
Policy training finished
---------------------
gamma: 0.1349454449644596
training start after waiting for 1.1461467742919922 seconds
policy loss:112.49293518066406
value loss:8.31180191040039
entropies:21.823970794677734
Policy training finished
---------------------
gamma: 0.1349454449644596
training start after waiting for 1.1841235160827637 seconds
policy loss:4.312061309814453
value loss:16.761756896972656
entropies:16.917030334472656
Policy training finished
---------------------
gamma: 0.1349454449644596
training start after waiting for 1.142641544342041 seconds
policy loss:-1394.7889404296875
value loss:31.591594696044922
entropies:24.31882095336914
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1147.7950)
ToM Target loss= tensor(2291.4678)
optimized based on ToM loss
---------------------
gamma: 0.13521533585438852
training start after waiting for 1.155811071395874 seconds
policy loss:-204.45541381835938
value loss:57.62262725830078
entropies:23.548351287841797
Policy training finished
---------------------
gamma: 0.13521533585438852
training start after waiting for 1.1602368354797363 seconds
policy loss:-573.5567016601562
value loss:11.061644554138184
entropies:20.577762603759766
Policy training finished
---------------------
gamma: 0.13521533585438852
training start after waiting for 1.210212230682373 seconds
policy loss:40.09098815917969
value loss:9.346927642822266
entropies:20.354785919189453
Policy training finished
---------------------
gamma: 0.13521533585438852
training start after waiting for 1.1954808235168457 seconds
policy loss:-51.76846694946289
value loss:9.055817604064941
entropies:20.180992126464844
Policy training finished
---------------------
gamma: 0.13521533585438852
training start after waiting for 1.1867930889129639 seconds
policy loss:-869.9268798828125
value loss:25.3039608001709
entropies:38.96022033691406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1199.1106)
ToM Target loss= tensor(2396.1807)
optimized based on ToM loss
---------------------
gamma: 0.1354857665260973
training start after waiting for 1.1925153732299805 seconds
policy loss:223.17080688476562
value loss:8.815093994140625
entropies:30.724809646606445
Policy training finished
---------------------
gamma: 0.1354857665260973
training start after waiting for 1.1905157566070557 seconds
policy loss:-365.1855773925781
value loss:16.826242446899414
entropies:34.479732513427734
Policy training finished
---------------------
gamma: 0.1354857665260973
training start after waiting for 1.1521406173706055 seconds
policy loss:-1331.940185546875
value loss:31.627662658691406
entropies:34.178443908691406
Policy training finished
---------------------
gamma: 0.1354857665260973
training start after waiting for 1.1504631042480469 seconds
policy loss:-504.5787353515625
value loss:11.830090522766113
entropies:25.1712646484375
Policy training finished
---------------------
gamma: 0.1354857665260973
training start after waiting for 1.1900451183319092 seconds
policy loss:-203.65753173828125
value loss:17.470829010009766
entropies:28.855335235595703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1269.9559)
ToM Target loss= tensor(2354.3083)
optimized based on ToM loss
---------------------
gamma: 0.1357567380591495
training start after waiting for 1.197258710861206 seconds
policy loss:-81.53701782226562
value loss:17.5106201171875
entropies:31.51278305053711
Policy training finished
---------------------
gamma: 0.1357567380591495
training start after waiting for 1.1739654541015625 seconds
policy loss:-341.464599609375
value loss:13.18056583404541
entropies:21.820789337158203
Policy training finished
---------------------
gamma: 0.1357567380591495
training start after waiting for 1.2104790210723877 seconds
policy loss:250.0435791015625
value loss:6.886564254760742
entropies:15.046760559082031
Policy training finished
---------------------
gamma: 0.1357567380591495
training start after waiting for 1.1726548671722412 seconds
policy loss:-118.43892669677734
value loss:11.628678321838379
entropies:23.99895477294922
Policy training finished
---------------------
gamma: 0.1357567380591495
training start after waiting for 1.2011992931365967 seconds
policy loss:99.75284576416016
value loss:8.494306564331055
entropies:25.401447296142578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1147.3843)
ToM Target loss= tensor(2403.2615)
optimized based on ToM loss
---------------------
gamma: 0.13602825153526782
training start after waiting for 1.1510875225067139 seconds
policy loss:81.61482238769531
value loss:3.5409748554229736
entropies:15.21314811706543
Policy training finished
---------------------
gamma: 0.13602825153526782
training start after waiting for 1.1830694675445557 seconds
policy loss:-746.018310546875
value loss:31.513795852661133
entropies:37.14453887939453
Policy training finished
---------------------
gamma: 0.13602825153526782
training start after waiting for 1.2035236358642578 seconds
policy loss:-499.09979248046875
value loss:7.830801486968994
entropies:32.05181884765625
Policy training finished
---------------------
gamma: 0.13602825153526782
training start after waiting for 1.2077240943908691 seconds
policy loss:-943.1818237304688
value loss:30.015153884887695
entropies:42.34336471557617
Policy training finished
---------------------
gamma: 0.13602825153526782
training start after waiting for 1.2085742950439453 seconds
policy loss:35.66240692138672
value loss:3.670804977416992
entropies:14.502256393432617
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1154.0123)
ToM Target loss= tensor(2367.0020)
optimized based on ToM loss
---------------------
gamma: 0.13630030803833834
training start after waiting for 1.2423069477081299 seconds
policy loss:-1205.5821533203125
value loss:59.02267074584961
entropies:25.581565856933594
Policy training finished
---------------------
gamma: 0.13630030803833834
training start after waiting for 1.2126119136810303 seconds
policy loss:-17.219215393066406
value loss:11.894320487976074
entropies:13.588532447814941
Policy training finished
---------------------
gamma: 0.13630030803833834
training start after waiting for 1.215782642364502 seconds
policy loss:-89.33585357666016
value loss:12.984980583190918
entropies:30.18792724609375
Policy training finished
---------------------
gamma: 0.13630030803833834
training start after waiting for 1.1971025466918945 seconds
policy loss:37.992130279541016
value loss:9.803627014160156
entropies:29.99789047241211
Policy training finished
---------------------
gamma: 0.13630030803833834
training start after waiting for 1.2024405002593994 seconds
policy loss:55.49747085571289
value loss:9.922616004943848
entropies:27.320327758789062
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1081.4703)
ToM Target loss= tensor(2270.1145)
optimized based on ToM loss
---------------------
gamma: 0.13657290865441502
training start after waiting for 1.1404476165771484 seconds
policy loss:-73.37051391601562
value loss:16.794401168823242
entropies:25.380325317382812
Policy training finished
---------------------
gamma: 0.13657290865441502
training start after waiting for 1.1915466785430908 seconds
policy loss:-518.9345092773438
value loss:12.499357223510742
entropies:32.59510040283203
Policy training finished
---------------------
gamma: 0.13657290865441502
training start after waiting for 1.2070841789245605 seconds
policy loss:-1468.444580078125
value loss:28.363222122192383
entropies:54.691688537597656
Policy training finished
---------------------
gamma: 0.13657290865441502
training start after waiting for 1.1750576496124268 seconds
policy loss:33.679039001464844
value loss:6.5139007568359375
entropies:12.525300979614258
Policy training finished
---------------------
gamma: 0.13657290865441502
training start after waiting for 1.1453814506530762 seconds
policy loss:-234.333740234375
value loss:16.69766616821289
entropies:24.213802337646484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1168.2214)
ToM Target loss= tensor(2308.6453)
optimized based on ToM loss
---------------------
gamma: 0.13684605447172385
training start after waiting for 1.1697783470153809 seconds
policy loss:-437.5673522949219
value loss:18.613018035888672
entropies:34.6103515625
Policy training finished
---------------------
gamma: 0.13684605447172385
training start after waiting for 1.178391456604004 seconds
policy loss:-45.37401580810547
value loss:15.538962364196777
entropies:24.565671920776367
Policy training finished
---------------------
gamma: 0.13684605447172385
training start after waiting for 1.1773629188537598 seconds
policy loss:-276.84478759765625
value loss:4.5582709312438965
entropies:23.43490219116211
Policy training finished
---------------------
gamma: 0.13684605447172385
training start after waiting for 1.1582529544830322 seconds
policy loss:-129.73182678222656
value loss:11.346421241760254
entropies:26.226943969726562
Policy training finished
---------------------
gamma: 0.13684605447172385
training start after waiting for 1.1841692924499512 seconds
policy loss:-952.5311889648438
value loss:37.11406326293945
entropies:31.20318031311035
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1180.9744)
ToM Target loss= tensor(2294.1250)
optimized based on ToM loss
---------------------
gamma: 0.1371197465806673
training start after waiting for 1.2304375171661377 seconds
policy loss:154.9583282470703
value loss:2.309234619140625
entropies:12.271072387695312
Policy training finished
---------------------
gamma: 0.1371197465806673
training start after waiting for 1.203395128250122 seconds
policy loss:-58.57107162475586
value loss:20.716245651245117
entropies:20.541580200195312
Policy training finished
---------------------
gamma: 0.1371197465806673
training start after waiting for 1.1514487266540527 seconds
policy loss:-29.03174591064453
value loss:6.6722917556762695
entropies:18.159950256347656
Policy training finished
---------------------
gamma: 0.1371197465806673
training start after waiting for 1.14393949508667 seconds
policy loss:-1287.3746337890625
value loss:11.840436935424805
entropies:30.54961395263672
Policy training finished
---------------------
gamma: 0.1371197465806673
training start after waiting for 1.182847261428833 seconds
policy loss:-583.080078125
value loss:21.194679260253906
entropies:38.187679290771484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1153.4556)
ToM Target loss= tensor(2349.0063)
optimized based on ToM loss
---------------------
gamma: 0.13739398607382863
training start after waiting for 1.148402214050293 seconds
policy loss:-370.6106262207031
value loss:11.482667922973633
entropies:22.881717681884766
Policy training finished
---------------------
gamma: 0.13739398607382863
training start after waiting for 1.1455800533294678 seconds
policy loss:282.3009338378906
value loss:8.28676986694336
entropies:18.347280502319336
Policy training finished
---------------------
gamma: 0.13739398607382863
training start after waiting for 1.167529582977295 seconds
policy loss:-968.0618896484375
value loss:21.096067428588867
entropies:39.168704986572266
Policy training finished
---------------------
gamma: 0.13739398607382863
training start after waiting for 1.1858487129211426 seconds
policy loss:-733.0020751953125
value loss:7.669127464294434
entropies:22.49079704284668
Policy training finished
---------------------
gamma: 0.13739398607382863
training start after waiting for 1.1994967460632324 seconds
policy loss:-523.6739501953125
value loss:23.400959014892578
entropies:30.053653717041016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1113.9174)
ToM Target loss= tensor(2236.5737)
optimized based on ToM loss
---------------------
gamma: 0.1376687740459763
training start after waiting for 1.1437177658081055 seconds
policy loss:46.27619552612305
value loss:9.274847984313965
entropies:18.00884437561035
Policy training finished
---------------------
gamma: 0.1376687740459763
training start after waiting for 1.150381088256836 seconds
policy loss:-184.36619567871094
value loss:15.953889846801758
entropies:32.689056396484375
Policy training finished
---------------------
gamma: 0.1376687740459763
training start after waiting for 1.1463708877563477 seconds
policy loss:-170.39474487304688
value loss:8.480629920959473
entropies:25.94454574584961
Policy training finished
---------------------
gamma: 0.1376687740459763
training start after waiting for 1.1770858764648438 seconds
policy loss:-98.18304443359375
value loss:4.03961181640625
entropies:18.951255798339844
Policy training finished
---------------------
gamma: 0.1376687740459763
training start after waiting for 1.1415901184082031 seconds
policy loss:-462.9435729980469
value loss:7.813977241516113
entropies:25.509422302246094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1078.1724)
ToM Target loss= tensor(2250.1326)
optimized based on ToM loss
---------------------
gamma: 0.13794411159406825
training start after waiting for 1.194150447845459 seconds
policy loss:-614.7192993164062
value loss:12.551238059997559
entropies:16.142671585083008
Policy training finished
---------------------
gamma: 0.13794411159406825
training start after waiting for 1.1922883987426758 seconds
policy loss:-48.241886138916016
value loss:6.128750801086426
entropies:11.202667236328125
Policy training finished
---------------------
gamma: 0.13794411159406825
training start after waiting for 1.1424486637115479 seconds
policy loss:-1110.84423828125
value loss:20.006864547729492
entropies:36.43855285644531
Policy training finished
---------------------
gamma: 0.13794411159406825
training start after waiting for 1.144376516342163 seconds
policy loss:-43.458683013916016
value loss:9.523386001586914
entropies:21.113494873046875
Policy training finished
---------------------
gamma: 0.13794411159406825
training start after waiting for 1.143240213394165 seconds
policy loss:-432.4883728027344
value loss:12.640290260314941
entropies:17.68712043762207
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1160.4419)
ToM Target loss= tensor(2408.8379)
optimized based on ToM loss
---------------------
gamma: 0.13821999981725638
training start after waiting for 1.212766170501709 seconds
policy loss:-669.3429565429688
value loss:26.945411682128906
entropies:30.687904357910156
Policy training finished
---------------------
gamma: 0.13821999981725638
training start after waiting for 1.1446261405944824 seconds
policy loss:-0.5247688293457031
value loss:10.90876293182373
entropies:18.17552947998047
Policy training finished
---------------------
gamma: 0.13821999981725638
training start after waiting for 1.1478757858276367 seconds
policy loss:-43.70724105834961
value loss:10.517683029174805
entropies:17.60562515258789
Policy training finished
---------------------
gamma: 0.13821999981725638
training start after waiting for 1.1706068515777588 seconds
policy loss:-164.87493896484375
value loss:14.67821979522705
entropies:22.931495666503906
Policy training finished
---------------------
gamma: 0.13821999981725638
training start after waiting for 1.146819829940796 seconds
policy loss:-241.7637939453125
value loss:9.739489555358887
entropies:30.93193817138672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1231.1083)
ToM Target loss= tensor(2413.5554)
optimized based on ToM loss
---------------------
gamma: 0.1384964398168909
training start after waiting for 1.2043137550354004 seconds
policy loss:-624.489501953125
value loss:15.829874038696289
entropies:27.23137855529785
Policy training finished
---------------------
gamma: 0.1384964398168909
training start after waiting for 1.2084870338439941 seconds
policy loss:-627.0094604492188
value loss:16.504638671875
entropies:32.38554382324219
Policy training finished
---------------------
gamma: 0.1384964398168909
training start after waiting for 1.2175989151000977 seconds
policy loss:-961.0587768554688
value loss:28.2816104888916
entropies:37.172142028808594
Policy training finished
---------------------
gamma: 0.1384964398168909
training start after waiting for 1.1408421993255615 seconds
policy loss:-251.77462768554688
value loss:20.271896362304688
entropies:23.778324127197266
Policy training finished
---------------------
gamma: 0.1384964398168909
training start after waiting for 1.1815705299377441 seconds
policy loss:-447.3594970703125
value loss:13.256677627563477
entropies:32.71665954589844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1199.0413)
ToM Target loss= tensor(2297.4236)
optimized based on ToM loss
---------------------
gamma: 0.13877343269652467
training start after waiting for 1.1973927021026611 seconds
policy loss:-61.388343811035156
value loss:16.354503631591797
entropies:35.316322326660156
Policy training finished
---------------------
gamma: 0.13877343269652467
training start after waiting for 1.137533187866211 seconds
policy loss:-225.71774291992188
value loss:13.56942367553711
entropies:29.650409698486328
Policy training finished
---------------------
gamma: 0.13877343269652467
training start after waiting for 1.2025747299194336 seconds
policy loss:-449.6495666503906
value loss:14.01264762878418
entropies:34.672950744628906
Policy training finished
---------------------
gamma: 0.13877343269652467
training start after waiting for 1.2096052169799805 seconds
policy loss:39.423179626464844
value loss:9.859310150146484
entropies:32.7252311706543
Policy training finished
---------------------
gamma: 0.13877343269652467
training start after waiting for 1.1743643283843994 seconds
policy loss:75.33338165283203
value loss:10.68559455871582
entropies:29.904949188232422
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1194.8893)
ToM Target loss= tensor(2305.6306)
optimized based on ToM loss
---------------------
gamma: 0.13905097956191773
training start after waiting for 1.2584350109100342 seconds
policy loss:-996.1375732421875
value loss:25.980640411376953
entropies:54.34500503540039
Policy training finished
---------------------
gamma: 0.13905097956191773
training start after waiting for 1.2088711261749268 seconds
policy loss:-1102.33349609375
value loss:19.675321578979492
entropies:30.501882553100586
Policy training finished
---------------------
gamma: 0.13905097956191773
training start after waiting for 1.1531665325164795 seconds
policy loss:-2899.015380859375
value loss:45.359375
entropies:62.46648406982422
Policy training finished
---------------------
gamma: 0.13905097956191773
training start after waiting for 1.1998262405395508 seconds
policy loss:-529.0386352539062
value loss:16.578594207763672
entropies:24.452640533447266
Policy training finished
---------------------
gamma: 0.13905097956191773
training start after waiting for 1.1634917259216309 seconds
policy loss:-847.6737060546875
value loss:25.005794525146484
entropies:34.50788879394531
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1329.9734)
ToM Target loss= tensor(2351.7808)
optimized based on ToM loss
---------------------
gamma: 0.13932908152104156
training start after waiting for 1.1976211071014404 seconds
policy loss:-218.61807250976562
value loss:11.216567039489746
entropies:28.01671028137207
Policy training finished
---------------------
gamma: 0.13932908152104156
training start after waiting for 1.1762843132019043 seconds
policy loss:-893.9596557617188
value loss:26.881732940673828
entropies:46.93613052368164
Policy training finished
---------------------
gamma: 0.13932908152104156
training start after waiting for 1.211686611175537 seconds
policy loss:194.8255157470703
value loss:14.66211223602295
entropies:26.138290405273438
Policy training finished
---------------------
gamma: 0.13932908152104156
training start after waiting for 1.1660077571868896 seconds
policy loss:5.657651424407959
value loss:23.614871978759766
entropies:30.383808135986328
Policy training finished
---------------------
gamma: 0.13932908152104156
training start after waiting for 1.137932300567627 seconds
policy loss:-632.6470947265625
value loss:10.605724334716797
entropies:20.308670043945312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1150.0031)
ToM Target loss= tensor(2315.1545)
optimized based on ToM loss
---------------------
gamma: 0.13960773968408366
training start after waiting for 1.1980273723602295 seconds
policy loss:-1458.1187744140625
value loss:21.511638641357422
entropies:31.373035430908203
Policy training finished
---------------------
gamma: 0.13960773968408366
training start after waiting for 1.1505534648895264 seconds
policy loss:-803.0193481445312
value loss:10.760024070739746
entropies:28.616456985473633
Policy training finished
---------------------
gamma: 0.13960773968408366
training start after waiting for 1.1872363090515137 seconds
policy loss:-624.0519409179688
value loss:14.223226547241211
entropies:44.45050048828125
Policy training finished
---------------------
gamma: 0.13960773968408366
training start after waiting for 1.152165174484253 seconds
policy loss:228.36663818359375
value loss:8.08066177368164
entropies:27.612674713134766
Policy training finished
---------------------
gamma: 0.13960773968408366
training start after waiting for 1.2081499099731445 seconds
policy loss:-192.70745849609375
value loss:16.60163116455078
entropies:31.967388153076172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1201.5876)
ToM Target loss= tensor(2271.8464)
optimized based on ToM loss
---------------------
gamma: 0.13988695516345181
training start after waiting for 1.1432862281799316 seconds
policy loss:-109.53267669677734
value loss:8.804770469665527
entropies:25.328086853027344
Policy training finished
---------------------
gamma: 0.13988695516345181
training start after waiting for 1.2043979167938232 seconds
policy loss:216.0037078857422
value loss:5.201809883117676
entropies:16.834148406982422
Policy training finished
---------------------
gamma: 0.13988695516345181
training start after waiting for 1.1480932235717773 seconds
policy loss:-146.97964477539062
value loss:27.771015167236328
entropies:39.47197723388672
Policy training finished
---------------------
gamma: 0.13988695516345181
training start after waiting for 1.1503655910491943 seconds
policy loss:-366.97503662109375
value loss:22.338775634765625
entropies:25.90612030029297
Policy training finished
---------------------
gamma: 0.13988695516345181
training start after waiting for 1.2023224830627441 seconds
policy loss:54.20677947998047
value loss:15.858758926391602
entropies:29.039342880249023
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1273.2268)
ToM Target loss= tensor(2329.0801)
optimized based on ToM loss
---------------------
gamma: 0.14016672907377872
training start after waiting for 1.1423919200897217 seconds
policy loss:-278.6363220214844
value loss:6.363499641418457
entropies:30.621658325195312
Policy training finished
---------------------
gamma: 0.14016672907377872
training start after waiting for 1.1800971031188965 seconds
policy loss:-327.6106262207031
value loss:4.95041561126709
entropies:22.798320770263672
Policy training finished
---------------------
gamma: 0.14016672907377872
training start after waiting for 1.1882867813110352 seconds
policy loss:-158.1094512939453
value loss:7.904202461242676
entropies:31.791194915771484
Policy training finished
---------------------
gamma: 0.14016672907377872
training start after waiting for 1.1499848365783691 seconds
policy loss:-146.312744140625
value loss:2.8034064769744873
entropies:20.22287368774414
Policy training finished
---------------------
gamma: 0.14016672907377872
training start after waiting for 1.1489026546478271 seconds
policy loss:-292.1214904785156
value loss:6.440345764160156
entropies:22.85033416748047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1157.2633)
ToM Target loss= tensor(2366.4773)
optimized based on ToM loss
---------------------
gamma: 0.1404470625319263
training start after waiting for 1.1520576477050781 seconds
policy loss:-728.6979370117188
value loss:14.174310684204102
entropies:31.059972763061523
Policy training finished
---------------------
gamma: 0.1404470625319263
training start after waiting for 1.1973648071289062 seconds
policy loss:-55.65793991088867
value loss:7.8661274909973145
entropies:29.99829864501953
Policy training finished
---------------------
gamma: 0.1404470625319263
training start after waiting for 1.145951271057129 seconds
policy loss:-1178.6287841796875
value loss:42.117740631103516
entropies:33.982906341552734
Policy training finished
---------------------
gamma: 0.1404470625319263
training start after waiting for 1.1895732879638672 seconds
policy loss:234.94992065429688
value loss:7.055860996246338
entropies:24.399429321289062
Policy training finished
---------------------
gamma: 0.1404470625319263
training start after waiting for 1.193638563156128 seconds
policy loss:-90.13835144042969
value loss:10.889320373535156
entropies:20.80865478515625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1178.1067)
ToM Target loss= tensor(2367.6362)
optimized based on ToM loss
---------------------
gamma: 0.14072795665699014
training start after waiting for 1.1489098072052002 seconds
policy loss:-221.54641723632812
value loss:16.875476837158203
entropies:30.623512268066406
Policy training finished
---------------------
gamma: 0.14072795665699014
training start after waiting for 1.2107150554656982 seconds
policy loss:-854.6607666015625
value loss:20.302257537841797
entropies:30.90027618408203
Policy training finished
---------------------
gamma: 0.14072795665699014
training start after waiting for 1.173933744430542 seconds
policy loss:-736.3460083007812
value loss:24.18191146850586
entropies:39.18986511230469
Policy training finished
---------------------
gamma: 0.14072795665699014
training start after waiting for 1.1790223121643066 seconds
policy loss:-1039.580810546875
value loss:47.474544525146484
entropies:31.314180374145508
Policy training finished
---------------------
gamma: 0.14072795665699014
training start after waiting for 1.1654903888702393 seconds
policy loss:-160.83201599121094
value loss:7.446237564086914
entropies:22.730724334716797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1294.6130)
ToM Target loss= tensor(2347.2400)
optimized based on ToM loss
---------------------
gamma: 0.14100941257030414
training start after waiting for 1.1570477485656738 seconds
policy loss:-697.5498046875
value loss:39.22412109375
entropies:32.42940902709961
Policy training finished
---------------------
gamma: 0.14100941257030414
training start after waiting for 1.1517770290374756 seconds
policy loss:-1297.1710205078125
value loss:22.620132446289062
entropies:41.117733001708984
Policy training finished
---------------------
gamma: 0.14100941257030414
training start after waiting for 1.1515822410583496 seconds
policy loss:-380.4952392578125
value loss:23.842863082885742
entropies:31.325599670410156
Policy training finished
---------------------
gamma: 0.14100941257030414
training start after waiting for 1.1917786598205566 seconds
policy loss:-213.5199432373047
value loss:10.42641830444336
entropies:28.338003158569336
Policy training finished
---------------------
gamma: 0.14100941257030414
training start after waiting for 1.1422555446624756 seconds
policy loss:-128.33499145507812
value loss:9.27778434753418
entropies:25.759601593017578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1277.8308)
ToM Target loss= tensor(2371.4036)
optimized based on ToM loss
---------------------
gamma: 0.14129143139544476
training start after waiting for 1.1525764465332031 seconds
policy loss:-1240.4296875
value loss:33.96597671508789
entropies:47.49848937988281
Policy training finished
---------------------
gamma: 0.14129143139544476
training start after waiting for 1.191009283065796 seconds
policy loss:-428.8175964355469
value loss:12.687356948852539
entropies:32.518638610839844
Policy training finished
---------------------
gamma: 0.14129143139544476
training start after waiting for 1.1834888458251953 seconds
policy loss:-434.97259521484375
value loss:16.258487701416016
entropies:27.749208450317383
Policy training finished
---------------------
gamma: 0.14129143139544476
training start after waiting for 1.2034711837768555 seconds
policy loss:-1121.322265625
value loss:23.539579391479492
entropies:55.989219665527344
Policy training finished
---------------------
gamma: 0.14129143139544476
training start after waiting for 1.2133233547210693 seconds
policy loss:-1715.6602783203125
value loss:39.23981475830078
entropies:51.193572998046875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1340.9631)
ToM Target loss= tensor(2386.0493)
optimized based on ToM loss
---------------------
gamma: 0.14157401425823565
training start after waiting for 1.1587226390838623 seconds
policy loss:-629.883544921875
value loss:30.276872634887695
entropies:30.96902847290039
Policy training finished
---------------------
gamma: 0.14157401425823565
training start after waiting for 1.1288139820098877 seconds
policy loss:369.01727294921875
value loss:13.748939514160156
entropies:34.190799713134766
Policy training finished
---------------------
gamma: 0.14157401425823565
training start after waiting for 1.1517224311828613 seconds
policy loss:-1004.041015625
value loss:22.95070457458496
entropies:45.08203125
Policy training finished
---------------------
gamma: 0.14157401425823565
training start after waiting for 1.193962574005127 seconds
policy loss:-892.9715576171875
value loss:17.243267059326172
entropies:41.517051696777344
Policy training finished
---------------------
gamma: 0.14157401425823565
training start after waiting for 1.1740567684173584 seconds
policy loss:-243.7451171875
value loss:15.748844146728516
entropies:26.759357452392578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1281.6189)
ToM Target loss= tensor(2383.2751)
optimized based on ToM loss
---------------------
gamma: 0.14185716228675213
training start after waiting for 1.1791536808013916 seconds
policy loss:-981.8026123046875
value loss:23.17383575439453
entropies:52.20590591430664
Policy training finished
---------------------
gamma: 0.14185716228675213
training start after waiting for 1.1418569087982178 seconds
policy loss:-1710.8106689453125
value loss:39.9136848449707
entropies:36.565574645996094
Policy training finished
---------------------
gamma: 0.14185716228675213
training start after waiting for 1.20158052444458 seconds
policy loss:-239.33071899414062
value loss:14.812736511230469
entropies:31.234737396240234
Policy training finished
---------------------
gamma: 0.14185716228675213
training start after waiting for 1.1838452816009521 seconds
policy loss:-234.9622802734375
value loss:12.599355697631836
entropies:26.941539764404297
Policy training finished
---------------------
gamma: 0.14185716228675213
training start after waiting for 1.1973645687103271 seconds
policy loss:-269.68994140625
value loss:11.544806480407715
entropies:26.566198348999023
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1357.9270)
ToM Target loss= tensor(2417.0823)
optimized based on ToM loss
---------------------
gamma: 0.14214087661132563
training start after waiting for 1.2007696628570557 seconds
policy loss:-128.95840454101562
value loss:14.00214958190918
entropies:25.248167037963867
Policy training finished
---------------------
gamma: 0.14214087661132563
training start after waiting for 1.2066214084625244 seconds
policy loss:393.3689880371094
value loss:5.882791996002197
entropies:20.48348617553711
Policy training finished
---------------------
gamma: 0.14214087661132563
training start after waiting for 1.1697266101837158 seconds
policy loss:-881.446044921875
value loss:21.665363311767578
entropies:35.58039855957031
Policy training finished
---------------------
gamma: 0.14214087661132563
training start after waiting for 1.1825401782989502 seconds
policy loss:286.69232177734375
value loss:6.860353469848633
entropies:23.961467742919922
Policy training finished
---------------------
gamma: 0.14214087661132563
training start after waiting for 1.1362907886505127 seconds
policy loss:-901.0569458007812
value loss:12.516251564025879
entropies:31.062225341796875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1218.1300)
ToM Target loss= tensor(2507.0596)
optimized based on ToM loss
---------------------
gamma: 0.14242515836454828
training start after waiting for 1.1558210849761963 seconds
policy loss:-192.8907928466797
value loss:10.141843795776367
entropies:22.688858032226562
Policy training finished
---------------------
gamma: 0.14242515836454828
training start after waiting for 1.1981847286224365 seconds
policy loss:-2568.3115234375
value loss:51.996341705322266
entropies:51.93334197998047
Policy training finished
---------------------
gamma: 0.14242515836454828
training start after waiting for 1.17872953414917 seconds
policy loss:-655.9639282226562
value loss:20.307947158813477
entropies:33.88502502441406
Policy training finished
---------------------
gamma: 0.14242515836454828
training start after waiting for 1.1896007061004639 seconds
policy loss:152.9543914794922
value loss:13.941021919250488
entropies:26.656421661376953
Policy training finished
---------------------
gamma: 0.14242515836454828
training start after waiting for 1.1709105968475342 seconds
policy loss:-186.2109375
value loss:16.79599952697754
entropies:32.92465591430664
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1252.1069)
ToM Target loss= tensor(2564.2466)
optimized based on ToM loss
---------------------
gamma: 0.14271000868127737
training start after waiting for 1.2017836570739746 seconds
policy loss:-338.15313720703125
value loss:26.759233474731445
entropies:39.24761962890625
Policy training finished
---------------------
gamma: 0.14271000868127737
training start after waiting for 1.1670184135437012 seconds
policy loss:-520.5118408203125
value loss:12.333316802978516
entropies:29.717212677001953
Policy training finished
---------------------
gamma: 0.14271000868127737
training start after waiting for 1.1602983474731445 seconds
policy loss:-56.57842254638672
value loss:12.53036880493164
entropies:46.774566650390625
Policy training finished
---------------------
gamma: 0.14271000868127737
training start after waiting for 1.2096867561340332 seconds
policy loss:-42.72540283203125
value loss:10.07536506652832
entropies:27.20657730102539
Policy training finished
---------------------
gamma: 0.14271000868127737
training start after waiting for 1.2007689476013184 seconds
policy loss:-368.0156555175781
value loss:15.296005249023438
entropies:32.82392120361328
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1269.3647)
ToM Target loss= tensor(2425.6057)
optimized based on ToM loss
---------------------
gamma: 0.14299542869863993
training start after waiting for 1.2317988872528076 seconds
policy loss:-654.5621948242188
value loss:19.198083877563477
entropies:41.8563117980957
Policy training finished
---------------------
gamma: 0.14299542869863993
training start after waiting for 1.207141637802124 seconds
policy loss:-187.1776580810547
value loss:9.05013370513916
entropies:29.90033531188965
Policy training finished
---------------------
gamma: 0.14299542869863993
training start after waiting for 1.1685497760772705 seconds
policy loss:-184.37020874023438
value loss:20.49764060974121
entropies:31.13075828552246
Policy training finished
---------------------
gamma: 0.14299542869863993
training start after waiting for 1.1875605583190918 seconds
policy loss:143.83535766601562
value loss:13.058601379394531
entropies:25.68478012084961
Policy training finished
---------------------
gamma: 0.14299542869863993
training start after waiting for 1.2040364742279053 seconds
policy loss:381.86212158203125
value loss:11.684402465820312
entropies:16.631608963012695
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1202.7911)
ToM Target loss= tensor(2310.5811)
optimized based on ToM loss
---------------------
gamma: 0.1432814195560372
training start after waiting for 1.1938166618347168 seconds
policy loss:204.557861328125
value loss:14.539958000183105
entropies:25.869365692138672
Policy training finished
---------------------
gamma: 0.1432814195560372
training start after waiting for 1.1902098655700684 seconds
policy loss:21.103395462036133
value loss:13.141510963439941
entropies:21.493202209472656
Policy training finished
---------------------
gamma: 0.1432814195560372
training start after waiting for 1.1444487571716309 seconds
policy loss:-368.6536865234375
value loss:14.38262939453125
entropies:32.6467170715332
Policy training finished
---------------------
gamma: 0.1432814195560372
training start after waiting for 1.1979506015777588 seconds
policy loss:-603.9424438476562
value loss:17.897056579589844
entropies:32.795440673828125
Policy training finished
---------------------
gamma: 0.1432814195560372
training start after waiting for 1.1802868843078613 seconds
policy loss:309.1575012207031
value loss:11.121807098388672
entropies:14.048206329345703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1084.4486)
ToM Target loss= tensor(2413.5215)
optimized based on ToM loss
---------------------
gamma: 0.14356798239514929
training start after waiting for 1.1815271377563477 seconds
policy loss:-744.6170654296875
value loss:17.384868621826172
entropies:36.413543701171875
Policy training finished
---------------------
gamma: 0.14356798239514929
training start after waiting for 1.1905159950256348 seconds
policy loss:-88.84475708007812
value loss:14.05276107788086
entropies:19.84744644165039
Policy training finished
---------------------
gamma: 0.14356798239514929
training start after waiting for 1.1429743766784668 seconds
policy loss:-908.95849609375
value loss:25.403379440307617
entropies:37.907264709472656
Policy training finished
---------------------
gamma: 0.14356798239514929
training start after waiting for 1.167203664779663 seconds
policy loss:-46.20741271972656
value loss:15.66375732421875
entropies:28.61378288269043
Policy training finished
---------------------
gamma: 0.14356798239514929
training start after waiting for 1.187878131866455 seconds
policy loss:-355.57391357421875
value loss:20.48815155029297
entropies:30.060222625732422
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1191.7327)
ToM Target loss= tensor(2421.3735)
optimized based on ToM loss
---------------------
gamma: 0.14385511835993958
training start after waiting for 1.2093441486358643 seconds
policy loss:-449.54901123046875
value loss:67.11321258544922
entropies:28.614238739013672
Policy training finished
---------------------
gamma: 0.14385511835993958
training start after waiting for 1.1449060440063477 seconds
policy loss:-1075.7421875
value loss:45.425899505615234
entropies:43.33106231689453
Policy training finished
---------------------
gamma: 0.14385511835993958
training start after waiting for 1.1915130615234375 seconds
policy loss:-341.1337585449219
value loss:27.298389434814453
entropies:37.07951354980469
Policy training finished
---------------------
gamma: 0.14385511835993958
training start after waiting for 1.1742188930511475 seconds
policy loss:-57.718421936035156
value loss:12.580883026123047
entropies:20.038188934326172
Policy training finished
---------------------
gamma: 0.14385511835993958
training start after waiting for 1.1590111255645752 seconds
policy loss:-622.5771484375
value loss:22.023807525634766
entropies:39.105224609375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1342.1260)
ToM Target loss= tensor(2565.9617)
optimized based on ToM loss
---------------------
gamma: 0.14414282859665947
training start after waiting for 1.2049412727355957 seconds
policy loss:63.115936279296875
value loss:10.291040420532227
entropies:27.641902923583984
Policy training finished
---------------------
gamma: 0.14414282859665947
training start after waiting for 1.1746819019317627 seconds
policy loss:-1995.2987060546875
value loss:32.4832649230957
entropies:52.15096664428711
Policy training finished
---------------------
gamma: 0.14414282859665947
training start after waiting for 1.1883008480072021 seconds
policy loss:-309.6188049316406
value loss:9.232749938964844
entropies:15.62374210357666
Policy training finished
---------------------
gamma: 0.14414282859665947
training start after waiting for 1.1405494213104248 seconds
policy loss:-315.89501953125
value loss:6.2791924476623535
entropies:29.843494415283203
Policy training finished
---------------------
gamma: 0.14414282859665947
training start after waiting for 1.2179136276245117 seconds
policy loss:-1428.525390625
value loss:17.84968376159668
entropies:36.022674560546875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1276.1747)
ToM Target loss= tensor(2593.8875)
optimized based on ToM loss
---------------------
gamma: 0.1444311142538528
training start after waiting for 1.190659761428833 seconds
policy loss:-143.6918487548828
value loss:32.37211608886719
entropies:50.83085632324219
Policy training finished
---------------------
gamma: 0.1444311142538528
training start after waiting for 1.164377212524414 seconds
policy loss:-966.527099609375
value loss:24.276853561401367
entropies:42.92693328857422
Policy training finished
---------------------
gamma: 0.1444311142538528
training start after waiting for 1.180311679840088 seconds
policy loss:-425.9009704589844
value loss:20.4639949798584
entropies:31.13892364501953
Policy training finished
---------------------
gamma: 0.1444311142538528
training start after waiting for 1.1510522365570068 seconds
policy loss:-809.6304321289062
value loss:18.22324562072754
entropies:40.97554397583008
Policy training finished
---------------------
gamma: 0.1444311142538528
training start after waiting for 1.181091547012329 seconds
policy loss:-541.935546875
value loss:17.176124572753906
entropies:35.29463195800781
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1255.1467)
ToM Target loss= tensor(2462.2585)
optimized based on ToM loss
---------------------
gamma: 0.1447199764823605
training start after waiting for 1.1990063190460205 seconds
policy loss:-259.1092834472656
value loss:19.395122528076172
entropies:43.697792053222656
Policy training finished
---------------------
gamma: 0.1447199764823605
training start after waiting for 1.1913609504699707 seconds
policy loss:-372.31195068359375
value loss:9.214642524719238
entropies:23.196508407592773
Policy training finished
---------------------
gamma: 0.1447199764823605
training start after waiting for 1.184746503829956 seconds
policy loss:-76.68724822998047
value loss:7.788370609283447
entropies:21.4437313079834
Policy training finished
---------------------
gamma: 0.1447199764823605
training start after waiting for 1.186112880706787 seconds
policy loss:-516.989501953125
value loss:10.311678886413574
entropies:29.620712280273438
Policy training finished
---------------------
gamma: 0.1447199764823605
training start after waiting for 1.1822309494018555 seconds
policy loss:-367.3232421875
value loss:9.50705337524414
entropies:25.115205764770508
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1178.8235)
ToM Target loss= tensor(2433.6531)
optimized based on ToM loss
---------------------
gamma: 0.14500941643532522
training start after waiting for 1.1434056758880615 seconds
policy loss:-4.486084938049316
value loss:5.3624114990234375
entropies:17.19963836669922
Policy training finished
---------------------
gamma: 0.14500941643532522
training start after waiting for 1.1663715839385986 seconds
policy loss:-42.07966232299805
value loss:10.382359504699707
entropies:37.64225769042969
Policy training finished
---------------------
gamma: 0.14500941643532522
training start after waiting for 1.1958775520324707 seconds
policy loss:-499.8377990722656
value loss:10.837406158447266
entropies:27.591617584228516
Policy training finished
---------------------
gamma: 0.14500941643532522
training start after waiting for 1.2139360904693604 seconds
policy loss:-737.9046020507812
value loss:16.168506622314453
entropies:37.6094970703125
Policy training finished
---------------------
gamma: 0.14500941643532522
training start after waiting for 1.1825196743011475 seconds
policy loss:-305.187744140625
value loss:10.572355270385742
entropies:43.03644561767578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1242.5475)
ToM Target loss= tensor(2413.9753)
optimized based on ToM loss
---------------------
gamma: 0.1452994352681959
training start after waiting for 1.1788287162780762 seconds
policy loss:-268.3863525390625
value loss:14.767152786254883
entropies:32.40912628173828
Policy training finished
---------------------
gamma: 0.1452994352681959
training start after waiting for 1.1953768730163574 seconds
policy loss:-345.9039611816406
value loss:21.51858901977539
entropies:30.781360626220703
Policy training finished
---------------------
gamma: 0.1452994352681959
training start after waiting for 1.1837780475616455 seconds
policy loss:167.52996826171875
value loss:21.045528411865234
entropies:43.19013977050781
Policy training finished
---------------------
gamma: 0.1452994352681959
training start after waiting for 1.1499085426330566 seconds
policy loss:183.19898986816406
value loss:8.077573776245117
entropies:32.141357421875
Policy training finished
---------------------
gamma: 0.1452994352681959
training start after waiting for 1.1847014427185059 seconds
policy loss:-662.0380859375
value loss:23.674264907836914
entropies:30.605369567871094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1271.1180)
ToM Target loss= tensor(2417.9639)
optimized based on ToM loss
---------------------
gamma: 0.14559003413873228
training start after waiting for 1.2035701274871826 seconds
policy loss:-801.0753784179688
value loss:17.52387237548828
entropies:38.65058135986328
Policy training finished
---------------------
gamma: 0.14559003413873228
training start after waiting for 1.1854896545410156 seconds
policy loss:-1155.1571044921875
value loss:16.915884017944336
entropies:31.349349975585938
Policy training finished
---------------------
gamma: 0.14559003413873228
training start after waiting for 1.2079968452453613 seconds
policy loss:-495.23919677734375
value loss:13.886528015136719
entropies:36.771324157714844
Policy training finished
---------------------
gamma: 0.14559003413873228
training start after waiting for 1.1385669708251953 seconds
policy loss:-6.607178211212158
value loss:10.769670486450195
entropies:21.50473403930664
Policy training finished
---------------------
gamma: 0.14559003413873228
training start after waiting for 1.196333408355713 seconds
policy loss:-943.2103881835938
value loss:19.66261100769043
entropies:28.01428985595703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1132.8269)
ToM Target loss= tensor(2390.2117)
optimized based on ToM loss
---------------------
gamma: 0.14588121420700975
training start after waiting for 1.1913244724273682 seconds
policy loss:164.4716339111328
value loss:18.902135848999023
entropies:38.597537994384766
Policy training finished
---------------------
gamma: 0.14588121420700975
training start after waiting for 1.230424165725708 seconds
policy loss:52.684349060058594
value loss:13.705451011657715
entropies:34.8653678894043
Policy training finished
---------------------
gamma: 0.14588121420700975
training start after waiting for 1.191373348236084 seconds
policy loss:239.13992309570312
value loss:6.060235023498535
entropies:11.379903793334961
Policy training finished
---------------------
gamma: 0.14588121420700975
training start after waiting for 1.1810812950134277 seconds
policy loss:-490.8568420410156
value loss:13.853630065917969
entropies:35.90211868286133
Policy training finished
---------------------
gamma: 0.14588121420700975
training start after waiting for 1.174478530883789 seconds
policy loss:-1655.7010498046875
value loss:35.77876663208008
entropies:47.997249603271484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1222.7703)
ToM Target loss= tensor(2377.8787)
optimized based on ToM loss
---------------------
gamma: 0.14617297663542378
training start after waiting for 1.181758165359497 seconds
policy loss:-506.131103515625
value loss:17.050777435302734
entropies:37.29893112182617
Policy training finished
---------------------
gamma: 0.14617297663542378
training start after waiting for 1.2141966819763184 seconds
policy loss:-214.21139526367188
value loss:7.193333625793457
entropies:18.2711181640625
Policy training finished
---------------------
gamma: 0.14617297663542378
training start after waiting for 1.2052607536315918 seconds
policy loss:-289.3920593261719
value loss:12.949020385742188
entropies:18.33672523498535
Policy training finished
---------------------
gamma: 0.14617297663542378
training start after waiting for 1.1498188972473145 seconds
policy loss:-111.3298110961914
value loss:11.435870170593262
entropies:25.517494201660156
Policy training finished
---------------------
gamma: 0.14617297663542378
training start after waiting for 1.2003822326660156 seconds
policy loss:-260.70733642578125
value loss:14.415034294128418
entropies:27.07809829711914
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1209.6840)
ToM Target loss= tensor(2431.2336)
optimized based on ToM loss
---------------------
gamma: 0.14646532258869463
training start after waiting for 1.1449053287506104 seconds
policy loss:44.59000778198242
value loss:8.417861938476562
entropies:20.154314041137695
Policy training finished
---------------------
gamma: 0.14646532258869463
training start after waiting for 1.1786057949066162 seconds
policy loss:-670.4302978515625
value loss:13.398990631103516
entropies:26.03586769104004
Policy training finished
---------------------
gamma: 0.14646532258869463
training start after waiting for 1.1875438690185547 seconds
policy loss:-734.5430908203125
value loss:7.365802764892578
entropies:11.192916870117188
Policy training finished
---------------------
gamma: 0.14646532258869463
training start after waiting for 1.200685977935791 seconds
policy loss:-115.62230682373047
value loss:6.389713764190674
entropies:18.20149040222168
Policy training finished
---------------------
gamma: 0.14646532258869463
training start after waiting for 1.183121919631958 seconds
policy loss:-1016.3759765625
value loss:38.18536376953125
entropies:25.36553955078125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1250.9226)
ToM Target loss= tensor(2506.4185)
optimized based on ToM loss
---------------------
gamma: 0.146758253233872
training start after waiting for 1.201848030090332 seconds
policy loss:-60.81189727783203
value loss:2.361963987350464
entropies:13.45141315460205
Policy training finished
---------------------
gamma: 0.146758253233872
training start after waiting for 1.1977412700653076 seconds
policy loss:-1305.6917724609375
value loss:24.16083526611328
entropies:29.882808685302734
Policy training finished
---------------------
gamma: 0.146758253233872
training start after waiting for 1.177875280380249 seconds
policy loss:-585.8822631835938
value loss:21.691795349121094
entropies:30.192447662353516
Policy training finished
---------------------
gamma: 0.146758253233872
training start after waiting for 1.1467862129211426 seconds
policy loss:-458.94537353515625
value loss:16.651813507080078
entropies:32.21025085449219
Policy training finished
---------------------
gamma: 0.146758253233872
training start after waiting for 1.1412243843078613 seconds
policy loss:-460.7259216308594
value loss:19.029102325439453
entropies:23.510433197021484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1199.3015)
ToM Target loss= tensor(2422.9380)
optimized based on ToM loss
---------------------
gamma: 0.14705176974033976
training start after waiting for 1.1890034675598145 seconds
policy loss:308.44195556640625
value loss:14.413444519042969
entropies:34.10432434082031
Policy training finished
---------------------
gamma: 0.14705176974033976
training start after waiting for 1.1509373188018799 seconds
policy loss:287.3209533691406
value loss:15.865952491760254
entropies:23.014799118041992
Policy training finished
---------------------
gamma: 0.14705176974033976
training start after waiting for 1.1620655059814453 seconds
policy loss:-16.981689453125
value loss:14.133305549621582
entropies:25.075647354125977
Policy training finished
---------------------
gamma: 0.14705176974033976
training start after waiting for 1.213552713394165 seconds
policy loss:-183.6359405517578
value loss:18.349544525146484
entropies:37.27814865112305
Policy training finished
---------------------
gamma: 0.14705176974033976
training start after waiting for 1.1904940605163574 seconds
policy loss:40.34791946411133
value loss:9.662332534790039
entropies:19.026723861694336
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1212.5332)
ToM Target loss= tensor(2425.6775)
optimized based on ToM loss
---------------------
gamma: 0.14734587327982043
training start after waiting for 1.197063684463501 seconds
policy loss:85.0261459350586
value loss:4.532610893249512
entropies:20.029560089111328
Policy training finished
---------------------
gamma: 0.14734587327982043
training start after waiting for 1.1647014617919922 seconds
policy loss:-943.9254760742188
value loss:14.160944938659668
entropies:18.8586368560791
Policy training finished
---------------------
gamma: 0.14734587327982043
training start after waiting for 1.191667079925537 seconds
policy loss:-855.4570922851562
value loss:22.243894577026367
entropies:33.01663589477539
Policy training finished
---------------------
gamma: 0.14734587327982043
training start after waiting for 1.174628496170044 seconds
policy loss:-496.22503662109375
value loss:20.859153747558594
entropies:43.80242156982422
Policy training finished
---------------------
gamma: 0.14734587327982043
training start after waiting for 1.17368745803833 seconds
policy loss:158.00999450683594
value loss:9.282989501953125
entropies:22.731189727783203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1234.0682)
ToM Target loss= tensor(2359.0789)
optimized based on ToM loss
---------------------
gamma: 0.14764056502638007
training start after waiting for 1.2181167602539062 seconds
policy loss:125.64815521240234
value loss:10.770549774169922
entropies:28.948577880859375
Policy training finished
---------------------
gamma: 0.14764056502638007
training start after waiting for 1.1333248615264893 seconds
policy loss:-1251.182373046875
value loss:29.404117584228516
entropies:50.62014389038086
Policy training finished
---------------------
gamma: 0.14764056502638007
training start after waiting for 1.184671401977539 seconds
policy loss:-650.1851806640625
value loss:21.83605194091797
entropies:48.355003356933594
Policy training finished
---------------------
gamma: 0.14764056502638007
training start after waiting for 1.1696879863739014 seconds
policy loss:-15.140703201293945
value loss:19.431358337402344
entropies:25.382509231567383
Policy training finished
---------------------
gamma: 0.14764056502638007
training start after waiting for 1.137662410736084 seconds
policy loss:-453.4469909667969
value loss:26.657917022705078
entropies:27.774566650390625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1225.7897)
ToM Target loss= tensor(2292.2881)
optimized based on ToM loss
---------------------
gamma: 0.14793584615643282
training start after waiting for 1.1461901664733887 seconds
policy loss:-598.6837158203125
value loss:29.564254760742188
entropies:40.400020599365234
Policy training finished
---------------------
gamma: 0.14793584615643282
training start after waiting for 1.1536939144134521 seconds
policy loss:-177.86651611328125
value loss:26.075159072875977
entropies:44.79533004760742
Policy training finished
---------------------
gamma: 0.14793584615643282
training start after waiting for 1.1406497955322266 seconds
policy loss:-370.4537353515625
value loss:17.532928466796875
entropies:16.2210693359375
Policy training finished
---------------------
gamma: 0.14793584615643282
training start after waiting for 1.2220170497894287 seconds
policy loss:-1470.7396240234375
value loss:37.5687370300293
entropies:41.692962646484375
Policy training finished
---------------------
gamma: 0.14793584615643282
training start after waiting for 1.2184028625488281 seconds
policy loss:39.286109924316406
value loss:19.419361114501953
entropies:21.101655960083008
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1262.0156)
ToM Target loss= tensor(2237.1582)
optimized based on ToM loss
---------------------
gamma: 0.1482317178487457
training start after waiting for 1.2100670337677002 seconds
policy loss:-1316.4451904296875
value loss:29.38515281677246
entropies:41.370384216308594
Policy training finished
---------------------
gamma: 0.1482317178487457
training start after waiting for 1.148756742477417 seconds
policy loss:-1110.389404296875
value loss:28.563032150268555
entropies:39.4760627746582
Policy training finished
---------------------
gamma: 0.1482317178487457
training start after waiting for 1.2359094619750977 seconds
policy loss:-1142.2491455078125
value loss:44.68128204345703
entropies:24.322696685791016
Policy training finished
---------------------
gamma: 0.1482317178487457
training start after waiting for 1.2130882740020752 seconds
policy loss:-247.5182647705078
value loss:28.540348052978516
entropies:40.20369338989258
Policy training finished
---------------------
gamma: 0.1482317178487457
training start after waiting for 1.1343297958374023 seconds
policy loss:-348.8756103515625
value loss:14.992429733276367
entropies:25.193605422973633
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1234.2058)
ToM Target loss= tensor(2196.8582)
optimized based on ToM loss
---------------------
gamma: 0.1485281812844432
training start after waiting for 1.1877999305725098 seconds
policy loss:105.18478393554688
value loss:21.955869674682617
entropies:27.012102127075195
Policy training finished
---------------------
gamma: 0.1485281812844432
training start after waiting for 1.153458833694458 seconds
policy loss:-172.9769744873047
value loss:18.49243927001953
entropies:43.932716369628906
Policy training finished
---------------------
gamma: 0.1485281812844432
training start after waiting for 1.1459953784942627 seconds
policy loss:-1557.528564453125
value loss:16.314002990722656
entropies:32.8326301574707
Policy training finished
---------------------
gamma: 0.1485281812844432
training start after waiting for 1.1385893821716309 seconds
policy loss:79.8417739868164
value loss:7.6059041023254395
entropies:18.35422706604004
Policy training finished
---------------------
gamma: 0.1485281812844432
training start after waiting for 1.183854579925537 seconds
policy loss:-906.3682861328125
value loss:17.564111709594727
entropies:34.704994201660156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1241.8829)
ToM Target loss= tensor(2325.0466)
optimized based on ToM loss
---------------------
gamma: 0.14882523764701208
training start after waiting for 1.1537299156188965 seconds
policy loss:-111.8224105834961
value loss:8.389037132263184
entropies:29.468097686767578
Policy training finished
---------------------
gamma: 0.14882523764701208
training start after waiting for 1.1564857959747314 seconds
policy loss:-1262.045166015625
value loss:15.783178329467773
entropies:32.41301727294922
Policy training finished
---------------------
gamma: 0.14882523764701208
training start after waiting for 1.1352932453155518 seconds
policy loss:-542.3260498046875
value loss:10.416154861450195
entropies:24.389772415161133
Policy training finished
---------------------
gamma: 0.14882523764701208
training start after waiting for 1.189591884613037 seconds
policy loss:-13.270188331604004
value loss:13.208357810974121
entropies:33.00306701660156
Policy training finished
---------------------
gamma: 0.14882523764701208
training start after waiting for 1.1377923488616943 seconds
policy loss:-283.4353332519531
value loss:15.086419105529785
entropies:33.21308135986328
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1231.7966)
ToM Target loss= tensor(2310.2415)
optimized based on ToM loss
---------------------
gamma: 0.14912288812230612
training start after waiting for 1.2433550357818604 seconds
policy loss:177.9076690673828
value loss:4.717234134674072
entropies:10.76167106628418
Policy training finished
---------------------
gamma: 0.14912288812230612
training start after waiting for 1.1977765560150146 seconds
policy loss:81.03816986083984
value loss:12.179666519165039
entropies:20.15607452392578
Policy training finished
---------------------
gamma: 0.14912288812230612
training start after waiting for 1.1726865768432617 seconds
policy loss:-262.0295715332031
value loss:6.870387554168701
entropies:19.832487106323242
Policy training finished
---------------------
gamma: 0.14912288812230612
training start after waiting for 1.181121826171875 seconds
policy loss:-792.186279296875
value loss:17.59126853942871
entropies:30.669015884399414
Policy training finished
---------------------
gamma: 0.14912288812230612
training start after waiting for 1.1779224872589111 seconds
policy loss:-275.7105407714844
value loss:9.213855743408203
entropies:34.246891021728516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1185.9288)
ToM Target loss= tensor(2351.8560)
optimized based on ToM loss
---------------------
gamma: 0.14942113389855072
training start after waiting for 1.1415956020355225 seconds
policy loss:-43.9887809753418
value loss:6.718867301940918
entropies:19.49899673461914
Policy training finished
---------------------
gamma: 0.14942113389855072
training start after waiting for 1.1980388164520264 seconds
policy loss:-360.0351257324219
value loss:11.559187889099121
entropies:42.730552673339844
Policy training finished
---------------------
gamma: 0.14942113389855072
training start after waiting for 1.1905596256256104 seconds
policy loss:-44.86204147338867
value loss:8.272784233093262
entropies:20.966060638427734
Policy training finished
---------------------
gamma: 0.14942113389855072
training start after waiting for 1.182868480682373 seconds
policy loss:-348.1658020019531
value loss:15.117918014526367
entropies:28.82585906982422
Policy training finished
---------------------
gamma: 0.14942113389855072
training start after waiting for 1.1728076934814453 seconds
policy loss:-98.7794189453125
value loss:16.81011962890625
entropies:21.855533599853516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1175.3224)
ToM Target loss= tensor(2250.6536)
optimized based on ToM loss
---------------------
gamma: 0.14971997616634783
training start after waiting for 1.1546454429626465 seconds
policy loss:-402.3405456542969
value loss:19.088924407958984
entropies:42.943843841552734
Policy training finished
---------------------
gamma: 0.14971997616634783
training start after waiting for 1.2065322399139404 seconds
policy loss:-583.5712890625
value loss:11.515660285949707
entropies:36.955078125
Policy training finished
---------------------
gamma: 0.14971997616634783
training start after waiting for 1.2024359703063965 seconds
policy loss:-1032.4451904296875
value loss:17.17721939086914
entropies:37.01559829711914
Policy training finished
---------------------
gamma: 0.14971997616634783
training start after waiting for 1.14732027053833 seconds
policy loss:-150.84585571289062
value loss:10.203381538391113
entropies:36.99761962890625
Policy training finished
---------------------
gamma: 0.14971997616634783
training start after waiting for 1.1855823993682861 seconds
policy loss:-443.1025695800781
value loss:11.132798194885254
entropies:33.197349548339844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1309.8342)
ToM Target loss= tensor(2279.2632)
optimized based on ToM loss
---------------------
gamma: 0.15001941611868053
training start after waiting for 1.1785674095153809 seconds
policy loss:-1394.342041015625
value loss:19.184274673461914
entropies:35.89868927001953
Policy training finished
---------------------
gamma: 0.15001941611868053
training start after waiting for 1.2143924236297607 seconds
policy loss:-292.618896484375
value loss:9.949753761291504
entropies:23.38888931274414
Policy training finished
---------------------
gamma: 0.15001941611868053
training start after waiting for 1.1945948600769043 seconds
policy loss:-1228.0521240234375
value loss:20.918306350708008
entropies:39.23247528076172
Policy training finished
---------------------
gamma: 0.15001941611868053
training start after waiting for 1.2373788356781006 seconds
policy loss:-380.4493713378906
value loss:14.093066215515137
entropies:18.78250503540039
Policy training finished
---------------------
gamma: 0.15001941611868053
training start after waiting for 1.2019171714782715 seconds
policy loss:256.5748596191406
value loss:8.005227088928223
entropies:26.530988693237305
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1168.1512)
ToM Target loss= tensor(2313.3342)
optimized based on ToM loss
---------------------
gamma: 0.1503194549509179
training start after waiting for 1.1814048290252686 seconds
policy loss:-22.839111328125
value loss:7.555661201477051
entropies:18.562992095947266
Policy training finished
---------------------
gamma: 0.1503194549509179
training start after waiting for 1.1476356983184814 seconds
policy loss:-1153.8170166015625
value loss:15.649085998535156
entropies:18.981060028076172
Policy training finished
---------------------
gamma: 0.1503194549509179
training start after waiting for 1.2113213539123535 seconds
policy loss:-281.112548828125
value loss:9.413887977600098
entropies:20.98511505126953
Policy training finished
---------------------
gamma: 0.1503194549509179
training start after waiting for 1.1913130283355713 seconds
policy loss:-320.3784484863281
value loss:14.566643714904785
entropies:19.3913516998291
Policy training finished
---------------------
gamma: 0.1503194549509179
training start after waiting for 1.1779146194458008 seconds
policy loss:-140.8538818359375
value loss:6.983911991119385
entropies:18.440614700317383
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1123.4717)
ToM Target loss= tensor(2367.6536)
optimized based on ToM loss
---------------------
gamma: 0.15062009386081973
training start after waiting for 1.108888864517212 seconds
policy loss:-692.2803344726562
value loss:13.641326904296875
entropies:22.558544158935547
Policy training finished
---------------------
gamma: 0.15062009386081973
training start after waiting for 1.148956060409546 seconds
policy loss:-762.327880859375
value loss:13.14686107635498
entropies:39.52471923828125
Policy training finished
---------------------
gamma: 0.15062009386081973
training start after waiting for 1.2040290832519531 seconds
policy loss:169.28347778320312
value loss:8.223201751708984
entropies:15.69133186340332
Policy training finished
---------------------
gamma: 0.15062009386081973
training start after waiting for 1.1425912380218506 seconds
policy loss:-549.1812133789062
value loss:29.889328002929688
entropies:25.66515350341797
Policy training finished
---------------------
gamma: 0.15062009386081973
training start after waiting for 1.2046160697937012 seconds
policy loss:-1430.64794921875
value loss:23.68867301940918
entropies:33.926753997802734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1195.0820)
ToM Target loss= tensor(2322.0156)
optimized based on ToM loss
---------------------
gamma: 0.15092133404854138
training start after waiting for 1.1491765975952148 seconds
policy loss:-124.13664245605469
value loss:13.02969741821289
entropies:25.375654220581055
Policy training finished
---------------------
gamma: 0.15092133404854138
training start after waiting for 1.1548588275909424 seconds
policy loss:304.02862548828125
value loss:18.069440841674805
entropies:20.339092254638672
Policy training finished
---------------------
gamma: 0.15092133404854138
training start after waiting for 1.1962931156158447 seconds
policy loss:-985.98388671875
value loss:39.4143180847168
entropies:31.04570770263672
Policy training finished
---------------------
gamma: 0.15092133404854138
training start after waiting for 1.106149673461914 seconds
policy loss:-879.4149169921875
value loss:26.981731414794922
entropies:37.54096221923828
Policy training finished
---------------------
gamma: 0.15092133404854138
training start after waiting for 1.1978766918182373 seconds
policy loss:-1009.8359375
value loss:21.147132873535156
entropies:30.518272399902344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1178.8817)
ToM Target loss= tensor(2214.1104)
optimized based on ToM loss
---------------------
gamma: 0.15122317671663846
training start after waiting for 1.1770586967468262 seconds
policy loss:-495.6666259765625
value loss:70.95545959472656
entropies:31.213787078857422
Policy training finished
---------------------
gamma: 0.15122317671663846
training start after waiting for 1.134054183959961 seconds
policy loss:-241.46495056152344
value loss:34.511146545410156
entropies:44.05518341064453
Policy training finished
---------------------
gamma: 0.15122317671663846
training start after waiting for 1.1921463012695312 seconds
policy loss:46.012359619140625
value loss:29.56381607055664
entropies:28.7335262298584
Policy training finished
---------------------
gamma: 0.15122317671663846
training start after waiting for 1.2096867561340332 seconds
policy loss:440.661865234375
value loss:31.719703674316406
entropies:24.698429107666016
Policy training finished
---------------------
gamma: 0.15122317671663846
training start after waiting for 1.1573615074157715 seconds
policy loss:283.6157531738281
value loss:25.773494720458984
entropies:13.723723411560059
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1287.8425)
ToM Target loss= tensor(2205.8938)
optimized based on ToM loss
---------------------
gamma: 0.15152562307007172
training start after waiting for 1.1854643821716309 seconds
policy loss:-429.4740295410156
value loss:52.77323532104492
entropies:28.902873992919922
Policy training finished
---------------------
gamma: 0.15152562307007172
training start after waiting for 1.2079083919525146 seconds
policy loss:72.76934814453125
value loss:28.760475158691406
entropies:25.196548461914062
Policy training finished
---------------------
gamma: 0.15152562307007172
training start after waiting for 1.1349411010742188 seconds
policy loss:-1342.3355712890625
value loss:32.485477447509766
entropies:30.05548858642578
Policy training finished
---------------------
gamma: 0.15152562307007172
training start after waiting for 1.1715948581695557 seconds
policy loss:377.43267822265625
value loss:12.51555061340332
entropies:21.854705810546875
Policy training finished
---------------------
gamma: 0.15152562307007172
training start after waiting for 1.1427724361419678 seconds
policy loss:-82.02027893066406
value loss:7.655137062072754
entropies:19.265947341918945
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1141.5562)
ToM Target loss= tensor(2270.8916)
optimized based on ToM loss
---------------------
gamma: 0.15182867431621186
training start after waiting for 1.177018642425537 seconds
policy loss:-569.0718383789062
value loss:15.119756698608398
entropies:32.77094650268555
Policy training finished
---------------------
gamma: 0.15182867431621186
training start after waiting for 1.192528486251831 seconds
policy loss:130.30148315429688
value loss:5.187506675720215
entropies:28.197044372558594
Policy training finished
---------------------
gamma: 0.15182867431621186
training start after waiting for 1.136554479598999 seconds
policy loss:-712.6011962890625
value loss:20.396167755126953
entropies:37.658634185791016
Policy training finished
---------------------
gamma: 0.15182867431621186
training start after waiting for 1.1746115684509277 seconds
policy loss:-2195.697998046875
value loss:46.1429443359375
entropies:46.45183563232422
Policy training finished
---------------------
gamma: 0.15182867431621186
training start after waiting for 1.149259328842163 seconds
policy loss:-512.405029296875
value loss:20.41680145263672
entropies:27.7528076171875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1171.9675)
ToM Target loss= tensor(2212.3398)
optimized based on ToM loss
---------------------
gamma: 0.15213233166484427
training start after waiting for 1.1850037574768066 seconds
policy loss:-1424.233642578125
value loss:26.208703994750977
entropies:41.1176643371582
Policy training finished
---------------------
gamma: 0.15213233166484427
training start after waiting for 1.189453125 seconds
policy loss:-387.8706970214844
value loss:8.181974411010742
entropies:35.94056701660156
Policy training finished
---------------------
gamma: 0.15213233166484427
training start after waiting for 1.188133955001831 seconds
policy loss:335.75714111328125
value loss:14.758149147033691
entropies:19.12881088256836
Policy training finished
---------------------
gamma: 0.15213233166484427
training start after waiting for 1.0954251289367676 seconds
policy loss:-182.12484741210938
value loss:10.557246208190918
entropies:27.897918701171875
Policy training finished
---------------------
gamma: 0.15213233166484427
training start after waiting for 1.147413969039917 seconds
policy loss:1105.0714111328125
value loss:37.333740234375
entropies:42.88703918457031
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1311.0607)
ToM Target loss= tensor(2289.0261)
optimized based on ToM loss
---------------------
gamma: 0.15243659632817397
training start after waiting for 1.1865808963775635 seconds
policy loss:-406.70538330078125
value loss:23.77783966064453
entropies:31.369338989257812
Policy training finished
---------------------
gamma: 0.15243659632817397
training start after waiting for 1.2136046886444092 seconds
policy loss:146.2525634765625
value loss:15.06775951385498
entropies:35.75082778930664
Policy training finished
---------------------
gamma: 0.15243659632817397
training start after waiting for 1.1434528827667236 seconds
policy loss:-325.53448486328125
value loss:26.10433578491211
entropies:33.2294921875
Policy training finished
---------------------
gamma: 0.15243659632817397
training start after waiting for 1.1905066967010498 seconds
policy loss:229.295654296875
value loss:8.666584968566895
entropies:24.916072845458984
Policy training finished
---------------------
gamma: 0.15243659632817397
training start after waiting for 1.2005248069763184 seconds
policy loss:-165.74240112304688
value loss:9.926848411560059
entropies:26.300880432128906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1221.1019)
ToM Target loss= tensor(2226.1848)
optimized based on ToM loss
---------------------
gamma: 0.15274146952083031
training start after waiting for 1.17484712600708 seconds
policy loss:-1474.8914794921875
value loss:33.9263916015625
entropies:46.317909240722656
Policy training finished
---------------------
gamma: 0.15274146952083031
training start after waiting for 1.1540231704711914 seconds
policy loss:-1244.260009765625
value loss:22.10934066772461
entropies:21.98633575439453
Policy training finished
---------------------
gamma: 0.15274146952083031
training start after waiting for 1.2095928192138672 seconds
policy loss:-980.2431030273438
value loss:20.272342681884766
entropies:32.471336364746094
Policy training finished
---------------------
gamma: 0.15274146952083031
training start after waiting for 1.1424832344055176 seconds
policy loss:-1264.793212890625
value loss:25.61090850830078
entropies:25.67673110961914
Policy training finished
---------------------
gamma: 0.15274146952083031
training start after waiting for 1.0955252647399902 seconds
policy loss:-478.2552185058594
value loss:13.850987434387207
entropies:26.558393478393555
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1182.1428)
ToM Target loss= tensor(2286.5474)
optimized based on ToM loss
---------------------
gamma: 0.153046952459872
training start after waiting for 1.1900460720062256 seconds
policy loss:14.86233901977539
value loss:15.89244270324707
entropies:28.224313735961914
Policy training finished
---------------------
gamma: 0.153046952459872
training start after waiting for 1.1414434909820557 seconds
policy loss:-726.5984497070312
value loss:13.585309028625488
entropies:28.176664352416992
Policy training finished
---------------------
gamma: 0.153046952459872
training start after waiting for 1.1864676475524902 seconds
policy loss:69.56948852539062
value loss:15.957939147949219
entropies:25.987398147583008
Policy training finished
---------------------
gamma: 0.153046952459872
training start after waiting for 1.1471855640411377 seconds
policy loss:-136.1392364501953
value loss:12.397013664245605
entropies:32.46902847290039
Policy training finished
---------------------
gamma: 0.153046952459872
training start after waiting for 1.2097601890563965 seconds
policy loss:-3.378566265106201
value loss:18.499557495117188
entropies:26.211441040039062
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1124.9772)
ToM Target loss= tensor(2276.1948)
optimized based on ToM loss
---------------------
gamma: 0.15335304636479175
training start after waiting for 1.1485099792480469 seconds
policy loss:-234.8271484375
value loss:20.84130096435547
entropies:39.49919128417969
Policy training finished
---------------------
gamma: 0.15335304636479175
training start after waiting for 1.1397418975830078 seconds
policy loss:-432.9245300292969
value loss:22.72182846069336
entropies:34.95766830444336
Policy training finished
---------------------
gamma: 0.15335304636479175
training start after waiting for 1.1533994674682617 seconds
policy loss:-336.93719482421875
value loss:11.591777801513672
entropies:35.209163665771484
Policy training finished
---------------------
gamma: 0.15335304636479175
training start after waiting for 1.1708824634552002 seconds
policy loss:-614.227294921875
value loss:25.808124542236328
entropies:28.475866317749023
Policy training finished
---------------------
gamma: 0.15335304636479175
training start after waiting for 1.1116914749145508 seconds
policy loss:120.88903045654297
value loss:11.182504653930664
entropies:13.152191162109375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1140.4836)
ToM Target loss= tensor(2322.2356)
optimized based on ToM loss
---------------------
gamma: 0.15365975245752134
training start after waiting for 1.1981942653656006 seconds
policy loss:-1026.39208984375
value loss:21.86707878112793
entropies:40.41553497314453
Policy training finished
---------------------
gamma: 0.15365975245752134
training start after waiting for 1.14323091506958 seconds
policy loss:-454.57330322265625
value loss:10.539409637451172
entropies:30.2403507232666
Policy training finished
---------------------
gamma: 0.15365975245752134
training start after waiting for 1.165503978729248 seconds
policy loss:-151.3141632080078
value loss:10.72596549987793
entropies:18.24598503112793
Policy training finished
---------------------
gamma: 0.15365975245752134
training start after waiting for 1.1539011001586914 seconds
policy loss:-240.36727905273438
value loss:14.529010772705078
entropies:32.05509948730469
Policy training finished
---------------------
gamma: 0.15365975245752134
training start after waiting for 1.1444172859191895 seconds
policy loss:-335.6069641113281
value loss:13.085175514221191
entropies:33.35607147216797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1172.5049)
ToM Target loss= tensor(2242.2087)
optimized based on ToM loss
---------------------
gamma: 0.1539670719624364
training start after waiting for 1.176086664199829 seconds
policy loss:-383.0547180175781
value loss:5.87510871887207
entropies:14.855259895324707
Policy training finished
---------------------
gamma: 0.1539670719624364
training start after waiting for 1.1967337131500244 seconds
policy loss:-1315.3963623046875
value loss:66.93045043945312
entropies:51.061500549316406
Policy training finished
---------------------
gamma: 0.1539670719624364
training start after waiting for 1.1386313438415527 seconds
policy loss:-740.9895629882812
value loss:28.08399200439453
entropies:40.65412902832031
Policy training finished
---------------------
gamma: 0.1539670719624364
training start after waiting for 1.1895103454589844 seconds
policy loss:-481.9021301269531
value loss:62.85483932495117
entropies:29.826011657714844
Policy training finished
---------------------
gamma: 0.1539670719624364
training start after waiting for 1.2068157196044922 seconds
policy loss:466.8112487792969
value loss:25.963666915893555
entropies:31.735679626464844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1245.7727)
ToM Target loss= tensor(2189.7800)
optimized based on ToM loss
---------------------
gamma: 0.15427500610636127
training start after waiting for 1.149972677230835 seconds
policy loss:428.6393127441406
value loss:25.80950355529785
entropies:30.442697525024414
Policy training finished
---------------------
gamma: 0.15427500610636127
training start after waiting for 1.1667046546936035 seconds
policy loss:506.6803283691406
value loss:20.412872314453125
entropies:22.373912811279297
Policy training finished
---------------------
gamma: 0.15427500610636127
training start after waiting for 1.1808116436004639 seconds
policy loss:-294.3868713378906
value loss:20.183645248413086
entropies:35.60344314575195
Policy training finished
---------------------
gamma: 0.15427500610636127
training start after waiting for 1.173926591873169 seconds
policy loss:265.71893310546875
value loss:25.109437942504883
entropies:24.697574615478516
Policy training finished
---------------------
gamma: 0.15427500610636127
training start after waiting for 1.2067980766296387 seconds
policy loss:-706.2310791015625
value loss:31.639646530151367
entropies:47.77505874633789
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1149.6300)
ToM Target loss= tensor(2239.6731)
optimized based on ToM loss
---------------------
gamma: 0.154583556118574
training start after waiting for 1.1698834896087646 seconds
policy loss:217.7659454345703
value loss:11.362455368041992
entropies:38.075897216796875
Policy training finished
---------------------
gamma: 0.154583556118574
training start after waiting for 1.1932482719421387 seconds
policy loss:-733.5230102539062
value loss:30.483448028564453
entropies:41.846763610839844
Policy training finished
---------------------
gamma: 0.154583556118574
training start after waiting for 1.1459760665893555 seconds
policy loss:321.8532409667969
value loss:12.998506546020508
entropies:15.919358253479004
Policy training finished
---------------------
gamma: 0.154583556118574
training start after waiting for 1.159240484237671 seconds
policy loss:-168.66802978515625
value loss:23.070802688598633
entropies:26.845613479614258
Policy training finished
---------------------
gamma: 0.154583556118574
training start after waiting for 1.174710750579834 seconds
policy loss:177.91477966308594
value loss:10.381747245788574
entropies:33.504302978515625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1234.4279)
ToM Target loss= tensor(2276.3501)
optimized based on ToM loss
---------------------
gamma: 0.15489272323081113
training start after waiting for 1.1736397743225098 seconds
policy loss:-350.67864990234375
value loss:16.60416603088379
entropies:29.18528175354004
Policy training finished
---------------------
gamma: 0.15489272323081113
training start after waiting for 1.1754310131072998 seconds
policy loss:-293.4117736816406
value loss:16.427000045776367
entropies:21.190961837768555
Policy training finished
---------------------
gamma: 0.15489272323081113
training start after waiting for 1.206510066986084 seconds
policy loss:-756.9611206054688
value loss:21.063180923461914
entropies:23.202816009521484
Policy training finished
---------------------
gamma: 0.15489272323081113
training start after waiting for 1.2110083103179932 seconds
policy loss:-545.1924438476562
value loss:17.633033752441406
entropies:32.266300201416016
Policy training finished
---------------------
gamma: 0.15489272323081113
training start after waiting for 1.1594104766845703 seconds
policy loss:140.38644409179688
value loss:11.890596389770508
entropies:23.81584930419922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1226.7151)
ToM Target loss= tensor(2358.9341)
optimized based on ToM loss
---------------------
gamma: 0.15520250867727275
training start after waiting for 1.1808207035064697 seconds
policy loss:269.4841613769531
value loss:12.17053508758545
entropies:19.510894775390625
Policy training finished
---------------------
gamma: 0.15520250867727275
training start after waiting for 1.173121452331543 seconds
policy loss:69.80229187011719
value loss:4.610311985015869
entropies:15.417618751525879
Policy training finished
---------------------
gamma: 0.15520250867727275
training start after waiting for 1.2102270126342773 seconds
policy loss:-1781.9166259765625
value loss:38.8930549621582
entropies:37.62245178222656
Policy training finished
---------------------
gamma: 0.15520250867727275
training start after waiting for 1.2170522212982178 seconds
policy loss:-963.7008666992188
value loss:24.35163688659668
entropies:49.35468292236328
Policy training finished
---------------------
gamma: 0.15520250867727275
training start after waiting for 1.1858611106872559 seconds
policy loss:-178.7651824951172
value loss:6.405669689178467
entropies:26.06706428527832
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1229.9492)
ToM Target loss= tensor(2239.3857)
optimized based on ToM loss
---------------------
gamma: 0.1555129136946273
training start after waiting for 1.1791267395019531 seconds
policy loss:-456.5325927734375
value loss:13.356735229492188
entropies:21.760147094726562
Policy training finished
---------------------
gamma: 0.1555129136946273
training start after waiting for 1.1912882328033447 seconds
policy loss:-280.24322509765625
value loss:6.3023223876953125
entropies:23.576519012451172
Policy training finished
---------------------
gamma: 0.1555129136946273
training start after waiting for 1.1853203773498535 seconds
policy loss:39.26201629638672
value loss:9.598274230957031
entropies:29.05402374267578
Policy training finished
---------------------
gamma: 0.1555129136946273
training start after waiting for 1.180290699005127 seconds
policy loss:120.73192596435547
value loss:6.816269874572754
entropies:23.925880432128906
Policy training finished
---------------------
gamma: 0.1555129136946273
training start after waiting for 1.209529161453247 seconds
policy loss:-1089.3138427734375
value loss:24.413406372070312
entropies:37.18781661987305
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1145.0438)
ToM Target loss= tensor(2283.8806)
optimized based on ToM loss
---------------------
gamma: 0.15582393952201656
training start after waiting for 1.160261869430542 seconds
policy loss:-596.0765991210938
value loss:19.018041610717773
entropies:30.202136993408203
Policy training finished
---------------------
gamma: 0.15582393952201656
training start after waiting for 1.1484923362731934 seconds
policy loss:68.29801940917969
value loss:8.326894760131836
entropies:30.054929733276367
Policy training finished
---------------------
gamma: 0.15582393952201656
training start after waiting for 1.1366283893585205 seconds
policy loss:-1077.7318115234375
value loss:33.1573371887207
entropies:37.467071533203125
Policy training finished
---------------------
gamma: 0.15582393952201656
training start after waiting for 1.1767842769622803 seconds
policy loss:-10.373751640319824
value loss:11.978671073913574
entropies:29.669687271118164
Policy training finished
---------------------
gamma: 0.15582393952201656
training start after waiting for 1.1706550121307373 seconds
policy loss:-322.01568603515625
value loss:19.186996459960938
entropies:23.630435943603516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1194.9487)
ToM Target loss= tensor(2273.6960)
optimized based on ToM loss
---------------------
gamma: 0.1561355874010606
training start after waiting for 1.1385324001312256 seconds
policy loss:-1280.0804443359375
value loss:31.531160354614258
entropies:30.922687530517578
Policy training finished
---------------------
gamma: 0.1561355874010606
training start after waiting for 1.1802005767822266 seconds
policy loss:-321.4981384277344
value loss:13.301566123962402
entropies:39.42853927612305
Policy training finished
---------------------
gamma: 0.1561355874010606
training start after waiting for 1.1868090629577637 seconds
policy loss:-499.4541015625
value loss:15.098336219787598
entropies:20.175613403320312
Policy training finished
---------------------
gamma: 0.1561355874010606
training start after waiting for 1.1927738189697266 seconds
policy loss:-472.0449523925781
value loss:17.469301223754883
entropies:26.833049774169922
Policy training finished
---------------------
gamma: 0.1561355874010606
training start after waiting for 1.2133941650390625 seconds
policy loss:-454.63397216796875
value loss:12.462352752685547
entropies:32.58586883544922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1214.1770)
ToM Target loss= tensor(2301.7852)
optimized based on ToM loss
---------------------
gamma: 0.1564478585758627
training start after waiting for 1.2042062282562256 seconds
policy loss:-449.8192443847656
value loss:18.500551223754883
entropies:45.042205810546875
Policy training finished
---------------------
gamma: 0.1564478585758627
training start after waiting for 1.2148568630218506 seconds
policy loss:382.45526123046875
value loss:12.285663604736328
entropies:28.205413818359375
Policy training finished
---------------------
gamma: 0.1564478585758627
training start after waiting for 1.1763195991516113 seconds
policy loss:175.30380249023438
value loss:11.159002304077148
entropies:33.0782356262207
Policy training finished
---------------------
gamma: 0.1564478585758627
training start after waiting for 1.224721908569336 seconds
policy loss:203.6921844482422
value loss:14.467000961303711
entropies:30.06604766845703
Policy training finished
---------------------
gamma: 0.1564478585758627
training start after waiting for 1.1758742332458496 seconds
policy loss:219.26499938964844
value loss:8.410633087158203
entropies:18.466543197631836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1189.5065)
ToM Target loss= tensor(2184.3665)
optimized based on ToM loss
---------------------
gamma: 0.15676075429301445
training start after waiting for 1.1459410190582275 seconds
policy loss:-120.87857818603516
value loss:4.8840765953063965
entropies:29.081954956054688
Policy training finished
---------------------
gamma: 0.15676075429301445
training start after waiting for 1.1844241619110107 seconds
policy loss:86.93983459472656
value loss:6.791525363922119
entropies:21.28864097595215
Policy training finished
---------------------
gamma: 0.15676075429301445
training start after waiting for 1.1626157760620117 seconds
policy loss:-1137.681640625
value loss:36.423301696777344
entropies:58.92613983154297
Policy training finished
---------------------
gamma: 0.15676075429301445
training start after waiting for 1.1954753398895264 seconds
policy loss:-847.09033203125
value loss:20.285541534423828
entropies:36.13599395751953
Policy training finished
---------------------
gamma: 0.15676075429301445
training start after waiting for 1.1932039260864258 seconds
policy loss:53.357810974121094
value loss:5.815965175628662
entropies:30.64318084716797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1188.4232)
ToM Target loss= tensor(2222.5662)
optimized based on ToM loss
---------------------
gamma: 0.15707427580160047
training start after waiting for 1.1519267559051514 seconds
policy loss:140.251953125
value loss:8.897661209106445
entropies:24.84088897705078
Policy training finished
---------------------
gamma: 0.15707427580160047
training start after waiting for 1.1522283554077148 seconds
policy loss:-1113.3516845703125
value loss:20.031513214111328
entropies:22.929527282714844
Policy training finished
---------------------
gamma: 0.15707427580160047
training start after waiting for 1.1868655681610107 seconds
policy loss:-805.1572875976562
value loss:25.2685489654541
entropies:29.580169677734375
Policy training finished
---------------------
gamma: 0.15707427580160047
training start after waiting for 1.2008178234100342 seconds
policy loss:-363.3392028808594
value loss:18.505233764648438
entropies:43.53093719482422
Policy training finished
---------------------
gamma: 0.15707427580160047
training start after waiting for 1.1670587062835693 seconds
policy loss:-303.1589660644531
value loss:16.76322364807129
entropies:32.78790283203125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1154.3268)
ToM Target loss= tensor(2310.3528)
optimized based on ToM loss
---------------------
gamma: 0.15738842435320366
training start after waiting for 1.189915657043457 seconds
policy loss:-712.6859130859375
value loss:27.388547897338867
entropies:38.812278747558594
Policy training finished
---------------------
gamma: 0.15738842435320366
training start after waiting for 1.192680835723877 seconds
policy loss:-142.600341796875
value loss:13.776188850402832
entropies:31.596393585205078
Policy training finished
---------------------
gamma: 0.15738842435320366
training start after waiting for 1.139652967453003 seconds
policy loss:107.05270385742188
value loss:26.12682342529297
entropies:33.9443359375
Policy training finished
---------------------
gamma: 0.15738842435320366
training start after waiting for 1.1777698993682861 seconds
policy loss:-690.5413208007812
value loss:16.739097595214844
entropies:34.454307556152344
Policy training finished
---------------------
gamma: 0.15738842435320366
training start after waiting for 1.1437945365905762 seconds
policy loss:-24.044095993041992
value loss:9.242563247680664
entropies:24.932846069335938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1209.1177)
ToM Target loss= tensor(2326.0923)
optimized based on ToM loss
---------------------
gamma: 0.15770320120191006
training start after waiting for 1.1975924968719482 seconds
policy loss:-1303.9111328125
value loss:11.727483749389648
entropies:32.567928314208984
Policy training finished
---------------------
gamma: 0.15770320120191006
training start after waiting for 1.160071849822998 seconds
policy loss:-635.7776489257812
value loss:28.72635841369629
entropies:37.13963317871094
Policy training finished
---------------------
gamma: 0.15770320120191006
training start after waiting for 1.1378626823425293 seconds
policy loss:-25.7960205078125
value loss:9.521867752075195
entropies:32.961090087890625
Policy training finished
---------------------
gamma: 0.15770320120191006
training start after waiting for 1.1826081275939941 seconds
policy loss:210.82696533203125
value loss:11.416193008422852
entropies:27.46450424194336
Policy training finished
---------------------
gamma: 0.15770320120191006
training start after waiting for 1.1798691749572754 seconds
policy loss:-190.87403869628906
value loss:28.1898136138916
entropies:38.06229782104492
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1276.1991)
ToM Target loss= tensor(2262.8503)
optimized based on ToM loss
---------------------
gamma: 0.15801860760431388
training start after waiting for 1.1484813690185547 seconds
policy loss:264.01495361328125
value loss:5.099210262298584
entropies:20.696781158447266
Policy training finished
---------------------
gamma: 0.15801860760431388
training start after waiting for 1.1697101593017578 seconds
policy loss:-196.17227172851562
value loss:18.590824127197266
entropies:38.66392517089844
Policy training finished
---------------------
gamma: 0.15801860760431388
training start after waiting for 1.1499075889587402 seconds
policy loss:-308.8734436035156
value loss:21.624853134155273
entropies:36.91737365722656
Policy training finished
---------------------
gamma: 0.15801860760431388
training start after waiting for 1.188382625579834 seconds
policy loss:-1280.000244140625
value loss:46.35483169555664
entropies:41.54725646972656
Policy training finished
---------------------
gamma: 0.15801860760431388
training start after waiting for 1.2163317203521729 seconds
policy loss:11.419379234313965
value loss:6.063399314880371
entropies:17.28704833984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1184.5989)
ToM Target loss= tensor(2292.7944)
optimized based on ToM loss
---------------------
gamma: 0.1583346448195225
training start after waiting for 1.2156589031219482 seconds
policy loss:-831.8200073242188
value loss:19.84173011779785
entropies:34.86138153076172
Policy training finished
---------------------
gamma: 0.1583346448195225
training start after waiting for 1.210153341293335 seconds
policy loss:-999.818359375
value loss:25.330421447753906
entropies:49.70718765258789
Policy training finished
---------------------
gamma: 0.1583346448195225
training start after waiting for 1.188227891921997 seconds
policy loss:93.7500228881836
value loss:16.835241317749023
entropies:33.87411880493164
Policy training finished
---------------------
gamma: 0.1583346448195225
training start after waiting for 1.13594388961792 seconds
policy loss:209.00350952148438
value loss:7.57363748550415
entropies:15.21608829498291
Policy training finished
---------------------
gamma: 0.1583346448195225
training start after waiting for 1.1850764751434326 seconds
policy loss:-113.79624938964844
value loss:14.167532920837402
entropies:22.911518096923828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1197.2415)
ToM Target loss= tensor(2320.8135)
optimized based on ToM loss
---------------------
gamma: 0.15865131410916156
training start after waiting for 1.1511492729187012 seconds
policy loss:-597.0104370117188
value loss:12.286083221435547
entropies:31.141704559326172
Policy training finished
---------------------
gamma: 0.15865131410916156
training start after waiting for 1.1615674495697021 seconds
policy loss:-700.4075927734375
value loss:19.079458236694336
entropies:43.176143646240234
Policy training finished
---------------------
gamma: 0.15865131410916156
training start after waiting for 1.1709403991699219 seconds
policy loss:220.69873046875
value loss:3.6526803970336914
entropies:18.848981857299805
Policy training finished
---------------------
gamma: 0.15865131410916156
training start after waiting for 1.2119786739349365 seconds
policy loss:-883.547119140625
value loss:15.373246192932129
entropies:29.392513275146484
Policy training finished
---------------------
gamma: 0.15865131410916156
training start after waiting for 1.1904525756835938 seconds
policy loss:-672.8758544921875
value loss:13.167086601257324
entropies:29.161401748657227
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1306.7772)
ToM Target loss= tensor(2307.2156)
optimized based on ToM loss
---------------------
gamma: 0.1589686167373799
training start after waiting for 1.1536521911621094 seconds
policy loss:-1588.4891357421875
value loss:25.182710647583008
entropies:36.853294372558594
Policy training finished
---------------------
gamma: 0.1589686167373799
training start after waiting for 1.1898925304412842 seconds
policy loss:-677.5367431640625
value loss:18.28084945678711
entropies:43.313934326171875
Policy training finished
---------------------
gamma: 0.1589686167373799
training start after waiting for 1.1484999656677246 seconds
policy loss:-717.2006225585938
value loss:14.547195434570312
entropies:33.394859313964844
Policy training finished
---------------------
gamma: 0.1589686167373799
training start after waiting for 1.1931190490722656 seconds
policy loss:-788.0587158203125
value loss:11.413750648498535
entropies:33.4373893737793
Policy training finished
---------------------
gamma: 0.1589686167373799
training start after waiting for 1.1666207313537598 seconds
policy loss:406.1796875
value loss:15.311177253723145
entropies:29.114591598510742
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1282.4462)
ToM Target loss= tensor(2202.5095)
optimized based on ToM loss
---------------------
gamma: 0.15928655397085464
training start after waiting for 1.1809959411621094 seconds
policy loss:-1196.9395751953125
value loss:21.182750701904297
entropies:43.99432373046875
Policy training finished
---------------------
gamma: 0.15928655397085464
training start after waiting for 1.1545357704162598 seconds
policy loss:-1638.8480224609375
value loss:25.213359832763672
entropies:35.17127990722656
Policy training finished
---------------------
gamma: 0.15928655397085464
training start after waiting for 1.1898314952850342 seconds
policy loss:-877.4918212890625
value loss:19.330303192138672
entropies:31.195018768310547
Policy training finished
---------------------
gamma: 0.15928655397085464
training start after waiting for 1.2189371585845947 seconds
policy loss:-650.5972290039062
value loss:25.06792640686035
entropies:32.039398193359375
Policy training finished
---------------------
gamma: 0.15928655397085464
training start after waiting for 1.1836862564086914 seconds
policy loss:-316.56201171875
value loss:19.0190372467041
entropies:31.570510864257812
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1266.3004)
ToM Target loss= tensor(2302.7327)
optimized based on ToM loss
---------------------
gamma: 0.15960512707879634
training start after waiting for 1.1362409591674805 seconds
policy loss:-418.2994384765625
value loss:15.017017364501953
entropies:27.55122184753418
Policy training finished
---------------------
gamma: 0.15960512707879634
training start after waiting for 1.1848783493041992 seconds
policy loss:-900.9833374023438
value loss:25.814260482788086
entropies:41.17408752441406
Policy training finished
---------------------
gamma: 0.15960512707879634
training start after waiting for 1.2001783847808838 seconds
policy loss:-85.94095611572266
value loss:10.6279878616333
entropies:30.180145263671875
Policy training finished
---------------------
gamma: 0.15960512707879634
training start after waiting for 1.152498483657837 seconds
policy loss:-1291.69873046875
value loss:19.344345092773438
entropies:30.375324249267578
Policy training finished
---------------------
gamma: 0.15960512707879634
training start after waiting for 1.2304298877716064 seconds
policy loss:-997.710693359375
value loss:19.343612670898438
entropies:42.782127380371094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1291.8201)
ToM Target loss= tensor(2280.4097)
optimized based on ToM loss
---------------------
gamma: 0.15992433733295394
training start after waiting for 1.1504552364349365 seconds
policy loss:-1318.50341796875
value loss:24.084339141845703
entropies:45.68350601196289
Policy training finished
---------------------
gamma: 0.15992433733295394
training start after waiting for 1.162940263748169 seconds
policy loss:-238.47613525390625
value loss:16.09971809387207
entropies:26.074119567871094
Policy training finished
---------------------
gamma: 0.15992433733295394
training start after waiting for 1.212095022201538 seconds
policy loss:78.54214477539062
value loss:20.52703857421875
entropies:22.425350189208984
Policy training finished
---------------------
gamma: 0.15992433733295394
training start after waiting for 1.2096285820007324 seconds
policy loss:-1093.45703125
value loss:37.74407196044922
entropies:39.35710144042969
Policy training finished
---------------------
gamma: 0.15992433733295394
training start after waiting for 1.1916000843048096 seconds
policy loss:-74.36941528320312
value loss:26.669111251831055
entropies:54.73857879638672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1178.6193)
ToM Target loss= tensor(2222.9216)
optimized based on ToM loss
---------------------
gamma: 0.16024418600761986
training start after waiting for 1.1698884963989258 seconds
policy loss:-1030.6978759765625
value loss:26.659425735473633
entropies:27.15454864501953
Policy training finished
---------------------
gamma: 0.16024418600761986
training start after waiting for 1.1914737224578857 seconds
policy loss:296.4290771484375
value loss:7.415659427642822
entropies:19.615921020507812
Policy training finished
---------------------
gamma: 0.16024418600761986
training start after waiting for 1.1480703353881836 seconds
policy loss:-347.7408142089844
value loss:9.364959716796875
entropies:20.651592254638672
Policy training finished
---------------------
gamma: 0.16024418600761986
training start after waiting for 1.1599795818328857 seconds
policy loss:-420.5800476074219
value loss:17.080055236816406
entropies:35.16558837890625
Policy training finished
---------------------
gamma: 0.16024418600761986
training start after waiting for 1.1833465099334717 seconds
policy loss:-190.9071807861328
value loss:5.309700965881348
entropies:16.44492530822754
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1201.5133)
ToM Target loss= tensor(2390.9067)
optimized based on ToM loss
---------------------
gamma: 0.1605646743796351
training start after waiting for 1.1590912342071533 seconds
policy loss:-269.66851806640625
value loss:11.460954666137695
entropies:26.972442626953125
Policy training finished
---------------------
gamma: 0.1605646743796351
training start after waiting for 1.185077428817749 seconds
policy loss:-445.4171142578125
value loss:12.201251029968262
entropies:25.06591796875
Policy training finished
---------------------
gamma: 0.1605646743796351
training start after waiting for 1.1435174942016602 seconds
policy loss:-774.530029296875
value loss:27.597280502319336
entropies:31.42296600341797
Policy training finished
---------------------
gamma: 0.1605646743796351
training start after waiting for 1.1421589851379395 seconds
policy loss:8.292150497436523
value loss:11.879528045654297
entropies:29.843097686767578
Policy training finished
---------------------
gamma: 0.1605646743796351
training start after waiting for 1.1772387027740479 seconds
policy loss:189.6730499267578
value loss:6.595890998840332
entropies:17.27291488647461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1133.4712)
ToM Target loss= tensor(2312.5298)
optimized based on ToM loss
---------------------
gamma: 0.16088580372839437
training start after waiting for 1.1385571956634521 seconds
policy loss:-127.40042877197266
value loss:10.871011734008789
entropies:28.21767807006836
Policy training finished
---------------------
gamma: 0.16088580372839437
training start after waiting for 1.1502749919891357 seconds
policy loss:-460.9775390625
value loss:40.964534759521484
entropies:41.42946243286133
Policy training finished
---------------------
gamma: 0.16088580372839437
training start after waiting for 1.2059967517852783 seconds
policy loss:-26.33275032043457
value loss:8.469117164611816
entropies:21.384376525878906
Policy training finished
---------------------
gamma: 0.16088580372839437
training start after waiting for 1.1750788688659668 seconds
policy loss:-276.3951110839844
value loss:12.723939895629883
entropies:28.2410888671875
Policy training finished
---------------------
gamma: 0.16088580372839437
training start after waiting for 1.1903975009918213 seconds
policy loss:-1114.473388671875
value loss:24.504175186157227
entropies:25.90387725830078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1283.8303)
ToM Target loss= tensor(2494.9917)
optimized based on ToM loss
---------------------
gamma: 0.16120757533585114
training start after waiting for 1.1495916843414307 seconds
policy loss:-591.6566772460938
value loss:18.56760025024414
entropies:25.572790145874023
Policy training finished
---------------------
gamma: 0.16120757533585114
training start after waiting for 1.1945116519927979 seconds
policy loss:301.4986572265625
value loss:8.81578254699707
entropies:21.27096939086914
Policy training finished
---------------------
gamma: 0.16120757533585114
training start after waiting for 1.1609158515930176 seconds
policy loss:-492.927490234375
value loss:14.672172546386719
entropies:23.936458587646484
Policy training finished
---------------------
gamma: 0.16120757533585114
training start after waiting for 1.179429292678833 seconds
policy loss:-217.67649841308594
value loss:13.562700271606445
entropies:39.499656677246094
Policy training finished
---------------------
gamma: 0.16120757533585114
training start after waiting for 1.2323870658874512 seconds
policy loss:-208.8787841796875
value loss:9.936911582946777
entropies:27.077295303344727
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1183.8718)
ToM Target loss= tensor(2273.7773)
optimized based on ToM loss
---------------------
gamma: 0.16152999048652283
training start after waiting for 1.153576135635376 seconds
policy loss:-375.8417053222656
value loss:12.552936553955078
entropies:19.44708824157715
Policy training finished
---------------------
gamma: 0.16152999048652283
training start after waiting for 1.1416194438934326 seconds
policy loss:89.40113067626953
value loss:7.505971431732178
entropies:19.2471981048584
Policy training finished
---------------------
gamma: 0.16152999048652283
training start after waiting for 1.1683032512664795 seconds
policy loss:-236.4084930419922
value loss:5.166013240814209
entropies:18.195255279541016
Policy training finished
---------------------
gamma: 0.16152999048652283
training start after waiting for 1.1510992050170898 seconds
policy loss:-586.5343017578125
value loss:8.9738130569458
entropies:28.102827072143555
Policy training finished
---------------------
gamma: 0.16152999048652283
training start after waiting for 1.1441981792449951 seconds
policy loss:-1026.3173828125
value loss:38.72671890258789
entropies:47.42729187011719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1054.5667)
ToM Target loss= tensor(2299.2505)
optimized based on ToM loss
---------------------
gamma: 0.16185305046749587
training start after waiting for 1.14933443069458 seconds
policy loss:-885.830322265625
value loss:9.769832611083984
entropies:28.545696258544922
Policy training finished
---------------------
gamma: 0.16185305046749587
training start after waiting for 1.180513858795166 seconds
policy loss:-2.20330810546875
value loss:14.726652145385742
entropies:23.793304443359375
Policy training finished
---------------------
gamma: 0.16185305046749587
training start after waiting for 1.18638014793396 seconds
policy loss:-1123.5518798828125
value loss:25.918521881103516
entropies:43.12969207763672
Policy training finished
---------------------
gamma: 0.16185305046749587
training start after waiting for 1.2159528732299805 seconds
policy loss:-403.5295104980469
value loss:36.50552749633789
entropies:30.143878936767578
Policy training finished
---------------------
gamma: 0.16185305046749587
training start after waiting for 1.146005392074585 seconds
policy loss:-707.5536499023438
value loss:18.555830001831055
entropies:33.72452163696289
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1234.8972)
ToM Target loss= tensor(2337.9170)
optimized based on ToM loss
---------------------
gamma: 0.16217675656843086
training start after waiting for 1.1989314556121826 seconds
policy loss:106.53631591796875
value loss:18.45529556274414
entropies:35.62517547607422
Policy training finished
---------------------
gamma: 0.16217675656843086
training start after waiting for 1.1511485576629639 seconds
policy loss:-143.34986877441406
value loss:14.03172492980957
entropies:30.134788513183594
Policy training finished
---------------------
gamma: 0.16217675656843086
training start after waiting for 1.2057058811187744 seconds
policy loss:-839.2379760742188
value loss:14.502528190612793
entropies:35.67097091674805
Policy training finished
---------------------
gamma: 0.16217675656843086
training start after waiting for 1.1500771045684814 seconds
policy loss:102.40888977050781
value loss:5.996397972106934
entropies:13.168489456176758
Policy training finished
---------------------
gamma: 0.16217675656843086
training start after waiting for 1.1312596797943115 seconds
policy loss:-445.30181884765625
value loss:23.384540557861328
entropies:26.603879928588867
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1243.1169)
ToM Target loss= tensor(2281.7839)
optimized based on ToM loss
---------------------
gamma: 0.16250111008156773
training start after waiting for 1.1397936344146729 seconds
policy loss:-228.55532836914062
value loss:12.94448184967041
entropies:30.350929260253906
Policy training finished
---------------------
gamma: 0.16250111008156773
training start after waiting for 1.2076613903045654 seconds
policy loss:-1228.230224609375
value loss:42.18465042114258
entropies:38.85201644897461
Policy training finished
---------------------
gamma: 0.16250111008156773
training start after waiting for 1.2027628421783447 seconds
policy loss:-1053.4515380859375
value loss:36.55368423461914
entropies:32.553462982177734
Policy training finished
---------------------
gamma: 0.16250111008156773
training start after waiting for 1.175931453704834 seconds
policy loss:50.75827407836914
value loss:7.553266525268555
entropies:18.870019912719727
Policy training finished
---------------------
gamma: 0.16250111008156773
training start after waiting for 1.1696186065673828 seconds
policy loss:-402.637451171875
value loss:13.40457534790039
entropies:30.181840896606445
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1159.7218)
ToM Target loss= tensor(2253.6226)
optimized based on ToM loss
---------------------
gamma: 0.16282611230173086
training start after waiting for 1.173957347869873 seconds
policy loss:-548.78271484375
value loss:24.33751678466797
entropies:41.266441345214844
Policy training finished
---------------------
gamma: 0.16282611230173086
training start after waiting for 1.205873727798462 seconds
policy loss:-285.3931884765625
value loss:10.910554885864258
entropies:21.70897674560547
Policy training finished
---------------------
gamma: 0.16282611230173086
training start after waiting for 1.17974853515625 seconds
policy loss:-827.696533203125
value loss:17.301557540893555
entropies:31.128095626831055
Policy training finished
---------------------
gamma: 0.16282611230173086
training start after waiting for 1.1779072284698486 seconds
policy loss:-136.1857147216797
value loss:10.469217300415039
entropies:15.258316040039062
Policy training finished
---------------------
gamma: 0.16282611230173086
training start after waiting for 1.1544904708862305 seconds
policy loss:-15.551464080810547
value loss:14.961492538452148
entropies:36.63294982910156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1231.2401)
ToM Target loss= tensor(2347.1353)
optimized based on ToM loss
---------------------
gamma: 0.16315176452633431
training start after waiting for 1.1690244674682617 seconds
policy loss:-639.7711791992188
value loss:20.258453369140625
entropies:25.833213806152344
Policy training finished
---------------------
gamma: 0.16315176452633431
training start after waiting for 1.1772723197937012 seconds
policy loss:-123.36485290527344
value loss:6.65543270111084
entropies:21.25526237487793
Policy training finished
---------------------
gamma: 0.16315176452633431
training start after waiting for 1.1429646015167236 seconds
policy loss:118.02275085449219
value loss:5.781085968017578
entropies:22.87792205810547
Policy training finished
---------------------
gamma: 0.16315176452633431
training start after waiting for 1.2100145816802979 seconds
policy loss:-1278.880126953125
value loss:27.510311126708984
entropies:35.63385009765625
Policy training finished
---------------------
gamma: 0.16315176452633431
training start after waiting for 1.1504621505737305 seconds
policy loss:45.954551696777344
value loss:11.003135681152344
entropies:29.115150451660156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1195.3853)
ToM Target loss= tensor(2363.1062)
optimized based on ToM loss
---------------------
gamma: 0.16347806805538698
training start after waiting for 1.1849210262298584 seconds
policy loss:-130.06451416015625
value loss:10.116771697998047
entropies:25.666309356689453
Policy training finished
---------------------
gamma: 0.16347806805538698
training start after waiting for 1.2105979919433594 seconds
policy loss:198.93536376953125
value loss:9.749429702758789
entropies:29.656749725341797
Policy training finished
---------------------
gamma: 0.16347806805538698
training start after waiting for 1.2085990905761719 seconds
policy loss:-308.04168701171875
value loss:9.856232643127441
entropies:31.156856536865234
Policy training finished
---------------------
gamma: 0.16347806805538698
training start after waiting for 1.211327314376831 seconds
policy loss:-507.9473571777344
value loss:17.90863609313965
entropies:28.75421142578125
Policy training finished
---------------------
gamma: 0.16347806805538698
training start after waiting for 1.1879687309265137 seconds
policy loss:-443.4142761230469
value loss:11.397664070129395
entropies:29.32072639465332
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1174.8184)
ToM Target loss= tensor(2343.9177)
optimized based on ToM loss
---------------------
gamma: 0.16380502419149776
training start after waiting for 1.2294893264770508 seconds
policy loss:-353.5312194824219
value loss:9.395259857177734
entropies:26.452251434326172
Policy training finished
---------------------
gamma: 0.16380502419149776
training start after waiting for 1.2035608291625977 seconds
policy loss:-781.6997680664062
value loss:18.409055709838867
entropies:34.280364990234375
Policy training finished
---------------------
gamma: 0.16380502419149776
training start after waiting for 1.1661956310272217 seconds
policy loss:104.22769927978516
value loss:7.342411994934082
entropies:30.590003967285156
Policy training finished
---------------------
gamma: 0.16380502419149776
training start after waiting for 1.1896469593048096 seconds
policy loss:148.27813720703125
value loss:12.607603073120117
entropies:26.779098510742188
Policy training finished
---------------------
gamma: 0.16380502419149776
training start after waiting for 1.186004400253296 seconds
policy loss:-571.4517211914062
value loss:18.627025604248047
entropies:31.382333755493164
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1210.0135)
ToM Target loss= tensor(2331.9065)
optimized based on ToM loss
---------------------
gamma: 0.16413263423988075
training start after waiting for 1.202636957168579 seconds
policy loss:-855.3569946289062
value loss:10.836287498474121
entropies:41.29573059082031
Policy training finished
---------------------
gamma: 0.16413263423988075
training start after waiting for 1.186988353729248 seconds
policy loss:-660.2454223632812
value loss:29.2712459564209
entropies:38.61326599121094
Policy training finished
---------------------
gamma: 0.16413263423988075
training start after waiting for 1.1465659141540527 seconds
policy loss:574.6681518554688
value loss:6.919292449951172
entropies:24.9916934967041
Policy training finished
---------------------
gamma: 0.16413263423988075
training start after waiting for 1.1467084884643555 seconds
policy loss:-285.1829833984375
value loss:22.3911075592041
entropies:40.86871337890625
Policy training finished
---------------------
gamma: 0.16413263423988075
training start after waiting for 1.1848084926605225 seconds
policy loss:-422.1878662109375
value loss:35.76994705200195
entropies:36.387542724609375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1269.5142)
ToM Target loss= tensor(2247.8967)
optimized based on ToM loss
---------------------
gamma: 0.16446089950836051
training start after waiting for 1.1930491924285889 seconds
policy loss:7.14351224899292
value loss:8.874794960021973
entropies:35.52040100097656
Policy training finished
---------------------
gamma: 0.16446089950836051
training start after waiting for 1.1392080783843994 seconds
policy loss:176.93197631835938
value loss:4.808766841888428
entropies:21.153051376342773
Policy training finished
---------------------
gamma: 0.16446089950836051
training start after waiting for 1.147930383682251 seconds
policy loss:-1017.6887817382812
value loss:19.592451095581055
entropies:30.887859344482422
Policy training finished
---------------------
gamma: 0.16446089950836051
training start after waiting for 1.1799101829528809 seconds
policy loss:7.760643482208252
value loss:12.598949432373047
entropies:19.561260223388672
Policy training finished
---------------------
gamma: 0.16446089950836051
training start after waiting for 1.1774160861968994 seconds
policy loss:-545.251220703125
value loss:5.74733304977417
entropies:18.03970718383789
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1183.3379)
ToM Target loss= tensor(2322.8621)
optimized based on ToM loss
---------------------
gamma: 0.16478982130737724
training start after waiting for 1.2133121490478516 seconds
policy loss:-64.84919738769531
value loss:11.021965026855469
entropies:30.458282470703125
Policy training finished
---------------------
gamma: 0.16478982130737724
training start after waiting for 1.2013521194458008 seconds
policy loss:-255.90426635742188
value loss:5.756082534790039
entropies:13.093950271606445
Policy training finished
---------------------
gamma: 0.16478982130737724
training start after waiting for 1.1681599617004395 seconds
policy loss:-166.8782958984375
value loss:6.763646602630615
entropies:27.817733764648438
Policy training finished
---------------------
gamma: 0.16478982130737724
training start after waiting for 1.1295225620269775 seconds
policy loss:-362.10198974609375
value loss:6.503541946411133
entropies:22.14824867248535
Policy training finished
---------------------
gamma: 0.16478982130737724
training start after waiting for 1.2337925434112549 seconds
policy loss:-924.7845458984375
value loss:14.940469741821289
entropies:27.67129898071289
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1079.5739)
ToM Target loss= tensor(2291.3616)
optimized based on ToM loss
---------------------
gamma: 0.165119400949992
training start after waiting for 1.1433799266815186 seconds
policy loss:-750.78759765625
value loss:11.821002006530762
entropies:19.60845184326172
Policy training finished
---------------------
gamma: 0.165119400949992
training start after waiting for 1.1906318664550781 seconds
policy loss:-665.5277709960938
value loss:19.789663314819336
entropies:23.964645385742188
Policy training finished
---------------------
gamma: 0.165119400949992
training start after waiting for 1.2162303924560547 seconds
policy loss:-138.2547607421875
value loss:4.921407222747803
entropies:8.525444030761719
Policy training finished
---------------------
gamma: 0.165119400949992
training start after waiting for 1.1780292987823486 seconds
policy loss:-697.7191162109375
value loss:14.614822387695312
entropies:33.61383056640625
Policy training finished
---------------------
gamma: 0.165119400949992
training start after waiting for 1.1771023273468018 seconds
policy loss:-98.37418365478516
value loss:11.613717079162598
entropies:22.506187438964844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1142.0919)
ToM Target loss= tensor(2357.4182)
optimized based on ToM loss
---------------------
gamma: 0.16544963975189197
training start after waiting for 1.243208646774292 seconds
policy loss:-441.18048095703125
value loss:11.938350677490234
entropies:26.195846557617188
Policy training finished
---------------------
gamma: 0.16544963975189197
training start after waiting for 1.1970632076263428 seconds
policy loss:-138.443603515625
value loss:13.549124717712402
entropies:35.31103515625
Policy training finished
---------------------
gamma: 0.16544963975189197
training start after waiting for 1.1839444637298584 seconds
policy loss:151.13319396972656
value loss:9.224254608154297
entropies:27.33155059814453
Policy training finished
---------------------
gamma: 0.16544963975189197
training start after waiting for 1.1919856071472168 seconds
policy loss:-412.2680358886719
value loss:15.121417999267578
entropies:36.59294128417969
Policy training finished
---------------------
gamma: 0.16544963975189197
training start after waiting for 1.181396245956421 seconds
policy loss:-1067.300537109375
value loss:21.47107696533203
entropies:33.51886749267578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1204.5475)
ToM Target loss= tensor(2193.3628)
optimized based on ToM loss
---------------------
gamma: 0.16578053903139575
training start after waiting for 1.194275140762329 seconds
policy loss:-745.4522094726562
value loss:24.998273849487305
entropies:27.25449562072754
Policy training finished
---------------------
gamma: 0.16578053903139575
training start after waiting for 1.1498169898986816 seconds
policy loss:-919.4513549804688
value loss:25.808319091796875
entropies:48.07339859008789
Policy training finished
---------------------
gamma: 0.16578053903139575
training start after waiting for 1.2142419815063477 seconds
policy loss:-243.71890258789062
value loss:7.317821502685547
entropies:29.20975685119629
Policy training finished
---------------------
gamma: 0.16578053903139575
training start after waiting for 1.1566295623779297 seconds
policy loss:-798.3644409179688
value loss:16.079204559326172
entropies:31.428428649902344
Policy training finished
---------------------
gamma: 0.16578053903139575
training start after waiting for 1.2000494003295898 seconds
policy loss:-509.95892333984375
value loss:8.965763092041016
entropies:43.15966796875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1194.2463)
ToM Target loss= tensor(2222.9143)
optimized based on ToM loss
---------------------
gamma: 0.16611210010945854
training start after waiting for 1.1907060146331787 seconds
policy loss:81.36284637451172
value loss:11.353490829467773
entropies:20.577220916748047
Policy training finished
---------------------
gamma: 0.16611210010945854
training start after waiting for 1.1880526542663574 seconds
policy loss:-442.79693603515625
value loss:10.917298316955566
entropies:39.31810760498047
Policy training finished
---------------------
gamma: 0.16611210010945854
training start after waiting for 1.1822843551635742 seconds
policy loss:-127.85684204101562
value loss:12.55012321472168
entropies:32.90203857421875
Policy training finished
---------------------
gamma: 0.16611210010945854
training start after waiting for 1.1888048648834229 seconds
policy loss:78.35362243652344
value loss:6.236393451690674
entropies:15.950197219848633
Policy training finished
---------------------
gamma: 0.16611210010945854
training start after waiting for 1.167233943939209 seconds
policy loss:-88.72518157958984
value loss:4.24553108215332
entropies:23.856414794921875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1114.5596)
ToM Target loss= tensor(2258.1313)
optimized based on ToM loss
---------------------
gamma: 0.16644432430967745
training start after waiting for 1.1771316528320312 seconds
policy loss:-2074.8447265625
value loss:33.70475769042969
entropies:53.589317321777344
Policy training finished
---------------------
gamma: 0.16644432430967745
training start after waiting for 1.1416494846343994 seconds
policy loss:-1671.9981689453125
value loss:57.66941833496094
entropies:47.446353912353516
Policy training finished
---------------------
gamma: 0.16644432430967745
training start after waiting for 1.142890214920044 seconds
policy loss:-494.6731262207031
value loss:7.796875476837158
entropies:24.46170997619629
Policy training finished
---------------------
gamma: 0.16644432430967745
training start after waiting for 1.200467824935913 seconds
policy loss:-1470.45947265625
value loss:19.425378799438477
entropies:34.08411407470703
Policy training finished
---------------------
gamma: 0.16644432430967745
training start after waiting for 1.1640994548797607 seconds
policy loss:-537.4458618164062
value loss:36.52632141113281
entropies:43.85118865966797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1368.6738)
ToM Target loss= tensor(2234.5415)
optimized based on ToM loss
---------------------
gamma: 0.1667772129582968
training start after waiting for 1.1632664203643799 seconds
policy loss:135.12841796875
value loss:26.58626937866211
entropies:41.68721389770508
Policy training finished
---------------------
gamma: 0.1667772129582968
training start after waiting for 1.198045253753662 seconds
policy loss:99.73587799072266
value loss:21.971342086791992
entropies:23.241756439208984
Policy training finished
---------------------
gamma: 0.1667772129582968
training start after waiting for 1.2074580192565918 seconds
policy loss:-346.2342529296875
value loss:13.748117446899414
entropies:33.40618133544922
Policy training finished
---------------------
gamma: 0.1667772129582968
training start after waiting for 1.1761116981506348 seconds
policy loss:-458.31402587890625
value loss:13.358542442321777
entropies:32.23551940917969
Policy training finished
---------------------
gamma: 0.1667772129582968
training start after waiting for 1.1935973167419434 seconds
policy loss:447.0397644042969
value loss:7.415578842163086
entropies:21.20634651184082
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1165.1006)
ToM Target loss= tensor(2310.7971)
optimized based on ToM loss
---------------------
gamma: 0.1671107673842134
training start after waiting for 1.208794355392456 seconds
policy loss:-616.7307739257812
value loss:14.50629997253418
entropies:40.606536865234375
Policy training finished
---------------------
gamma: 0.1671107673842134
training start after waiting for 1.2100508213043213 seconds
policy loss:-765.698486328125
value loss:16.914039611816406
entropies:36.79085922241211
Policy training finished
---------------------
gamma: 0.1671107673842134
training start after waiting for 1.1402699947357178 seconds
policy loss:-373.47222900390625
value loss:12.905932426452637
entropies:26.550020217895508
Policy training finished
---------------------
gamma: 0.1671107673842134
training start after waiting for 1.197618007659912 seconds
policy loss:-582.4733276367188
value loss:13.06187915802002
entropies:34.88115692138672
Policy training finished
---------------------
gamma: 0.1671107673842134
training start after waiting for 1.1371610164642334 seconds
policy loss:167.69570922851562
value loss:5.15958309173584
entropies:16.67367935180664
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1283.6852)
ToM Target loss= tensor(2280.1147)
optimized based on ToM loss
---------------------
gamma: 0.16744498891898182
training start after waiting for 1.145287036895752 seconds
policy loss:80.8121109008789
value loss:6.599226951599121
entropies:26.20958709716797
Policy training finished
---------------------
gamma: 0.16744498891898182
training start after waiting for 1.210052728652954 seconds
policy loss:-928.43310546875
value loss:53.02290344238281
entropies:35.097862243652344
Policy training finished
---------------------
gamma: 0.16744498891898182
training start after waiting for 1.1738603115081787 seconds
policy loss:-125.53191375732422
value loss:11.107231140136719
entropies:23.069435119628906
Policy training finished
---------------------
gamma: 0.16744498891898182
training start after waiting for 1.185210943222046 seconds
policy loss:166.20668029785156
value loss:8.9337739944458
entropies:22.301612854003906
Policy training finished
---------------------
gamma: 0.16744498891898182
training start after waiting for 1.1762382984161377 seconds
policy loss:-265.8883972167969
value loss:12.169936180114746
entropies:28.909313201904297
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1194.5610)
ToM Target loss= tensor(2325.3926)
optimized based on ToM loss
---------------------
gamma: 0.16777987889681978
training start after waiting for 1.196939468383789 seconds
policy loss:-454.58099365234375
value loss:11.025392532348633
entropies:18.20208168029785
Policy training finished
---------------------
gamma: 0.16777987889681978
training start after waiting for 1.1998119354248047 seconds
policy loss:-170.05313110351562
value loss:24.35878562927246
entropies:29.525592803955078
Policy training finished
---------------------
gamma: 0.16777987889681978
training start after waiting for 1.2042369842529297 seconds
policy loss:-410.5080261230469
value loss:8.526337623596191
entropies:24.510881423950195
Policy training finished
---------------------
gamma: 0.16777987889681978
training start after waiting for 1.1436347961425781 seconds
policy loss:-193.88327026367188
value loss:11.388137817382812
entropies:19.194232940673828
Policy training finished
---------------------
gamma: 0.16777987889681978
training start after waiting for 1.1858413219451904 seconds
policy loss:-1175.9129638671875
value loss:37.63320541381836
entropies:23.781362533569336
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1175.4939)
ToM Target loss= tensor(2266.3276)
optimized based on ToM loss
---------------------
gamma: 0.16811543865461342
training start after waiting for 1.1769177913665771 seconds
policy loss:-25.1270694732666
value loss:14.218666076660156
entropies:29.211017608642578
Policy training finished
---------------------
gamma: 0.16811543865461342
training start after waiting for 1.2112696170806885 seconds
policy loss:-967.7163696289062
value loss:33.56109619140625
entropies:42.96512222290039
Policy training finished
---------------------
gamma: 0.16811543865461342
training start after waiting for 1.1767466068267822 seconds
policy loss:-254.98248291015625
value loss:9.864343643188477
entropies:32.38787078857422
Policy training finished
---------------------
gamma: 0.16811543865461342
training start after waiting for 1.1792526245117188 seconds
policy loss:610.7464599609375
value loss:12.845062255859375
entropies:27.107086181640625
Policy training finished
---------------------
gamma: 0.16811543865461342
training start after waiting for 1.2132904529571533 seconds
policy loss:354.7023620605469
value loss:7.286545753479004
entropies:19.10003662109375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1157.8472)
ToM Target loss= tensor(2225.6938)
optimized based on ToM loss
---------------------
gamma: 0.16845166953192264
training start after waiting for 1.200303077697754 seconds
policy loss:-1634.7095947265625
value loss:28.602325439453125
entropies:43.4481086730957
Policy training finished
---------------------
gamma: 0.16845166953192264
training start after waiting for 1.183584451675415 seconds
policy loss:-429.1317443847656
value loss:18.861209869384766
entropies:26.519275665283203
Policy training finished
---------------------
gamma: 0.16845166953192264
training start after waiting for 1.1482436656951904 seconds
policy loss:-39.585838317871094
value loss:14.418257713317871
entropies:28.07722282409668
Policy training finished
---------------------
gamma: 0.16845166953192264
training start after waiting for 1.2088429927825928 seconds
policy loss:-258.4005126953125
value loss:14.895651817321777
entropies:28.286235809326172
Policy training finished
---------------------
gamma: 0.16845166953192264
training start after waiting for 1.1940727233886719 seconds
policy loss:-1504.8743896484375
value loss:29.933242797851562
entropies:33.84485626220703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1219.3942)
ToM Target loss= tensor(2291.9067)
optimized based on ToM loss
---------------------
gamma: 0.16878857287098648
training start after waiting for 1.182183027267456 seconds
policy loss:-546.8035888671875
value loss:14.482691764831543
entropies:21.994483947753906
Policy training finished
---------------------
gamma: 0.16878857287098648
training start after waiting for 1.1358277797698975 seconds
policy loss:-62.42452621459961
value loss:9.169641494750977
entropies:20.180212020874023
Policy training finished
---------------------
gamma: 0.16878857287098648
training start after waiting for 1.134183406829834 seconds
policy loss:-562.4189453125
value loss:28.428874969482422
entropies:35.47018051147461
Policy training finished
---------------------
gamma: 0.16878857287098648
training start after waiting for 1.151099681854248 seconds
policy loss:-105.6065673828125
value loss:12.74039077758789
entropies:36.567665100097656
Policy training finished
---------------------
gamma: 0.16878857287098648
training start after waiting for 1.1860806941986084 seconds
policy loss:-101.21861267089844
value loss:16.171648025512695
entropies:14.097554206848145
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1207.3024)
ToM Target loss= tensor(2281.3132)
optimized based on ToM loss
---------------------
gamma: 0.16912615001672845
training start after waiting for 1.186826467514038 seconds
policy loss:-198.66140747070312
value loss:8.242283821105957
entropies:18.65713119506836
Policy training finished
---------------------
gamma: 0.16912615001672845
training start after waiting for 1.1438424587249756 seconds
policy loss:-169.02316284179688
value loss:15.972319602966309
entropies:30.05691909790039
Policy training finished
---------------------
gamma: 0.16912615001672845
training start after waiting for 1.1927011013031006 seconds
policy loss:-117.63740539550781
value loss:12.77314281463623
entropies:21.41994857788086
Policy training finished
---------------------
gamma: 0.16912615001672845
training start after waiting for 1.1817562580108643 seconds
policy loss:-924.0980834960938
value loss:26.166606903076172
entropies:37.823612213134766
Policy training finished
---------------------
gamma: 0.16912615001672845
training start after waiting for 1.1926560401916504 seconds
policy loss:-55.348060607910156
value loss:9.932852745056152
entropies:29.51736831665039
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1133.2429)
ToM Target loss= tensor(2235.2053)
optimized based on ToM loss
---------------------
gamma: 0.1694644023167619
training start after waiting for 1.1927874088287354 seconds
policy loss:73.49004364013672
value loss:13.004752159118652
entropies:21.35157012939453
Policy training finished
---------------------
gamma: 0.1694644023167619
training start after waiting for 1.190704584121704 seconds
policy loss:-172.29290771484375
value loss:12.218046188354492
entropies:36.50325012207031
Policy training finished
---------------------
gamma: 0.1694644023167619
training start after waiting for 1.1498842239379883 seconds
policy loss:-359.5632629394531
value loss:19.56955909729004
entropies:31.726272583007812
Policy training finished
---------------------
gamma: 0.1694644023167619
training start after waiting for 1.179624080657959 seconds
policy loss:-774.168701171875
value loss:21.904613494873047
entropies:33.945735931396484
Policy training finished
---------------------
gamma: 0.1694644023167619
training start after waiting for 1.1895999908447266 seconds
policy loss:-554.4791259765625
value loss:18.398019790649414
entropies:36.790470123291016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1225.0262)
ToM Target loss= tensor(2234.1819)
optimized based on ToM loss
---------------------
gamma: 0.16980333112139542
training start after waiting for 1.1770458221435547 seconds
policy loss:-308.6532897949219
value loss:15.357268333435059
entropies:32.297264099121094
Policy training finished
---------------------
gamma: 0.16980333112139542
training start after waiting for 1.2075583934783936 seconds
policy loss:-592.56591796875
value loss:12.963064193725586
entropies:25.59130859375
Policy training finished
---------------------
gamma: 0.16980333112139542
training start after waiting for 1.163971185684204 seconds
policy loss:-788.3372802734375
value loss:39.5568962097168
entropies:35.165348052978516
Policy training finished
---------------------
gamma: 0.16980333112139542
training start after waiting for 1.1869745254516602 seconds
policy loss:-828.8819580078125
value loss:16.13346290588379
entropies:40.43294906616211
Policy training finished
---------------------
gamma: 0.16980333112139542
training start after waiting for 1.1118338108062744 seconds
policy loss:-385.241943359375
value loss:19.667993545532227
entropies:38.67625045776367
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1226.5703)
ToM Target loss= tensor(2204.1365)
optimized based on ToM loss
---------------------
gamma: 0.1701429377836382
training start after waiting for 1.195291519165039 seconds
policy loss:-912.390869140625
value loss:24.993444442749023
entropies:21.282699584960938
Policy training finished
---------------------
gamma: 0.1701429377836382
training start after waiting for 1.1899654865264893 seconds
policy loss:-508.28277587890625
value loss:13.486751556396484
entropies:23.90064239501953
Policy training finished
---------------------
gamma: 0.1701429377836382
training start after waiting for 1.1492319107055664 seconds
policy loss:-744.1825561523438
value loss:18.407581329345703
entropies:37.42137145996094
Policy training finished
---------------------
gamma: 0.1701429377836382
training start after waiting for 1.1901519298553467 seconds
policy loss:-704.5881958007812
value loss:13.153547286987305
entropies:22.88288116455078
Policy training finished
---------------------
gamma: 0.1701429377836382
training start after waiting for 1.1963143348693848 seconds
policy loss:-1825.6456298828125
value loss:42.804866790771484
entropies:35.19266891479492
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1174.2937)
ToM Target loss= tensor(2259.1187)
optimized based on ToM loss
---------------------
gamma: 0.17048322365920549
training start after waiting for 1.1685030460357666 seconds
policy loss:-449.5679931640625
value loss:11.458549499511719
entropies:29.464900970458984
Policy training finished
---------------------
gamma: 0.17048322365920549
training start after waiting for 1.1494855880737305 seconds
policy loss:-389.1527099609375
value loss:12.078266143798828
entropies:23.278820037841797
Policy training finished
---------------------
gamma: 0.17048322365920549
training start after waiting for 1.141920566558838 seconds
policy loss:-463.983154296875
value loss:6.171595573425293
entropies:21.106403350830078
Policy training finished
---------------------
gamma: 0.17048322365920549
training start after waiting for 1.1528329849243164 seconds
policy loss:-468.527587890625
value loss:20.1456356048584
entropies:32.14678955078125
Policy training finished
---------------------
gamma: 0.17048322365920549
training start after waiting for 1.1837882995605469 seconds
policy loss:-501.0619201660156
value loss:42.35298156738281
entropies:27.755718231201172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1170.1790)
ToM Target loss= tensor(2257.1130)
optimized based on ToM loss
---------------------
gamma: 0.1708241901065239
training start after waiting for 1.147087812423706 seconds
policy loss:-166.74179077148438
value loss:23.35963249206543
entropies:19.42528533935547
Policy training finished
---------------------
gamma: 0.1708241901065239
training start after waiting for 1.123474359512329 seconds
policy loss:-139.3700714111328
value loss:20.61934471130371
entropies:37.11492919921875
Policy training finished
---------------------
gamma: 0.1708241901065239
training start after waiting for 1.148315191268921 seconds
policy loss:-261.1496887207031
value loss:15.103955268859863
entropies:19.96004867553711
Policy training finished
---------------------
gamma: 0.1708241901065239
training start after waiting for 1.209946632385254 seconds
policy loss:-204.15313720703125
value loss:17.440698623657227
entropies:35.20026779174805
Policy training finished
---------------------
gamma: 0.1708241901065239
training start after waiting for 1.1847641468048096 seconds
policy loss:-203.95860290527344
value loss:11.598162651062012
entropies:28.490802764892578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1163.3365)
ToM Target loss= tensor(2282.3279)
optimized based on ToM loss
---------------------
gamma: 0.17116583848673694
training start after waiting for 1.1455676555633545 seconds
policy loss:-708.0315551757812
value loss:10.584609985351562
entropies:28.09833335876465
Policy training finished
---------------------
gamma: 0.17116583848673694
training start after waiting for 1.1446714401245117 seconds
policy loss:-1352.040283203125
value loss:36.18764877319336
entropies:41.666465759277344
Policy training finished
---------------------
gamma: 0.17116583848673694
training start after waiting for 1.2036018371582031 seconds
policy loss:-1310.0030517578125
value loss:19.333148956298828
entropies:37.10654067993164
Policy training finished
---------------------
gamma: 0.17116583848673694
training start after waiting for 1.2058823108673096 seconds
policy loss:-1437.5697021484375
value loss:35.412574768066406
entropies:33.51831817626953
Policy training finished
---------------------
gamma: 0.17116583848673694
training start after waiting for 1.2200396060943604 seconds
policy loss:-1169.4891357421875
value loss:35.61925506591797
entropies:30.758487701416016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1257.3840)
ToM Target loss= tensor(2290.0330)
optimized based on ToM loss
---------------------
gamma: 0.17150817016371042
training start after waiting for 1.1413288116455078 seconds
policy loss:217.68707275390625
value loss:11.438652992248535
entropies:30.51934814453125
Policy training finished
---------------------
gamma: 0.17150817016371042
training start after waiting for 1.1824843883514404 seconds
policy loss:-1370.0828857421875
value loss:22.754451751708984
entropies:31.702482223510742
Policy training finished
---------------------
gamma: 0.17150817016371042
training start after waiting for 1.1889371871948242 seconds
policy loss:-179.69830322265625
value loss:16.109548568725586
entropies:21.92431640625
Policy training finished
---------------------
gamma: 0.17150817016371042
training start after waiting for 1.1883835792541504 seconds
policy loss:315.73809814453125
value loss:15.768539428710938
entropies:24.783599853515625
Policy training finished
---------------------
gamma: 0.17150817016371042
training start after waiting for 1.1451685428619385 seconds
policy loss:-496.0849609375
value loss:20.108476638793945
entropies:32.39308547973633
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1169.5282)
ToM Target loss= tensor(2301.5593)
optimized based on ToM loss
---------------------
gamma: 0.17185118650403786
training start after waiting for 1.1846280097961426 seconds
policy loss:-858.7826538085938
value loss:32.23230743408203
entropies:34.4212646484375
Policy training finished
---------------------
gamma: 0.17185118650403786
training start after waiting for 1.210899829864502 seconds
policy loss:-488.1368103027344
value loss:28.306846618652344
entropies:38.4914665222168
Policy training finished
---------------------
gamma: 0.17185118650403786
training start after waiting for 1.20046067237854 seconds
policy loss:-480.25714111328125
value loss:20.791439056396484
entropies:27.129743576049805
Policy training finished
---------------------
gamma: 0.17185118650403786
training start after waiting for 1.1845066547393799 seconds
policy loss:-248.58035278320312
value loss:23.21835708618164
entropies:31.117034912109375
Policy training finished
---------------------
gamma: 0.17185118650403786
training start after waiting for 1.2035627365112305 seconds
policy loss:-900.11376953125
value loss:19.31839370727539
entropies:49.954833984375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1220.4803)
ToM Target loss= tensor(2281.3979)
optimized based on ToM loss
---------------------
gamma: 0.17219488887704593
training start after waiting for 1.1386449337005615 seconds
policy loss:-130.02073669433594
value loss:5.333480358123779
entropies:13.176280975341797
Policy training finished
---------------------
gamma: 0.17219488887704593
training start after waiting for 1.187760353088379 seconds
policy loss:220.35719299316406
value loss:5.35921049118042
entropies:22.347793579101562
Policy training finished
---------------------
gamma: 0.17219488887704593
training start after waiting for 1.1605091094970703 seconds
policy loss:331.0220031738281
value loss:5.485978126525879
entropies:17.933895111083984
Policy training finished
---------------------
gamma: 0.17219488887704593
training start after waiting for 1.1743574142456055 seconds
policy loss:457.5157775878906
value loss:10.527488708496094
entropies:25.16054916381836
Policy training finished
---------------------
gamma: 0.17219488887704593
training start after waiting for 1.1886208057403564 seconds
policy loss:-325.45806884765625
value loss:6.341431617736816
entropies:26.20523452758789
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1235.2561)
ToM Target loss= tensor(2451.4417)
optimized based on ToM loss
---------------------
gamma: 0.1725392786548
training start after waiting for 1.145270824432373 seconds
policy loss:-1314.9818115234375
value loss:20.445175170898438
entropies:32.900352478027344
Policy training finished
---------------------
gamma: 0.1725392786548
training start after waiting for 1.1972131729125977 seconds
policy loss:-141.75782775878906
value loss:4.692989349365234
entropies:21.463783264160156
Policy training finished
---------------------
gamma: 0.1725392786548
training start after waiting for 1.1903786659240723 seconds
policy loss:-901.73974609375
value loss:13.290375709533691
entropies:42.307861328125
Policy training finished
---------------------
gamma: 0.1725392786548
training start after waiting for 1.1479716300964355 seconds
policy loss:-573.66015625
value loss:15.467952728271484
entropies:39.44763946533203
Policy training finished
---------------------
gamma: 0.1725392786548
training start after waiting for 1.2165064811706543 seconds
policy loss:-93.2297134399414
value loss:9.528582572937012
entropies:32.02733612060547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1198.3195)
ToM Target loss= tensor(2221.3657)
optimized based on ToM loss
---------------------
gamma: 0.17288435721210962
training start after waiting for 1.1356086730957031 seconds
policy loss:-123.6074447631836
value loss:21.57048988342285
entropies:35.21630096435547
Policy training finished
---------------------
gamma: 0.17288435721210962
training start after waiting for 1.1969895362854004 seconds
policy loss:-373.5813293457031
value loss:19.86295509338379
entropies:26.933284759521484
Policy training finished
---------------------
gamma: 0.17288435721210962
training start after waiting for 1.1379048824310303 seconds
policy loss:-596.9234619140625
value loss:26.437511444091797
entropies:30.463253021240234
Policy training finished
---------------------
gamma: 0.17288435721210962
training start after waiting for 1.1621437072753906 seconds
policy loss:-656.294189453125
value loss:19.80547523498535
entropies:33.153507232666016
Policy training finished
---------------------
gamma: 0.17288435721210962
training start after waiting for 1.142324686050415 seconds
policy loss:-637.2371826171875
value loss:15.575339317321777
entropies:39.31769943237305
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1221.7087)
ToM Target loss= tensor(2221.8201)
optimized based on ToM loss
---------------------
gamma: 0.17323012592653383
training start after waiting for 1.1874220371246338 seconds
policy loss:13.28434944152832
value loss:2.658628225326538
entropies:14.790997505187988
Policy training finished
---------------------
gamma: 0.17323012592653383
training start after waiting for 1.1743240356445312 seconds
policy loss:-350.6296691894531
value loss:12.466312408447266
entropies:25.581302642822266
Policy training finished
---------------------
gamma: 0.17323012592653383
training start after waiting for 1.183711051940918 seconds
policy loss:-904.2055053710938
value loss:14.118215560913086
entropies:34.67591857910156
Policy training finished
---------------------
gamma: 0.17323012592653383
training start after waiting for 1.1785578727722168 seconds
policy loss:-370.76470947265625
value loss:23.47419548034668
entropies:33.42168426513672
Policy training finished
---------------------
gamma: 0.17323012592653383
training start after waiting for 1.190260648727417 seconds
policy loss:260.42071533203125
value loss:6.586097717285156
entropies:25.20806884765625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1207.8254)
ToM Target loss= tensor(2251.8564)
optimized based on ToM loss
---------------------
gamma: 0.1735765861783869
training start after waiting for 1.1994647979736328 seconds
policy loss:-246.82916259765625
value loss:12.274617195129395
entropies:26.280370712280273
Policy training finished
---------------------
gamma: 0.1735765861783869
training start after waiting for 1.1931614875793457 seconds
policy loss:-236.8037109375
value loss:7.556647777557373
entropies:33.19845199584961
Policy training finished
---------------------
gamma: 0.1735765861783869
training start after waiting for 1.1385793685913086 seconds
policy loss:-1688.8565673828125
value loss:61.962806701660156
entropies:36.62995147705078
Policy training finished
---------------------
gamma: 0.1735765861783869
training start after waiting for 1.1918895244598389 seconds
policy loss:38.1906852722168
value loss:8.11312484741211
entropies:38.59626007080078
Policy training finished
---------------------
gamma: 0.1735765861783869
training start after waiting for 1.136425495147705 seconds
policy loss:-1289.2408447265625
value loss:10.666930198669434
entropies:35.29412078857422
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1205.8616)
ToM Target loss= tensor(2213.2588)
optimized based on ToM loss
---------------------
gamma: 0.17392373935074368
training start after waiting for 1.1800422668457031 seconds
policy loss:-75.87808227539062
value loss:14.486230850219727
entropies:27.842891693115234
Policy training finished
---------------------
gamma: 0.17392373935074368
training start after waiting for 1.1899268627166748 seconds
policy loss:162.74746704101562
value loss:8.61672592163086
entropies:21.50143814086914
Policy training finished
---------------------
gamma: 0.17392373935074368
training start after waiting for 1.1406164169311523 seconds
policy loss:-364.3874816894531
value loss:12.240559577941895
entropies:36.15299606323242
Policy training finished
---------------------
gamma: 0.17392373935074368
training start after waiting for 1.2727513313293457 seconds
policy loss:-1192.619873046875
value loss:28.692060470581055
entropies:41.087547302246094
Policy training finished
---------------------
gamma: 0.17392373935074368
training start after waiting for 1.1525390148162842 seconds
policy loss:-968.1065673828125
value loss:50.615257263183594
entropies:45.31242752075195
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1244.1698)
ToM Target loss= tensor(2204.0674)
optimized based on ToM loss
---------------------
gamma: 0.17427158682944516
training start after waiting for 1.148711919784546 seconds
policy loss:262.32086181640625
value loss:28.13601303100586
entropies:35.373817443847656
Policy training finished
---------------------
gamma: 0.17427158682944516
training start after waiting for 1.145806074142456 seconds
policy loss:-6.593135833740234
value loss:16.64838981628418
entropies:36.0327262878418
Policy training finished
---------------------
gamma: 0.17427158682944516
training start after waiting for 1.1690998077392578 seconds
policy loss:-31.106172561645508
value loss:15.42833137512207
entropies:30.69229507446289
Policy training finished
---------------------
gamma: 0.17427158682944516
training start after waiting for 1.1291370391845703 seconds
policy loss:-727.6344604492188
value loss:23.33399200439453
entropies:34.671085357666016
Policy training finished
---------------------
gamma: 0.17427158682944516
training start after waiting for 1.197584867477417 seconds
policy loss:-375.9087219238281
value loss:11.620415687561035
entropies:29.63729476928711
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1140.1266)
ToM Target loss= tensor(2205.9402)
optimized based on ToM loss
---------------------
gamma: 0.17462013000310406
training start after waiting for 1.18033766746521 seconds
policy loss:-94.7015380859375
value loss:6.676609039306641
entropies:24.007009506225586
Policy training finished
---------------------
gamma: 0.17462013000310406
training start after waiting for 1.2121644020080566 seconds
policy loss:-784.5850830078125
value loss:18.205875396728516
entropies:36.93673324584961
Policy training finished
---------------------
gamma: 0.17462013000310406
training start after waiting for 1.2040657997131348 seconds
policy loss:-794.4088134765625
value loss:13.457862854003906
entropies:35.09239959716797
Policy training finished
---------------------
gamma: 0.17462013000310406
training start after waiting for 1.1938602924346924 seconds
policy loss:-44.773746490478516
value loss:9.085556030273438
entropies:21.89348030090332
Policy training finished
---------------------
gamma: 0.17462013000310406
training start after waiting for 1.1433475017547607 seconds
policy loss:-1651.995361328125
value loss:42.179378509521484
entropies:39.82999038696289
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1285.9447)
ToM Target loss= tensor(2251.4541)
optimized based on ToM loss
---------------------
gamma: 0.17496937026311027
training start after waiting for 1.179300308227539 seconds
policy loss:-80.87619018554688
value loss:7.013614654541016
entropies:30.048927307128906
Policy training finished
---------------------
gamma: 0.17496937026311027
training start after waiting for 1.1580514907836914 seconds
policy loss:143.39292907714844
value loss:7.644074440002441
entropies:20.799835205078125
Policy training finished
---------------------
gamma: 0.17496937026311027
training start after waiting for 1.160698413848877 seconds
policy loss:362.65350341796875
value loss:8.3119535446167
entropies:22.467845916748047
Policy training finished
---------------------
gamma: 0.17496937026311027
training start after waiting for 1.1823935508728027 seconds
policy loss:-97.06642150878906
value loss:4.336453914642334
entropies:20.651535034179688
Policy training finished
---------------------
gamma: 0.17496937026311027
training start after waiting for 1.147404432296753 seconds
policy loss:-465.19329833984375
value loss:9.84520435333252
entropies:22.764690399169922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1136.8141)
ToM Target loss= tensor(2307.8142)
optimized based on ToM loss
---------------------
gamma: 0.1753193090036365
training start after waiting for 1.1469902992248535 seconds
policy loss:-413.90283203125
value loss:9.757499694824219
entropies:23.676240921020508
Policy training finished
---------------------
gamma: 0.1753193090036365
training start after waiting for 1.2046630382537842 seconds
policy loss:254.40272521972656
value loss:18.676029205322266
entropies:28.536691665649414
Policy training finished
---------------------
gamma: 0.1753193090036365
training start after waiting for 1.1805481910705566 seconds
policy loss:-1368.066650390625
value loss:45.25669479370117
entropies:18.51847267150879
Policy training finished
---------------------
gamma: 0.1753193090036365
training start after waiting for 1.2095279693603516 seconds
policy loss:-0.7490701675415039
value loss:13.629438400268555
entropies:28.674739837646484
Policy training finished
---------------------
gamma: 0.1753193090036365
training start after waiting for 1.1201984882354736 seconds
policy loss:-137.85313415527344
value loss:11.958776473999023
entropies:22.792823791503906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1154.7236)
ToM Target loss= tensor(2326.3804)
optimized based on ToM loss
---------------------
gamma: 0.17566994762164376
training start after waiting for 1.2134523391723633 seconds
policy loss:6.836749076843262
value loss:10.92546272277832
entropies:20.06208610534668
Policy training finished
---------------------
gamma: 0.17566994762164376
training start after waiting for 1.2065634727478027 seconds
policy loss:-746.51611328125
value loss:25.473905563354492
entropies:40.38801574707031
Policy training finished
---------------------
gamma: 0.17566994762164376
training start after waiting for 1.1457929611206055 seconds
policy loss:-621.9160766601562
value loss:15.031839370727539
entropies:27.963211059570312
Policy training finished
---------------------
gamma: 0.17566994762164376
training start after waiting for 1.1580801010131836 seconds
policy loss:-443.8961181640625
value loss:20.1456298828125
entropies:23.218223571777344
Policy training finished
---------------------
gamma: 0.17566994762164376
training start after waiting for 1.1664881706237793 seconds
policy loss:-874.8159790039062
value loss:26.598072052001953
entropies:34.670772552490234
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1143.6948)
ToM Target loss= tensor(2172.3684)
optimized based on ToM loss
---------------------
gamma: 0.17602128751688703
training start after waiting for 1.1449713706970215 seconds
policy loss:-787.7505493164062
value loss:29.826400756835938
entropies:36.11071014404297
Policy training finished
---------------------
gamma: 0.17602128751688703
training start after waiting for 1.206261157989502 seconds
policy loss:-126.13568878173828
value loss:21.660673141479492
entropies:40.32195281982422
Policy training finished
---------------------
gamma: 0.17602128751688703
training start after waiting for 1.1647539138793945 seconds
policy loss:-316.9148254394531
value loss:10.919472694396973
entropies:28.376144409179688
Policy training finished
---------------------
gamma: 0.17602128751688703
training start after waiting for 1.1465506553649902 seconds
policy loss:-1046.864013671875
value loss:22.468067169189453
entropies:32.39558410644531
Policy training finished
---------------------
gamma: 0.17602128751688703
training start after waiting for 1.1852045059204102 seconds
policy loss:-250.45542907714844
value loss:12.8450927734375
entropies:25.509462356567383
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1242.7347)
ToM Target loss= tensor(2291.5425)
optimized based on ToM loss
---------------------
gamma: 0.1763733300919208
training start after waiting for 1.169816255569458 seconds
policy loss:-592.929443359375
value loss:14.641974449157715
entropies:30.087371826171875
Policy training finished
---------------------
gamma: 0.1763733300919208
training start after waiting for 1.1751079559326172 seconds
policy loss:-1160.559326171875
value loss:25.476797103881836
entropies:38.047367095947266
Policy training finished
---------------------
gamma: 0.1763733300919208
training start after waiting for 1.1655073165893555 seconds
policy loss:-6.7688307762146
value loss:10.950485229492188
entropies:23.16584014892578
Policy training finished
---------------------
gamma: 0.1763733300919208
training start after waiting for 1.1807861328125 seconds
policy loss:-511.0171813964844
value loss:18.56460189819336
entropies:30.394786834716797
Policy training finished
---------------------
gamma: 0.1763733300919208
training start after waiting for 1.1945757865905762 seconds
policy loss:-298.1877136230469
value loss:30.781959533691406
entropies:23.912220001220703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1148.7145)
ToM Target loss= tensor(2210.7983)
optimized based on ToM loss
---------------------
gamma: 0.17672607675210464
training start after waiting for 1.1693322658538818 seconds
policy loss:-1602.082275390625
value loss:25.537353515625
entropies:37.732879638671875
Policy training finished
---------------------
gamma: 0.17672607675210464
training start after waiting for 1.1401739120483398 seconds
policy loss:-462.41888427734375
value loss:26.23845863342285
entropies:40.084808349609375
Policy training finished
---------------------
gamma: 0.17672607675210464
training start after waiting for 1.1339082717895508 seconds
policy loss:-145.0003662109375
value loss:17.985755920410156
entropies:38.351951599121094
Policy training finished
---------------------
gamma: 0.17672607675210464
training start after waiting for 1.147240161895752 seconds
policy loss:-62.99924850463867
value loss:7.95622444152832
entropies:32.81688690185547
Policy training finished
---------------------
gamma: 0.17672607675210464
training start after waiting for 1.173853874206543 seconds
policy loss:-682.1881103515625
value loss:17.889339447021484
entropies:26.703510284423828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1284.8411)
ToM Target loss= tensor(2264.4819)
optimized based on ToM loss
---------------------
gamma: 0.17707952890560885
training start after waiting for 1.1783063411712646 seconds
policy loss:-1430.915283203125
value loss:24.473791122436523
entropies:32.233970642089844
Policy training finished
---------------------
gamma: 0.17707952890560885
training start after waiting for 1.1706888675689697 seconds
policy loss:-750.0629272460938
value loss:15.349871635437012
entropies:26.65203857421875
Policy training finished
---------------------
gamma: 0.17707952890560885
training start after waiting for 1.1369805335998535 seconds
policy loss:-769.5982666015625
value loss:12.127609252929688
entropies:34.49858474731445
Policy training finished
---------------------
gamma: 0.17707952890560885
training start after waiting for 1.2060527801513672 seconds
policy loss:-321.52947998046875
value loss:5.877331256866455
entropies:26.252262115478516
Policy training finished
---------------------
gamma: 0.17707952890560885
training start after waiting for 1.1402993202209473 seconds
policy loss:-414.0733947753906
value loss:10.900264739990234
entropies:21.058387756347656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1215.9858)
ToM Target loss= tensor(2308.3086)
optimized based on ToM loss
---------------------
gamma: 0.17743368796342007
training start after waiting for 1.1884393692016602 seconds
policy loss:-482.5241394042969
value loss:19.02142333984375
entropies:37.251068115234375
Policy training finished
---------------------
gamma: 0.17743368796342007
training start after waiting for 1.2393124103546143 seconds
policy loss:-180.1562957763672
value loss:21.99699592590332
entropies:38.76210021972656
Policy training finished
---------------------
gamma: 0.17743368796342007
training start after waiting for 1.1436619758605957 seconds
policy loss:652.8319702148438
value loss:20.297361373901367
entropies:31.538833618164062
Policy training finished
---------------------
gamma: 0.17743368796342007
training start after waiting for 1.2056195735931396 seconds
policy loss:221.6060333251953
value loss:25.17964744567871
entropies:34.474456787109375
Policy training finished
---------------------
gamma: 0.17743368796342007
training start after waiting for 1.1565978527069092 seconds
policy loss:-10.489248275756836
value loss:13.290214538574219
entropies:31.88152503967285
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1219.1194)
ToM Target loss= tensor(2168.4194)
optimized based on ToM loss
---------------------
gamma: 0.17778855533934693
training start after waiting for 1.1723015308380127 seconds
policy loss:-929.9052734375
value loss:28.216514587402344
entropies:50.861183166503906
Policy training finished
---------------------
gamma: 0.17778855533934693
training start after waiting for 1.1367363929748535 seconds
policy loss:-178.3477783203125
value loss:16.201187133789062
entropies:34.44437026977539
Policy training finished
---------------------
gamma: 0.17778855533934693
training start after waiting for 1.1498701572418213 seconds
policy loss:-891.7943725585938
value loss:23.072656631469727
entropies:41.26085662841797
Policy training finished
---------------------
gamma: 0.17778855533934693
training start after waiting for 1.2485878467559814 seconds
policy loss:339.0841369628906
value loss:10.171183586120605
entropies:24.58152198791504
Policy training finished
---------------------
gamma: 0.17778855533934693
training start after waiting for 1.1960458755493164 seconds
policy loss:-43.612335205078125
value loss:17.440547943115234
entropies:25.466995239257812
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1293.9509)
ToM Target loss= tensor(2224.8860)
optimized based on ToM loss
---------------------
gamma: 0.1781441324500256
training start after waiting for 1.1826262474060059 seconds
policy loss:343.14093017578125
value loss:13.299373626708984
entropies:31.000404357910156
Policy training finished
---------------------
gamma: 0.1781441324500256
training start after waiting for 1.2010321617126465 seconds
policy loss:296.1820373535156
value loss:9.796667098999023
entropies:22.519981384277344
Policy training finished
---------------------
gamma: 0.1781441324500256
training start after waiting for 1.1723880767822266 seconds
policy loss:108.47398376464844
value loss:8.607192039489746
entropies:23.677797317504883
Policy training finished
---------------------
gamma: 0.1781441324500256
training start after waiting for 1.1555814743041992 seconds
policy loss:-1120.6693115234375
value loss:19.482545852661133
entropies:25.70266342163086
Policy training finished
---------------------
gamma: 0.1781441324500256
training start after waiting for 1.140105962753296 seconds
policy loss:-90.0749740600586
value loss:3.6872105598449707
entropies:15.62850570678711
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1096.9736)
ToM Target loss= tensor(2230.5730)
optimized based on ToM loss
---------------------
gamma: 0.17850042071492567
training start after waiting for 1.2110545635223389 seconds
policy loss:-916.258056640625
value loss:15.483189582824707
entropies:24.486919403076172
Policy training finished
---------------------
gamma: 0.17850042071492567
training start after waiting for 1.1965253353118896 seconds
policy loss:123.6314926147461
value loss:7.383692741394043
entropies:23.210172653198242
Policy training finished
---------------------
gamma: 0.17850042071492567
training start after waiting for 1.1510939598083496 seconds
policy loss:-563.5552978515625
value loss:22.354843139648438
entropies:22.134033203125
Policy training finished
---------------------
gamma: 0.17850042071492567
training start after waiting for 1.1684002876281738 seconds
policy loss:-588.3624267578125
value loss:15.53770923614502
entropies:29.286012649536133
Policy training finished
---------------------
gamma: 0.17850042071492567
training start after waiting for 1.151646375656128 seconds
policy loss:-245.04238891601562
value loss:29.521011352539062
entropies:18.610153198242188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1109.8772)
ToM Target loss= tensor(2227.6191)
optimized based on ToM loss
---------------------
gamma: 0.1788574215563555
training start after waiting for 1.1907446384429932 seconds
policy loss:-3020.046875
value loss:91.14949035644531
entropies:47.419620513916016
Policy training finished
---------------------
gamma: 0.1788574215563555
training start after waiting for 1.2008612155914307 seconds
policy loss:-374.07989501953125
value loss:22.39202880859375
entropies:31.141008377075195
Policy training finished
---------------------
gamma: 0.1788574215563555
training start after waiting for 1.185471773147583 seconds
policy loss:56.149986267089844
value loss:12.302884101867676
entropies:21.126998901367188
Policy training finished
---------------------
gamma: 0.1788574215563555
training start after waiting for 1.2004563808441162 seconds
policy loss:-977.8184204101562
value loss:57.279571533203125
entropies:40.58245849609375
Policy training finished
---------------------
gamma: 0.1788574215563555
training start after waiting for 1.168226718902588 seconds
policy loss:-493.3855895996094
value loss:30.595571517944336
entropies:25.98623275756836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1263.1443)
ToM Target loss= tensor(2176.2969)
optimized based on ToM loss
---------------------
gamma: 0.1792151363994682
training start after waiting for 1.19278883934021 seconds
policy loss:282.47137451171875
value loss:8.015229225158691
entropies:14.660652160644531
Policy training finished
---------------------
gamma: 0.1792151363994682
training start after waiting for 1.2050650119781494 seconds
policy loss:-764.3034057617188
value loss:23.80831527709961
entropies:45.0433349609375
Policy training finished
---------------------
gamma: 0.1792151363994682
training start after waiting for 1.1607983112335205 seconds
policy loss:-522.6195678710938
value loss:41.73573684692383
entropies:28.248146057128906
Policy training finished
---------------------
gamma: 0.1792151363994682
training start after waiting for 1.1618845462799072 seconds
policy loss:255.07769775390625
value loss:12.521178245544434
entropies:15.652460098266602
Policy training finished
---------------------
gamma: 0.1792151363994682
training start after waiting for 1.1428375244140625 seconds
policy loss:-49.428489685058594
value loss:9.445754051208496
entropies:17.8546142578125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1061.7401)
ToM Target loss= tensor(2206.7222)
optimized based on ToM loss
---------------------
gamma: 0.17957356667226715
training start after waiting for 1.215252161026001 seconds
policy loss:-614.0050048828125
value loss:14.92502212524414
entropies:42.959190368652344
Policy training finished
---------------------
gamma: 0.17957356667226715
training start after waiting for 1.1989331245422363 seconds
policy loss:-702.254638671875
value loss:18.36947250366211
entropies:30.98054313659668
Policy training finished
---------------------
gamma: 0.17957356667226715
training start after waiting for 1.1801514625549316 seconds
policy loss:-705.606201171875
value loss:25.320707321166992
entropies:32.08467102050781
Policy training finished
---------------------
gamma: 0.17957356667226715
training start after waiting for 1.1688547134399414 seconds
policy loss:-87.56141662597656
value loss:17.867963790893555
entropies:33.673709869384766
Policy training finished
---------------------
gamma: 0.17957356667226715
training start after waiting for 1.1820826530456543 seconds
policy loss:-610.2923583984375
value loss:28.45880126953125
entropies:34.42156219482422
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1201.6812)
ToM Target loss= tensor(2210.7532)
optimized based on ToM loss
---------------------
gamma: 0.17993271380561168
training start after waiting for 1.1811857223510742 seconds
policy loss:-122.05838775634766
value loss:7.650968074798584
entropies:17.001056671142578
Policy training finished
---------------------
gamma: 0.17993271380561168
training start after waiting for 1.1500074863433838 seconds
policy loss:-132.54698181152344
value loss:7.77965784072876
entropies:23.87713623046875
Policy training finished
---------------------
gamma: 0.17993271380561168
training start after waiting for 1.1583783626556396 seconds
policy loss:-965.4531860351562
value loss:31.499452590942383
entropies:32.92576599121094
Policy training finished
---------------------
gamma: 0.17993271380561168
training start after waiting for 1.1545345783233643 seconds
policy loss:-258.16912841796875
value loss:16.530223846435547
entropies:36.615318298339844
Policy training finished
---------------------
gamma: 0.17993271380561168
training start after waiting for 1.1799321174621582 seconds
policy loss:214.0066375732422
value loss:13.939144134521484
entropies:27.01346206665039
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1190.2339)
ToM Target loss= tensor(2231.3301)
optimized based on ToM loss
---------------------
gamma: 0.1802925792332229
training start after waiting for 1.181546926498413 seconds
policy loss:-1466.9869384765625
value loss:32.25795364379883
entropies:33.745628356933594
Policy training finished
---------------------
gamma: 0.1802925792332229
training start after waiting for 1.1418569087982178 seconds
policy loss:-457.0510559082031
value loss:16.127492904663086
entropies:27.617599487304688
Policy training finished
---------------------
gamma: 0.1802925792332229
training start after waiting for 1.1951141357421875 seconds
policy loss:-268.2535705566406
value loss:27.01709747314453
entropies:43.260066986083984
Policy training finished
---------------------
gamma: 0.1802925792332229
training start after waiting for 1.205639123916626 seconds
policy loss:-773.06005859375
value loss:14.832918167114258
entropies:35.426692962646484
Policy training finished
---------------------
gamma: 0.1802925792332229
training start after waiting for 1.1485013961791992 seconds
policy loss:-785.0792236328125
value loss:24.285598754882812
entropies:27.474201202392578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1243.6676)
ToM Target loss= tensor(2264.1831)
optimized based on ToM loss
---------------------
gamma: 0.18065316439168935
training start after waiting for 1.1741142272949219 seconds
policy loss:-339.63922119140625
value loss:27.432758331298828
entropies:28.513927459716797
Policy training finished
---------------------
gamma: 0.18065316439168935
training start after waiting for 1.1925699710845947 seconds
policy loss:103.00225067138672
value loss:8.428565979003906
entropies:23.408512115478516
Policy training finished
---------------------
gamma: 0.18065316439168935
training start after waiting for 1.1910207271575928 seconds
policy loss:-890.12158203125
value loss:17.463668823242188
entropies:26.443931579589844
Policy training finished
---------------------
gamma: 0.18065316439168935
training start after waiting for 1.156376600265503 seconds
policy loss:-210.52891540527344
value loss:12.308101654052734
entropies:24.156471252441406
Policy training finished
---------------------
gamma: 0.18065316439168935
training start after waiting for 1.149916410446167 seconds
policy loss:-822.7974243164062
value loss:14.50046157836914
entropies:45.31126022338867
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1164.3668)
ToM Target loss= tensor(2254.4099)
optimized based on ToM loss
---------------------
gamma: 0.18101447072047272
training start after waiting for 1.1766223907470703 seconds
policy loss:25.432682037353516
value loss:14.58937931060791
entropies:21.543798446655273
Policy training finished
---------------------
gamma: 0.18101447072047272
training start after waiting for 1.1846411228179932 seconds
policy loss:-999.75146484375
value loss:18.212142944335938
entropies:32.76820755004883
Policy training finished
---------------------
gamma: 0.18101447072047272
training start after waiting for 1.144233226776123 seconds
policy loss:-718.6178588867188
value loss:18.391386032104492
entropies:28.481918334960938
Policy training finished
---------------------
gamma: 0.18101447072047272
training start after waiting for 1.1960365772247314 seconds
policy loss:-599.6959228515625
value loss:19.362075805664062
entropies:29.348831176757812
Policy training finished
---------------------
gamma: 0.18101447072047272
training start after waiting for 1.187753677368164 seconds
policy loss:163.00502014160156
value loss:9.199532508850098
entropies:22.421485900878906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1118.2523)
ToM Target loss= tensor(2296.3335)
optimized based on ToM loss
---------------------
gamma: 0.18137649966191366
training start after waiting for 1.1846210956573486 seconds
policy loss:200.657470703125
value loss:21.472457885742188
entropies:22.458816528320312
Policy training finished
---------------------
gamma: 0.18137649966191366
training start after waiting for 1.1955373287200928 seconds
policy loss:165.5594482421875
value loss:15.058781623840332
entropies:34.864845275878906
Policy training finished
---------------------
gamma: 0.18137649966191366
training start after waiting for 1.2139129638671875 seconds
policy loss:-689.4495849609375
value loss:25.767152786254883
entropies:39.840213775634766
Policy training finished
---------------------
gamma: 0.18137649966191366
training start after waiting for 1.1506733894348145 seconds
policy loss:-158.14707946777344
value loss:20.992660522460938
entropies:24.740306854248047
Policy training finished
---------------------
gamma: 0.18137649966191366
training start after waiting for 1.222318410873413 seconds
policy loss:-360.52862548828125
value loss:8.672576904296875
entropies:23.86975860595703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1096.3831)
ToM Target loss= tensor(2174.2932)
optimized based on ToM loss
---------------------
gamma: 0.1817392526612375
training start after waiting for 1.1809992790222168 seconds
policy loss:-728.6022338867188
value loss:35.78557205200195
entropies:32.81259536743164
Policy training finished
---------------------
gamma: 0.1817392526612375
training start after waiting for 1.141674518585205 seconds
policy loss:-544.5191650390625
value loss:19.121566772460938
entropies:40.10918045043945
Policy training finished
---------------------
gamma: 0.1817392526612375
training start after waiting for 1.1502830982208252 seconds
policy loss:-648.2316284179688
value loss:33.4085807800293
entropies:39.427146911621094
Policy training finished
---------------------
gamma: 0.1817392526612375
training start after waiting for 1.2075121402740479 seconds
policy loss:282.14788818359375
value loss:13.378437995910645
entropies:24.247419357299805
Policy training finished
---------------------
gamma: 0.1817392526612375
training start after waiting for 1.2056598663330078 seconds
policy loss:467.17333984375
value loss:20.075700759887695
entropies:28.70995330810547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1170.5133)
ToM Target loss= tensor(2130.8127)
optimized based on ToM loss
---------------------
gamma: 0.18210273116655998
training start after waiting for 1.1511995792388916 seconds
policy loss:632.3388061523438
value loss:23.838253021240234
entropies:31.84541130065918
Policy training finished
---------------------
gamma: 0.18210273116655998
training start after waiting for 1.1474668979644775 seconds
policy loss:240.88619995117188
value loss:20.07661247253418
entropies:25.522748947143555
Policy training finished
---------------------
gamma: 0.18210273116655998
training start after waiting for 1.1840274333953857 seconds
policy loss:192.05128479003906
value loss:22.1622314453125
entropies:32.99656295776367
Policy training finished
---------------------
gamma: 0.18210273116655998
training start after waiting for 1.1410489082336426 seconds
policy loss:-268.3022155761719
value loss:11.548083305358887
entropies:23.4544620513916
Policy training finished
---------------------
gamma: 0.18210273116655998
training start after waiting for 1.1777665615081787 seconds
policy loss:-1.464550495147705
value loss:10.262385368347168
entropies:21.63531494140625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1132.6239)
ToM Target loss= tensor(2145.8774)
optimized based on ToM loss
---------------------
gamma: 0.1824669366288931
training start after waiting for 1.2063665390014648 seconds
policy loss:-6.901727199554443
value loss:23.831125259399414
entropies:23.012210845947266
Policy training finished
---------------------
gamma: 0.1824669366288931
training start after waiting for 1.1657533645629883 seconds
policy loss:-75.90830993652344
value loss:19.627565383911133
entropies:31.628623962402344
Policy training finished
---------------------
gamma: 0.1824669366288931
training start after waiting for 1.1515750885009766 seconds
policy loss:120.39270782470703
value loss:13.014273643493652
entropies:21.105789184570312
Policy training finished
---------------------
gamma: 0.1824669366288931
training start after waiting for 1.1695525646209717 seconds
policy loss:-238.06564331054688
value loss:18.710155487060547
entropies:16.453947067260742
Policy training finished
---------------------
gamma: 0.1824669366288931
training start after waiting for 1.138174057006836 seconds
policy loss:-201.81871032714844
value loss:14.796027183532715
entropies:16.0078067779541
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1093.4653)
ToM Target loss= tensor(2299.9724)
optimized based on ToM loss
---------------------
gamma: 0.18283187050215088
training start after waiting for 1.202927827835083 seconds
policy loss:-409.9284362792969
value loss:10.803132057189941
entropies:21.87153434753418
Policy training finished
---------------------
gamma: 0.18283187050215088
training start after waiting for 1.2018702030181885 seconds
policy loss:-457.72802734375
value loss:8.779155731201172
entropies:31.26626968383789
Policy training finished
---------------------
gamma: 0.18283187050215088
training start after waiting for 1.2099757194519043 seconds
policy loss:-693.30712890625
value loss:14.334148406982422
entropies:19.413291931152344
Policy training finished
---------------------
gamma: 0.18283187050215088
training start after waiting for 1.1454737186431885 seconds
policy loss:-682.1192016601562
value loss:17.732973098754883
entropies:34.148353576660156
Policy training finished
---------------------
gamma: 0.18283187050215088
training start after waiting for 1.2067511081695557 seconds
policy loss:-417.5324401855469
value loss:22.804737091064453
entropies:32.46117401123047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1095.9349)
ToM Target loss= tensor(2210.7861)
optimized based on ToM loss
---------------------
gamma: 0.18319753424315519
training start after waiting for 1.187546968460083 seconds
policy loss:-162.88369750976562
value loss:25.77043914794922
entropies:31.524566650390625
Policy training finished
---------------------
gamma: 0.18319753424315519
training start after waiting for 1.2100210189819336 seconds
policy loss:564.5638427734375
value loss:18.494260787963867
entropies:31.864219665527344
Policy training finished
---------------------
gamma: 0.18319753424315519
training start after waiting for 1.1531591415405273 seconds
policy loss:-27.37091827392578
value loss:13.671751022338867
entropies:20.517940521240234
Policy training finished
---------------------
gamma: 0.18319753424315519
training start after waiting for 1.1772470474243164 seconds
policy loss:-85.36824035644531
value loss:10.998804092407227
entropies:16.826332092285156
Policy training finished
---------------------
gamma: 0.18319753424315519
training start after waiting for 1.1717424392700195 seconds
policy loss:99.22103881835938
value loss:7.365170478820801
entropies:17.244413375854492
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1017.7587)
ToM Target loss= tensor(2156.9968)
optimized based on ToM loss
---------------------
gamma: 0.1835639293116415
training start after waiting for 1.197305679321289 seconds
policy loss:303.5753479003906
value loss:5.108080863952637
entropies:14.044591903686523
Policy training finished
---------------------
gamma: 0.1835639293116415
training start after waiting for 1.153672695159912 seconds
policy loss:-594.3973388671875
value loss:37.15890884399414
entropies:27.04197883605957
Policy training finished
---------------------
gamma: 0.1835639293116415
training start after waiting for 1.190181016921997 seconds
policy loss:-531.6451416015625
value loss:14.329964637756348
entropies:16.87353515625
Policy training finished
---------------------
gamma: 0.1835639293116415
training start after waiting for 1.2176480293273926 seconds
policy loss:-724.6131591796875
value loss:14.998292922973633
entropies:21.677753448486328
Policy training finished
---------------------
gamma: 0.1835639293116415
training start after waiting for 1.1587395668029785 seconds
policy loss:-1411.337646484375
value loss:31.943090438842773
entropies:33.01611328125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1062.6799)
ToM Target loss= tensor(2237.6628)
optimized based on ToM loss
---------------------
gamma: 0.18393105717026478
training start after waiting for 1.170022964477539 seconds
policy loss:104.67085266113281
value loss:6.840460300445557
entropies:20.631547927856445
Policy training finished
---------------------
gamma: 0.18393105717026478
training start after waiting for 1.2045347690582275 seconds
policy loss:11.807981491088867
value loss:5.461235523223877
entropies:19.16872787475586
Policy training finished
---------------------
gamma: 0.18393105717026478
training start after waiting for 1.1748762130737305 seconds
policy loss:-793.4019165039062
value loss:30.542293548583984
entropies:35.10445022583008
Policy training finished
---------------------
gamma: 0.18393105717026478
training start after waiting for 1.2119932174682617 seconds
policy loss:-363.75885009765625
value loss:12.809311866760254
entropies:26.228410720825195
Policy training finished
---------------------
gamma: 0.18393105717026478
training start after waiting for 1.1722872257232666 seconds
policy loss:-115.76897430419922
value loss:12.414130210876465
entropies:26.486677169799805
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1119.0258)
ToM Target loss= tensor(2133.0435)
optimized based on ToM loss
---------------------
gamma: 0.18429891928460532
training start after waiting for 1.1471951007843018 seconds
policy loss:-70.37319946289062
value loss:14.77441692352295
entropies:33.88261795043945
Policy training finished
---------------------
gamma: 0.18429891928460532
training start after waiting for 1.1564807891845703 seconds
policy loss:-458.96661376953125
value loss:24.42681884765625
entropies:29.5550594329834
Policy training finished
---------------------
gamma: 0.18429891928460532
training start after waiting for 1.199622631072998 seconds
policy loss:-1094.384033203125
value loss:16.552183151245117
entropies:34.10215377807617
Policy training finished
---------------------
gamma: 0.18429891928460532
training start after waiting for 1.1623923778533936 seconds
policy loss:-318.5922546386719
value loss:15.939306259155273
entropies:30.285667419433594
Policy training finished
---------------------
gamma: 0.18429891928460532
training start after waiting for 1.17741060256958 seconds
policy loss:-382.05743408203125
value loss:8.604174613952637
entropies:22.44080352783203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1163.5673)
ToM Target loss= tensor(2201.1646)
optimized based on ToM loss
---------------------
gamma: 0.18466751712317453
training start after waiting for 1.176433801651001 seconds
policy loss:-50.692569732666016
value loss:4.36385440826416
entropies:9.140571594238281
Policy training finished
---------------------
gamma: 0.18466751712317453
training start after waiting for 1.150625228881836 seconds
policy loss:-1734.80810546875
value loss:30.755767822265625
entropies:35.69322967529297
Policy training finished
---------------------
gamma: 0.18466751712317453
training start after waiting for 1.1600761413574219 seconds
policy loss:-318.4618835449219
value loss:10.77383804321289
entropies:17.571727752685547
Policy training finished
---------------------
gamma: 0.18466751712317453
training start after waiting for 1.1775693893432617 seconds
policy loss:-315.72662353515625
value loss:14.30112361907959
entropies:23.404504776000977
Policy training finished
---------------------
gamma: 0.18466751712317453
training start after waiting for 1.1550405025482178 seconds
policy loss:-582.0155029296875
value loss:25.843948364257812
entropies:28.353271484375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1185.7822)
ToM Target loss= tensor(2319.7742)
optimized based on ToM loss
---------------------
gamma: 0.18503685215742088
training start after waiting for 1.144629716873169 seconds
policy loss:-102.35995483398438
value loss:5.623355388641357
entropies:15.888172149658203
Policy training finished
---------------------
gamma: 0.18503685215742088
training start after waiting for 1.228623628616333 seconds
policy loss:-313.804443359375
value loss:7.825041770935059
entropies:23.6965389251709
Policy training finished
---------------------
gamma: 0.18503685215742088
training start after waiting for 1.1799240112304688 seconds
policy loss:302.2378234863281
value loss:8.053362846374512
entropies:11.723978042602539
Policy training finished
---------------------
gamma: 0.18503685215742088
training start after waiting for 1.180321216583252 seconds
policy loss:-276.249267578125
value loss:20.198701858520508
entropies:23.647947311401367
Policy training finished
---------------------
gamma: 0.18503685215742088
training start after waiting for 1.184053897857666 seconds
policy loss:-217.7276611328125
value loss:6.958908557891846
entropies:16.55168342590332
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1217.2007)
ToM Target loss= tensor(2343.6362)
optimized based on ToM loss
---------------------
gamma: 0.18540692586173574
training start after waiting for 1.1706993579864502 seconds
policy loss:11.588824272155762
value loss:6.600076675415039
entropies:16.67009162902832
Policy training finished
---------------------
gamma: 0.18540692586173574
training start after waiting for 1.1429104804992676 seconds
policy loss:-987.1809692382812
value loss:17.757314682006836
entropies:43.47985076904297
Policy training finished
---------------------
gamma: 0.18540692586173574
training start after waiting for 1.141608476638794 seconds
policy loss:-865.1436767578125
value loss:23.79479217529297
entropies:36.26266860961914
Policy training finished
---------------------
gamma: 0.18540692586173574
training start after waiting for 1.2075817584991455 seconds
policy loss:-1341.942626953125
value loss:24.306352615356445
entropies:27.78464126586914
Policy training finished
---------------------
gamma: 0.18540692586173574
training start after waiting for 1.1456923484802246 seconds
policy loss:68.88253021240234
value loss:9.471171379089355
entropies:30.48785400390625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1135.8430)
ToM Target loss= tensor(2177.2769)
optimized based on ToM loss
---------------------
gamma: 0.1857777397134592
training start after waiting for 1.1711232662200928 seconds
policy loss:-115.07670593261719
value loss:12.67340087890625
entropies:22.35838508605957
Policy training finished
---------------------
gamma: 0.1857777397134592
training start after waiting for 1.1799845695495605 seconds
policy loss:244.23680114746094
value loss:10.492572784423828
entropies:16.647377014160156
Policy training finished
---------------------
gamma: 0.1857777397134592
training start after waiting for 1.1426541805267334 seconds
policy loss:-331.29107666015625
value loss:30.087480545043945
entropies:34.20631790161133
Policy training finished
---------------------
gamma: 0.1857777397134592
training start after waiting for 1.1516153812408447 seconds
policy loss:-525.2749633789062
value loss:18.985692977905273
entropies:33.563270568847656
Policy training finished
---------------------
gamma: 0.1857777397134592
training start after waiting for 1.141679286956787 seconds
policy loss:-874.0388793945312
value loss:21.764263153076172
entropies:30.623685836791992
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1185.0940)
ToM Target loss= tensor(2174.1072)
optimized based on ToM loss
---------------------
gamma: 0.18614929519288612
training start after waiting for 1.1639297008514404 seconds
policy loss:-112.28487396240234
value loss:23.109174728393555
entropies:33.728004455566406
Policy training finished
---------------------
gamma: 0.18614929519288612
training start after waiting for 1.1668126583099365 seconds
policy loss:549.9623413085938
value loss:11.904756546020508
entropies:20.690059661865234
Policy training finished
---------------------
gamma: 0.18614929519288612
training start after waiting for 1.148331642150879 seconds
policy loss:-549.6885375976562
value loss:12.254436492919922
entropies:31.406787872314453
Policy training finished
---------------------
gamma: 0.18614929519288612
training start after waiting for 1.1804447174072266 seconds
policy loss:-1668.124755859375
value loss:32.1219596862793
entropies:48.48622131347656
Policy training finished
---------------------
gamma: 0.18614929519288612
training start after waiting for 1.204937219619751 seconds
policy loss:-1314.551025390625
value loss:26.97954559326172
entropies:40.24435043334961
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1189.5793)
ToM Target loss= tensor(2102.6707)
optimized based on ToM loss
---------------------
gamma: 0.18652159378327188
training start after waiting for 1.138267993927002 seconds
policy loss:-222.63589477539062
value loss:4.334113597869873
entropies:16.842769622802734
Policy training finished
---------------------
gamma: 0.18652159378327188
training start after waiting for 1.221977949142456 seconds
policy loss:-366.51129150390625
value loss:6.642068862915039
entropies:28.22838592529297
Policy training finished
---------------------
gamma: 0.18652159378327188
training start after waiting for 1.213829755783081 seconds
policy loss:-787.620361328125
value loss:20.542984008789062
entropies:22.338275909423828
Policy training finished
---------------------
gamma: 0.18652159378327188
training start after waiting for 1.1535074710845947 seconds
policy loss:-459.9735107421875
value loss:24.960275650024414
entropies:34.55647277832031
Policy training finished
---------------------
gamma: 0.18652159378327188
training start after waiting for 1.1964545249938965 seconds
policy loss:27.576763153076172
value loss:22.023488998413086
entropies:28.9342041015625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1166.6172)
ToM Target loss= tensor(2174.5781)
optimized based on ToM loss
---------------------
gamma: 0.1868946369708384
training start after waiting for 1.1934399604797363 seconds
policy loss:3.976128101348877
value loss:5.906010150909424
entropies:17.86742401123047
Policy training finished
---------------------
gamma: 0.1868946369708384
training start after waiting for 1.1501226425170898 seconds
policy loss:-712.228759765625
value loss:20.12204933166504
entropies:34.680843353271484
Policy training finished
---------------------
gamma: 0.1868946369708384
training start after waiting for 1.177656650543213 seconds
policy loss:-1175.2777099609375
value loss:39.69224166870117
entropies:51.55451583862305
Policy training finished
---------------------
gamma: 0.1868946369708384
training start after waiting for 1.158754587173462 seconds
policy loss:-984.7360229492188
value loss:44.312015533447266
entropies:48.025814056396484
Policy training finished
---------------------
gamma: 0.1868946369708384
training start after waiting for 1.1548774242401123 seconds
policy loss:153.72933959960938
value loss:8.012795448303223
entropies:20.662216186523438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1273.7047)
ToM Target loss= tensor(2270.7031)
optimized based on ToM loss
---------------------
gamma: 0.1872684262447801
training start after waiting for 1.1831443309783936 seconds
policy loss:-194.2432403564453
value loss:17.021488189697266
entropies:27.755950927734375
Policy training finished
---------------------
gamma: 0.1872684262447801
training start after waiting for 1.1487586498260498 seconds
policy loss:-176.19822692871094
value loss:12.155418395996094
entropies:25.62995719909668
Policy training finished
---------------------
gamma: 0.1872684262447801
training start after waiting for 1.1315927505493164 seconds
policy loss:-819.3602905273438
value loss:23.191757202148438
entropies:32.08976745605469
Policy training finished
---------------------
gamma: 0.1872684262447801
training start after waiting for 1.150172472000122 seconds
policy loss:-1366.8157958984375
value loss:32.011783599853516
entropies:41.00716018676758
Policy training finished
---------------------
gamma: 0.1872684262447801
training start after waiting for 1.1713216304779053 seconds
policy loss:-1010.2572021484375
value loss:24.934309005737305
entropies:25.72601318359375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1203.4851)
ToM Target loss= tensor(2381.0750)
optimized based on ToM loss
---------------------
gamma: 0.18764296309726966
training start after waiting for 1.190030574798584 seconds
policy loss:59.21373748779297
value loss:16.51553726196289
entropies:47.13235855102539
Policy training finished
---------------------
gamma: 0.18764296309726966
training start after waiting for 1.2052521705627441 seconds
policy loss:227.3580780029297
value loss:19.86211585998535
entropies:45.01626968383789
Policy training finished
---------------------
gamma: 0.18764296309726966
training start after waiting for 1.1347668170928955 seconds
policy loss:160.56651306152344
value loss:11.490556716918945
entropies:19.7066650390625
Policy training finished
---------------------
gamma: 0.18764296309726966
training start after waiting for 1.2000682353973389 seconds
policy loss:-466.6871032714844
value loss:20.173444747924805
entropies:36.17329025268555
Policy training finished
---------------------
gamma: 0.18764296309726966
training start after waiting for 1.2107431888580322 seconds
policy loss:-1182.466796875
value loss:28.77213478088379
entropies:54.863792419433594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1377.1245)
ToM Target loss= tensor(2202.3240)
optimized based on ToM loss
---------------------
gamma: 0.1880182490234642
training start after waiting for 1.1440861225128174 seconds
policy loss:235.2747039794922
value loss:12.325721740722656
entropies:25.318058013916016
Policy training finished
---------------------
gamma: 0.1880182490234642
training start after waiting for 1.1429524421691895 seconds
policy loss:-79.19922637939453
value loss:7.842942714691162
entropies:18.54353904724121
Policy training finished
---------------------
gamma: 0.1880182490234642
training start after waiting for 1.1626625061035156 seconds
policy loss:-207.50686645507812
value loss:17.6343936920166
entropies:47.17184066772461
Policy training finished
---------------------
gamma: 0.1880182490234642
training start after waiting for 1.1746289730072021 seconds
policy loss:-1125.2523193359375
value loss:25.77569580078125
entropies:32.52406692504883
Policy training finished
---------------------
gamma: 0.1880182490234642
training start after waiting for 1.175950050354004 seconds
policy loss:-452.2945251464844
value loss:11.031881332397461
entropies:25.338342666625977
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1229.1995)
ToM Target loss= tensor(2212.6492)
optimized based on ToM loss
---------------------
gamma: 0.18839428552151113
training start after waiting for 1.1474370956420898 seconds
policy loss:-258.8631591796875
value loss:15.918500900268555
entropies:22.630430221557617
Policy training finished
---------------------
gamma: 0.18839428552151113
training start after waiting for 1.1804869174957275 seconds
policy loss:-352.6536865234375
value loss:33.103309631347656
entropies:38.488914489746094
Policy training finished
---------------------
gamma: 0.18839428552151113
training start after waiting for 1.2031230926513672 seconds
policy loss:256.6937255859375
value loss:12.332538604736328
entropies:14.528221130371094
Policy training finished
---------------------
gamma: 0.18839428552151113
training start after waiting for 1.1859846115112305 seconds
policy loss:-124.50894165039062
value loss:17.72177505493164
entropies:27.399784088134766
Policy training finished
---------------------
gamma: 0.18839428552151113
training start after waiting for 1.2191216945648193 seconds
policy loss:-275.0340576171875
value loss:20.74073600769043
entropies:25.560043334960938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1115.0454)
ToM Target loss= tensor(2172.0251)
optimized based on ToM loss
---------------------
gamma: 0.18877107409255417
training start after waiting for 1.1801576614379883 seconds
policy loss:-312.604248046875
value loss:9.018272399902344
entropies:18.237323760986328
Policy training finished
---------------------
gamma: 0.18877107409255417
training start after waiting for 1.2334766387939453 seconds
policy loss:-61.47042465209961
value loss:13.070075988769531
entropies:30.810359954833984
Policy training finished
---------------------
gamma: 0.18877107409255417
training start after waiting for 1.1739501953125 seconds
policy loss:-1574.9482421875
value loss:29.2337703704834
entropies:30.851043701171875
Policy training finished
---------------------
gamma: 0.18877107409255417
training start after waiting for 1.1655454635620117 seconds
policy loss:-1235.512451171875
value loss:31.10501480102539
entropies:31.893306732177734
Policy training finished
---------------------
gamma: 0.18877107409255417
training start after waiting for 1.2206659317016602 seconds
policy loss:33.93706130981445
value loss:4.768396377563477
entropies:17.41299057006836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1146.5225)
ToM Target loss= tensor(2166.3777)
optimized based on ToM loss
---------------------
gamma: 0.18914861624073928
training start after waiting for 1.1944384574890137 seconds
policy loss:-458.1842956542969
value loss:12.544563293457031
entropies:13.755664825439453
Policy training finished
---------------------
gamma: 0.18914861624073928
training start after waiting for 1.1664297580718994 seconds
policy loss:-448.61090087890625
value loss:10.812156677246094
entropies:25.21746253967285
Policy training finished
---------------------
gamma: 0.18914861624073928
training start after waiting for 1.1649608612060547 seconds
policy loss:-172.8808135986328
value loss:12.86779499053955
entropies:17.967144012451172
Policy training finished
---------------------
gamma: 0.18914861624073928
training start after waiting for 1.1905243396759033 seconds
policy loss:188.7269744873047
value loss:10.065682411193848
entropies:22.213523864746094
Policy training finished
---------------------
gamma: 0.18914861624073928
training start after waiting for 1.1916916370391846 seconds
policy loss:-48.866737365722656
value loss:10.943039894104004
entropies:33.38130187988281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(971.2563)
ToM Target loss= tensor(2172.2119)
optimized based on ToM loss
---------------------
gamma: 0.18952691347322076
training start after waiting for 1.1443207263946533 seconds
policy loss:-1332.56103515625
value loss:40.73411178588867
entropies:31.227699279785156
Policy training finished
---------------------
gamma: 0.18952691347322076
training start after waiting for 1.2064611911773682 seconds
policy loss:-176.07913208007812
value loss:27.133956909179688
entropies:31.510116577148438
Policy training finished
---------------------
gamma: 0.18952691347322076
training start after waiting for 1.1878180503845215 seconds
policy loss:-152.6401824951172
value loss:9.93749713897705
entropies:22.85370445251465
Policy training finished
---------------------
gamma: 0.18952691347322076
training start after waiting for 1.1761798858642578 seconds
policy loss:-350.1501770019531
value loss:16.873929977416992
entropies:28.056838989257812
Policy training finished
---------------------
gamma: 0.18952691347322076
training start after waiting for 1.1779909133911133 seconds
policy loss:-914.6765747070312
value loss:12.819504737854004
entropies:24.166723251342773
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1139.9713)
ToM Target loss= tensor(2128.0547)
optimized based on ToM loss
---------------------
gamma: 0.1899059673001672
training start after waiting for 1.1498043537139893 seconds
policy loss:-219.24008178710938
value loss:8.49756145477295
entropies:24.911888122558594
Policy training finished
---------------------
gamma: 0.1899059673001672
training start after waiting for 1.1898913383483887 seconds
policy loss:-888.4631958007812
value loss:22.339073181152344
entropies:38.44585037231445
Policy training finished
---------------------
gamma: 0.1899059673001672
training start after waiting for 1.1729085445404053 seconds
policy loss:-287.8594665527344
value loss:34.35218048095703
entropies:28.15243148803711
Policy training finished
---------------------
gamma: 0.1899059673001672
training start after waiting for 1.147047996520996 seconds
policy loss:-230.91505432128906
value loss:15.096515655517578
entropies:49.358055114746094
Policy training finished
---------------------
gamma: 0.1899059673001672
training start after waiting for 1.1904470920562744 seconds
policy loss:-254.10281372070312
value loss:25.77962875366211
entropies:37.04277420043945
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1222.2247)
ToM Target loss= tensor(2335.2354)
optimized based on ToM loss
---------------------
gamma: 0.19028577923476753
training start after waiting for 1.1406326293945312 seconds
policy loss:157.33621215820312
value loss:11.078316688537598
entropies:19.854509353637695
Policy training finished
---------------------
gamma: 0.19028577923476753
training start after waiting for 1.1525311470031738 seconds
policy loss:-353.4241638183594
value loss:12.901400566101074
entropies:31.198291778564453
Policy training finished
---------------------
gamma: 0.19028577923476753
training start after waiting for 1.1522655487060547 seconds
policy loss:-933.7000732421875
value loss:20.85773468017578
entropies:45.30888748168945
Policy training finished
---------------------
gamma: 0.19028577923476753
training start after waiting for 1.146183967590332 seconds
policy loss:-1467.1055908203125
value loss:28.93638801574707
entropies:34.500709533691406
Policy training finished
---------------------
gamma: 0.19028577923476753
training start after waiting for 1.1942355632781982 seconds
policy loss:-409.2275085449219
value loss:20.8404483795166
entropies:34.438026428222656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1304.8112)
ToM Target loss= tensor(2437.3049)
optimized based on ToM loss
---------------------
gamma: 0.19066635079323707
training start after waiting for 1.1986384391784668 seconds
policy loss:-955.5946044921875
value loss:21.471290588378906
entropies:35.654605865478516
Policy training finished
---------------------
gamma: 0.19066635079323707
training start after waiting for 1.1359362602233887 seconds
policy loss:-901.4903564453125
value loss:22.856992721557617
entropies:50.51354217529297
Policy training finished
---------------------
gamma: 0.19066635079323707
training start after waiting for 1.1915054321289062 seconds
policy loss:-1294.2919921875
value loss:23.95671844482422
entropies:41.00151824951172
Policy training finished
---------------------
gamma: 0.19066635079323707
training start after waiting for 1.1747958660125732 seconds
policy loss:-323.6474304199219
value loss:14.253859519958496
entropies:38.20547103881836
Policy training finished
---------------------
gamma: 0.19066635079323707
training start after waiting for 1.189225196838379 seconds
policy loss:-265.7652893066406
value loss:8.539222717285156
entropies:27.86104393005371
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1309.3168)
ToM Target loss= tensor(2256.8704)
optimized based on ToM loss
---------------------
gamma: 0.19104768349482354
training start after waiting for 1.1270573139190674 seconds
policy loss:-101.94119262695312
value loss:8.517162322998047
entropies:30.323772430419922
Policy training finished
---------------------
gamma: 0.19104768349482354
training start after waiting for 1.1389143466949463 seconds
policy loss:-264.7771301269531
value loss:22.394763946533203
entropies:36.31148910522461
Policy training finished
---------------------
gamma: 0.19104768349482354
training start after waiting for 1.1768805980682373 seconds
policy loss:-877.53857421875
value loss:21.03215789794922
entropies:30.346216201782227
Policy training finished
---------------------
gamma: 0.19104768349482354
training start after waiting for 1.171968698501587 seconds
policy loss:-88.5963363647461
value loss:6.1355485916137695
entropies:30.123062133789062
Policy training finished
---------------------
gamma: 0.19104768349482354
training start after waiting for 1.1906089782714844 seconds
policy loss:-1161.8045654296875
value loss:34.130271911621094
entropies:48.25597381591797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1185.5679)
ToM Target loss= tensor(2270.8811)
optimized based on ToM loss
---------------------
gamma: 0.1914297788618132
training start after waiting for 1.1812384128570557 seconds
policy loss:70.12040710449219
value loss:6.590336799621582
entropies:22.349626541137695
Policy training finished
---------------------
gamma: 0.1914297788618132
training start after waiting for 1.17685866355896 seconds
policy loss:-53.49995422363281
value loss:4.125084400177002
entropies:22.97808837890625
Policy training finished
---------------------
gamma: 0.1914297788618132
training start after waiting for 1.2029967308044434 seconds
policy loss:-1125.87109375
value loss:24.173763275146484
entropies:37.94661331176758
Policy training finished
---------------------
gamma: 0.1914297788618132
training start after waiting for 1.1742744445800781 seconds
policy loss:-101.7951431274414
value loss:13.140716552734375
entropies:20.506088256835938
Policy training finished
---------------------
gamma: 0.1914297788618132
training start after waiting for 1.1921100616455078 seconds
policy loss:-395.21197509765625
value loss:13.339694023132324
entropies:28.640411376953125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1194.4221)
ToM Target loss= tensor(2331.8457)
optimized based on ToM loss
---------------------
gamma: 0.19181263841953683
training start after waiting for 1.201047420501709 seconds
policy loss:-1171.361083984375
value loss:25.952980041503906
entropies:39.40082550048828
Policy training finished
---------------------
gamma: 0.19181263841953683
training start after waiting for 1.2099344730377197 seconds
policy loss:-537.0518798828125
value loss:17.770931243896484
entropies:33.4609375
Policy training finished
---------------------
gamma: 0.19181263841953683
training start after waiting for 1.142327070236206 seconds
policy loss:-1252.2767333984375
value loss:26.986366271972656
entropies:40.5602912902832
Policy training finished
---------------------
gamma: 0.19181263841953683
training start after waiting for 1.183499813079834 seconds
policy loss:-215.8994903564453
value loss:12.534777641296387
entropies:32.85527420043945
Policy training finished
---------------------
gamma: 0.19181263841953683
training start after waiting for 1.1379787921905518 seconds
policy loss:-668.5079956054688
value loss:24.268394470214844
entropies:29.238666534423828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1258.6738)
ToM Target loss= tensor(2292.4089)
optimized based on ToM loss
---------------------
gamma: 0.1921962636963759
training start after waiting for 1.1702117919921875 seconds
policy loss:-963.2416381835938
value loss:19.009105682373047
entropies:32.05678176879883
Policy training finished
---------------------
gamma: 0.1921962636963759
training start after waiting for 1.1845602989196777 seconds
policy loss:387.6223449707031
value loss:12.435678482055664
entropies:17.265052795410156
Policy training finished
---------------------
gamma: 0.1921962636963759
training start after waiting for 1.1913824081420898 seconds
policy loss:50.48749542236328
value loss:9.751762390136719
entropies:15.902177810668945
Policy training finished
---------------------
gamma: 0.1921962636963759
training start after waiting for 1.1660385131835938 seconds
policy loss:-398.0179443359375
value loss:9.176841735839844
entropies:20.495975494384766
Policy training finished
---------------------
gamma: 0.1921962636963759
training start after waiting for 1.197197675704956 seconds
policy loss:-219.95176696777344
value loss:15.732645988464355
entropies:22.719295501708984
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1048.6614)
ToM Target loss= tensor(2253.9297)
optimized based on ToM loss
---------------------
gamma: 0.19258065622376866
training start after waiting for 1.1897189617156982 seconds
policy loss:-422.0154113769531
value loss:18.790496826171875
entropies:19.056161880493164
Policy training finished
---------------------
gamma: 0.19258065622376866
training start after waiting for 1.1808578968048096 seconds
policy loss:-323.1816101074219
value loss:11.476774215698242
entropies:27.623031616210938
Policy training finished
---------------------
gamma: 0.19258065622376866
training start after waiting for 1.1968095302581787 seconds
policy loss:-170.49546813964844
value loss:12.005035400390625
entropies:24.461444854736328
Policy training finished
---------------------
gamma: 0.19258065622376866
training start after waiting for 1.168461561203003 seconds
policy loss:14.374713897705078
value loss:8.961502075195312
entropies:20.306621551513672
Policy training finished
---------------------
gamma: 0.19258065622376866
training start after waiting for 1.148555040359497 seconds
policy loss:-634.1929931640625
value loss:22.12291717529297
entropies:29.59185791015625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1096.0242)
ToM Target loss= tensor(2229.8530)
optimized based on ToM loss
---------------------
gamma: 0.1929658175362162
training start after waiting for 1.1356475353240967 seconds
policy loss:276.3799133300781
value loss:6.401249408721924
entropies:22.172645568847656
Policy training finished
---------------------
gamma: 0.1929658175362162
training start after waiting for 1.2081553936004639 seconds
policy loss:472.1489562988281
value loss:15.010759353637695
entropies:25.71137046813965
Policy training finished
---------------------
gamma: 0.1929658175362162
training start after waiting for 1.1432290077209473 seconds
policy loss:237.49819946289062
value loss:11.343452453613281
entropies:19.032188415527344
Policy training finished
---------------------
gamma: 0.1929658175362162
training start after waiting for 1.147348165512085 seconds
policy loss:-70.99227142333984
value loss:11.293272018432617
entropies:30.400806427001953
Policy training finished
---------------------
gamma: 0.1929658175362162
training start after waiting for 1.1890878677368164 seconds
policy loss:93.6014633178711
value loss:11.27039909362793
entropies:30.566795349121094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1130.0856)
ToM Target loss= tensor(2297.2224)
optimized based on ToM loss
---------------------
gamma: 0.19335174917128864
training start after waiting for 1.17685866355896 seconds
policy loss:-816.2869873046875
value loss:13.001577377319336
entropies:26.85733985900879
Policy training finished
---------------------
gamma: 0.19335174917128864
training start after waiting for 1.1468088626861572 seconds
policy loss:-345.83001708984375
value loss:20.953044891357422
entropies:37.63764190673828
Policy training finished
---------------------
gamma: 0.19335174917128864
training start after waiting for 1.162132740020752 seconds
policy loss:-2375.552001953125
value loss:42.932247161865234
entropies:53.925743103027344
Policy training finished
---------------------
gamma: 0.19335174917128864
training start after waiting for 1.1688003540039062 seconds
policy loss:-793.7006225585938
value loss:19.350963592529297
entropies:31.816614151000977
Policy training finished
---------------------
gamma: 0.19335174917128864
training start after waiting for 1.1919584274291992 seconds
policy loss:-301.26068115234375
value loss:7.747276782989502
entropies:24.70343780517578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1102.5903)
ToM Target loss= tensor(2095.4097)
optimized based on ToM loss
---------------------
gamma: 0.19373845266963122
training start after waiting for 1.1831893920898438 seconds
policy loss:-797.5364990234375
value loss:18.884050369262695
entropies:23.652496337890625
Policy training finished
---------------------
gamma: 0.19373845266963122
training start after waiting for 1.1496822834014893 seconds
policy loss:-148.1149139404297
value loss:11.006178855895996
entropies:26.833175659179688
Policy training finished
---------------------
gamma: 0.19373845266963122
training start after waiting for 1.2574801445007324 seconds
policy loss:37.57341384887695
value loss:15.831382751464844
entropies:21.710161209106445
Policy training finished
---------------------
gamma: 0.19373845266963122
training start after waiting for 1.170896291732788 seconds
policy loss:-522.4739990234375
value loss:23.829437255859375
entropies:34.1461067199707
Policy training finished
---------------------
gamma: 0.19373845266963122
training start after waiting for 1.1823956966400146 seconds
policy loss:-865.296142578125
value loss:34.647369384765625
entropies:62.16999053955078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1208.4874)
ToM Target loss= tensor(2278.3892)
optimized based on ToM loss
---------------------
gamma: 0.1941259295749705
training start after waiting for 1.2114205360412598 seconds
policy loss:215.1567840576172
value loss:10.37881851196289
entropies:25.610897064208984
Policy training finished
---------------------
gamma: 0.1941259295749705
training start after waiting for 1.1774444580078125 seconds
policy loss:17.153776168823242
value loss:10.746737480163574
entropies:38.29066467285156
Policy training finished
---------------------
gamma: 0.1941259295749705
training start after waiting for 1.1478724479675293 seconds
policy loss:-495.8437194824219
value loss:21.098127365112305
entropies:34.95505142211914
Policy training finished
---------------------
gamma: 0.1941259295749705
training start after waiting for 1.1716701984405518 seconds
policy loss:-717.1666870117188
value loss:17.242395401000977
entropies:37.804317474365234
Policy training finished
---------------------
gamma: 0.1941259295749705
training start after waiting for 1.1452116966247559 seconds
policy loss:-499.74249267578125
value loss:28.44732093811035
entropies:40.569156646728516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1148.2257)
ToM Target loss= tensor(2152.8699)
optimized based on ToM loss
---------------------
gamma: 0.19451418143412044
training start after waiting for 1.1439766883850098 seconds
policy loss:-1156.19970703125
value loss:24.217357635498047
entropies:38.85828399658203
Policy training finished
---------------------
gamma: 0.19451418143412044
training start after waiting for 1.174952745437622 seconds
policy loss:-483.6107177734375
value loss:11.731032371520996
entropies:36.79967498779297
Policy training finished
---------------------
gamma: 0.19451418143412044
training start after waiting for 1.212695598602295 seconds
policy loss:-941.768798828125
value loss:21.40027618408203
entropies:30.361234664916992
Policy training finished
---------------------
gamma: 0.19451418143412044
training start after waiting for 1.2129180431365967 seconds
policy loss:-309.9372253417969
value loss:12.794391632080078
entropies:30.033790588378906
Policy training finished
---------------------
gamma: 0.19451418143412044
training start after waiting for 1.1900057792663574 seconds
policy loss:165.2360382080078
value loss:18.576091766357422
entropies:20.939071655273438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1074.3068)
ToM Target loss= tensor(2153.2371)
optimized based on ToM loss
---------------------
gamma: 0.1949032097969887
training start after waiting for 1.1930158138275146 seconds
policy loss:243.77841186523438
value loss:9.950065612792969
entropies:22.093055725097656
Policy training finished
---------------------
gamma: 0.1949032097969887
training start after waiting for 1.1368536949157715 seconds
policy loss:179.41571044921875
value loss:8.523008346557617
entropies:25.019351959228516
Policy training finished
---------------------
gamma: 0.1949032097969887
training start after waiting for 1.1708638668060303 seconds
policy loss:-306.8299865722656
value loss:17.19263458251953
entropies:26.581201553344727
Policy training finished
---------------------
gamma: 0.1949032097969887
training start after waiting for 1.2116377353668213 seconds
policy loss:-1144.80224609375
value loss:39.628597259521484
entropies:34.395179748535156
Policy training finished
---------------------
gamma: 0.1949032097969887
training start after waiting for 1.208068609237671 seconds
policy loss:-120.7581558227539
value loss:7.986232757568359
entropies:23.288192749023438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1169.0175)
ToM Target loss= tensor(2273.5259)
optimized based on ToM loss
---------------------
gamma: 0.19529301621658268
training start after waiting for 1.1555862426757812 seconds
policy loss:-1573.1375732421875
value loss:33.205753326416016
entropies:35.35946273803711
Policy training finished
---------------------
gamma: 0.19529301621658268
training start after waiting for 1.1846773624420166 seconds
policy loss:-598.8680419921875
value loss:23.10272789001465
entropies:27.43254280090332
Policy training finished
---------------------
gamma: 0.19529301621658268
training start after waiting for 1.171166181564331 seconds
policy loss:-294.0749206542969
value loss:28.513240814208984
entropies:21.906911849975586
Policy training finished
---------------------
gamma: 0.19529301621658268
training start after waiting for 1.1916494369506836 seconds
policy loss:-21.70522689819336
value loss:7.857508659362793
entropies:15.581676483154297
Policy training finished
---------------------
gamma: 0.19529301621658268
training start after waiting for 1.1454997062683105 seconds
policy loss:-558.546875
value loss:34.880638122558594
entropies:46.402591705322266
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1186.4478)
ToM Target loss= tensor(2258.8579)
optimized based on ToM loss
---------------------
gamma: 0.19568360224901585
training start after waiting for 1.1901040077209473 seconds
policy loss:-562.0803833007812
value loss:16.038665771484375
entropies:36.76323699951172
Policy training finished
---------------------
gamma: 0.19568360224901585
training start after waiting for 1.2206871509552002 seconds
policy loss:-6.882775783538818
value loss:8.754783630371094
entropies:30.830482482910156
Policy training finished
---------------------
gamma: 0.19568360224901585
training start after waiting for 1.1478002071380615 seconds
policy loss:-38.30039978027344
value loss:17.401344299316406
entropies:24.097261428833008
Policy training finished
---------------------
gamma: 0.19568360224901585
training start after waiting for 1.1905028820037842 seconds
policy loss:409.36566162109375
value loss:13.53283977508545
entropies:23.256492614746094
Policy training finished
---------------------
gamma: 0.19568360224901585
training start after waiting for 1.1809043884277344 seconds
policy loss:-827.5391845703125
value loss:38.15177917480469
entropies:41.259185791015625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1107.5687)
ToM Target loss= tensor(2144.8608)
optimized based on ToM loss
---------------------
gamma: 0.19607496945351388
training start after waiting for 1.1687507629394531 seconds
policy loss:-133.29026794433594
value loss:12.992547035217285
entropies:16.29180908203125
Policy training finished
---------------------
gamma: 0.19607496945351388
training start after waiting for 1.194262981414795 seconds
policy loss:-1071.814208984375
value loss:43.7177619934082
entropies:41.142417907714844
Policy training finished
---------------------
gamma: 0.19607496945351388
training start after waiting for 1.1596412658691406 seconds
policy loss:-86.894775390625
value loss:11.6693754196167
entropies:21.772701263427734
Policy training finished
---------------------
gamma: 0.19607496945351388
training start after waiting for 1.1569790840148926 seconds
policy loss:-21.51423454284668
value loss:13.025358200073242
entropies:15.767026901245117
Policy training finished
---------------------
gamma: 0.19607496945351388
training start after waiting for 1.1362669467926025 seconds
policy loss:49.206729888916016
value loss:4.831439971923828
entropies:19.72344398498535
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1134.6243)
ToM Target loss= tensor(2309.6731)
optimized based on ToM loss
---------------------
gamma: 0.1964671193924209
training start after waiting for 1.1823852062225342 seconds
policy loss:-1005.4027709960938
value loss:33.132991790771484
entropies:40.25697708129883
Policy training finished
---------------------
gamma: 0.1964671193924209
training start after waiting for 1.1434249877929688 seconds
policy loss:-355.59033203125
value loss:22.15972328186035
entropies:38.65052032470703
Policy training finished
---------------------
gamma: 0.1964671193924209
training start after waiting for 1.1862008571624756 seconds
policy loss:-459.7597351074219
value loss:22.277286529541016
entropies:26.173357009887695
Policy training finished
---------------------
gamma: 0.1964671193924209
training start after waiting for 1.16078782081604 seconds
policy loss:-159.06748962402344
value loss:13.749979972839355
entropies:16.458480834960938
Policy training finished
---------------------
gamma: 0.1964671193924209
training start after waiting for 1.1676795482635498 seconds
policy loss:-381.48577880859375
value loss:13.772330284118652
entropies:25.667354583740234
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1229.9901)
ToM Target loss= tensor(2309.4949)
optimized based on ToM loss
---------------------
gamma: 0.19686005363120573
training start after waiting for 1.1840875148773193 seconds
policy loss:-84.59919738769531
value loss:14.954309463500977
entropies:27.914363861083984
Policy training finished
---------------------
gamma: 0.19686005363120573
training start after waiting for 1.1828773021697998 seconds
policy loss:66.21857452392578
value loss:12.643787384033203
entropies:11.620162963867188
Policy training finished
---------------------
gamma: 0.19686005363120573
training start after waiting for 1.2423007488250732 seconds
policy loss:-298.9369812011719
value loss:21.96880340576172
entropies:35.0479736328125
Policy training finished
---------------------
gamma: 0.19686005363120573
training start after waiting for 1.210557460784912 seconds
policy loss:-310.516357421875
value loss:9.58424186706543
entropies:18.831327438354492
Policy training finished
---------------------
gamma: 0.19686005363120573
training start after waiting for 1.1794679164886475 seconds
policy loss:-364.2933654785156
value loss:27.7465877532959
entropies:30.98102569580078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1226.8993)
ToM Target loss= tensor(2240.7910)
optimized based on ToM loss
---------------------
gamma: 0.19725377373846814
training start after waiting for 1.1472117900848389 seconds
policy loss:-552.2965087890625
value loss:8.782685279846191
entropies:21.640239715576172
Policy training finished
---------------------
gamma: 0.19725377373846814
training start after waiting for 1.139714241027832 seconds
policy loss:-660.74609375
value loss:15.70639705657959
entropies:29.784700393676758
Policy training finished
---------------------
gamma: 0.19725377373846814
training start after waiting for 1.1893644332885742 seconds
policy loss:-1347.1588134765625
value loss:52.16423034667969
entropies:44.45934295654297
Policy training finished
---------------------
gamma: 0.19725377373846814
training start after waiting for 1.2270104885101318 seconds
policy loss:-1296.427001953125
value loss:22.25494956970215
entropies:43.80164337158203
Policy training finished
---------------------
gamma: 0.19725377373846814
training start after waiting for 1.1835815906524658 seconds
policy loss:113.46192932128906
value loss:10.823783874511719
entropies:23.999744415283203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1219.8386)
ToM Target loss= tensor(2239.0154)
optimized based on ToM loss
---------------------
gamma: 0.19764828128594508
training start after waiting for 1.1790597438812256 seconds
policy loss:-643.5978393554688
value loss:14.464510917663574
entropies:31.99739646911621
Policy training finished
---------------------
gamma: 0.19764828128594508
training start after waiting for 1.164335012435913 seconds
policy loss:-23.28912925720215
value loss:12.459423065185547
entropies:25.604496002197266
Policy training finished
---------------------
gamma: 0.19764828128594508
training start after waiting for 1.139805555343628 seconds
policy loss:-2200.62451171875
value loss:44.95579528808594
entropies:51.84081268310547
Policy training finished
---------------------
gamma: 0.19764828128594508
training start after waiting for 1.1373682022094727 seconds
policy loss:-413.6640625
value loss:12.046055793762207
entropies:30.52049446105957
Policy training finished
---------------------
gamma: 0.19764828128594508
training start after waiting for 1.2101852893829346 seconds
policy loss:-115.14433288574219
value loss:12.049927711486816
entropies:30.786571502685547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1276.1299)
ToM Target loss= tensor(2262.7649)
optimized based on ToM loss
---------------------
gamma: 0.19804357784851698
training start after waiting for 1.143911600112915 seconds
policy loss:-239.28138732910156
value loss:15.742144584655762
entropies:30.868314743041992
Policy training finished
---------------------
gamma: 0.19804357784851698
training start after waiting for 1.17586088180542 seconds
policy loss:-199.94139099121094
value loss:12.408967018127441
entropies:26.691152572631836
Policy training finished
---------------------
gamma: 0.19804357784851698
training start after waiting for 1.1435465812683105 seconds
policy loss:11.021333694458008
value loss:10.51900863647461
entropies:29.45493507385254
Policy training finished
---------------------
gamma: 0.19804357784851698
training start after waiting for 1.1952381134033203 seconds
policy loss:-1623.105712890625
value loss:27.451772689819336
entropies:36.68760681152344
Policy training finished
---------------------
gamma: 0.19804357784851698
training start after waiting for 1.2244899272918701 seconds
policy loss:-1964.4443359375
value loss:56.052734375
entropies:39.15121078491211
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1179.7162)
ToM Target loss= tensor(2150.7319)
optimized based on ToM loss
---------------------
gamma: 0.198439665004214
training start after waiting for 1.160719633102417 seconds
policy loss:-1934.214599609375
value loss:34.516666412353516
entropies:33.353858947753906
Policy training finished
---------------------
gamma: 0.198439665004214
training start after waiting for 1.1790075302124023 seconds
policy loss:-778.60888671875
value loss:25.527036666870117
entropies:49.17481994628906
Policy training finished
---------------------
gamma: 0.198439665004214
training start after waiting for 1.1411080360412598 seconds
policy loss:-495.02496337890625
value loss:20.47241973876953
entropies:29.075380325317383
Policy training finished
---------------------
gamma: 0.198439665004214
training start after waiting for 1.1655604839324951 seconds
policy loss:4.152960777282715
value loss:17.787216186523438
entropies:41.97439193725586
Policy training finished
---------------------
gamma: 0.198439665004214
training start after waiting for 1.1462111473083496 seconds
policy loss:168.2278594970703
value loss:14.26097297668457
entropies:35.70430374145508
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1233.8911)
ToM Target loss= tensor(2146.5215)
optimized based on ToM loss
---------------------
gamma: 0.19883654433422243
training start after waiting for 1.206963300704956 seconds
policy loss:210.7758331298828
value loss:12.60782527923584
entropies:23.245479583740234
Policy training finished
---------------------
gamma: 0.19883654433422243
training start after waiting for 1.0998845100402832 seconds
policy loss:522.6495361328125
value loss:12.085164070129395
entropies:14.448824882507324
Policy training finished
---------------------
gamma: 0.19883654433422243
training start after waiting for 1.1610708236694336 seconds
policy loss:-1092.14306640625
value loss:17.08135986328125
entropies:38.273746490478516
Policy training finished
---------------------
gamma: 0.19883654433422243
training start after waiting for 1.1568078994750977 seconds
policy loss:-421.51617431640625
value loss:20.155397415161133
entropies:31.168357849121094
Policy training finished
---------------------
gamma: 0.19883654433422243
training start after waiting for 1.1860878467559814 seconds
policy loss:390.49627685546875
value loss:5.8611626625061035
entropies:16.253833770751953
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1110.5493)
ToM Target loss= tensor(2271.2698)
optimized based on ToM loss
---------------------
gamma: 0.19923421742289088
training start after waiting for 1.1401853561401367 seconds
policy loss:-143.282470703125
value loss:9.574838638305664
entropies:25.044113159179688
Policy training finished
---------------------
gamma: 0.19923421742289088
training start after waiting for 1.2072303295135498 seconds
policy loss:-694.892333984375
value loss:18.07958984375
entropies:30.872270584106445
Policy training finished
---------------------
gamma: 0.19923421742289088
training start after waiting for 1.140061616897583 seconds
policy loss:-1729.152587890625
value loss:41.262977600097656
entropies:31.166168212890625
Policy training finished
---------------------
gamma: 0.19923421742289088
training start after waiting for 1.215538740158081 seconds
policy loss:-1350.9534912109375
value loss:24.552473068237305
entropies:45.53456497192383
Policy training finished
---------------------
gamma: 0.19923421742289088
training start after waiting for 1.2193372249603271 seconds
policy loss:-242.38424682617188
value loss:7.908297538757324
entropies:18.901273727416992
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1161.0381)
ToM Target loss= tensor(2197.1028)
optimized based on ToM loss
---------------------
gamma: 0.19963268585773666
training start after waiting for 1.1853845119476318 seconds
policy loss:-268.5283508300781
value loss:29.750537872314453
entropies:26.725849151611328
Policy training finished
---------------------
gamma: 0.19963268585773666
training start after waiting for 1.175459861755371 seconds
policy loss:-1260.9722900390625
value loss:30.440170288085938
entropies:34.841278076171875
Policy training finished
---------------------
gamma: 0.19963268585773666
training start after waiting for 1.170527696609497 seconds
policy loss:-637.214111328125
value loss:21.365402221679688
entropies:35.7451057434082
Policy training finished
---------------------
gamma: 0.19963268585773666
training start after waiting for 1.207646369934082 seconds
policy loss:-727.3300170898438
value loss:20.98586654663086
entropies:37.645912170410156
Policy training finished
---------------------
gamma: 0.19963268585773666
training start after waiting for 1.177229404449463 seconds
policy loss:-692.753662109375
value loss:23.902006149291992
entropies:34.04487991333008
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1184.9368)
ToM Target loss= tensor(2113.7124)
optimized based on ToM loss
---------------------
gamma: 0.20003195122945214
training start after waiting for 1.1978411674499512 seconds
policy loss:525.6455078125
value loss:14.476737022399902
entropies:20.2183895111084
Policy training finished
---------------------
gamma: 0.20003195122945214
training start after waiting for 1.197641134262085 seconds
policy loss:-830.634033203125
value loss:30.61793327331543
entropies:42.30195236206055
Policy training finished
---------------------
gamma: 0.20003195122945214
training start after waiting for 1.2098867893218994 seconds
policy loss:-508.9979248046875
value loss:24.812545776367188
entropies:24.8040714263916
Policy training finished
---------------------
gamma: 0.20003195122945214
training start after waiting for 1.1399850845336914 seconds
policy loss:-240.3323974609375
value loss:12.486838340759277
entropies:24.970840454101562
Policy training finished
---------------------
gamma: 0.20003195122945214
training start after waiting for 1.2058777809143066 seconds
policy loss:-38.89972686767578
value loss:14.184209823608398
entropies:30.299362182617188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1217.5935)
ToM Target loss= tensor(2247.7717)
optimized based on ToM loss
---------------------
gamma: 0.20043201513191103
training start after waiting for 1.1456172466278076 seconds
policy loss:-939.8089599609375
value loss:13.263038635253906
entropies:30.592498779296875
Policy training finished
---------------------
gamma: 0.20043201513191103
training start after waiting for 1.1634626388549805 seconds
policy loss:-46.828094482421875
value loss:10.228989601135254
entropies:32.17876434326172
Policy training finished
---------------------
gamma: 0.20043201513191103
training start after waiting for 1.2056822776794434 seconds
policy loss:-190.04904174804688
value loss:11.477572441101074
entropies:30.308307647705078
Policy training finished
---------------------
gamma: 0.20043201513191103
training start after waiting for 1.1499483585357666 seconds
policy loss:-1299.817626953125
value loss:42.425437927246094
entropies:44.85822296142578
Policy training finished
---------------------
gamma: 0.20043201513191103
training start after waiting for 1.1769633293151855 seconds
policy loss:397.3988037109375
value loss:13.574308395385742
entropies:29.4790096282959
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1183.0934)
ToM Target loss= tensor(2238.9912)
optimized based on ToM loss
---------------------
gamma: 0.20083287916217485
training start after waiting for 1.1956419944763184 seconds
policy loss:206.01808166503906
value loss:8.44407844543457
entropies:19.382488250732422
Policy training finished
---------------------
gamma: 0.20083287916217485
training start after waiting for 1.1938228607177734 seconds
policy loss:-283.2182312011719
value loss:15.309123992919922
entropies:25.960372924804688
Policy training finished
---------------------
gamma: 0.20083287916217485
training start after waiting for 1.1827168464660645 seconds
policy loss:766.5953979492188
value loss:18.778850555419922
entropies:33.97556686401367
Policy training finished
---------------------
gamma: 0.20083287916217485
training start after waiting for 1.1869797706604004 seconds
policy loss:-63.77031707763672
value loss:16.54010009765625
entropies:26.619314193725586
Policy training finished
---------------------
gamma: 0.20083287916217485
training start after waiting for 1.1682355403900146 seconds
policy loss:-930.4630126953125
value loss:9.703548431396484
entropies:21.769428253173828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1155.6632)
ToM Target loss= tensor(2235.9883)
optimized based on ToM loss
---------------------
gamma: 0.2012345449204992
training start after waiting for 1.1988084316253662 seconds
policy loss:-804.8611450195312
value loss:20.388856887817383
entropies:27.86078643798828
Policy training finished
---------------------
gamma: 0.2012345449204992
training start after waiting for 1.2152037620544434 seconds
policy loss:-1167.7935791015625
value loss:24.31390953063965
entropies:32.33006286621094
Policy training finished
---------------------
gamma: 0.2012345449204992
training start after waiting for 1.2107901573181152 seconds
policy loss:258.2782897949219
value loss:9.540881156921387
entropies:27.68376922607422
Policy training finished
---------------------
gamma: 0.2012345449204992
training start after waiting for 1.1445586681365967 seconds
policy loss:-1866.09814453125
value loss:40.1059684753418
entropies:37.211544036865234
Policy training finished
---------------------
gamma: 0.2012345449204992
training start after waiting for 1.158691644668579 seconds
policy loss:-374.83343505859375
value loss:25.186372756958008
entropies:34.558773040771484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1188.3762)
ToM Target loss= tensor(2246.5049)
optimized based on ToM loss
---------------------
gamma: 0.2016370140103402
training start after waiting for 1.1730308532714844 seconds
policy loss:105.45111846923828
value loss:12.761252403259277
entropies:30.92985725402832
Policy training finished
---------------------
gamma: 0.2016370140103402
training start after waiting for 1.2106075286865234 seconds
policy loss:115.52589416503906
value loss:12.630825996398926
entropies:28.92852020263672
Policy training finished
---------------------
gamma: 0.2016370140103402
training start after waiting for 1.1964290142059326 seconds
policy loss:-260.30926513671875
value loss:12.316279411315918
entropies:29.05373764038086
Policy training finished
---------------------
gamma: 0.2016370140103402
training start after waiting for 1.1873435974121094 seconds
policy loss:187.3831787109375
value loss:12.465780258178711
entropies:28.570045471191406
Policy training finished
---------------------
gamma: 0.2016370140103402
training start after waiting for 1.218106985092163 seconds
policy loss:-750.220703125
value loss:23.29940414428711
entropies:39.17924499511719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1130.9484)
ToM Target loss= tensor(2083.0476)
optimized based on ToM loss
---------------------
gamma: 0.20204028803836088
training start after waiting for 1.1877079010009766 seconds
policy loss:21.026493072509766
value loss:10.660810470581055
entropies:28.700061798095703
Policy training finished
---------------------
gamma: 0.20204028803836088
training start after waiting for 1.203312635421753 seconds
policy loss:-447.5285949707031
value loss:9.066445350646973
entropies:20.97879409790039
Policy training finished
---------------------
gamma: 0.20204028803836088
training start after waiting for 1.1827459335327148 seconds
policy loss:-1297.77197265625
value loss:24.428577423095703
entropies:37.62057876586914
Policy training finished
---------------------
gamma: 0.20204028803836088
training start after waiting for 1.1814684867858887 seconds
policy loss:-801.226806640625
value loss:23.93008041381836
entropies:23.96304702758789
Policy training finished
---------------------
gamma: 0.20204028803836088
training start after waiting for 1.1906821727752686 seconds
policy loss:164.33834838867188
value loss:13.026491165161133
entropies:33.442623138427734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1221.5834)
ToM Target loss= tensor(2171.6011)
optimized based on ToM loss
---------------------
gamma: 0.2024443686144376
training start after waiting for 1.1477437019348145 seconds
policy loss:-932.6640625
value loss:21.584243774414062
entropies:48.365169525146484
Policy training finished
---------------------
gamma: 0.2024443686144376
training start after waiting for 1.1632368564605713 seconds
policy loss:9.51421070098877
value loss:7.203058242797852
entropies:26.57770347595215
Policy training finished
---------------------
gamma: 0.2024443686144376
training start after waiting for 1.2142813205718994 seconds
policy loss:-234.47738647460938
value loss:17.191892623901367
entropies:32.973960876464844
Policy training finished
---------------------
gamma: 0.2024443686144376
training start after waiting for 1.1791939735412598 seconds
policy loss:-509.779052734375
value loss:27.325698852539062
entropies:32.689002990722656
Policy training finished
---------------------
gamma: 0.2024443686144376
training start after waiting for 1.1448018550872803 seconds
policy loss:-694.7876586914062
value loss:21.01967430114746
entropies:24.646709442138672
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1162.4121)
ToM Target loss= tensor(2154.4797)
optimized based on ToM loss
---------------------
gamma: 0.20284925735166648
training start after waiting for 1.2168142795562744 seconds
policy loss:-118.56582641601562
value loss:23.928613662719727
entropies:25.110424041748047
Policy training finished
---------------------
gamma: 0.20284925735166648
training start after waiting for 1.1670284271240234 seconds
policy loss:-190.0818634033203
value loss:12.508090019226074
entropies:24.5734806060791
Policy training finished
---------------------
gamma: 0.20284925735166648
training start after waiting for 1.1868855953216553 seconds
policy loss:-97.49412536621094
value loss:9.994430541992188
entropies:14.652912139892578
Policy training finished
---------------------
gamma: 0.20284925735166648
training start after waiting for 1.1469664573669434 seconds
policy loss:-835.3515625
value loss:16.262998580932617
entropies:46.05913162231445
Policy training finished
---------------------
gamma: 0.20284925735166648
training start after waiting for 1.1731455326080322 seconds
policy loss:-467.76593017578125
value loss:12.816488265991211
entropies:35.44648742675781
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1222.8584)
ToM Target loss= tensor(2236.7278)
optimized based on ToM loss
---------------------
gamma: 0.20325495586636982
training start after waiting for 1.1784977912902832 seconds
policy loss:-1359.33935546875
value loss:61.254730224609375
entropies:35.20375061035156
Policy training finished
---------------------
gamma: 0.20325495586636982
training start after waiting for 1.1457419395446777 seconds
policy loss:-693.842041015625
value loss:19.15297508239746
entropies:35.93415832519531
Policy training finished
---------------------
gamma: 0.20325495586636982
training start after waiting for 1.164968490600586 seconds
policy loss:-528.3888549804688
value loss:18.921781539916992
entropies:30.565797805786133
Policy training finished
---------------------
gamma: 0.20325495586636982
training start after waiting for 1.2108392715454102 seconds
policy loss:-432.969482421875
value loss:38.55105972290039
entropies:34.57042694091797
Policy training finished
---------------------
gamma: 0.20325495586636982
training start after waiting for 1.1665911674499512 seconds
policy loss:117.38074493408203
value loss:11.986156463623047
entropies:21.059261322021484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1164.5078)
ToM Target loss= tensor(2158.1538)
optimized based on ToM loss
---------------------
gamma: 0.20366146577810257
training start after waiting for 1.205939769744873 seconds
policy loss:-449.3564453125
value loss:26.036441802978516
entropies:35.337860107421875
Policy training finished
---------------------
gamma: 0.20366146577810257
training start after waiting for 1.1497535705566406 seconds
policy loss:-101.59571838378906
value loss:16.21167755126953
entropies:19.294729232788086
Policy training finished
---------------------
gamma: 0.20366146577810257
training start after waiting for 1.2126781940460205 seconds
policy loss:636.3868408203125
value loss:22.883892059326172
entropies:22.01276969909668
Policy training finished
---------------------
gamma: 0.20366146577810257
training start after waiting for 1.1779298782348633 seconds
policy loss:252.81173706054688
value loss:15.888606071472168
entropies:19.743000030517578
Policy training finished
---------------------
gamma: 0.20366146577810257
training start after waiting for 1.1426162719726562 seconds
policy loss:-18.15320587158203
value loss:11.185735702514648
entropies:30.208431243896484
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1183.9799)
ToM Target loss= tensor(2363.7876)
optimized based on ToM loss
---------------------
gamma: 0.20406878870965878
training start after waiting for 1.1952497959136963 seconds
policy loss:37.981468200683594
value loss:13.247444152832031
entropies:21.91942024230957
Policy training finished
---------------------
gamma: 0.20406878870965878
training start after waiting for 1.1836061477661133 seconds
policy loss:-730.7066650390625
value loss:23.018836975097656
entropies:39.276432037353516
Policy training finished
---------------------
gamma: 0.20406878870965878
training start after waiting for 1.189807415008545 seconds
policy loss:-236.3645477294922
value loss:16.798381805419922
entropies:23.106769561767578
Policy training finished
---------------------
gamma: 0.20406878870965878
training start after waiting for 1.1805660724639893 seconds
policy loss:-1617.808837890625
value loss:26.07709503173828
entropies:42.55744552612305
Policy training finished
---------------------
gamma: 0.20406878870965878
training start after waiting for 1.1417653560638428 seconds
policy loss:-53.838966369628906
value loss:12.879836082458496
entropies:26.20975685119629
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1175.3102)
ToM Target loss= tensor(2175.5574)
optimized based on ToM loss
---------------------
gamma: 0.2044769262870781
training start after waiting for 1.1341652870178223 seconds
policy loss:-294.5406799316406
value loss:13.036502838134766
entropies:35.561424255371094
Policy training finished
---------------------
gamma: 0.2044769262870781
training start after waiting for 1.184659719467163 seconds
policy loss:469.43011474609375
value loss:8.85693645477295
entropies:30.504615783691406
Policy training finished
---------------------
gamma: 0.2044769262870781
training start after waiting for 1.1858508586883545 seconds
policy loss:-1621.02294921875
value loss:49.77262496948242
entropies:50.67521667480469
Policy training finished
---------------------
gamma: 0.2044769262870781
training start after waiting for 1.1521100997924805 seconds
policy loss:-697.857421875
value loss:34.215003967285156
entropies:44.42005920410156
Policy training finished
---------------------
gamma: 0.2044769262870781
training start after waiting for 1.1794819831848145 seconds
policy loss:-971.96142578125
value loss:62.03031921386719
entropies:39.40232467651367
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1273.8821)
ToM Target loss= tensor(2155.2209)
optimized based on ToM loss
---------------------
gamma: 0.20488588013965225
training start after waiting for 1.1828556060791016 seconds
policy loss:149.34219360351562
value loss:30.886350631713867
entropies:33.018592834472656
Policy training finished
---------------------
gamma: 0.20488588013965225
training start after waiting for 1.2136530876159668 seconds
policy loss:250.6090087890625
value loss:24.724132537841797
entropies:30.521854400634766
Policy training finished
---------------------
gamma: 0.20488588013965225
training start after waiting for 1.2087154388427734 seconds
policy loss:-214.18106079101562
value loss:22.891611099243164
entropies:28.866615295410156
Policy training finished
---------------------
gamma: 0.20488588013965225
training start after waiting for 1.1422193050384521 seconds
policy loss:176.7769317626953
value loss:17.480972290039062
entropies:18.465797424316406
Policy training finished
---------------------
gamma: 0.20488588013965225
training start after waiting for 1.216043472290039 seconds
policy loss:-165.7383575439453
value loss:27.595218658447266
entropies:26.316030502319336
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1092.9838)
ToM Target loss= tensor(2139.2297)
optimized based on ToM loss
---------------------
gamma: 0.20529565189993157
training start after waiting for 1.1669881343841553 seconds
policy loss:189.00413513183594
value loss:15.09622859954834
entropies:28.319778442382812
Policy training finished
---------------------
gamma: 0.20529565189993157
training start after waiting for 1.2290260791778564 seconds
policy loss:-484.5994567871094
value loss:18.774755477905273
entropies:48.780731201171875
Policy training finished
---------------------
gamma: 0.20529565189993157
training start after waiting for 1.1466259956359863 seconds
policy loss:-821.3933715820312
value loss:23.467388153076172
entropies:25.83026885986328
Policy training finished
---------------------
gamma: 0.20529565189993157
training start after waiting for 1.1762478351593018 seconds
policy loss:-1814.9964599609375
value loss:47.34767150878906
entropies:41.91980743408203
Policy training finished
---------------------
gamma: 0.20529565189993157
training start after waiting for 1.1380252838134766 seconds
policy loss:-2691.100830078125
value loss:56.69364547729492
entropies:46.94361114501953
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1263.4802)
ToM Target loss= tensor(2184.8525)
optimized based on ToM loss
---------------------
gamma: 0.20570624320373143
training start after waiting for 1.1747186183929443 seconds
policy loss:-1304.3563232421875
value loss:25.432159423828125
entropies:26.6660099029541
Policy training finished
---------------------
gamma: 0.20570624320373143
training start after waiting for 1.1530125141143799 seconds
policy loss:-91.84992980957031
value loss:15.836137771606445
entropies:36.5924186706543
Policy training finished
---------------------
gamma: 0.20570624320373143
training start after waiting for 1.1473515033721924 seconds
policy loss:-2419.62548828125
value loss:70.48181915283203
entropies:48.139671325683594
Policy training finished
---------------------
gamma: 0.20570624320373143
training start after waiting for 1.1453497409820557 seconds
policy loss:-1168.8226318359375
value loss:16.814899444580078
entropies:33.319793701171875
Policy training finished
---------------------
gamma: 0.20570624320373143
training start after waiting for 1.1344990730285645 seconds
policy loss:-36.765296936035156
value loss:13.162559509277344
entropies:30.78883934020996
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1248.6812)
ToM Target loss= tensor(2258.2483)
optimized based on ToM loss
---------------------
gamma: 0.2061176556901389
training start after waiting for 1.1870744228363037 seconds
policy loss:-57.21267318725586
value loss:31.872907638549805
entropies:35.91783905029297
Policy training finished
---------------------
gamma: 0.2061176556901389
training start after waiting for 1.1948931217193604 seconds
policy loss:-2200.08447265625
value loss:35.022918701171875
entropies:50.724853515625
Policy training finished
---------------------
gamma: 0.2061176556901389
training start after waiting for 1.1586430072784424 seconds
policy loss:732.6531372070312
value loss:34.80757141113281
entropies:35.59300231933594
Policy training finished
---------------------
gamma: 0.2061176556901389
training start after waiting for 1.2049696445465088 seconds
policy loss:-361.7373962402344
value loss:20.182527542114258
entropies:34.34547805786133
Policy training finished
---------------------
gamma: 0.2061176556901389
training start after waiting for 1.140456199645996 seconds
policy loss:352.44281005859375
value loss:18.929227828979492
entropies:34.557151794433594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1214.0630)
ToM Target loss= tensor(2127.9375)
optimized based on ToM loss
---------------------
gamma: 0.20652989100151917
training start after waiting for 1.185539722442627 seconds
policy loss:157.73001098632812
value loss:14.423213005065918
entropies:25.01918601989746
Policy training finished
---------------------
gamma: 0.20652989100151917
training start after waiting for 1.203226089477539 seconds
policy loss:-455.5536193847656
value loss:20.71026039123535
entropies:33.21516036987305
Policy training finished
---------------------
gamma: 0.20652989100151917
training start after waiting for 1.175483226776123 seconds
policy loss:-75.66558837890625
value loss:18.014829635620117
entropies:27.399005889892578
Policy training finished
---------------------
gamma: 0.20652989100151917
training start after waiting for 1.1519381999969482 seconds
policy loss:-1230.778564453125
value loss:32.373321533203125
entropies:44.195735931396484
Policy training finished
---------------------
gamma: 0.20652989100151917
training start after waiting for 1.1541385650634766 seconds
policy loss:-1470.6629638671875
value loss:26.493364334106445
entropies:33.87004089355469
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1124.8634)
ToM Target loss= tensor(2160.7717)
optimized based on ToM loss
---------------------
gamma: 0.2069429507835222
training start after waiting for 1.1754963397979736 seconds
policy loss:-1222.7618408203125
value loss:26.05003547668457
entropies:36.51213073730469
Policy training finished
---------------------
gamma: 0.2069429507835222
training start after waiting for 1.1803789138793945 seconds
policy loss:-128.1348419189453
value loss:8.994514465332031
entropies:30.003015518188477
Policy training finished
---------------------
gamma: 0.2069429507835222
training start after waiting for 1.2052314281463623 seconds
policy loss:-874.9197998046875
value loss:11.064310073852539
entropies:28.015056610107422
Policy training finished
---------------------
gamma: 0.2069429507835222
training start after waiting for 1.1821627616882324 seconds
policy loss:-69.21131896972656
value loss:28.310457229614258
entropies:38.741729736328125
Policy training finished
---------------------
gamma: 0.2069429507835222
training start after waiting for 1.2029640674591064 seconds
policy loss:195.63970947265625
value loss:6.577603340148926
entropies:24.270124435424805
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1139.9271)
ToM Target loss= tensor(2077.8765)
optimized based on ToM loss
---------------------
gamma: 0.20735683668508925
training start after waiting for 1.1408729553222656 seconds
policy loss:764.4705200195312
value loss:25.131183624267578
entropies:32.47554016113281
Policy training finished
---------------------
gamma: 0.20735683668508925
training start after waiting for 1.2073662281036377 seconds
policy loss:-500.7156982421875
value loss:30.832443237304688
entropies:27.628957748413086
Policy training finished
---------------------
gamma: 0.20735683668508925
training start after waiting for 1.211810827255249 seconds
policy loss:-130.11669921875
value loss:17.38261604309082
entropies:20.599685668945312
Policy training finished
---------------------
gamma: 0.20735683668508925
training start after waiting for 1.194319486618042 seconds
policy loss:-586.0147705078125
value loss:21.991708755493164
entropies:31.045143127441406
Policy training finished
---------------------
gamma: 0.20735683668508925
training start after waiting for 1.1601264476776123 seconds
policy loss:-422.30645751953125
value loss:10.909212112426758
entropies:43.73002624511719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1200.4027)
ToM Target loss= tensor(2131.8760)
optimized based on ToM loss
---------------------
gamma: 0.20777155035845943
training start after waiting for 1.147876262664795 seconds
policy loss:-581.1538696289062
value loss:15.168530464172363
entropies:27.1908016204834
Policy training finished
---------------------
gamma: 0.20777155035845943
training start after waiting for 1.185211181640625 seconds
policy loss:-431.4922180175781
value loss:22.380382537841797
entropies:23.825498580932617
Policy training finished
---------------------
gamma: 0.20777155035845943
training start after waiting for 1.1733424663543701 seconds
policy loss:-4.2157979011535645
value loss:4.764426231384277
entropies:18.585662841796875
Policy training finished
---------------------
gamma: 0.20777155035845943
training start after waiting for 1.1997935771942139 seconds
policy loss:-736.59619140625
value loss:20.276931762695312
entropies:36.01588439941406
Policy training finished
---------------------
gamma: 0.20777155035845943
training start after waiting for 1.1398682594299316 seconds
policy loss:-750.565673828125
value loss:10.657204627990723
entropies:31.318359375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1153.4200)
ToM Target loss= tensor(2212.2588)
optimized based on ToM loss
---------------------
gamma: 0.20818709345917635
training start after waiting for 1.1446869373321533 seconds
policy loss:-605.9487915039062
value loss:14.105751991271973
entropies:19.592735290527344
Policy training finished
---------------------
gamma: 0.20818709345917635
training start after waiting for 1.2318618297576904 seconds
policy loss:-460.5130615234375
value loss:10.497148513793945
entropies:38.38391876220703
Policy training finished
---------------------
gamma: 0.20818709345917635
training start after waiting for 1.1774845123291016 seconds
policy loss:68.93549346923828
value loss:7.114247798919678
entropies:34.298439025878906
Policy training finished
---------------------
gamma: 0.20818709345917635
training start after waiting for 1.1395466327667236 seconds
policy loss:249.3677978515625
value loss:13.475234985351562
entropies:30.772972106933594
Policy training finished
---------------------
gamma: 0.20818709345917635
training start after waiting for 1.134835958480835 seconds
policy loss:246.01260375976562
value loss:20.42009735107422
entropies:39.79396057128906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1177.4934)
ToM Target loss= tensor(2092.6748)
optimized based on ToM loss
---------------------
gamma: 0.20860346764609472
training start after waiting for 1.1918349266052246 seconds
policy loss:-640.6246948242188
value loss:22.1787166595459
entropies:41.427276611328125
Policy training finished
---------------------
gamma: 0.20860346764609472
training start after waiting for 1.2013590335845947 seconds
policy loss:201.16285705566406
value loss:14.093667984008789
entropies:21.36425018310547
Policy training finished
---------------------
gamma: 0.20860346764609472
training start after waiting for 1.1386222839355469 seconds
policy loss:-1097.8592529296875
value loss:24.999330520629883
entropies:45.52338790893555
Policy training finished
---------------------
gamma: 0.20860346764609472
training start after waiting for 1.1998481750488281 seconds
policy loss:-67.28953552246094
value loss:13.139202117919922
entropies:39.79663848876953
Policy training finished
---------------------
gamma: 0.20860346764609472
training start after waiting for 1.1829588413238525 seconds
policy loss:269.91619873046875
value loss:10.599239349365234
entropies:26.347238540649414
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1238.0740)
ToM Target loss= tensor(2199.9114)
optimized based on ToM loss
---------------------
gamma: 0.2090206745813869
training start after waiting for 1.149935245513916 seconds
policy loss:-110.19212341308594
value loss:8.998077392578125
entropies:25.06888198852539
Policy training finished
---------------------
gamma: 0.2090206745813869
training start after waiting for 1.1954913139343262 seconds
policy loss:-568.71142578125
value loss:23.907733917236328
entropies:47.1820182800293
Policy training finished
---------------------
gamma: 0.2090206745813869
training start after waiting for 1.171849250793457 seconds
policy loss:284.7299499511719
value loss:7.374913215637207
entropies:20.790218353271484
Policy training finished
---------------------
gamma: 0.2090206745813869
training start after waiting for 1.155341625213623 seconds
policy loss:-958.5812377929688
value loss:27.686534881591797
entropies:55.80085754394531
Policy training finished
---------------------
gamma: 0.2090206745813869
training start after waiting for 1.2036097049713135 seconds
policy loss:-831.1910400390625
value loss:43.99666976928711
entropies:33.80426025390625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1238.5325)
ToM Target loss= tensor(2220.3542)
optimized based on ToM loss
---------------------
gamma: 0.20943871593054966
training start after waiting for 1.1986384391784668 seconds
policy loss:-129.14846801757812
value loss:9.365209579467773
entropies:19.569358825683594
Policy training finished
---------------------
gamma: 0.20943871593054966
training start after waiting for 1.1962172985076904 seconds
policy loss:131.79454040527344
value loss:22.64054298400879
entropies:34.585182189941406
Policy training finished
---------------------
gamma: 0.20943871593054966
training start after waiting for 1.19586181640625 seconds
policy loss:113.27637481689453
value loss:6.975231647491455
entropies:11.905439376831055
Policy training finished
---------------------
gamma: 0.20943871593054966
training start after waiting for 1.1841561794281006 seconds
policy loss:-151.88548278808594
value loss:8.377744674682617
entropies:22.9918270111084
Policy training finished
---------------------
gamma: 0.20943871593054966
training start after waiting for 1.1922028064727783 seconds
policy loss:-371.99407958984375
value loss:20.24657440185547
entropies:24.376419067382812
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1160.8909)
ToM Target loss= tensor(2266.8337)
optimized based on ToM loss
---------------------
gamma: 0.20985759336241075
training start after waiting for 1.1859095096588135 seconds
policy loss:-804.72607421875
value loss:24.107521057128906
entropies:35.917076110839844
Policy training finished
---------------------
gamma: 0.20985759336241075
training start after waiting for 1.1837031841278076 seconds
policy loss:210.80418395996094
value loss:11.065051078796387
entropies:25.892763137817383
Policy training finished
---------------------
gamma: 0.20985759336241075
training start after waiting for 1.2072947025299072 seconds
policy loss:-1021.7443237304688
value loss:20.975561141967773
entropies:32.45097351074219
Policy training finished
---------------------
gamma: 0.20985759336241075
training start after waiting for 1.1842148303985596 seconds
policy loss:-498.438232421875
value loss:29.760595321655273
entropies:33.272621154785156
Policy training finished
---------------------
gamma: 0.20985759336241075
training start after waiting for 1.2121272087097168 seconds
policy loss:-769.2745361328125
value loss:10.06509780883789
entropies:20.870372772216797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1282.4021)
ToM Target loss= tensor(2222.3274)
optimized based on ToM loss
---------------------
gamma: 0.21027730854913557
training start after waiting for 1.1407346725463867 seconds
policy loss:-895.4894409179688
value loss:19.858196258544922
entropies:28.398908615112305
Policy training finished
---------------------
gamma: 0.21027730854913557
training start after waiting for 1.1904187202453613 seconds
policy loss:-8.799683570861816
value loss:3.0156683921813965
entropies:17.82251739501953
Policy training finished
---------------------
gamma: 0.21027730854913557
training start after waiting for 1.1763420104980469 seconds
policy loss:-283.5826416015625
value loss:10.588165283203125
entropies:22.110652923583984
Policy training finished
---------------------
gamma: 0.21027730854913557
training start after waiting for 1.1808373928070068 seconds
policy loss:98.767578125
value loss:13.312053680419922
entropies:25.149044036865234
Policy training finished
---------------------
gamma: 0.21027730854913557
training start after waiting for 1.1487410068511963 seconds
policy loss:487.7665710449219
value loss:9.732086181640625
entropies:17.648929595947266
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1155.5499)
ToM Target loss= tensor(2361.0037)
optimized based on ToM loss
---------------------
gamma: 0.21069786316623385
training start after waiting for 1.1996886730194092 seconds
policy loss:-895.5396118164062
value loss:36.04568862915039
entropies:35.453102111816406
Policy training finished
---------------------
gamma: 0.21069786316623385
training start after waiting for 1.1945395469665527 seconds
policy loss:-42.66584777832031
value loss:10.555041313171387
entropies:11.673269271850586
Policy training finished
---------------------
gamma: 0.21069786316623385
training start after waiting for 1.14750075340271 seconds
policy loss:59.392669677734375
value loss:15.214492797851562
entropies:22.912090301513672
Policy training finished
---------------------
gamma: 0.21069786316623385
training start after waiting for 1.2105247974395752 seconds
policy loss:-172.1781463623047
value loss:10.461027145385742
entropies:28.326610565185547
Policy training finished
---------------------
gamma: 0.21069786316623385
training start after waiting for 1.1912462711334229 seconds
policy loss:50.53254699707031
value loss:12.068524360656738
entropies:29.745460510253906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1239.7943)
ToM Target loss= tensor(2267.3728)
optimized based on ToM loss
---------------------
gamma: 0.2111192588925663
training start after waiting for 1.205007553100586 seconds
policy loss:-2648.44921875
value loss:107.84611511230469
entropies:36.5523567199707
Policy training finished
---------------------
gamma: 0.2111192588925663
training start after waiting for 1.1775507926940918 seconds
policy loss:-769.6061401367188
value loss:17.10336685180664
entropies:23.256484985351562
Policy training finished
---------------------
gamma: 0.2111192588925663
training start after waiting for 1.2191481590270996 seconds
policy loss:-120.70555877685547
value loss:15.084598541259766
entropies:21.97673797607422
Policy training finished
---------------------
gamma: 0.2111192588925663
training start after waiting for 1.1861259937286377 seconds
policy loss:139.46820068359375
value loss:22.746095657348633
entropies:24.67201805114746
Policy training finished
---------------------
gamma: 0.2111192588925663
training start after waiting for 1.1475892066955566 seconds
policy loss:-187.35780334472656
value loss:31.684024810791016
entropies:34.37818908691406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1079.2736)
ToM Target loss= tensor(2177.5676)
optimized based on ToM loss
---------------------
gamma: 0.21154149741035144
training start after waiting for 1.1445233821868896 seconds
policy loss:-203.75140380859375
value loss:24.829612731933594
entropies:31.285335540771484
Policy training finished
---------------------
gamma: 0.21154149741035144
training start after waiting for 1.1808359622955322 seconds
policy loss:273.8924560546875
value loss:19.575639724731445
entropies:26.79483413696289
Policy training finished
---------------------
gamma: 0.21154149741035144
training start after waiting for 1.1696186065673828 seconds
policy loss:167.29833984375
value loss:11.355363845825195
entropies:18.386812210083008
Policy training finished
---------------------
gamma: 0.21154149741035144
training start after waiting for 1.1745939254760742 seconds
policy loss:-225.98287963867188
value loss:12.957256317138672
entropies:19.718481063842773
Policy training finished
---------------------
gamma: 0.21154149741035144
training start after waiting for 1.1950368881225586 seconds
policy loss:20.037647247314453
value loss:10.122945785522461
entropies:26.01464080810547
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1075.7219)
ToM Target loss= tensor(2135.5596)
optimized based on ToM loss
---------------------
gamma: 0.21196458040517216
training start after waiting for 1.1387221813201904 seconds
policy loss:-781.1697998046875
value loss:27.97775650024414
entropies:23.915891647338867
Policy training finished
---------------------
gamma: 0.21196458040517216
training start after waiting for 1.1774098873138428 seconds
policy loss:-1261.8408203125
value loss:57.7403678894043
entropies:46.784645080566406
Policy training finished
---------------------
gamma: 0.21196458040517216
training start after waiting for 1.146667718887329 seconds
policy loss:-415.3633728027344
value loss:18.179611206054688
entropies:24.273021697998047
Policy training finished
---------------------
gamma: 0.21196458040517216
training start after waiting for 1.1436338424682617 seconds
policy loss:-28.37073516845703
value loss:6.030232906341553
entropies:19.868194580078125
Policy training finished
---------------------
gamma: 0.21196458040517216
training start after waiting for 1.138441562652588 seconds
policy loss:-48.816864013671875
value loss:10.313008308410645
entropies:26.312602996826172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1200.6252)
ToM Target loss= tensor(2203.0767)
optimized based on ToM loss
---------------------
gamma: 0.21238850956598251
training start after waiting for 1.184279441833496 seconds
policy loss:-591.943359375
value loss:28.03638458251953
entropies:30.145084381103516
Policy training finished
---------------------
gamma: 0.21238850956598251
training start after waiting for 1.2125458717346191 seconds
policy loss:-396.00628662109375
value loss:13.666403770446777
entropies:16.071340560913086
Policy training finished
---------------------
gamma: 0.21238850956598251
training start after waiting for 1.180828332901001 seconds
policy loss:7.8221330642700195
value loss:8.64807415008545
entropies:14.123536109924316
Policy training finished
---------------------
gamma: 0.21238850956598251
training start after waiting for 1.2356486320495605 seconds
policy loss:-818.6046142578125
value loss:19.857744216918945
entropies:26.65079689025879
Policy training finished
---------------------
gamma: 0.21238850956598251
training start after waiting for 1.142188310623169 seconds
policy loss:-260.47662353515625
value loss:10.591259002685547
entropies:23.857219696044922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1173.3093)
ToM Target loss= tensor(2288.5842)
optimized based on ToM loss
---------------------
gamma: 0.21281328658511448
training start after waiting for 1.2505340576171875 seconds
policy loss:-996.5983276367188
value loss:30.554582595825195
entropies:29.921344757080078
Policy training finished
---------------------
gamma: 0.21281328658511448
training start after waiting for 1.1444239616394043 seconds
policy loss:-729.6868286132812
value loss:18.678905487060547
entropies:23.703977584838867
Policy training finished
---------------------
gamma: 0.21281328658511448
training start after waiting for 1.1821486949920654 seconds
policy loss:-1872.029541015625
value loss:32.21685791015625
entropies:31.065898895263672
Policy training finished
---------------------
gamma: 0.21281328658511448
training start after waiting for 1.1977059841156006 seconds
policy loss:204.01092529296875
value loss:19.796430587768555
entropies:38.18785095214844
Policy training finished
---------------------
gamma: 0.21281328658511448
training start after waiting for 1.1843585968017578 seconds
policy loss:-4.5409369468688965
value loss:19.642135620117188
entropies:29.05414581298828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1220.1412)
ToM Target loss= tensor(2260.8635)
optimized based on ToM loss
---------------------
gamma: 0.2132389131582847
training start after waiting for 1.1972861289978027 seconds
policy loss:371.7550964355469
value loss:7.374241352081299
entropies:24.103748321533203
Policy training finished
---------------------
gamma: 0.2132389131582847
training start after waiting for 1.1944689750671387 seconds
policy loss:-891.5306396484375
value loss:29.62630844116211
entropies:31.125133514404297
Policy training finished
---------------------
gamma: 0.2132389131582847
training start after waiting for 1.1906962394714355 seconds
policy loss:7.968285083770752
value loss:15.066335678100586
entropies:20.833328247070312
Policy training finished
---------------------
gamma: 0.2132389131582847
training start after waiting for 1.187211036682129 seconds
policy loss:187.5692901611328
value loss:8.175528526306152
entropies:23.880168914794922
Policy training finished
---------------------
gamma: 0.2132389131582847
training start after waiting for 1.150226354598999 seconds
policy loss:-386.3408508300781
value loss:8.343063354492188
entropies:32.56711196899414
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1041.0459)
ToM Target loss= tensor(2193.9297)
optimized based on ToM loss
---------------------
gamma: 0.21366539098460127
training start after waiting for 1.1800546646118164 seconds
policy loss:62.99073791503906
value loss:10.66164779663086
entropies:20.037256240844727
Policy training finished
---------------------
gamma: 0.21366539098460127
training start after waiting for 1.204075813293457 seconds
policy loss:-222.73516845703125
value loss:17.193151473999023
entropies:25.986682891845703
Policy training finished
---------------------
gamma: 0.21366539098460127
training start after waiting for 1.158127784729004 seconds
policy loss:-624.4739379882812
value loss:11.767319679260254
entropies:33.780181884765625
Policy training finished
---------------------
gamma: 0.21366539098460127
training start after waiting for 1.1803064346313477 seconds
policy loss:-246.65966796875
value loss:8.269648551940918
entropies:16.43187141418457
Policy training finished
---------------------
gamma: 0.21366539098460127
training start after waiting for 1.1859204769134521 seconds
policy loss:213.53944396972656
value loss:7.304320335388184
entropies:22.399490356445312
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1110.7592)
ToM Target loss= tensor(2169.9888)
optimized based on ToM loss
---------------------
gamma: 0.2140927217665705
training start after waiting for 1.1446278095245361 seconds
policy loss:-1119.3603515625
value loss:30.617666244506836
entropies:36.126708984375
Policy training finished
---------------------
gamma: 0.2140927217665705
training start after waiting for 1.206925868988037 seconds
policy loss:139.45950317382812
value loss:16.91150665283203
entropies:27.11463165283203
Policy training finished
---------------------
gamma: 0.2140927217665705
training start after waiting for 1.19626784324646 seconds
policy loss:100.91041564941406
value loss:11.834343910217285
entropies:24.276485443115234
Policy training finished
---------------------
gamma: 0.2140927217665705
training start after waiting for 1.1894407272338867 seconds
policy loss:277.4804382324219
value loss:14.200743675231934
entropies:13.991291046142578
Policy training finished
---------------------
gamma: 0.2140927217665705
training start after waiting for 1.18520188331604 seconds
policy loss:122.12945556640625
value loss:37.118377685546875
entropies:21.642860412597656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1134.9714)
ToM Target loss= tensor(2222.6836)
optimized based on ToM loss
---------------------
gamma: 0.21452090721010364
training start after waiting for 1.2095494270324707 seconds
policy loss:-290.2874450683594
value loss:33.638877868652344
entropies:35.62403106689453
Policy training finished
---------------------
gamma: 0.21452090721010364
training start after waiting for 1.2226078510284424 seconds
policy loss:-287.33270263671875
value loss:19.226608276367188
entropies:25.844013214111328
Policy training finished
---------------------
gamma: 0.21452090721010364
training start after waiting for 1.1691687107086182 seconds
policy loss:-419.2149658203125
value loss:9.136117935180664
entropies:26.72250747680664
Policy training finished
---------------------
gamma: 0.21452090721010364
training start after waiting for 1.1807072162628174 seconds
policy loss:-1769.451171875
value loss:43.127838134765625
entropies:40.98631286621094
Policy training finished
---------------------
gamma: 0.21452090721010364
training start after waiting for 1.2410616874694824 seconds
policy loss:16.98499870300293
value loss:25.6677303314209
entropies:23.46837615966797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1129.5554)
ToM Target loss= tensor(2186.4585)
optimized based on ToM loss
---------------------
gamma: 0.21494994902452386
training start after waiting for 1.1055762767791748 seconds
policy loss:-1317.77392578125
value loss:33.1212043762207
entropies:31.113014221191406
Policy training finished
---------------------
gamma: 0.21494994902452386
training start after waiting for 1.1661016941070557 seconds
policy loss:-238.39976501464844
value loss:37.428829193115234
entropies:33.988792419433594
Policy training finished
---------------------
gamma: 0.21494994902452386
training start after waiting for 1.181135654449463 seconds
policy loss:-855.73779296875
value loss:23.56070327758789
entropies:25.657644271850586
Policy training finished
---------------------
gamma: 0.21494994902452386
training start after waiting for 1.1921765804290771 seconds
policy loss:71.29962921142578
value loss:21.31954574584961
entropies:24.981670379638672
Policy training finished
---------------------
gamma: 0.21494994902452386
training start after waiting for 1.190042495727539 seconds
policy loss:-473.0177001953125
value loss:21.37875747680664
entropies:32.521087646484375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1172.4434)
ToM Target loss= tensor(2142.9980)
optimized based on ToM loss
---------------------
gamma: 0.2153798489225729
training start after waiting for 1.1558496952056885 seconds
policy loss:468.0510559082031
value loss:18.81229591369629
entropies:21.91104507446289
Policy training finished
---------------------
gamma: 0.2153798489225729
training start after waiting for 1.1802198886871338 seconds
policy loss:-387.9569396972656
value loss:16.63913345336914
entropies:27.936599731445312
Policy training finished
---------------------
gamma: 0.2153798489225729
training start after waiting for 1.181086778640747 seconds
policy loss:-611.6438598632812
value loss:15.355603218078613
entropies:33.162391662597656
Policy training finished
---------------------
gamma: 0.2153798489225729
training start after waiting for 1.1374897956848145 seconds
policy loss:-537.0736083984375
value loss:13.816035270690918
entropies:38.954673767089844
Policy training finished
---------------------
gamma: 0.2153798489225729
training start after waiting for 1.2103781700134277 seconds
policy loss:-937.9908447265625
value loss:29.073148727416992
entropies:44.4642448425293
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1129.7068)
ToM Target loss= tensor(2153.5046)
optimized based on ToM loss
---------------------
gamma: 0.21581060862041804
training start after waiting for 1.2002942562103271 seconds
policy loss:-102.5978775024414
value loss:13.646665573120117
entropies:27.54095458984375
Policy training finished
---------------------
gamma: 0.21581060862041804
training start after waiting for 1.1806554794311523 seconds
policy loss:-108.53805541992188
value loss:20.694847106933594
entropies:46.34040451049805
Policy training finished
---------------------
gamma: 0.21581060862041804
training start after waiting for 1.150223970413208 seconds
policy loss:-591.6895751953125
value loss:9.735210418701172
entropies:29.995494842529297
Policy training finished
---------------------
gamma: 0.21581060862041804
training start after waiting for 1.2001781463623047 seconds
policy loss:-411.2685546875
value loss:16.034103393554688
entropies:45.264644622802734
Policy training finished
---------------------
gamma: 0.21581060862041804
training start after waiting for 1.1748650074005127 seconds
policy loss:-299.1582336425781
value loss:7.967095375061035
entropies:25.687244415283203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1185.4441)
ToM Target loss= tensor(2179.1365)
optimized based on ToM loss
---------------------
gamma: 0.21624222983765887
training start after waiting for 1.1434361934661865 seconds
policy loss:-504.8017578125
value loss:11.43846321105957
entropies:31.12224578857422
Policy training finished
---------------------
gamma: 0.21624222983765887
training start after waiting for 1.150270938873291 seconds
policy loss:-372.975830078125
value loss:34.360382080078125
entropies:33.70330047607422
Policy training finished
---------------------
gamma: 0.21624222983765887
training start after waiting for 1.191141128540039 seconds
policy loss:228.51698303222656
value loss:8.13670539855957
entropies:28.507320404052734
Policy training finished
---------------------
gamma: 0.21624222983765887
training start after waiting for 1.201303482055664 seconds
policy loss:-1407.1265869140625
value loss:45.30876159667969
entropies:40.66988754272461
Policy training finished
---------------------
gamma: 0.21624222983765887
training start after waiting for 1.146677017211914 seconds
policy loss:-1551.65380859375
value loss:21.298738479614258
entropies:36.60619354248047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1183.6543)
ToM Target loss= tensor(2239.8542)
optimized based on ToM loss
---------------------
gamma: 0.21667471429733418
training start after waiting for 1.1740570068359375 seconds
policy loss:66.15592193603516
value loss:11.032200813293457
entropies:30.512893676757812
Policy training finished
---------------------
gamma: 0.21667471429733418
training start after waiting for 1.2084672451019287 seconds
policy loss:54.8223991394043
value loss:12.14212703704834
entropies:23.29383087158203
Policy training finished
---------------------
gamma: 0.21667471429733418
training start after waiting for 1.1449649333953857 seconds
policy loss:-849.9947509765625
value loss:17.42606544494629
entropies:35.847904205322266
Policy training finished
---------------------
gamma: 0.21667471429733418
training start after waiting for 1.1772804260253906 seconds
policy loss:150.29339599609375
value loss:8.735478401184082
entropies:38.60426330566406
Policy training finished
---------------------
gamma: 0.21667471429733418
training start after waiting for 1.1974480152130127 seconds
policy loss:-650.783935546875
value loss:20.565502166748047
entropies:38.656455993652344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1113.4236)
ToM Target loss= tensor(2096.3557)
optimized based on ToM loss
---------------------
gamma: 0.21710806372592883
training start after waiting for 1.2037341594696045 seconds
policy loss:-397.50616455078125
value loss:13.476418495178223
entropies:34.756446838378906
Policy training finished
---------------------
gamma: 0.21710806372592883
training start after waiting for 1.1999635696411133 seconds
policy loss:-367.19586181640625
value loss:21.771970748901367
entropies:34.54143524169922
Policy training finished
---------------------
gamma: 0.21710806372592883
training start after waiting for 1.1711716651916504 seconds
policy loss:-173.81402587890625
value loss:24.146785736083984
entropies:17.840742111206055
Policy training finished
---------------------
gamma: 0.21710806372592883
training start after waiting for 1.171046495437622 seconds
policy loss:-807.3411865234375
value loss:16.953420639038086
entropies:36.27574920654297
Policy training finished
---------------------
gamma: 0.21710806372592883
training start after waiting for 1.1950106620788574 seconds
policy loss:85.25008392333984
value loss:6.708215236663818
entropies:22.258346557617188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1075.7472)
ToM Target loss= tensor(2106.2261)
optimized based on ToM loss
---------------------
gamma: 0.2175422798533807
training start after waiting for 1.1827614307403564 seconds
policy loss:-208.43482971191406
value loss:18.585554122924805
entropies:23.260948181152344
Policy training finished
---------------------
gamma: 0.2175422798533807
training start after waiting for 1.1770594120025635 seconds
policy loss:-325.8887939453125
value loss:18.191709518432617
entropies:18.17809295654297
Policy training finished
---------------------
gamma: 0.2175422798533807
training start after waiting for 1.1744801998138428 seconds
policy loss:-158.36732482910156
value loss:14.575404167175293
entropies:16.673511505126953
Policy training finished
---------------------
gamma: 0.2175422798533807
training start after waiting for 1.1949570178985596 seconds
policy loss:-878.0596313476562
value loss:40.08456039428711
entropies:39.92780303955078
Policy training finished
---------------------
gamma: 0.2175422798533807
training start after waiting for 1.1932106018066406 seconds
policy loss:-194.57444763183594
value loss:17.093496322631836
entropies:16.23177146911621
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1156.0812)
ToM Target loss= tensor(2288.4536)
optimized based on ToM loss
---------------------
gamma: 0.21797736441308746
training start after waiting for 1.1265652179718018 seconds
policy loss:66.1536865234375
value loss:10.333417892456055
entropies:8.149215698242188
Policy training finished
---------------------
gamma: 0.21797736441308746
training start after waiting for 1.1966164112091064 seconds
policy loss:285.2299499511719
value loss:19.573135375976562
entropies:30.72722053527832
Policy training finished
---------------------
gamma: 0.21797736441308746
training start after waiting for 1.2033724784851074 seconds
policy loss:-299.48223876953125
value loss:8.206315040588379
entropies:16.55649757385254
Policy training finished
---------------------
gamma: 0.21797736441308746
training start after waiting for 1.1693994998931885 seconds
policy loss:-280.66534423828125
value loss:39.84358596801758
entropies:30.315608978271484
Policy training finished
---------------------
gamma: 0.21797736441308746
training start after waiting for 1.2108399868011475 seconds
policy loss:-2189.877197265625
value loss:111.4868392944336
entropies:43.57756805419922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1079.3363)
ToM Target loss= tensor(2279.0107)
optimized based on ToM loss
---------------------
gamma: 0.21841331914191364
training start after waiting for 1.1432487964630127 seconds
policy loss:270.3499450683594
value loss:16.63382339477539
entropies:21.086946487426758
Policy training finished
---------------------
gamma: 0.21841331914191364
training start after waiting for 1.1762340068817139 seconds
policy loss:-106.41213989257812
value loss:18.132396697998047
entropies:19.80304718017578
Policy training finished
---------------------
gamma: 0.21841331914191364
training start after waiting for 1.1633212566375732 seconds
policy loss:-65.44884490966797
value loss:13.96256160736084
entropies:22.18876838684082
Policy training finished
---------------------
gamma: 0.21841331914191364
training start after waiting for 1.173719882965088 seconds
policy loss:-327.8969421386719
value loss:17.624500274658203
entropies:29.6015625
Policy training finished
---------------------
gamma: 0.21841331914191364
training start after waiting for 1.1586766242980957 seconds
policy loss:-640.5018310546875
value loss:12.114863395690918
entropies:24.07962417602539
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1085.7844)
ToM Target loss= tensor(2313.5488)
optimized based on ToM loss
---------------------
gamma: 0.21885014578019746
training start after waiting for 1.2076497077941895 seconds
policy loss:-756.0184326171875
value loss:36.754756927490234
entropies:25.807952880859375
Policy training finished
---------------------
gamma: 0.21885014578019746
training start after waiting for 1.1613552570343018 seconds
policy loss:281.7657775878906
value loss:16.996355056762695
entropies:21.891483306884766
Policy training finished
---------------------
gamma: 0.21885014578019746
training start after waiting for 1.1901359558105469 seconds
policy loss:-152.70626831054688
value loss:17.696849822998047
entropies:39.18207931518555
Policy training finished
---------------------
gamma: 0.21885014578019746
training start after waiting for 1.1326136589050293 seconds
policy loss:-939.3062744140625
value loss:34.85315704345703
entropies:31.645767211914062
Policy training finished
---------------------
gamma: 0.21885014578019746
training start after waiting for 1.1810986995697021 seconds
policy loss:53.045555114746094
value loss:9.53353500366211
entropies:18.976428985595703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1166.0671)
ToM Target loss= tensor(2259.8242)
optimized based on ToM loss
---------------------
gamma: 0.21928784607175786
training start after waiting for 1.153587818145752 seconds
policy loss:154.10452270507812
value loss:10.215242385864258
entropies:24.759836196899414
Policy training finished
---------------------
gamma: 0.21928784607175786
training start after waiting for 1.2129161357879639 seconds
policy loss:-2278.864013671875
value loss:64.27098083496094
entropies:59.731502532958984
Policy training finished
---------------------
gamma: 0.21928784607175786
training start after waiting for 1.2093727588653564 seconds
policy loss:-561.6878051757812
value loss:15.32276439666748
entropies:25.352378845214844
Policy training finished
---------------------
gamma: 0.21928784607175786
training start after waiting for 1.2104339599609375 seconds
policy loss:-21.93288803100586
value loss:5.109692573547363
entropies:18.922218322753906
Policy training finished
---------------------
gamma: 0.21928784607175786
training start after waiting for 1.190274953842163 seconds
policy loss:-698.98828125
value loss:23.484941482543945
entropies:26.321279525756836
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1091.5570)
ToM Target loss= tensor(2158.3259)
optimized based on ToM loss
---------------------
gamma: 0.21972642176390136
training start after waiting for 1.1861662864685059 seconds
policy loss:-64.4792709350586
value loss:7.523054122924805
entropies:21.801698684692383
Policy training finished
---------------------
gamma: 0.21972642176390136
training start after waiting for 1.201845645904541 seconds
policy loss:-1001.7664794921875
value loss:42.56230545043945
entropies:50.18960189819336
Policy training finished
---------------------
gamma: 0.21972642176390136
training start after waiting for 1.1871273517608643 seconds
policy loss:-110.85787200927734
value loss:13.0941162109375
entropies:37.51591491699219
Policy training finished
---------------------
gamma: 0.21972642176390136
training start after waiting for 1.175917625427246 seconds
policy loss:-503.7627258300781
value loss:23.51717758178711
entropies:44.49678039550781
Policy training finished
---------------------
gamma: 0.21972642176390136
training start after waiting for 1.1572771072387695 seconds
policy loss:-434.29534912109375
value loss:13.321205139160156
entropies:34.33789825439453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1193.1705)
ToM Target loss= tensor(2158.5618)
optimized based on ToM loss
---------------------
gamma: 0.22016587460742917
training start after waiting for 1.2131164073944092 seconds
policy loss:-1248.7589111328125
value loss:40.41061019897461
entropies:38.708984375
Policy training finished
---------------------
gamma: 0.22016587460742917
training start after waiting for 1.158376693725586 seconds
policy loss:-1691.28173828125
value loss:42.931697845458984
entropies:45.101016998291016
Policy training finished
---------------------
gamma: 0.22016587460742917
training start after waiting for 1.1827597618103027 seconds
policy loss:200.91162109375
value loss:12.705947875976562
entropies:33.27033233642578
Policy training finished
---------------------
gamma: 0.22016587460742917
training start after waiting for 1.149658441543579 seconds
policy loss:-220.48202514648438
value loss:27.14198875427246
entropies:32.455078125
Policy training finished
---------------------
gamma: 0.22016587460742917
training start after waiting for 1.191901683807373 seconds
policy loss:200.14385986328125
value loss:8.865697860717773
entropies:18.536666870117188
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1138.6887)
ToM Target loss= tensor(2196.5820)
optimized based on ToM loss
---------------------
gamma: 0.22060620635664402
training start after waiting for 1.1375923156738281 seconds
policy loss:-927.8954467773438
value loss:22.67477035522461
entropies:42.884521484375
Policy training finished
---------------------
gamma: 0.22060620635664402
training start after waiting for 1.15431547164917 seconds
policy loss:-780.6676635742188
value loss:15.347792625427246
entropies:30.29258155822754
Policy training finished
---------------------
gamma: 0.22060620635664402
training start after waiting for 1.1746997833251953 seconds
policy loss:-227.122314453125
value loss:9.254226684570312
entropies:18.905550003051758
Policy training finished
---------------------
gamma: 0.22060620635664402
training start after waiting for 1.155017375946045 seconds
policy loss:-179.62002563476562
value loss:22.351221084594727
entropies:22.50944709777832
Policy training finished
---------------------
gamma: 0.22060620635664402
training start after waiting for 1.1893248558044434 seconds
policy loss:-554.6988525390625
value loss:29.042356491088867
entropies:30.68697166442871
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1195.0691)
ToM Target loss= tensor(2292.9380)
optimized based on ToM loss
---------------------
gamma: 0.2210474187693573
training start after waiting for 1.2092111110687256 seconds
policy loss:-1583.0902099609375
value loss:31.017744064331055
entropies:39.073936462402344
Policy training finished
---------------------
gamma: 0.2210474187693573
training start after waiting for 1.1773405075073242 seconds
policy loss:-237.6579132080078
value loss:8.555896759033203
entropies:19.56787109375
Policy training finished
---------------------
gamma: 0.2210474187693573
training start after waiting for 1.1738710403442383 seconds
policy loss:-276.5176696777344
value loss:12.28142261505127
entropies:22.525001525878906
Policy training finished
---------------------
gamma: 0.2210474187693573
training start after waiting for 1.1370797157287598 seconds
policy loss:246.39089965820312
value loss:11.76291561126709
entropies:19.951576232910156
Policy training finished
---------------------
gamma: 0.2210474187693573
training start after waiting for 1.1921000480651855 seconds
policy loss:-579.763427734375
value loss:27.39065933227539
entropies:34.61964416503906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1088.3588)
ToM Target loss= tensor(2185.4490)
optimized based on ToM loss
---------------------
gamma: 0.221489513606896
training start after waiting for 1.199620246887207 seconds
policy loss:-1399.609130859375
value loss:48.03303146362305
entropies:43.421913146972656
Policy training finished
---------------------
gamma: 0.221489513606896
training start after waiting for 1.192253828048706 seconds
policy loss:198.85842895507812
value loss:28.24333953857422
entropies:23.763347625732422
Policy training finished
---------------------
gamma: 0.221489513606896
training start after waiting for 1.1898999214172363 seconds
policy loss:-33.46336364746094
value loss:22.241331100463867
entropies:20.275413513183594
Policy training finished
---------------------
gamma: 0.221489513606896
training start after waiting for 1.1726648807525635 seconds
policy loss:-167.4134979248047
value loss:25.880809783935547
entropies:36.80626678466797
Policy training finished
---------------------
gamma: 0.221489513606896
training start after waiting for 1.1446850299835205 seconds
policy loss:359.61572265625
value loss:11.187458038330078
entropies:17.68025016784668
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1105.2590)
ToM Target loss= tensor(2101.1401)
optimized based on ToM loss
---------------------
gamma: 0.2219324926341098
training start after waiting for 1.188272476196289 seconds
policy loss:-414.77459716796875
value loss:13.584192276000977
entropies:30.606006622314453
Policy training finished
---------------------
gamma: 0.2219324926341098
training start after waiting for 1.2198922634124756 seconds
policy loss:-185.7442169189453
value loss:26.753929138183594
entropies:34.70307159423828
Policy training finished
---------------------
gamma: 0.2219324926341098
training start after waiting for 1.1954760551452637 seconds
policy loss:-147.7707977294922
value loss:14.792051315307617
entropies:26.737503051757812
Policy training finished
---------------------
gamma: 0.2219324926341098
training start after waiting for 1.1926124095916748 seconds
policy loss:5.351650714874268
value loss:25.17051887512207
entropies:20.711395263671875
Policy training finished
---------------------
gamma: 0.2219324926341098
training start after waiting for 1.1825637817382812 seconds
policy loss:105.53431701660156
value loss:4.966512203216553
entropies:18.221603393554688
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1127.7365)
ToM Target loss= tensor(2207.5979)
optimized based on ToM loss
---------------------
gamma: 0.22237635761937802
training start after waiting for 1.1389045715332031 seconds
policy loss:9.680030822753906
value loss:10.426043510437012
entropies:20.734113693237305
Policy training finished
---------------------
gamma: 0.22237635761937802
training start after waiting for 1.2081472873687744 seconds
policy loss:-147.65115356445312
value loss:12.420477867126465
entropies:29.674468994140625
Policy training finished
---------------------
gamma: 0.22237635761937802
training start after waiting for 1.189558744430542 seconds
policy loss:-1156.002685546875
value loss:23.83759307861328
entropies:31.139455795288086
Policy training finished
---------------------
gamma: 0.22237635761937802
training start after waiting for 1.2139837741851807 seconds
policy loss:-953.8440551757812
value loss:21.208580017089844
entropies:36.6420783996582
Policy training finished
---------------------
gamma: 0.22237635761937802
training start after waiting for 1.1701180934906006 seconds
policy loss:174.85130310058594
value loss:7.190426826477051
entropies:20.291276931762695
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1088.9274)
ToM Target loss= tensor(2113.7544)
optimized based on ToM loss
---------------------
gamma: 0.22282111033461677
training start after waiting for 1.1950106620788574 seconds
policy loss:-418.0717468261719
value loss:22.719093322753906
entropies:33.51101303100586
Policy training finished
---------------------
gamma: 0.22282111033461677
training start after waiting for 1.1488163471221924 seconds
policy loss:-24.96600341796875
value loss:8.986617088317871
entropies:27.63968849182129
Policy training finished
---------------------
gamma: 0.22282111033461677
training start after waiting for 1.1946079730987549 seconds
policy loss:-66.51387023925781
value loss:7.449155330657959
entropies:27.382007598876953
Policy training finished
---------------------
gamma: 0.22282111033461677
training start after waiting for 1.169161319732666 seconds
policy loss:-457.16375732421875
value loss:11.373590469360352
entropies:28.03580093383789
Policy training finished
---------------------
gamma: 0.22282111033461677
training start after waiting for 1.149796962738037 seconds
policy loss:19.718521118164062
value loss:6.4704508781433105
entropies:24.478416442871094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1135.0502)
ToM Target loss= tensor(2183.2090)
optimized based on ToM loss
---------------------
gamma: 0.223266752555286
training start after waiting for 1.1893558502197266 seconds
policy loss:-10.312448501586914
value loss:3.7514989376068115
entropies:27.044384002685547
Policy training finished
---------------------
gamma: 0.223266752555286
training start after waiting for 1.1897523403167725 seconds
policy loss:-446.4544677734375
value loss:32.145599365234375
entropies:26.08148956298828
Policy training finished
---------------------
gamma: 0.223266752555286
training start after waiting for 1.1908862590789795 seconds
policy loss:-392.759521484375
value loss:27.47879981994629
entropies:28.265342712402344
Policy training finished
---------------------
gamma: 0.223266752555286
training start after waiting for 1.184394121170044 seconds
policy loss:-182.23641967773438
value loss:7.2115159034729
entropies:18.72699737548828
Policy training finished
---------------------
gamma: 0.223266752555286
training start after waiting for 1.1731531620025635 seconds
policy loss:-563.61865234375
value loss:22.753429412841797
entropies:32.30244827270508
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1108.0400)
ToM Target loss= tensor(2200.0205)
optimized based on ToM loss
---------------------
gamma: 0.22371328606039656
training start after waiting for 1.1507837772369385 seconds
policy loss:-816.5588989257812
value loss:15.091245651245117
entropies:26.521732330322266
Policy training finished
---------------------
gamma: 0.22371328606039656
training start after waiting for 1.1911063194274902 seconds
policy loss:-3084.328369140625
value loss:76.23445892333984
entropies:60.1850700378418
Policy training finished
---------------------
gamma: 0.22371328606039656
training start after waiting for 1.2142655849456787 seconds
policy loss:174.7947998046875
value loss:9.200881958007812
entropies:25.050743103027344
Policy training finished
---------------------
gamma: 0.22371328606039656
training start after waiting for 1.1470744609832764 seconds
policy loss:-41.320220947265625
value loss:9.229230880737305
entropies:19.479717254638672
Policy training finished
---------------------
gamma: 0.22371328606039656
training start after waiting for 1.199472427368164 seconds
policy loss:-1262.3712158203125
value loss:25.632221221923828
entropies:37.986122131347656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1194.4231)
ToM Target loss= tensor(2111.6011)
optimized based on ToM loss
---------------------
gamma: 0.22416071263251736
training start after waiting for 1.1424901485443115 seconds
policy loss:-277.47442626953125
value loss:16.1628360748291
entropies:34.149959564208984
Policy training finished
---------------------
gamma: 0.22416071263251736
training start after waiting for 1.179539680480957 seconds
policy loss:212.49276733398438
value loss:14.06231689453125
entropies:21.67380142211914
Policy training finished
---------------------
gamma: 0.22416071263251736
training start after waiting for 1.2052428722381592 seconds
policy loss:47.30592346191406
value loss:7.455291748046875
entropies:22.89080810546875
Policy training finished
---------------------
gamma: 0.22416071263251736
training start after waiting for 1.1532959938049316 seconds
policy loss:95.34368896484375
value loss:5.6604413986206055
entropies:12.264419555664062
Policy training finished
---------------------
gamma: 0.22416071263251736
training start after waiting for 1.1820800304412842 seconds
policy loss:-214.25192260742188
value loss:8.181727409362793
entropies:20.305875778198242
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1087.8197)
ToM Target loss= tensor(2204.1370)
optimized based on ToM loss
---------------------
gamma: 0.2246090340577824
training start after waiting for 1.158177375793457 seconds
policy loss:-274.4559020996094
value loss:12.768449783325195
entropies:16.89305877685547
Policy training finished
---------------------
gamma: 0.2246090340577824
training start after waiting for 1.1919221878051758 seconds
policy loss:25.349267959594727
value loss:5.423344612121582
entropies:15.079317092895508
Policy training finished
---------------------
gamma: 0.2246090340577824
training start after waiting for 1.1997547149658203 seconds
policy loss:-633.6022338867188
value loss:17.849607467651367
entropies:41.65775680541992
Policy training finished
---------------------
gamma: 0.2246090340577824
training start after waiting for 1.1437208652496338 seconds
policy loss:-260.2032470703125
value loss:16.50411033630371
entropies:28.600454330444336
Policy training finished
---------------------
gamma: 0.2246090340577824
training start after waiting for 1.1484603881835938 seconds
policy loss:-1476.6702880859375
value loss:38.684471130371094
entropies:40.81857681274414
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1104.4673)
ToM Target loss= tensor(2190.5837)
optimized based on ToM loss
---------------------
gamma: 0.22505825212589797
training start after waiting for 1.1736772060394287 seconds
policy loss:-1078.5982666015625
value loss:44.653526306152344
entropies:33.22525405883789
Policy training finished
---------------------
gamma: 0.22505825212589797
training start after waiting for 1.202746868133545 seconds
policy loss:261.7048034667969
value loss:13.60805606842041
entropies:22.99079132080078
Policy training finished
---------------------
gamma: 0.22505825212589797
training start after waiting for 1.128866195678711 seconds
policy loss:-113.64629364013672
value loss:25.70877456665039
entropies:19.839988708496094
Policy training finished
---------------------
gamma: 0.22505825212589797
training start after waiting for 1.1439497470855713 seconds
policy loss:-1219.83740234375
value loss:49.64588165283203
entropies:30.498743057250977
Policy training finished
---------------------
gamma: 0.22505825212589797
training start after waiting for 1.2111220359802246 seconds
policy loss:329.0160827636719
value loss:17.515785217285156
entropies:18.438634872436523
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1041.7998)
ToM Target loss= tensor(2123.1541)
optimized based on ToM loss
---------------------
gamma: 0.22550836863014978
training start after waiting for 1.1448254585266113 seconds
policy loss:588.066650390625
value loss:28.5133056640625
entropies:27.944896697998047
Policy training finished
---------------------
gamma: 0.22550836863014978
training start after waiting for 1.195406198501587 seconds
policy loss:-2526.881591796875
value loss:56.96729278564453
entropies:37.667415618896484
Policy training finished
---------------------
gamma: 0.22550836863014978
training start after waiting for 1.1737337112426758 seconds
policy loss:-159.9460906982422
value loss:20.960851669311523
entropies:28.718955993652344
Policy training finished
---------------------
gamma: 0.22550836863014978
training start after waiting for 1.1709721088409424 seconds
policy loss:14.953372955322266
value loss:13.462827682495117
entropies:31.417116165161133
Policy training finished
---------------------
gamma: 0.22550836863014978
training start after waiting for 1.1725902557373047 seconds
policy loss:-382.37774658203125
value loss:14.135196685791016
entropies:16.39263153076172
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1167.3639)
ToM Target loss= tensor(2152.3433)
optimized based on ToM loss
---------------------
gamma: 0.2259593853674101
training start after waiting for 1.1450934410095215 seconds
policy loss:-971.8944702148438
value loss:21.877647399902344
entropies:26.338329315185547
Policy training finished
---------------------
gamma: 0.2259593853674101
training start after waiting for 1.2044529914855957 seconds
policy loss:-747.4586181640625
value loss:10.607659339904785
entropies:20.607959747314453
Policy training finished
---------------------
gamma: 0.2259593853674101
training start after waiting for 1.1499390602111816 seconds
policy loss:-1106.236328125
value loss:33.18290328979492
entropies:39.01588821411133
Policy training finished
---------------------
gamma: 0.2259593853674101
training start after waiting for 1.1380813121795654 seconds
policy loss:-1471.35400390625
value loss:26.999296188354492
entropies:30.209836959838867
Policy training finished
---------------------
gamma: 0.2259593853674101
training start after waiting for 1.1715993881225586 seconds
policy loss:-841.3460693359375
value loss:18.43281364440918
entropies:40.713661193847656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1147.0443)
ToM Target loss= tensor(2230.7966)
optimized based on ToM loss
---------------------
gamma: 0.22641130413814492
training start after waiting for 1.2112624645233154 seconds
policy loss:-433.32025146484375
value loss:8.578017234802246
entropies:23.972557067871094
Policy training finished
---------------------
gamma: 0.22641130413814492
training start after waiting for 1.2146875858306885 seconds
policy loss:-76.8820571899414
value loss:8.106544494628906
entropies:10.658483505249023
Policy training finished
---------------------
gamma: 0.22641130413814492
training start after waiting for 1.2052125930786133 seconds
policy loss:-736.9232788085938
value loss:30.98667335510254
entropies:38.91455841064453
Policy training finished
---------------------
gamma: 0.22641130413814492
training start after waiting for 1.1868722438812256 seconds
policy loss:-223.31605529785156
value loss:21.031002044677734
entropies:37.38897705078125
Policy training finished
---------------------
gamma: 0.22641130413814492
training start after waiting for 1.104353427886963 seconds
policy loss:209.72608947753906
value loss:10.163039207458496
entropies:18.905853271484375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1131.5435)
ToM Target loss= tensor(2170.1680)
optimized based on ToM loss
---------------------
gamma: 0.2268641267464212
training start after waiting for 1.1961755752563477 seconds
policy loss:-381.935546875
value loss:9.697538375854492
entropies:22.51703453063965
Policy training finished
---------------------
gamma: 0.2268641267464212
training start after waiting for 1.1995689868927002 seconds
policy loss:-608.1942749023438
value loss:17.418481826782227
entropies:27.750465393066406
Policy training finished
---------------------
gamma: 0.2268641267464212
training start after waiting for 1.143127679824829 seconds
policy loss:427.8177185058594
value loss:7.010010719299316
entropies:24.23569107055664
Policy training finished
---------------------
gamma: 0.2268641267464212
training start after waiting for 1.0805842876434326 seconds
policy loss:-856.255126953125
value loss:29.193113327026367
entropies:20.66084098815918
Policy training finished
---------------------
gamma: 0.2268641267464212
training start after waiting for 1.2015695571899414 seconds
policy loss:-1928.90380859375
value loss:42.156517028808594
entropies:40.46627426147461
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1101.9574)
ToM Target loss= tensor(2139.8152)
optimized based on ToM loss
---------------------
gamma: 0.22731785499991405
training start after waiting for 1.1412084102630615 seconds
policy loss:-340.7431945800781
value loss:8.564504623413086
entropies:16.055503845214844
Policy training finished
---------------------
gamma: 0.22731785499991405
training start after waiting for 1.206838607788086 seconds
policy loss:-1158.2601318359375
value loss:36.24427032470703
entropies:27.97904396057129
Policy training finished
---------------------
gamma: 0.22731785499991405
training start after waiting for 1.15397310256958 seconds
policy loss:-1207.7930908203125
value loss:25.547950744628906
entropies:44.755374908447266
Policy training finished
---------------------
gamma: 0.22731785499991405
training start after waiting for 1.147416353225708 seconds
policy loss:-60.72820281982422
value loss:7.829056262969971
entropies:19.363208770751953
Policy training finished
---------------------
gamma: 0.22731785499991405
training start after waiting for 1.2442529201507568 seconds
policy loss:-393.6291198730469
value loss:6.981709003448486
entropies:25.103771209716797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1061.4817)
ToM Target loss= tensor(2103.5496)
optimized based on ToM loss
---------------------
gamma: 0.22777249070991387
training start after waiting for 1.197314739227295 seconds
policy loss:-110.28803253173828
value loss:15.860175132751465
entropies:22.420005798339844
Policy training finished
---------------------
gamma: 0.22777249070991387
training start after waiting for 1.1089205741882324 seconds
policy loss:-255.42098999023438
value loss:17.18775177001953
entropies:39.7237434387207
Policy training finished
---------------------
gamma: 0.22777249070991387
training start after waiting for 1.206425428390503 seconds
policy loss:91.52600860595703
value loss:45.355445861816406
entropies:31.89337158203125
Policy training finished
---------------------
gamma: 0.22777249070991387
training start after waiting for 1.1457641124725342 seconds
policy loss:289.2539978027344
value loss:22.24990463256836
entropies:23.077713012695312
Policy training finished
---------------------
gamma: 0.22777249070991387
training start after waiting for 1.186004400253296 seconds
policy loss:106.06227111816406
value loss:9.792004585266113
entropies:21.88205909729004
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1180.1864)
ToM Target loss= tensor(2191.3828)
optimized based on ToM loss
---------------------
gamma: 0.2282280356913337
training start after waiting for 1.1997623443603516 seconds
policy loss:687.9627075195312
value loss:14.914325714111328
entropies:27.57032012939453
Policy training finished
---------------------
gamma: 0.2282280356913337
training start after waiting for 1.1490468978881836 seconds
policy loss:-270.53582763671875
value loss:19.93975067138672
entropies:21.98944854736328
Policy training finished
---------------------
gamma: 0.2282280356913337
training start after waiting for 1.2070183753967285 seconds
policy loss:-576.6859130859375
value loss:18.678913116455078
entropies:38.98744583129883
Policy training finished
---------------------
gamma: 0.2282280356913337
training start after waiting for 1.0981130599975586 seconds
policy loss:-267.61572265625
value loss:8.329008102416992
entropies:19.257827758789062
Policy training finished
---------------------
gamma: 0.2282280356913337
training start after waiting for 1.2219939231872559 seconds
policy loss:149.07899475097656
value loss:13.834334373474121
entropies:25.28148651123047
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1036.3369)
ToM Target loss= tensor(2159.1875)
optimized based on ToM loss
---------------------
gamma: 0.22868449176271638
training start after waiting for 1.1310505867004395 seconds
policy loss:-1370.26318359375
value loss:31.82834815979004
entropies:36.213321685791016
Policy training finished
---------------------
gamma: 0.22868449176271638
training start after waiting for 1.1463308334350586 seconds
policy loss:101.69715118408203
value loss:6.071507453918457
entropies:14.328744888305664
Policy training finished
---------------------
gamma: 0.22868449176271638
training start after waiting for 1.1416821479797363 seconds
policy loss:-755.361083984375
value loss:23.121110916137695
entropies:31.285221099853516
Policy training finished
---------------------
gamma: 0.22868449176271638
training start after waiting for 1.1838338375091553 seconds
policy loss:-845.0490112304688
value loss:25.785987854003906
entropies:31.791353225708008
Policy training finished
---------------------
gamma: 0.22868449176271638
training start after waiting for 1.171722412109375 seconds
policy loss:-431.5270080566406
value loss:23.441688537597656
entropies:28.27838134765625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1140.5747)
ToM Target loss= tensor(2179.7437)
optimized based on ToM loss
---------------------
gamma: 0.22914186074624182
training start after waiting for 1.1947040557861328 seconds
policy loss:387.74188232421875
value loss:10.330249786376953
entropies:20.58446502685547
Policy training finished
---------------------
gamma: 0.22914186074624182
training start after waiting for 1.2075953483581543 seconds
policy loss:-189.60797119140625
value loss:24.0677547454834
entropies:35.966407775878906
Policy training finished
---------------------
gamma: 0.22914186074624182
training start after waiting for 1.1440293788909912 seconds
policy loss:116.45026397705078
value loss:15.375200271606445
entropies:34.1978759765625
Policy training finished
---------------------
gamma: 0.22914186074624182
training start after waiting for 1.1591193675994873 seconds
policy loss:161.55105590820312
value loss:11.09623908996582
entropies:18.07164764404297
Policy training finished
---------------------
gamma: 0.22914186074624182
training start after waiting for 1.2083024978637695 seconds
policy loss:-85.32844543457031
value loss:43.163787841796875
entropies:34.280860900878906
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1207.5179)
ToM Target loss= tensor(2249.6011)
optimized based on ToM loss
---------------------
gamma: 0.2296001444677343
training start after waiting for 1.21187162399292 seconds
policy loss:535.5113525390625
value loss:24.257463455200195
entropies:29.443958282470703
Policy training finished
---------------------
gamma: 0.2296001444677343
training start after waiting for 1.1798672676086426 seconds
policy loss:-951.9647216796875
value loss:37.8946647644043
entropies:18.779430389404297
Policy training finished
---------------------
gamma: 0.2296001444677343
training start after waiting for 1.166773796081543 seconds
policy loss:90.1282730102539
value loss:4.516975402832031
entropies:12.488885879516602
Policy training finished
---------------------
gamma: 0.2296001444677343
training start after waiting for 1.151416540145874 seconds
policy loss:-1055.2625732421875
value loss:38.10334777832031
entropies:37.122474670410156
Policy training finished
---------------------
gamma: 0.2296001444677343
training start after waiting for 1.146254301071167 seconds
policy loss:-568.9783935546875
value loss:31.756818771362305
entropies:35.588714599609375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1204.4326)
ToM Target loss= tensor(2207.2485)
optimized based on ToM loss
---------------------
gamma: 0.23005934475666978
training start after waiting for 1.2074146270751953 seconds
policy loss:135.4136199951172
value loss:11.256816864013672
entropies:20.9885196685791
Policy training finished
---------------------
gamma: 0.23005934475666978
training start after waiting for 1.1754481792449951 seconds
policy loss:-929.0786743164062
value loss:14.716279983520508
entropies:50.68320083618164
Policy training finished
---------------------
gamma: 0.23005934475666978
training start after waiting for 1.2055943012237549 seconds
policy loss:126.08604431152344
value loss:19.393775939941406
entropies:13.751380920410156
Policy training finished
---------------------
gamma: 0.23005934475666978
training start after waiting for 1.1711971759796143 seconds
policy loss:-573.561767578125
value loss:19.990480422973633
entropies:26.24024200439453
Policy training finished
---------------------
gamma: 0.23005934475666978
training start after waiting for 1.1773300170898438 seconds
policy loss:-361.7567443847656
value loss:34.61237716674805
entropies:29.965797424316406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1117.4768)
ToM Target loss= tensor(2070.2893)
optimized based on ToM loss
---------------------
gamma: 0.23051946344618313
training start after waiting for 1.1762068271636963 seconds
policy loss:171.07760620117188
value loss:27.329631805419922
entropies:24.575340270996094
Policy training finished
---------------------
gamma: 0.23051946344618313
training start after waiting for 1.1407015323638916 seconds
policy loss:-342.96038818359375
value loss:28.30626678466797
entropies:33.15486145019531
Policy training finished
---------------------
gamma: 0.23051946344618313
training start after waiting for 1.1714389324188232 seconds
policy loss:-167.23483276367188
value loss:31.352432250976562
entropies:36.806488037109375
Policy training finished
---------------------
gamma: 0.23051946344618313
training start after waiting for 1.1826918125152588 seconds
policy loss:-705.9765625
value loss:28.252363204956055
entropies:29.079416275024414
Policy training finished
---------------------
gamma: 0.23051946344618313
training start after waiting for 1.1987440586090088 seconds
policy loss:-87.84953308105469
value loss:15.750566482543945
entropies:26.65873146057129
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1154.0479)
ToM Target loss= tensor(2118.2356)
optimized based on ToM loss
---------------------
gamma: 0.2309805023730755
training start after waiting for 1.148071527481079 seconds
policy loss:-915.7587280273438
value loss:19.908279418945312
entropies:42.460758209228516
Policy training finished
---------------------
gamma: 0.2309805023730755
training start after waiting for 1.1849706172943115 seconds
policy loss:-714.5309448242188
value loss:14.213127136230469
entropies:28.924219131469727
Policy training finished
---------------------
gamma: 0.2309805023730755
training start after waiting for 1.2037885189056396 seconds
policy loss:-1295.8572998046875
value loss:32.784889221191406
entropies:35.49211120605469
Policy training finished
---------------------
gamma: 0.2309805023730755
training start after waiting for 1.181410312652588 seconds
policy loss:-458.38995361328125
value loss:9.381354331970215
entropies:20.404661178588867
Policy training finished
---------------------
gamma: 0.2309805023730755
training start after waiting for 1.1814994812011719 seconds
policy loss:-1066.813232421875
value loss:36.18727493286133
entropies:34.871612548828125
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1244.3988)
ToM Target loss= tensor(2228.6572)
optimized based on ToM loss
---------------------
gamma: 0.23144246337782165
training start after waiting for 1.186896562576294 seconds
policy loss:687.20751953125
value loss:25.092323303222656
entropies:31.002708435058594
Policy training finished
---------------------
gamma: 0.23144246337782165
training start after waiting for 1.1560578346252441 seconds
policy loss:655.2332153320312
value loss:29.376384735107422
entropies:35.19814682006836
Policy training finished
---------------------
gamma: 0.23144246337782165
training start after waiting for 1.2100625038146973 seconds
policy loss:-33.91074752807617
value loss:23.906946182250977
entropies:29.430084228515625
Policy training finished
---------------------
gamma: 0.23144246337782165
training start after waiting for 1.178102970123291 seconds
policy loss:193.3405303955078
value loss:8.418231010437012
entropies:19.1912841796875
Policy training finished
---------------------
gamma: 0.23144246337782165
training start after waiting for 1.1937978267669678 seconds
policy loss:-518.3199462890625
value loss:28.181991577148438
entropies:46.08973693847656
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1151.6274)
ToM Target loss= tensor(2091.6768)
optimized based on ToM loss
---------------------
gamma: 0.2319053483045773
training start after waiting for 1.1523401737213135 seconds
policy loss:129.62994384765625
value loss:8.050238609313965
entropies:24.7208251953125
Policy training finished
---------------------
gamma: 0.2319053483045773
training start after waiting for 1.1925461292266846 seconds
policy loss:24.710777282714844
value loss:12.851122856140137
entropies:15.792961120605469
Policy training finished
---------------------
gamma: 0.2319053483045773
training start after waiting for 1.1393687725067139 seconds
policy loss:374.35284423828125
value loss:9.302339553833008
entropies:19.27191162109375
Policy training finished
---------------------
gamma: 0.2319053483045773
training start after waiting for 1.1414661407470703 seconds
policy loss:-370.6971435546875
value loss:13.097047805786133
entropies:27.278156280517578
Policy training finished
---------------------
gamma: 0.2319053483045773
training start after waiting for 1.2009472846984863 seconds
policy loss:-1219.736083984375
value loss:51.68552780151367
entropies:36.2166862487793
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1109.4087)
ToM Target loss= tensor(2230.4050)
optimized based on ToM loss
---------------------
gamma: 0.23236915900118646
training start after waiting for 1.211845874786377 seconds
policy loss:-155.4986572265625
value loss:4.010218620300293
entropies:15.953160285949707
Policy training finished
---------------------
gamma: 0.23236915900118646
training start after waiting for 1.1549193859100342 seconds
policy loss:92.21284484863281
value loss:8.251934051513672
entropies:19.07663345336914
Policy training finished
---------------------
gamma: 0.23236915900118646
training start after waiting for 1.2171740531921387 seconds
policy loss:152.05068969726562
value loss:8.513148307800293
entropies:15.842384338378906
Policy training finished
---------------------
gamma: 0.23236915900118646
training start after waiting for 1.1773757934570312 seconds
policy loss:-1183.752685546875
value loss:47.82353973388672
entropies:32.98783493041992
Policy training finished
---------------------
gamma: 0.23236915900118646
training start after waiting for 1.1439120769500732 seconds
policy loss:-237.0789337158203
value loss:10.984930038452148
entropies:25.17082977294922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1109.7858)
ToM Target loss= tensor(2305.5452)
optimized based on ToM loss
---------------------
gamma: 0.23283389731918883
training start after waiting for 1.1981546878814697 seconds
policy loss:-1397.5146484375
value loss:25.801925659179688
entropies:28.368366241455078
Policy training finished
---------------------
gamma: 0.23283389731918883
training start after waiting for 1.2011466026306152 seconds
policy loss:-57.80768585205078
value loss:9.311755180358887
entropies:15.773992538452148
Policy training finished
---------------------
gamma: 0.23283389731918883
training start after waiting for 1.2184622287750244 seconds
policy loss:-575.3497924804688
value loss:21.248126983642578
entropies:26.514545440673828
Policy training finished
---------------------
gamma: 0.23283389731918883
training start after waiting for 1.1812183856964111 seconds
policy loss:-2003.10546875
value loss:43.492374420166016
entropies:45.37009811401367
Policy training finished
---------------------
gamma: 0.23283389731918883
training start after waiting for 1.218632698059082 seconds
policy loss:-621.6544189453125
value loss:32.91605758666992
entropies:31.174161911010742
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1119.1394)
ToM Target loss= tensor(2171.4138)
optimized based on ToM loss
---------------------
gamma: 0.2332995651138272
training start after waiting for 1.1795408725738525 seconds
policy loss:100.39973449707031
value loss:7.116085052490234
entropies:23.029226303100586
Policy training finished
---------------------
gamma: 0.2332995651138272
training start after waiting for 1.2076129913330078 seconds
policy loss:-24.049604415893555
value loss:22.248580932617188
entropies:30.842256546020508
Policy training finished
---------------------
gamma: 0.2332995651138272
training start after waiting for 1.1430954933166504 seconds
policy loss:-1446.29345703125
value loss:36.52948760986328
entropies:29.7371826171875
Policy training finished
---------------------
gamma: 0.2332995651138272
training start after waiting for 1.1487030982971191 seconds
policy loss:-129.46536254882812
value loss:10.603460311889648
entropies:17.11069107055664
Policy training finished
---------------------
gamma: 0.2332995651138272
training start after waiting for 1.175304889678955 seconds
policy loss:35.51922607421875
value loss:3.1209914684295654
entropies:10.778116226196289
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1088.3560)
ToM Target loss= tensor(2184.4878)
optimized based on ToM loss
---------------------
gamma: 0.23376616424405486
training start after waiting for 1.2055189609527588 seconds
policy loss:192.72918701171875
value loss:6.031128883361816
entropies:18.827091217041016
Policy training finished
---------------------
gamma: 0.23376616424405486
training start after waiting for 1.1455419063568115 seconds
policy loss:-360.44854736328125
value loss:7.206916809082031
entropies:20.286746978759766
Policy training finished
---------------------
gamma: 0.23376616424405486
training start after waiting for 1.2117631435394287 seconds
policy loss:-266.21051025390625
value loss:12.494956016540527
entropies:26.224102020263672
Policy training finished
---------------------
gamma: 0.23376616424405486
training start after waiting for 1.211272954940796 seconds
policy loss:73.20263671875
value loss:9.645475387573242
entropies:30.55435562133789
Policy training finished
---------------------
gamma: 0.23376616424405486
training start after waiting for 1.162738561630249 seconds
policy loss:-1429.075927734375
value loss:46.37160110473633
entropies:24.075504302978516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1059.0356)
ToM Target loss= tensor(2088.4194)
optimized based on ToM loss
---------------------
gamma: 0.23423369657254298
training start after waiting for 1.1888487339019775 seconds
policy loss:-595.0250244140625
value loss:9.135684967041016
entropies:32.37900161743164
Policy training finished
---------------------
gamma: 0.23423369657254298
training start after waiting for 1.1886682510375977 seconds
policy loss:-2293.45654296875
value loss:48.51829147338867
entropies:35.41325759887695
Policy training finished
---------------------
gamma: 0.23423369657254298
training start after waiting for 1.1225683689117432 seconds
policy loss:6.772729873657227
value loss:5.66005802154541
entropies:18.635082244873047
Policy training finished
---------------------
gamma: 0.23423369657254298
training start after waiting for 1.1918995380401611 seconds
policy loss:-121.10729217529297
value loss:14.860429763793945
entropies:24.42473030090332
Policy training finished
---------------------
gamma: 0.23423369657254298
training start after waiting for 1.1877110004425049 seconds
policy loss:-53.87173843383789
value loss:11.962388038635254
entropies:31.35969352722168
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1184.0935)
ToM Target loss= tensor(2228.7192)
optimized based on ToM loss
---------------------
gamma: 0.23470216396568808
training start after waiting for 1.1489062309265137 seconds
policy loss:-20.125152587890625
value loss:13.513589859008789
entropies:25.59421730041504
Policy training finished
---------------------
gamma: 0.23470216396568808
training start after waiting for 1.144794225692749 seconds
policy loss:-464.7969055175781
value loss:9.136011123657227
entropies:25.526870727539062
Policy training finished
---------------------
gamma: 0.23470216396568808
training start after waiting for 1.2065958976745605 seconds
policy loss:246.23123168945312
value loss:13.97808837890625
entropies:18.03893280029297
Policy training finished
---------------------
gamma: 0.23470216396568808
training start after waiting for 1.1650350093841553 seconds
policy loss:-1724.6104736328125
value loss:35.128631591796875
entropies:44.29315948486328
Policy training finished
---------------------
gamma: 0.23470216396568808
training start after waiting for 1.1947588920593262 seconds
policy loss:-659.4434204101562
value loss:12.700668334960938
entropies:20.515491485595703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1140.2463)
ToM Target loss= tensor(2109.1184)
optimized based on ToM loss
---------------------
gamma: 0.23517156829361946
training start after waiting for 1.202810287475586 seconds
policy loss:-101.36314392089844
value loss:21.852066040039062
entropies:25.763507843017578
Policy training finished
---------------------
gamma: 0.23517156829361946
training start after waiting for 1.1372430324554443 seconds
policy loss:-554.8284912109375
value loss:8.351922035217285
entropies:23.11258888244629
Policy training finished
---------------------
gamma: 0.23517156829361946
training start after waiting for 1.1667609214782715 seconds
policy loss:-603.2075805664062
value loss:19.15873146057129
entropies:32.105384826660156
Policy training finished
---------------------
gamma: 0.23517156829361946
training start after waiting for 1.2051963806152344 seconds
policy loss:-166.37796020507812
value loss:14.260551452636719
entropies:24.230443954467773
Policy training finished
---------------------
gamma: 0.23517156829361946
training start after waiting for 1.1924834251403809 seconds
policy loss:-606.5216064453125
value loss:38.83645248413086
entropies:24.426183700561523
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1108.8446)
ToM Target loss= tensor(2174.0779)
optimized based on ToM loss
---------------------
gamma: 0.23564191143020669
training start after waiting for 1.1896295547485352 seconds
policy loss:362.6195068359375
value loss:12.132244110107422
entropies:14.886547088623047
Policy training finished
---------------------
gamma: 0.23564191143020669
training start after waiting for 1.1916513442993164 seconds
policy loss:-556.4676513671875
value loss:47.17881393432617
entropies:28.478778839111328
Policy training finished
---------------------
gamma: 0.23564191143020669
training start after waiting for 1.1484010219573975 seconds
policy loss:-192.0945587158203
value loss:22.786043167114258
entropies:36.93871307373047
Policy training finished
---------------------
gamma: 0.23564191143020669
training start after waiting for 1.2027015686035156 seconds
policy loss:364.1720886230469
value loss:11.249526023864746
entropies:17.3837833404541
Policy training finished
---------------------
gamma: 0.23564191143020669
training start after waiting for 1.1860322952270508 seconds
policy loss:-646.9738159179688
value loss:42.984683990478516
entropies:31.60401153564453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1145.5707)
ToM Target loss= tensor(2175.9534)
optimized based on ToM loss
---------------------
gamma: 0.2361131952530671
training start after waiting for 1.1544108390808105 seconds
policy loss:-60.27594757080078
value loss:9.63819408416748
entropies:15.199562072753906
Policy training finished
---------------------
gamma: 0.2361131952530671
training start after waiting for 1.1620588302612305 seconds
policy loss:-1223.785888671875
value loss:21.003440856933594
entropies:30.012454986572266
Policy training finished
---------------------
gamma: 0.2361131952530671
training start after waiting for 1.1688292026519775 seconds
policy loss:-220.97544860839844
value loss:14.909849166870117
entropies:22.028404235839844
Policy training finished
---------------------
gamma: 0.2361131952530671
training start after waiting for 1.137619972229004 seconds
policy loss:-635.6819458007812
value loss:27.323497772216797
entropies:32.25952911376953
Policy training finished
---------------------
gamma: 0.2361131952530671
training start after waiting for 1.1770262718200684 seconds
policy loss:96.76531982421875
value loss:8.247200965881348
entropies:26.96247100830078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1135.0408)
ToM Target loss= tensor(2196.5830)
optimized based on ToM loss
---------------------
gamma: 0.23658542164357324
training start after waiting for 1.1485629081726074 seconds
policy loss:4.235583782196045
value loss:8.710613250732422
entropies:17.412538528442383
Policy training finished
---------------------
gamma: 0.23658542164357324
training start after waiting for 1.1901593208312988 seconds
policy loss:-65.59417724609375
value loss:7.5563859939575195
entropies:15.566500663757324
Policy training finished
---------------------
gamma: 0.23658542164357324
training start after waiting for 1.1635327339172363 seconds
policy loss:218.1522216796875
value loss:8.188450813293457
entropies:23.221790313720703
Policy training finished
---------------------
gamma: 0.23658542164357324
training start after waiting for 1.144639492034912 seconds
policy loss:170.0463409423828
value loss:12.084044456481934
entropies:24.137035369873047
Policy training finished
---------------------
gamma: 0.23658542164357324
training start after waiting for 1.1932384967803955 seconds
policy loss:3.944117784500122
value loss:3.955874443054199
entropies:16.30685043334961
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1046.0994)
ToM Target loss= tensor(2209.0752)
optimized based on ToM loss
---------------------
gamma: 0.2370585924868604
training start after waiting for 1.173527717590332 seconds
policy loss:-3353.96533203125
value loss:67.2628173828125
entropies:45.33287811279297
Policy training finished
---------------------
gamma: 0.2370585924868604
training start after waiting for 1.1470894813537598 seconds
policy loss:-670.5397338867188
value loss:14.823094367980957
entropies:27.794891357421875
Policy training finished
---------------------
gamma: 0.2370585924868604
training start after waiting for 1.1888012886047363 seconds
policy loss:-547.4158325195312
value loss:25.97268295288086
entropies:30.84813117980957
Policy training finished
---------------------
gamma: 0.2370585924868604
training start after waiting for 1.1665029525756836 seconds
policy loss:-1815.441162109375
value loss:44.36536407470703
entropies:43.03554916381836
Policy training finished
---------------------
gamma: 0.2370585924868604
training start after waiting for 1.1898541450500488 seconds
policy loss:30.88218879699707
value loss:9.450666427612305
entropies:20.2760066986084
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1202.7288)
ToM Target loss= tensor(2186.5588)
optimized based on ToM loss
---------------------
gamma: 0.23753270967183412
training start after waiting for 1.2104887962341309 seconds
policy loss:-613.1963500976562
value loss:27.527381896972656
entropies:35.07049560546875
Policy training finished
---------------------
gamma: 0.23753270967183412
training start after waiting for 1.207801342010498 seconds
policy loss:-547.3995971679688
value loss:28.71327781677246
entropies:29.553058624267578
Policy training finished
---------------------
gamma: 0.23753270967183412
training start after waiting for 1.1425681114196777 seconds
policy loss:-307.19403076171875
value loss:27.950029373168945
entropies:17.189754486083984
Policy training finished
---------------------
gamma: 0.23753270967183412
training start after waiting for 1.1936209201812744 seconds
policy loss:314.5252990722656
value loss:11.66549015045166
entropies:20.094661712646484
Policy training finished
---------------------
gamma: 0.23753270967183412
training start after waiting for 1.186467170715332 seconds
policy loss:-197.77847290039062
value loss:17.893531799316406
entropies:29.63459014892578
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1168.0813)
ToM Target loss= tensor(2170.8916)
optimized based on ToM loss
---------------------
gamma: 0.2380077750911778
training start after waiting for 1.1930623054504395 seconds
policy loss:303.446533203125
value loss:9.472349166870117
entropies:20.64173126220703
Policy training finished
---------------------
gamma: 0.2380077750911778
training start after waiting for 1.152259349822998 seconds
policy loss:-1932.9190673828125
value loss:28.476320266723633
entropies:48.2667121887207
Policy training finished
---------------------
gamma: 0.2380077750911778
training start after waiting for 1.180022954940796 seconds
policy loss:-1971.007568359375
value loss:85.51862335205078
entropies:38.51285934448242
Policy training finished
---------------------
gamma: 0.2380077750911778
training start after waiting for 1.1866354942321777 seconds
policy loss:-1017.1691284179688
value loss:14.573578834533691
entropies:29.307846069335938
Policy training finished
---------------------
gamma: 0.2380077750911778
training start after waiting for 1.1772751808166504 seconds
policy loss:-775.3489990234375
value loss:9.545867919921875
entropies:33.23109436035156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1277.6096)
ToM Target loss= tensor(2136.0217)
optimized based on ToM loss
---------------------
gamma: 0.23848379064136016
training start after waiting for 1.1871881484985352 seconds
policy loss:-145.33895874023438
value loss:11.737626075744629
entropies:28.365877151489258
Policy training finished
---------------------
gamma: 0.23848379064136016
training start after waiting for 1.1584811210632324 seconds
policy loss:-226.51268005371094
value loss:11.314750671386719
entropies:21.17640495300293
Policy training finished
---------------------
gamma: 0.23848379064136016
training start after waiting for 1.1821990013122559 seconds
policy loss:164.23875427246094
value loss:7.527370929718018
entropies:15.721700668334961
Policy training finished
---------------------
gamma: 0.23848379064136016
training start after waiting for 1.1540353298187256 seconds
policy loss:-569.3084106445312
value loss:26.923994064331055
entropies:29.18560218811035
Policy training finished
---------------------
gamma: 0.23848379064136016
training start after waiting for 1.167520523071289 seconds
policy loss:-842.1635131835938
value loss:25.551738739013672
entropies:35.09541702270508
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1132.0516)
ToM Target loss= tensor(2133.7158)
optimized based on ToM loss
---------------------
gamma: 0.23896075822264287
training start after waiting for 1.192979335784912 seconds
policy loss:66.36414337158203
value loss:14.329890251159668
entropies:25.372026443481445
Policy training finished
---------------------
gamma: 0.23896075822264287
training start after waiting for 1.2018439769744873 seconds
policy loss:-1557.0557861328125
value loss:43.68378829956055
entropies:41.05332946777344
Policy training finished
---------------------
gamma: 0.23896075822264287
training start after waiting for 1.2032792568206787 seconds
policy loss:-909.6947631835938
value loss:22.35479164123535
entropies:30.825538635253906
Policy training finished
---------------------
gamma: 0.23896075822264287
training start after waiting for 1.1787724494934082 seconds
policy loss:-410.2003479003906
value loss:25.94869613647461
entropies:26.282520294189453
Policy training finished
---------------------
gamma: 0.23896075822264287
training start after waiting for 1.1857898235321045 seconds
policy loss:-1174.9412841796875
value loss:34.855369567871094
entropies:35.697509765625
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1166.7450)
ToM Target loss= tensor(2086.0552)
optimized based on ToM loss
---------------------
gamma: 0.23943867973908817
training start after waiting for 1.147815465927124 seconds
policy loss:67.69374084472656
value loss:20.272491455078125
entropies:32.07091522216797
Policy training finished
---------------------
gamma: 0.23943867973908817
training start after waiting for 1.142329216003418 seconds
policy loss:-243.00894165039062
value loss:14.611897468566895
entropies:31.75177764892578
Policy training finished
---------------------
gamma: 0.23943867973908817
training start after waiting for 1.1525521278381348 seconds
policy loss:-122.77104187011719
value loss:13.111578941345215
entropies:45.56257629394531
Policy training finished
---------------------
gamma: 0.23943867973908817
training start after waiting for 1.1913762092590332 seconds
policy loss:-398.47222900390625
value loss:24.619548797607422
entropies:26.356260299682617
Policy training finished
---------------------
gamma: 0.23943867973908817
training start after waiting for 1.2110035419464111 seconds
policy loss:-999.5459594726562
value loss:15.849190711975098
entropies:30.279747009277344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1234.9294)
ToM Target loss= tensor(2107.5737)
optimized based on ToM loss
---------------------
gamma: 0.23991755709856635
training start after waiting for 1.21860671043396 seconds
policy loss:338.0587463378906
value loss:8.749923706054688
entropies:27.407567977905273
Policy training finished
---------------------
gamma: 0.23991755709856635
training start after waiting for 1.2032344341278076 seconds
policy loss:-1689.330078125
value loss:30.18213653564453
entropies:48.696136474609375
Policy training finished
---------------------
gamma: 0.23991755709856635
training start after waiting for 1.1971409320831299 seconds
policy loss:190.1942138671875
value loss:8.231966018676758
entropies:23.4267578125
Policy training finished
---------------------
gamma: 0.23991755709856635
training start after waiting for 1.1462979316711426 seconds
policy loss:335.3038024902344
value loss:9.35760498046875
entropies:32.054405212402344
Policy training finished
---------------------
gamma: 0.23991755709856635
training start after waiting for 1.1547095775604248 seconds
policy loss:87.11417388916016
value loss:10.612825393676758
entropies:17.258411407470703
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1095.2620)
ToM Target loss= tensor(2176.8879)
optimized based on ToM loss
---------------------
gamma: 0.2403973922127635
training start after waiting for 1.16212797164917 seconds
policy loss:-515.5765380859375
value loss:14.89915657043457
entropies:20.14954376220703
Policy training finished
---------------------
gamma: 0.2403973922127635
training start after waiting for 1.2046940326690674 seconds
policy loss:-31.915250778198242
value loss:9.420260429382324
entropies:24.552825927734375
Policy training finished
---------------------
gamma: 0.2403973922127635
training start after waiting for 1.2042701244354248 seconds
policy loss:198.19992065429688
value loss:6.395838260650635
entropies:17.10671615600586
Policy training finished
---------------------
gamma: 0.2403973922127635
training start after waiting for 1.1466336250305176 seconds
policy loss:-1187.073974609375
value loss:35.59151840209961
entropies:37.15924072265625
Policy training finished
---------------------
gamma: 0.2403973922127635
training start after waiting for 1.1912713050842285 seconds
policy loss:291.6268310546875
value loss:9.093317031860352
entropies:18.516986846923828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1128.7885)
ToM Target loss= tensor(2236.0354)
optimized based on ToM loss
---------------------
gamma: 0.24087818699718902
training start after waiting for 1.1454417705535889 seconds
policy loss:33.02881622314453
value loss:10.373452186584473
entropies:27.922752380371094
Policy training finished
---------------------
gamma: 0.24087818699718902
training start after waiting for 1.1860475540161133 seconds
policy loss:-359.243408203125
value loss:19.56351661682129
entropies:29.847095489501953
Policy training finished
---------------------
gamma: 0.24087818699718902
training start after waiting for 1.2029578685760498 seconds
policy loss:-242.45263671875
value loss:10.028904914855957
entropies:30.347198486328125
Policy training finished
---------------------
gamma: 0.24087818699718902
training start after waiting for 1.1756386756896973 seconds
policy loss:-92.52789306640625
value loss:10.357680320739746
entropies:24.148887634277344
Policy training finished
---------------------
gamma: 0.24087818699718902
training start after waiting for 1.1838366985321045 seconds
policy loss:-9.811412811279297
value loss:9.591544151306152
entropies:20.501344680786133
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1174.8843)
ToM Target loss= tensor(2234.0195)
optimized based on ToM loss
---------------------
gamma: 0.2413599433711834
training start after waiting for 1.1427299976348877 seconds
policy loss:-507.537353515625
value loss:62.642066955566406
entropies:35.274959564208984
Policy training finished
---------------------
gamma: 0.2413599433711834
training start after waiting for 1.154329538345337 seconds
policy loss:-584.6100463867188
value loss:41.33979034423828
entropies:25.738018035888672
Policy training finished
---------------------
gamma: 0.2413599433711834
training start after waiting for 1.1500790119171143 seconds
policy loss:-502.9920349121094
value loss:11.773727416992188
entropies:18.213703155517578
Policy training finished
---------------------
gamma: 0.2413599433711834
training start after waiting for 1.2158348560333252 seconds
policy loss:269.3663330078125
value loss:8.96231460571289
entropies:21.590957641601562
Policy training finished
---------------------
gamma: 0.2413599433711834
training start after waiting for 1.1940343379974365 seconds
policy loss:-197.44287109375
value loss:7.775677680969238
entropies:18.323766708374023
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(988.3445)
ToM Target loss= tensor(2102.7532)
optimized based on ToM loss
---------------------
gamma: 0.24184266325792578
training start after waiting for 1.2188026905059814 seconds
policy loss:-451.5039978027344
value loss:27.300182342529297
entropies:29.594266891479492
Policy training finished
---------------------
gamma: 0.24184266325792578
training start after waiting for 1.2124965190887451 seconds
policy loss:18.71050453186035
value loss:4.051306247711182
entropies:19.946575164794922
Policy training finished
---------------------
gamma: 0.24184266325792578
training start after waiting for 1.1404812335968018 seconds
policy loss:-1734.0489501953125
value loss:32.333335876464844
entropies:35.379417419433594
Policy training finished
---------------------
gamma: 0.24184266325792578
training start after waiting for 1.1870083808898926 seconds
policy loss:-88.45366668701172
value loss:9.750926971435547
entropies:17.24927520751953
Policy training finished
---------------------
gamma: 0.24184266325792578
training start after waiting for 1.1336021423339844 seconds
policy loss:-37.56255340576172
value loss:10.557429313659668
entropies:25.064315795898438
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1062.3527)
ToM Target loss= tensor(2141.1167)
optimized based on ToM loss
---------------------
gamma: 0.24232634858444163
training start after waiting for 1.180211067199707 seconds
policy loss:-798.5023193359375
value loss:22.602418899536133
entropies:31.071096420288086
Policy training finished
---------------------
gamma: 0.24232634858444163
training start after waiting for 1.1659607887268066 seconds
policy loss:-856.3666381835938
value loss:20.977262496948242
entropies:35.92082977294922
Policy training finished
---------------------
gamma: 0.24232634858444163
training start after waiting for 1.1986663341522217 seconds
policy loss:-587.6013793945312
value loss:18.492013931274414
entropies:23.71607208251953
Policy training finished
---------------------
gamma: 0.24232634858444163
training start after waiting for 1.1074144840240479 seconds
policy loss:194.87327575683594
value loss:6.213393688201904
entropies:10.956396102905273
Policy training finished
---------------------
gamma: 0.24232634858444163
training start after waiting for 1.1489722728729248 seconds
policy loss:-602.5750122070312
value loss:33.74580383300781
entropies:32.33426284790039
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1129.9177)
ToM Target loss= tensor(2246.6860)
optimized based on ToM loss
---------------------
gamma: 0.2428110012816105
training start after waiting for 1.2366697788238525 seconds
policy loss:-293.1292724609375
value loss:13.240663528442383
entropies:30.156784057617188
Policy training finished
---------------------
gamma: 0.2428110012816105
training start after waiting for 1.2098803520202637 seconds
policy loss:97.56242370605469
value loss:8.17048168182373
entropies:12.951226234436035
Policy training finished
---------------------
gamma: 0.2428110012816105
training start after waiting for 1.1650724411010742 seconds
policy loss:-568.419677734375
value loss:15.263185501098633
entropies:32.64824676513672
Policy training finished
---------------------
gamma: 0.2428110012816105
training start after waiting for 1.1846511363983154 seconds
policy loss:-1013.2156982421875
value loss:34.92086410522461
entropies:43.34657287597656
Policy training finished
---------------------
gamma: 0.2428110012816105
training start after waiting for 1.2077610492706299 seconds
policy loss:-14.528029441833496
value loss:22.575029373168945
entropies:19.123355865478516
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1145.8405)
ToM Target loss= tensor(2136.0723)
optimized based on ToM loss
---------------------
gamma: 0.24329662328417373
training start after waiting for 1.1421244144439697 seconds
policy loss:-39.95550537109375
value loss:11.567946434020996
entropies:22.911720275878906
Policy training finished
---------------------
gamma: 0.24329662328417373
training start after waiting for 1.145467758178711 seconds
policy loss:-661.45556640625
value loss:23.176530838012695
entropies:38.26442337036133
Policy training finished
---------------------
gamma: 0.24329662328417373
training start after waiting for 1.1455607414245605 seconds
policy loss:-780.0182495117188
value loss:19.205717086791992
entropies:34.7641716003418
Policy training finished
---------------------
gamma: 0.24329662328417373
training start after waiting for 1.1383533477783203 seconds
policy loss:-568.7278442382812
value loss:23.9571533203125
entropies:29.694808959960938
Policy training finished
---------------------
gamma: 0.24329662328417373
training start after waiting for 1.215599536895752 seconds
policy loss:-722.0457153320312
value loss:32.79199981689453
entropies:48.41594696044922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1366.7369)
ToM Target loss= tensor(2199.8164)
optimized based on ToM loss
---------------------
gamma: 0.2437832165307421
training start after waiting for 1.2316277027130127 seconds
policy loss:-1621.00439453125
value loss:26.178264617919922
entropies:26.789817810058594
Policy training finished
---------------------
gamma: 0.2437832165307421
training start after waiting for 1.1378052234649658 seconds
policy loss:-879.6819458007812
value loss:32.34069061279297
entropies:38.51282501220703
Policy training finished
---------------------
gamma: 0.2437832165307421
training start after waiting for 1.172729730606079 seconds
policy loss:-380.2130126953125
value loss:6.298862457275391
entropies:19.36672592163086
Policy training finished
---------------------
gamma: 0.2437832165307421
training start after waiting for 1.1665372848510742 seconds
policy loss:40.611534118652344
value loss:7.343001365661621
entropies:34.437461853027344
Policy training finished
---------------------
gamma: 0.2437832165307421
training start after waiting for 1.1885647773742676 seconds
policy loss:370.6061706542969
value loss:13.72282886505127
entropies:24.277524948120117
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1151.0138)
ToM Target loss= tensor(2219.1147)
optimized based on ToM loss
---------------------
gamma: 0.24427078296380358
training start after waiting for 1.2139406204223633 seconds
policy loss:171.04193115234375
value loss:12.040230751037598
entropies:25.383037567138672
Policy training finished
---------------------
gamma: 0.24427078296380358
training start after waiting for 1.1595494747161865 seconds
policy loss:-1229.5965576171875
value loss:39.54668426513672
entropies:42.207489013671875
Policy training finished
---------------------
gamma: 0.24427078296380358
training start after waiting for 1.2368268966674805 seconds
policy loss:-382.185546875
value loss:22.660444259643555
entropies:29.963394165039062
Policy training finished
---------------------
gamma: 0.24427078296380358
training start after waiting for 1.1634242534637451 seconds
policy loss:46.554542541503906
value loss:10.814712524414062
entropies:14.411319732666016
Policy training finished
---------------------
gamma: 0.24427078296380358
training start after waiting for 1.2159805297851562 seconds
policy loss:-557.0156860351562
value loss:17.604026794433594
entropies:29.42001724243164
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1120.9742)
ToM Target loss= tensor(2184.6602)
optimized based on ToM loss
---------------------
gamma: 0.2447593245297312
training start after waiting for 1.202117681503296 seconds
policy loss:-1256.22021484375
value loss:32.513118743896484
entropies:37.001834869384766
Policy training finished
---------------------
gamma: 0.2447593245297312
training start after waiting for 1.1919701099395752 seconds
policy loss:-280.7809143066406
value loss:15.307847023010254
entropies:36.601654052734375
Policy training finished
---------------------
gamma: 0.2447593245297312
training start after waiting for 1.2243940830230713 seconds
policy loss:563.7137451171875
value loss:17.21700668334961
entropies:27.48629379272461
Policy training finished
---------------------
gamma: 0.2447593245297312
training start after waiting for 1.178800106048584 seconds
policy loss:-294.4884948730469
value loss:53.11892318725586
entropies:30.496309280395508
Policy training finished
---------------------
gamma: 0.2447593245297312
training start after waiting for 1.1466476917266846 seconds
policy loss:-1498.3880615234375
value loss:49.58012771606445
entropies:53.842281341552734
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1255.6899)
ToM Target loss= tensor(2174.8940)
optimized based on ToM loss
---------------------
gamma: 0.24524884317879067
training start after waiting for 1.1864674091339111 seconds
policy loss:-287.7870178222656
value loss:20.158437728881836
entropies:26.274330139160156
Policy training finished
---------------------
gamma: 0.24524884317879067
training start after waiting for 1.1921558380126953 seconds
policy loss:-119.91947174072266
value loss:19.502716064453125
entropies:22.192413330078125
Policy training finished
---------------------
gamma: 0.24524884317879067
training start after waiting for 1.176624059677124 seconds
policy loss:15.79293155670166
value loss:7.368645668029785
entropies:20.808012008666992
Policy training finished
---------------------
gamma: 0.24524884317879067
training start after waiting for 1.1998004913330078 seconds
policy loss:-490.4764099121094
value loss:15.726755142211914
entropies:13.423402786254883
Policy training finished
---------------------
gamma: 0.24524884317879067
training start after waiting for 1.1737534999847412 seconds
policy loss:-453.8644104003906
value loss:44.487918853759766
entropies:35.43220901489258
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1180.5109)
ToM Target loss= tensor(2257.3247)
optimized based on ToM loss
---------------------
gamma: 0.24573934086514826
training start after waiting for 1.2020885944366455 seconds
policy loss:-423.0860290527344
value loss:18.649375915527344
entropies:20.483043670654297
Policy training finished
---------------------
gamma: 0.24573934086514826
training start after waiting for 1.1822445392608643 seconds
policy loss:-388.76416015625
value loss:40.380210876464844
entropies:25.26368522644043
Policy training finished
---------------------
gamma: 0.24573934086514826
training start after waiting for 1.1613318920135498 seconds
policy loss:-213.18458557128906
value loss:11.602787017822266
entropies:25.787063598632812
Policy training finished
---------------------
gamma: 0.24573934086514826
training start after waiting for 1.1614978313446045 seconds
policy loss:-392.512451171875
value loss:17.165851593017578
entropies:20.918119430541992
Policy training finished
---------------------
gamma: 0.24573934086514826
training start after waiting for 1.1293134689331055 seconds
policy loss:-1181.5579833984375
value loss:45.966732025146484
entropies:39.330467224121094
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1150.5565)
ToM Target loss= tensor(2128.9521)
optimized based on ToM loss
---------------------
gamma: 0.24623081954687856
training start after waiting for 1.2023987770080566 seconds
policy loss:184.9580078125
value loss:9.635974884033203
entropies:30.007877349853516
Policy training finished
---------------------
gamma: 0.24623081954687856
training start after waiting for 1.1968319416046143 seconds
policy loss:-671.101318359375
value loss:27.760478973388672
entropies:38.974647521972656
Policy training finished
---------------------
gamma: 0.24623081954687856
training start after waiting for 1.2112104892730713 seconds
policy loss:235.81973266601562
value loss:14.04349136352539
entropies:26.960124969482422
Policy training finished
---------------------
gamma: 0.24623081954687856
training start after waiting for 1.1417450904846191 seconds
policy loss:31.886890411376953
value loss:10.121922492980957
entropies:14.887022018432617
Policy training finished
---------------------
gamma: 0.24623081954687856
training start after waiting for 1.1632640361785889 seconds
policy loss:-932.9695434570312
value loss:32.405155181884766
entropies:43.505828857421875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1169.4314)
ToM Target loss= tensor(2146.3196)
optimized based on ToM loss
---------------------
gamma: 0.2467232811859723
training start after waiting for 1.2082085609436035 seconds
policy loss:65.55589294433594
value loss:16.257476806640625
entropies:41.510108947753906
Policy training finished
---------------------
gamma: 0.2467232811859723
training start after waiting for 1.215773105621338 seconds
policy loss:128.2637939453125
value loss:7.455389022827148
entropies:22.210145950317383
Policy training finished
---------------------
gamma: 0.2467232811859723
training start after waiting for 1.147130012512207 seconds
policy loss:75.97645568847656
value loss:4.922318935394287
entropies:16.24428939819336
Policy training finished
---------------------
gamma: 0.2467232811859723
training start after waiting for 1.187690258026123 seconds
policy loss:-269.15106201171875
value loss:15.853503227233887
entropies:33.46586990356445
Policy training finished
---------------------
gamma: 0.2467232811859723
training start after waiting for 1.135948896408081 seconds
policy loss:-1145.190185546875
value loss:52.93259048461914
entropies:38.85587692260742
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1213.4061)
ToM Target loss= tensor(2114.8447)
optimized based on ToM loss
---------------------
gamma: 0.24721672774834424
training start after waiting for 1.1788477897644043 seconds
policy loss:66.60417175292969
value loss:13.391023635864258
entropies:20.68781280517578
Policy training finished
---------------------
gamma: 0.24721672774834424
training start after waiting for 1.1403615474700928 seconds
policy loss:-461.76312255859375
value loss:20.21375846862793
entropies:34.62434387207031
Policy training finished
---------------------
gamma: 0.24721672774834424
training start after waiting for 1.1794636249542236 seconds
policy loss:-972.1273803710938
value loss:19.406169891357422
entropies:32.125633239746094
Policy training finished
---------------------
gamma: 0.24721672774834424
training start after waiting for 1.152653694152832 seconds
policy loss:-42.310447692871094
value loss:12.007128715515137
entropies:26.903005599975586
Policy training finished
---------------------
gamma: 0.24721672774834424
training start after waiting for 1.197652816772461 seconds
policy loss:-338.71673583984375
value loss:11.661036491394043
entropies:40.863990783691406
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1123.0112)
ToM Target loss= tensor(2146.9893)
optimized based on ToM loss
---------------------
gamma: 0.24771116120384093
training start after waiting for 1.1811351776123047 seconds
policy loss:-956.5109252929688
value loss:37.10662841796875
entropies:44.443206787109375
Policy training finished
---------------------
gamma: 0.24771116120384093
training start after waiting for 1.1949732303619385 seconds
policy loss:-280.13677978515625
value loss:27.192642211914062
entropies:28.75315284729004
Policy training finished
---------------------
gamma: 0.24771116120384093
training start after waiting for 1.1412549018859863 seconds
policy loss:132.1507568359375
value loss:15.805334091186523
entropies:23.515615463256836
Policy training finished
---------------------
gamma: 0.24771116120384093
training start after waiting for 1.181096076965332 seconds
policy loss:228.49761962890625
value loss:18.756671905517578
entropies:37.299869537353516
Policy training finished
---------------------
gamma: 0.24771116120384093
training start after waiting for 1.141984224319458 seconds
policy loss:-19.028045654296875
value loss:18.1811580657959
entropies:22.913536071777344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1217.0979)
ToM Target loss= tensor(2146.2019)
optimized based on ToM loss
---------------------
gamma: 0.2482065835262486
training start after waiting for 1.190962791442871 seconds
policy loss:-1303.0201416015625
value loss:28.314041137695312
entropies:47.305946350097656
Policy training finished
---------------------
gamma: 0.2482065835262486
training start after waiting for 1.1996254920959473 seconds
policy loss:362.8904113769531
value loss:11.674920082092285
entropies:25.950725555419922
Policy training finished
---------------------
gamma: 0.2482065835262486
training start after waiting for 1.189655065536499 seconds
policy loss:-1449.86181640625
value loss:33.53481674194336
entropies:50.254703521728516
Policy training finished
---------------------
gamma: 0.2482065835262486
training start after waiting for 1.1835026741027832 seconds
policy loss:-962.1622314453125
value loss:31.130373001098633
entropies:26.68708038330078
Policy training finished
---------------------
gamma: 0.2482065835262486
training start after waiting for 1.2104039192199707 seconds
policy loss:-1.4132966995239258
value loss:13.789107322692871
entropies:32.20275115966797
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1192.9562)
ToM Target loss= tensor(2059.2532)
optimized based on ToM loss
---------------------
gamma: 0.2487029966933011
training start after waiting for 1.2165939807891846 seconds
policy loss:83.40371704101562
value loss:25.196456909179688
entropies:29.85833740234375
Policy training finished
---------------------
gamma: 0.2487029966933011
training start after waiting for 1.1958937644958496 seconds
policy loss:-679.9608764648438
value loss:14.002227783203125
entropies:44.90147399902344
Policy training finished
---------------------
gamma: 0.2487029966933011
training start after waiting for 1.2081236839294434 seconds
policy loss:-636.8941650390625
value loss:21.159080505371094
entropies:30.663055419921875
Policy training finished
---------------------
gamma: 0.2487029966933011
training start after waiting for 1.1691389083862305 seconds
policy loss:-531.5591430664062
value loss:23.703868865966797
entropies:32.51042175292969
Policy training finished
---------------------
gamma: 0.2487029966933011
training start after waiting for 1.1989352703094482 seconds
policy loss:-2145.42236328125
value loss:56.09453582763672
entropies:50.38188552856445
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1132.8656)
ToM Target loss= tensor(2009.2047)
optimized based on ToM loss
---------------------
gamma: 0.2492004026866877
training start after waiting for 1.167046070098877 seconds
policy loss:-202.0852508544922
value loss:28.86837387084961
entropies:51.00237274169922
Policy training finished
---------------------
gamma: 0.2492004026866877
training start after waiting for 1.176414966583252 seconds
policy loss:-146.44471740722656
value loss:43.62356185913086
entropies:36.427913665771484
Policy training finished
---------------------
gamma: 0.2492004026866877
training start after waiting for 1.181337833404541 seconds
policy loss:426.139892578125
value loss:11.910393714904785
entropies:26.790264129638672
Policy training finished
---------------------
gamma: 0.2492004026866877
training start after waiting for 1.1782505512237549 seconds
policy loss:-2540.425048828125
value loss:43.292137145996094
entropies:58.93150329589844
Policy training finished
---------------------
gamma: 0.2492004026866877
training start after waiting for 1.1814711093902588 seconds
policy loss:-631.0970458984375
value loss:22.19519805908203
entropies:45.12651062011719
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1216.5673)
ToM Target loss= tensor(1965.8610)
optimized based on ToM loss
---------------------
gamma: 0.24969880349206108
training start after waiting for 1.1462469100952148 seconds
policy loss:376.0269470214844
value loss:32.23515319824219
entropies:32.77598190307617
Policy training finished
---------------------
gamma: 0.24969880349206108
training start after waiting for 1.1724615097045898 seconds
policy loss:-248.7307586669922
value loss:17.266077041625977
entropies:22.063526153564453
Policy training finished
---------------------
gamma: 0.24969880349206108
training start after waiting for 1.1907296180725098 seconds
policy loss:-1425.5531005859375
value loss:36.07159423828125
entropies:42.78241729736328
Policy training finished
---------------------
gamma: 0.24969880349206108
training start after waiting for 1.2104854583740234 seconds
policy loss:335.5856018066406
value loss:8.483633041381836
entropies:21.641311645507812
Policy training finished
---------------------
gamma: 0.24969880349206108
training start after waiting for 1.187211275100708 seconds
policy loss:-322.7315979003906
value loss:13.218338966369629
entropies:21.093204498291016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1095.7202)
ToM Target loss= tensor(2065.5815)
optimized based on ToM loss
---------------------
gamma: 0.2501982010990452
training start after waiting for 1.205820083618164 seconds
policy loss:-656.281982421875
value loss:22.73366928100586
entropies:50.851356506347656
Policy training finished
---------------------
gamma: 0.2501982010990452
training start after waiting for 1.150801420211792 seconds
policy loss:-1141.4447021484375
value loss:20.708547592163086
entropies:30.265403747558594
Policy training finished
---------------------
gamma: 0.2501982010990452
training start after waiting for 1.1898906230926514 seconds
policy loss:-140.1878204345703
value loss:13.188138961791992
entropies:21.98975372314453
Policy training finished
---------------------
gamma: 0.2501982010990452
training start after waiting for 1.201078176498413 seconds
policy loss:-510.5017395019531
value loss:9.249234199523926
entropies:28.889081954956055
Policy training finished
---------------------
gamma: 0.2501982010990452
training start after waiting for 1.1947331428527832 seconds
policy loss:134.82791137695312
value loss:9.416171073913574
entropies:19.717182159423828
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1109.0712)
ToM Target loss= tensor(2067.3059)
optimized based on ToM loss
---------------------
gamma: 0.2506985975012433
training start after waiting for 1.150202751159668 seconds
policy loss:-1573.1365966796875
value loss:47.682518005371094
entropies:29.30464744567871
Policy training finished
---------------------
gamma: 0.2506985975012433
training start after waiting for 1.1526813507080078 seconds
policy loss:-391.7343444824219
value loss:18.406702041625977
entropies:26.105192184448242
Policy training finished
---------------------
gamma: 0.2506985975012433
training start after waiting for 1.1547977924346924 seconds
policy loss:-1688.5174560546875
value loss:33.62714767456055
entropies:36.22710037231445
Policy training finished
---------------------
gamma: 0.2506985975012433
training start after waiting for 1.1927037239074707 seconds
policy loss:-150.49911499023438
value loss:15.422999382019043
entropies:27.841354370117188
Policy training finished
---------------------
gamma: 0.2506985975012433
training start after waiting for 1.1748347282409668 seconds
policy loss:-26.128353118896484
value loss:19.000686645507812
entropies:19.38428497314453
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1132.8530)
ToM Target loss= tensor(2202.5796)
optimized based on ToM loss
---------------------
gamma: 0.25119999469624577
training start after waiting for 1.201390027999878 seconds
policy loss:350.4407043457031
value loss:14.739575386047363
entropies:27.111106872558594
Policy training finished
---------------------
gamma: 0.25119999469624577
training start after waiting for 1.1963744163513184 seconds
policy loss:430.9696960449219
value loss:16.52102279663086
entropies:27.61536407470703
Policy training finished
---------------------
gamma: 0.25119999469624577
training start after waiting for 1.1457533836364746 seconds
policy loss:135.98452758789062
value loss:31.862804412841797
entropies:30.78807830810547
Policy training finished
---------------------
gamma: 0.25119999469624577
training start after waiting for 1.176276445388794 seconds
policy loss:-791.88037109375
value loss:31.65302085876465
entropies:31.273988723754883
Policy training finished
---------------------
gamma: 0.25119999469624577
training start after waiting for 1.1810815334320068 seconds
policy loss:-275.31768798828125
value loss:8.269031524658203
entropies:20.668930053710938
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1006.2176)
ToM Target loss= tensor(2034.2904)
optimized based on ToM loss
---------------------
gamma: 0.25170239468563826
training start after waiting for 1.1918022632598877 seconds
policy loss:-588.0619506835938
value loss:14.440900802612305
entropies:29.948972702026367
Policy training finished
---------------------
gamma: 0.25170239468563826
training start after waiting for 1.1932823657989502 seconds
policy loss:-978.958251953125
value loss:27.166301727294922
entropies:30.81703758239746
Policy training finished
---------------------
gamma: 0.25170239468563826
training start after waiting for 1.17490816116333 seconds
policy loss:-1524.848876953125
value loss:36.67863082885742
entropies:40.09601974487305
Policy training finished
---------------------
gamma: 0.25170239468563826
training start after waiting for 1.2172060012817383 seconds
policy loss:-1.5545053482055664
value loss:6.524978160858154
entropies:20.49992561340332
Policy training finished
---------------------
gamma: 0.25170239468563826
training start after waiting for 1.2095348834991455 seconds
policy loss:-740.26513671875
value loss:34.01746368408203
entropies:33.40492248535156
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1122.2882)
ToM Target loss= tensor(2040.1068)
optimized based on ToM loss
---------------------
gamma: 0.25220579947500954
training start after waiting for 1.1937329769134521 seconds
policy loss:-131.30247497558594
value loss:10.328783988952637
entropies:11.728796005249023
Policy training finished
---------------------
gamma: 0.25220579947500954
training start after waiting for 1.1491093635559082 seconds
policy loss:390.0465087890625
value loss:16.587657928466797
entropies:20.321537017822266
Policy training finished
---------------------
gamma: 0.25220579947500954
training start after waiting for 1.2141544818878174 seconds
policy loss:-208.591064453125
value loss:16.20340347290039
entropies:23.326562881469727
Policy training finished
---------------------
gamma: 0.25220579947500954
training start after waiting for 1.1761736869812012 seconds
policy loss:-1107.3912353515625
value loss:11.826424598693848
entropies:30.72870635986328
Policy training finished
---------------------
gamma: 0.25220579947500954
training start after waiting for 1.1706929206848145 seconds
policy loss:-298.0810852050781
value loss:10.73367691040039
entropies:12.587444305419922
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1090.9290)
ToM Target loss= tensor(2240.2366)
optimized based on ToM loss
---------------------
gamma: 0.2527102110739596
training start after waiting for 1.1784508228302002 seconds
policy loss:-0.552513599395752
value loss:12.163589477539062
entropies:25.481264114379883
Policy training finished
---------------------
gamma: 0.2527102110739596
training start after waiting for 1.1780414581298828 seconds
policy loss:-10.77645492553711
value loss:15.170942306518555
entropies:33.01096725463867
Policy training finished
---------------------
gamma: 0.2527102110739596
training start after waiting for 1.178114652633667 seconds
policy loss:-1448.9542236328125
value loss:26.550148010253906
entropies:36.65707778930664
Policy training finished
---------------------
gamma: 0.2527102110739596
training start after waiting for 1.1736040115356445 seconds
policy loss:-89.41730499267578
value loss:9.428369522094727
entropies:21.389589309692383
Policy training finished
---------------------
gamma: 0.2527102110739596
training start after waiting for 1.2075235843658447 seconds
policy loss:-2683.854736328125
value loss:37.729793548583984
entropies:41.42815017700195
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1090.5847)
ToM Target loss= tensor(2107.6880)
optimized based on ToM loss
---------------------
gamma: 0.2532156314961075
training start after waiting for 1.1420683860778809 seconds
policy loss:-810.3800659179688
value loss:17.683507919311523
entropies:48.49139404296875
Policy training finished
---------------------
gamma: 0.2532156314961075
training start after waiting for 1.1894643306732178 seconds
policy loss:-589.3152465820312
value loss:16.191505432128906
entropies:29.025510787963867
Policy training finished
---------------------
gamma: 0.2532156314961075
training start after waiting for 1.1516175270080566 seconds
policy loss:99.96856689453125
value loss:13.977766990661621
entropies:25.963258743286133
Policy training finished
---------------------
gamma: 0.2532156314961075
training start after waiting for 1.2274441719055176 seconds
policy loss:-225.14393615722656
value loss:21.89708137512207
entropies:21.955554962158203
Policy training finished
---------------------
gamma: 0.2532156314961075
training start after waiting for 1.1529979705810547 seconds
policy loss:-124.3104248046875
value loss:28.30208969116211
entropies:36.20649719238281
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1138.3373)
ToM Target loss= tensor(2054.2212)
optimized based on ToM loss
---------------------
gamma: 0.2537220627590997
training start after waiting for 1.1551964282989502 seconds
policy loss:134.74960327148438
value loss:19.79929542541504
entropies:25.05706214904785
Policy training finished
---------------------
gamma: 0.2537220627590997
training start after waiting for 1.1931805610656738 seconds
policy loss:-17.997217178344727
value loss:28.67039680480957
entropies:36.21095275878906
Policy training finished
---------------------
gamma: 0.2537220627590997
training start after waiting for 1.1523663997650146 seconds
policy loss:-57.44425582885742
value loss:10.253238677978516
entropies:16.27355194091797
Policy training finished
---------------------
gamma: 0.2537220627590997
training start after waiting for 1.1854231357574463 seconds
policy loss:-455.26019287109375
value loss:21.30278968811035
entropies:27.611038208007812
Policy training finished
---------------------
gamma: 0.2537220627590997
training start after waiting for 1.1783747673034668 seconds
policy loss:-543.1342163085938
value loss:32.12195587158203
entropies:27.385101318359375
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1122.2839)
ToM Target loss= tensor(2101.7583)
optimized based on ToM loss
---------------------
gamma: 0.2542295068846179
training start after waiting for 1.203686237335205 seconds
policy loss:14.385859489440918
value loss:13.387310981750488
entropies:24.07805824279785
Policy training finished
---------------------
gamma: 0.2542295068846179
training start after waiting for 1.216853380203247 seconds
policy loss:-1006.006103515625
value loss:27.62630271911621
entropies:30.85220718383789
Policy training finished
---------------------
gamma: 0.2542295068846179
training start after waiting for 1.1428401470184326 seconds
policy loss:-954.7706298828125
value loss:15.490060806274414
entropies:18.007532119750977
Policy training finished
---------------------
gamma: 0.2542295068846179
training start after waiting for 1.1554675102233887 seconds
policy loss:-165.61111450195312
value loss:7.288379669189453
entropies:20.95690155029297
Policy training finished
---------------------
gamma: 0.2542295068846179
training start after waiting for 1.2114300727844238 seconds
policy loss:-454.0610656738281
value loss:28.25400733947754
entropies:39.82018280029297
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1096.8282)
ToM Target loss= tensor(2049.4883)
optimized based on ToM loss
---------------------
gamma: 0.2547379658983871
training start after waiting for 1.19071626663208 seconds
policy loss:-29.355792999267578
value loss:12.66032600402832
entropies:22.43914031982422
Policy training finished
---------------------
gamma: 0.2547379658983871
training start after waiting for 1.209226369857788 seconds
policy loss:-259.1257019042969
value loss:46.57982635498047
entropies:27.050567626953125
Policy training finished
---------------------
gamma: 0.2547379658983871
training start after waiting for 1.1435158252716064 seconds
policy loss:96.59015655517578
value loss:23.215435028076172
entropies:25.888002395629883
Policy training finished
---------------------
gamma: 0.2547379658983871
training start after waiting for 1.2063806056976318 seconds
policy loss:-87.70343780517578
value loss:18.388641357421875
entropies:23.70717430114746
Policy training finished
---------------------
gamma: 0.2547379658983871
training start after waiting for 1.1360318660736084 seconds
policy loss:-359.10174560546875
value loss:14.060612678527832
entropies:20.263660430908203
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1179.0079)
ToM Target loss= tensor(2224.7671)
optimized based on ToM loss
---------------------
gamma: 0.2552474418301839
training start after waiting for 1.1384532451629639 seconds
policy loss:-15.644118309020996
value loss:13.692731857299805
entropies:25.390640258789062
Policy training finished
---------------------
gamma: 0.2552474418301839
training start after waiting for 1.240321397781372 seconds
policy loss:-176.6367645263672
value loss:5.747983455657959
entropies:15.361968994140625
Policy training finished
---------------------
gamma: 0.2552474418301839
training start after waiting for 1.178361415863037 seconds
policy loss:-1037.6005859375
value loss:27.655712127685547
entropies:24.452913284301758
Policy training finished
---------------------
gamma: 0.2552474418301839
training start after waiting for 1.140695571899414 seconds
policy loss:-40.239620208740234
value loss:7.440795421600342
entropies:11.202095031738281
Policy training finished
---------------------
gamma: 0.2552474418301839
training start after waiting for 1.1889724731445312 seconds
policy loss:-396.6204833984375
value loss:14.933903694152832
entropies:20.113609313964844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1042.1387)
ToM Target loss= tensor(2165.4395)
optimized based on ToM loss
---------------------
gamma: 0.2557579367138443
training start after waiting for 1.208387851715088 seconds
policy loss:-507.862548828125
value loss:13.553727149963379
entropies:20.764385223388672
Policy training finished
---------------------
gamma: 0.2557579367138443
training start after waiting for 1.1399521827697754 seconds
policy loss:-328.6265869140625
value loss:15.384364128112793
entropies:19.954273223876953
Policy training finished
---------------------
gamma: 0.2557579367138443
training start after waiting for 1.14853835105896 seconds
policy loss:125.83981323242188
value loss:4.91453218460083
entropies:20.923587799072266
Policy training finished
---------------------
gamma: 0.2557579367138443
training start after waiting for 1.1966898441314697 seconds
policy loss:-531.3258056640625
value loss:18.610170364379883
entropies:26.248188018798828
Policy training finished
---------------------
gamma: 0.2557579367138443
training start after waiting for 1.1847724914550781 seconds
policy loss:-330.8358154296875
value loss:23.379419326782227
entropies:24.811241149902344
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1124.3995)
ToM Target loss= tensor(2262.9216)
optimized based on ToM loss
---------------------
gamma: 0.25626945258727196
training start after waiting for 1.206336498260498 seconds
policy loss:-6.666540145874023
value loss:12.189594268798828
entropies:23.16045570373535
Policy training finished
---------------------
gamma: 0.25626945258727196
training start after waiting for 1.1180334091186523 seconds
policy loss:-1570.8963623046875
value loss:33.68122863769531
entropies:37.56359100341797
Policy training finished
---------------------
gamma: 0.25626945258727196
training start after waiting for 1.1272547245025635 seconds
policy loss:-450.9470520019531
value loss:18.982892990112305
entropies:34.94048309326172
Policy training finished
---------------------
gamma: 0.25626945258727196
training start after waiting for 1.1507396697998047 seconds
policy loss:-748.9369506835938
value loss:53.00813293457031
entropies:38.94552230834961
Policy training finished
---------------------
gamma: 0.25626945258727196
training start after waiting for 1.1482820510864258 seconds
policy loss:39.66627883911133
value loss:19.431365966796875
entropies:21.305931091308594
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1120.6774)
ToM Target loss= tensor(2130.8555)
optimized based on ToM loss
---------------------
gamma: 0.2567819914924465
training start after waiting for 1.1558523178100586 seconds
policy loss:-178.21295166015625
value loss:13.523575782775879
entropies:13.859392166137695
Policy training finished
---------------------
gamma: 0.2567819914924465
training start after waiting for 1.1846764087677002 seconds
policy loss:528.735107421875
value loss:14.835527420043945
entropies:20.4419002532959
Policy training finished
---------------------
gamma: 0.2567819914924465
training start after waiting for 1.1806590557098389 seconds
policy loss:86.66393280029297
value loss:10.786483764648438
entropies:17.159751892089844
Policy training finished
---------------------
gamma: 0.2567819914924465
training start after waiting for 1.198927640914917 seconds
policy loss:-326.1840515136719
value loss:6.769107818603516
entropies:10.577741622924805
Policy training finished
---------------------
gamma: 0.2567819914924465
training start after waiting for 1.152940034866333 seconds
policy loss:-1395.239990234375
value loss:26.1741943359375
entropies:34.967124938964844
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1044.2172)
ToM Target loss= tensor(2242.6663)
optimized based on ToM loss
---------------------
gamma: 0.2572955554754314
training start after waiting for 1.1784942150115967 seconds
policy loss:-1474.7333984375
value loss:33.94763946533203
entropies:46.9206428527832
Policy training finished
---------------------
gamma: 0.2572955554754314
training start after waiting for 1.1527316570281982 seconds
policy loss:52.29726028442383
value loss:8.944141387939453
entropies:26.983734130859375
Policy training finished
---------------------
gamma: 0.2572955554754314
training start after waiting for 1.1226763725280762 seconds
policy loss:-390.96441650390625
value loss:14.676714897155762
entropies:24.020477294921875
Policy training finished
---------------------
gamma: 0.2572955554754314
training start after waiting for 1.1820461750030518 seconds
policy loss:132.4697723388672
value loss:14.155714988708496
entropies:20.19344711303711
Policy training finished
---------------------
gamma: 0.2572955554754314
training start after waiting for 1.144735336303711 seconds
policy loss:-340.8684387207031
value loss:37.95380401611328
entropies:28.669635772705078
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1129.0248)
ToM Target loss= tensor(2181.5784)
optimized based on ToM loss
---------------------
gamma: 0.2578101465863823
training start after waiting for 1.1790132522583008 seconds
policy loss:270.1752624511719
value loss:10.282703399658203
entropies:23.14092445373535
Policy training finished
---------------------
gamma: 0.2578101465863823
training start after waiting for 1.1469767093658447 seconds
policy loss:-85.923828125
value loss:23.99038314819336
entropies:25.247821807861328
Policy training finished
---------------------
gamma: 0.2578101465863823
training start after waiting for 1.1778533458709717 seconds
policy loss:-778.484130859375
value loss:28.06216049194336
entropies:27.954641342163086
Policy training finished
---------------------
gamma: 0.2578101465863823
training start after waiting for 1.2110395431518555 seconds
policy loss:340.13739013671875
value loss:12.985689163208008
entropies:24.6171817779541
Policy training finished
---------------------
gamma: 0.2578101465863823
training start after waiting for 1.168649673461914 seconds
policy loss:-982.8512573242188
value loss:26.917144775390625
entropies:37.6395263671875
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1090.0962)
ToM Target loss= tensor(2053.4480)
optimized based on ToM loss
---------------------
gamma: 0.25832576687955505
training start after waiting for 1.1821658611297607 seconds
policy loss:-216.88351440429688
value loss:6.450783729553223
entropies:19.847267150878906
Policy training finished
---------------------
gamma: 0.25832576687955505
training start after waiting for 1.1838977336883545 seconds
policy loss:-650.617431640625
value loss:17.858875274658203
entropies:29.641666412353516
Policy training finished
---------------------
gamma: 0.25832576687955505
training start after waiting for 1.181396484375 seconds
policy loss:133.39158630371094
value loss:10.093796730041504
entropies:14.761574745178223
Policy training finished
---------------------
gamma: 0.25832576687955505
training start after waiting for 1.198071002960205 seconds
policy loss:-104.90701293945312
value loss:9.557601928710938
entropies:28.897785186767578
Policy training finished
---------------------
gamma: 0.25832576687955505
training start after waiting for 1.151832103729248 seconds
policy loss:-824.2420654296875
value loss:16.259557723999023
entropies:33.0631217956543
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1145.1321)
ToM Target loss= tensor(2222.2927)
optimized based on ToM loss
---------------------
gamma: 0.2588424184133142
training start after waiting for 1.2187423706054688 seconds
policy loss:-578.164794921875
value loss:10.730340957641602
entropies:23.73952293395996
Policy training finished
---------------------
gamma: 0.2588424184133142
training start after waiting for 1.1951682567596436 seconds
policy loss:-999.1218872070312
value loss:25.564178466796875
entropies:29.64518928527832
Policy training finished
---------------------
gamma: 0.2588424184133142
training start after waiting for 1.170698642730713 seconds
policy loss:-206.97938537597656
value loss:31.330257415771484
entropies:45.612518310546875
Policy training finished
---------------------
gamma: 0.2588424184133142
training start after waiting for 1.145616054534912 seconds
policy loss:222.28073120117188
value loss:9.668085098266602
entropies:20.381412506103516
Policy training finished
---------------------
gamma: 0.2588424184133142
training start after waiting for 1.1522963047027588 seconds
policy loss:-730.7566528320312
value loss:20.399738311767578
entropies:25.132022857666016
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1156.0631)
ToM Target loss= tensor(2147.5637)
optimized based on ToM loss
---------------------
gamma: 0.2593601032501408
training start after waiting for 1.1816990375518799 seconds
policy loss:28.8740291595459
value loss:5.5090203285217285
entropies:14.289905548095703
Policy training finished
---------------------
gamma: 0.2593601032501408
training start after waiting for 1.1740925312042236 seconds
policy loss:46.19021987915039
value loss:4.4680962562561035
entropies:17.98868179321289
Policy training finished
---------------------
gamma: 0.2593601032501408
training start after waiting for 1.1908879280090332 seconds
policy loss:-530.36328125
value loss:24.384815216064453
entropies:40.53600311279297
Policy training finished
---------------------
gamma: 0.2593601032501408
training start after waiting for 1.2175464630126953 seconds
policy loss:584.8837280273438
value loss:18.138187408447266
entropies:29.9498291015625
Policy training finished
---------------------
gamma: 0.2593601032501408
training start after waiting for 1.1831934452056885 seconds
policy loss:-1279.8548583984375
value loss:21.992481231689453
entropies:28.262996673583984
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1164.4177)
ToM Target loss= tensor(2242.1985)
optimized based on ToM loss
---------------------
gamma: 0.2598788234566411
training start after waiting for 1.1713693141937256 seconds
policy loss:-389.1027526855469
value loss:17.03079605102539
entropies:32.68011474609375
Policy training finished
---------------------
gamma: 0.2598788234566411
training start after waiting for 1.1581637859344482 seconds
policy loss:-279.8661804199219
value loss:17.318689346313477
entropies:28.712078094482422
Policy training finished
---------------------
gamma: 0.2598788234566411
training start after waiting for 1.1533246040344238 seconds
policy loss:-2505.968017578125
value loss:42.06011962890625
entropies:36.569007873535156
Policy training finished
---------------------
gamma: 0.2598788234566411
training start after waiting for 1.1516969203948975 seconds
policy loss:-54.66260528564453
value loss:16.362178802490234
entropies:29.156707763671875
Policy training finished
---------------------
gamma: 0.2598788234566411
training start after waiting for 1.1292576789855957 seconds
policy loss:-368.72222900390625
value loss:17.595212936401367
entropies:21.14828872680664
Policy training finished
---------------------
ToM training started
ToM data loaded
batch_size =  300
ToM_loss = tensor(1241.2206)
ToM Target loss= tensor(2216.7534)
optimized based on ToM loss
---------------------
gamma: 2024-04-07 09:07:03,795 : Time 07h 48m 45s, ave eps reward [-3.88 -3.88 -3.88], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 8.87, mean reward -3.878273402154602, std reward 1.8452711141083882, AG 0.0
2024-04-07 09:07:30,683 : Time 07h 49m 12s, ave eps reward [-4.24 -4.24 -4.24], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 7.19, mean reward -4.235857230858177, std reward 2.128159405230697, AG 0.0
2024-04-07 09:07:56,545 : Time 07h 49m 38s, ave eps reward [-4.07 -4.07 -4.07], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 10.68, mean reward -4.066400048554824, std reward 2.3898642910708907, AG 0.0
2024-04-07 09:08:22,982 : Time 07h 50m 04s, ave eps reward [-3.72 -3.72 -3.72], ave eps length 10.0, reward step [-0.37 -0.37 -0.37], FPS 6.96, mean reward -3.715279639560534, std reward 2.2552894379502404, AG 0.0
2024-04-07 09:08:49,358 : Time 07h 50m 31s, ave eps reward [-4.49 -4.49 -4.49], ave eps length 10.0, reward step [-0.45 -0.45 -0.45], FPS 6.86, mean reward -4.490857427374061, std reward 3.553400305411508, AG 0.0
2024-04-07 09:09:15,710 : Time 07h 50m 57s, ave eps reward [-4.21 -4.21 -4.21], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 7.01, mean reward -4.20670619429586, std reward 3.068286135869954, AG 0.0
2024-04-07 09:09:42,078 : Time 07h 51m 24s, ave eps reward [-4.24 -4.24 -4.24], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.68, mean reward -4.235464395061042, std reward 3.8458662757849913, AG 0.0
2024-04-07 09:10:08,207 : Time 07h 51m 50s, ave eps reward [-3.49 -3.49 -3.49], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 8.01, mean reward -3.487676453800785, std reward 1.6101070310271128, AG 0.0
2024-04-07 09:10:34,293 : Time 07h 52m 16s, ave eps reward [-3.9 -3.9 -3.9], ave eps length 10.0, reward step [-0.39 -0.39 -0.39], FPS 9.35, mean reward -3.899520956548552, std reward 3.1224332974663342, AG 0.0
2024-04-07 09:11:00,760 : Time 07h 52m 42s, ave eps reward [-3.27 -3.27 -3.27], ave eps length 10.0, reward step [-0.33 -0.33 -0.33], FPS 6.54, mean reward -3.266394732308318, std reward 1.607346265746286, AG 0.0
2024-04-07 09:11:27,331 : Time 07h 53m 09s, ave eps reward [-4.11 -4.11 -4.11], ave eps length 10.0, reward step [-0.41 -0.41 -0.41], FPS 9.81, mean reward -4.107759415798056, std reward 2.5974756416387836, AG 0.0
2024-04-07 09:11:53,722 : Time 07h 53m 35s, ave eps reward [-4.93 -4.93 -4.93], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 7.8, mean reward -4.929362287393144, std reward 3.8909119936379866, AG 0.0
2024-04-07 09:12:20,163 : Time 07h 54m 02s, ave eps reward [-5.92 -5.92 -5.92], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 6.74, mean reward -5.917558464753505, std reward 5.806198613244671, AG 0.0
2024-04-07 09:12:46,482 : Time 07h 54m 28s, ave eps reward [-5.06 -5.06 -5.06], ave eps length 10.0, reward step [-0.51 -0.51 -0.51], FPS 7.22, mean reward -5.063194350201134, std reward 4.3281783255187225, AG 0.0
2024-04-07 09:13:13,002 : Time 07h 54m 54s, ave eps reward [-4.85 -4.85 -4.85], ave eps length 10.0, reward step [-0.49 -0.49 -0.49], FPS 6.77, mean reward -4.851339176436171, std reward 3.3521189371617983, AG 0.0
2024-04-07 09:13:39,132 : Time 07h 55m 21s, ave eps reward [-4.77 -4.77 -4.77], ave eps length 10.0, reward step [-0.48 -0.48 -0.48], FPS 6.96, mean reward -4.769779352069237, std reward 3.673382763204438, AG 0.0
2024-04-07 09:14:04,560 : Time 07h 55m 46s, ave eps reward [-4.41 -4.41 -4.41], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 10.99, mean reward -4.411977737898285, std reward 4.359143195329106, AG 0.0
2024-04-07 09:14:30,838 : Time 07h 56m 12s, ave eps reward [-4.37 -4.37 -4.37], ave eps length 10.0, reward step [-0.44 -0.44 -0.44], FPS 7.06, mean reward -4.372612201082599, std reward 2.8213617781874745, AG 0.0
2024-04-07 09:14:57,274 : Time 07h 56m 39s, ave eps reward [-4.22 -4.22 -4.22], ave eps length 10.0, reward step [-0.42 -0.42 -0.42], FPS 6.92, mean reward -4.223669432862302, std reward 2.7729102059108075, AG 0.0
2024-04-07 09:15:23,749 : Time 07h 57m 05s, ave eps reward [-3.3 -3.3 -3.3], ave eps length 10.0, reward step [-0.33 -0.33 -0.33], FPS 8.48, mean reward -3.295049552762432, std reward 1.4952343190426216, AG 0.0
2024-04-07 09:15:50,219 : Time 07h 57m 32s, ave eps reward [-5.87 -5.87 -5.87], ave eps length 10.0, reward step [-0.59 -0.59 -0.59], FPS 6.76, mean reward -5.8743051913715725, std reward 4.637494806280269, AG 0.0
2024-04-07 09:16:16,358 : Time 07h 57m 58s, ave eps reward [-4.69 -4.69 -4.69], ave eps length 10.0, reward step [-0.47 -0.47 -0.47], FPS 8.59, mean reward -4.694868791830751, std reward 3.117282805027046, AG 0.0
2024-04-07 09:16:42,517 : Time 07h 58m 24s, ave eps reward [-4.05 -4.05 -4.05], ave eps length 10.0, reward step [-0.4 -0.4 -0.4], FPS 8.78, mean reward -4.048069257984037, std reward 2.357571555192358, AG 0.0
2024-04-07 09:17:09,388 : Time 07h 58m 51s, ave eps reward [-5.04 -5.04 -5.04], ave eps length 10.0, reward step [-0.5 -0.5 -0.5], FPS 7.12, mean reward -5.03829782985199, std reward 5.046414138149237, AG 0.0
2024-04-07 09:17:35,471 : Time 07h 59m 17s, ave eps reward [-3.47 -3.47 -3.47], ave eps length 10.0, reward step [-0.35 -0.35 -0.35], FPS 10.56, mean reward -3.472843311579976, std reward 1.2685090608000276, AG 0.0
2024-04-07 09:18:02,083 : Time 07h 59m 44s, ave eps reward [-5.27 -5.27 -5.27], ave eps length 10.0, reward step [-0.53 -0.53 -0.53], FPS 7.71, mean reward -5.270069829413886, std reward 4.245550011635682, AG 0.0
slurmstepd: error: *** JOB 6496151 ON gl3021 CANCELLED AT 2024-04-07T09:18:24 DUE TO TIME LIMIT ***
